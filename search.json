[{"title":"2025.7.8学习日记","path":"/2025/07/08/学习日记25年7月/2025.7.8学习笔记/","content":"今日学习内容3DGS项目今天准备实现Filter实现请求日志记录.将所有的外部访问都记录在日志文件中 力扣每日一题:一个dp＋二分的困难题. 代码随想录做了一个N皇后. 项目相关看了两篇项目博客,然后准备明天开始写日志过滤器和用户名密码登录接口. 生活早上踢球颠球 长传 短传 晚上健身练的背 二头 核心,强度偏大.拉伸腿部为主.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.8学习日记","path":"/2025/07/08/学习日记25年7月/2025.7.9学习笔记/","content":"今日学习内容3DGS项目今天准备实现Filter实现请求日志记录.将所有的外部访问都记录在日志文件中 力扣每日一题:今天的每日是一个最大移动k次会议,使得最大连续空闲时间最长.抽象成功,和灵神的题解做的一样.转换成了对n+1个空闲时间区间滑窗求定长滑窗的最大和. 代码随想录项目相关看了两篇项目博客,然后准备明天开始写日志过滤器和用户名密码登录接口. 生活早上踢球颠球 长传 短传 晚上健身练的背 二头 核心,强度偏大.拉伸腿部为主.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.7学习日记","path":"/2025/07/07/学习日记25年7月/2025.7.7学习笔记/","content":"今日学习内容3DGS部署CityGaussian,学习了一下CityGaussian论文,这篇文章通过优化原版3DGS密度控制模块,防止训练时高斯数量爆炸性增长,将原版3DGS存储压缩10倍，训练速度提升25%，内存减半,渲染效果和原版接近. 项目再次梳理了全局视图的框架.明天准备实现验权和登录相关的功能. 力扣每日一题:今天的每日是一个贪心加优先队列的方法. 代码随想录三道题 生活健身主要练胸 肩 三头 ,强度偏大","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.5学习日记","path":"/2025/07/05/学习日记25年7月/2025.7.5学习笔记/","content":"今日学习内容3DGSDifix3D方法相当于在3DGS渲染脚本render.py执行获得2D渲染图像后,再对2D渲染图像使用单步扩散模型,对2D渲染图进行处理.所以这种方式无法做到像原版3DGS一样对3D模型的实时渲染.因为其本质是对2D图像进行去噪,而不是对3D空间中离散的高斯球进行剔除.而3DGS实时渲染的基础就是render.py的快速渲染. 项目力扣每日一题:简单题,遍历求出现次数即可.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.4学习日记","path":"/2025/07/04/学习日记25年7月/2025.7.4学习笔记/","content":"今日学习内容3DGS看了difix3D论文,相当于加了一个单步扩散模型. 项目把全局配置显示拦截器实现了,目前项目可以正常显示主页信息.实现主页显示的过程基本上把项目框架给搭完了. 力扣每日一题:昨天每日的困难版本. 项目","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.3学习日记","path":"/2025/07/03/学习日记25年7月/2025.7.3学习笔记/","content":"今日学习内容3DGS看了difix3D论文,相当于加了一个单步扩散模型. 项目做了一天的项目,完善主页的过程基本把大部分的接口都实现了. 力扣每日一题:二进制有关的. 项目","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.1学习日记","path":"/2025/07/01/学习日记25年7月/2025.7.1学习笔记/","content":"今日学习内容3DGS部署CF-3DGS项目,阅读论文.这篇论文通过​​局部-全局联合优化​​直接估计相机位姿，省去SfM预处理步骤,具体效果需要实验验证.部署环境过程遇到一些依赖问题. 力扣每日一题:今天的每日一个简单的遍历即可做出. 项目做了半天之后觉得vue3并不适合这个博客项目,所以又换回到了Thymeleaf作为前端的方案.但是对于项目的理解我觉得是有非常大的进步的.目前重新创建了一个Techub项目,然后基本理解了Thymeleaf的前端框架.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.6.30学习日记","path":"/2025/06/30/学习日记25年6月/2025.6.30学习笔记/","content":"今日学习内容3DGS学习taming-gs论文. 力扣每日一题:每日是一个简单的哈希题. 项目今天开发了一个返回全文分类的接口.虽然看起来只开发一个接口,但是保证开发规范的话,属于牵一发动全身的感觉,写一个内容需要写好对应的DO DTO,然后最好把所有的常量之类的写成更加直观地枚举类. 个人博客晚上把个人博客的分类和专栏功能加进去了,这回看起来作用非常完善. 整理算法笔记把刷题算法笔记重新重构了一下,保留重要的标题. 生活记录晚上健身今天练的胸 肩 核心 ,强度适中.","tags":["3DGS","日记","leetcode","项目","博客"],"categories":["学习日记","2025-06"]},{"title":"2025.6.29学习日记","path":"/2025/06/29/学习日记25年6月/2025.6.29学习笔记/","content":"力扣每日一题:今天的每日一个滑窗,还是比较好想的. 力扣周赛全回来了,这周周赛做了三道题,而且第三道是最近练的回溯,虽然超时了,dfs改写成dp就过了. 生活记录早上踢球md,我的球踢草丛里了.","tags":["日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"2025.6.28学习日记","path":"/2025/06/28/学习日记25年6月/2025.6.28学习笔记/","content":"今日学习内容3DGS对训练的模型进行了汇总。dash方法的训练速度是最快的，同时渲染质量和原版比略有提升；mip方法渲染质量得分最高，但其受视角局限性影响最严重，渲染视角和输入视角差的稍微多一些就会看到空间中很多严重的鬼影。 力扣每日一题:spring篇spring篇学习完一遍,记了一篇笔记. 项目被依赖项搞了一下午,发现配置子模块后并不会自己扫描子模块的包,需要先配置一个AutoConfig类,然后通过配置AutoConfig配置子模块的包路径,才能被扫描到.不过还是感觉不从0开始,这些问题都不会遇到,还是有用的,印象非常深刻. Unity开发对带球的bug进一步优化. 晚上学代码随想录生活记录steam夏促买了一堆游戏","tags":["3DGS","spring","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"2025.6.27学习日记","path":"/2025/06/27/学习日记25年6月/2025.6.27学习笔记/","content":"今日学习内容3DGS对训练的模型进行了汇总。dash方法的训练速度是最快的，同时渲染质量和原版比略有提升；mip方法渲染质量得分最高，但其受视角局限性影响最严重，渲染视角和输入视角差的稍微多一些就会看到空间中很多严重的鬼影。 力扣每日一题:项目添加了自动初始化数据库模块,使用liquibase来管理数据库sql的变更,并且在不开启liquibase的情况也做了考虑.感觉还是从项目里面学比较深刻,光看文档记不住. Unity开发把带球的逻辑重构了,通过切换带球人的图层来实现带球,先在看起来非常完美,然后把之前的踢球逻辑也加上了,看起来很不错. 晚上学代码随想录做了三道回溯 生活记录晚上健身练的背 肩膀 二头 三头 强度比较大.","tags":["3DGS","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"2025.6.26学习日记","path":"/2025/06/26/学习日记25年6月/2025.6.26学习笔记/","content":"今日学习内容3DGS简单跑跑代码,新部署了mip-gs 力扣每日一题:今天的每日和二进制有关的一个脑筋急转弯. Mysql学习54 项目整理项目结构,创建子模块. Unity开发早上把球跟随的逻辑重构了一下,打算的实现方案是通过切换图层,使得带球球员和球没有碰撞,但是能保持球的物理. 生活记录晚上回学校聚餐和球队哥们吃火锅玩阿瓦隆.","tags":["3DGS","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"TecHub项目笔记","path":"/2025/06/25/项目笔记/TecHub项目笔记/","content":"TecHub项目笔记大概会以一个时间线的方式,从0开始搭建项目. 贴一个复制 /** * @author JakicDong * @time 2025.6.25 * @description 启动类 * @version 1.0.0 */ 项目介绍以Spring3框架为基础,VUE3作为前端 配置本地Git仓库和GitHub远程仓库IDEA中建立项目根目录本地仓库,文件变红说明配置成功,然后提交一次.配置远程仓库,先在IDEA中登录GitHub账号,然后GitHub建立远程仓库然后在IDEA中配置远程仓库地址.git remote add origin git@github.com:xxx/xxx,git(origin后面替换成你刚复制的东西) git push -u origin master (解释:该脚本将本地的master 推到刚才设置的github远程仓库中)如果没有配过公钥的话可能会报错,配置一个公钥即可.然后就可以愉快的进行远程版本控制了. 子模块创建需要注意子模块和父模块pom文件的格式.子模块: parent groupIdcom.github.JakicDong.TecHub/groupId artifactIdTecHub/artifactId version1.0-SNAPSHOT/version/parent 父模块 parent groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter-parent/artifactId version3.0.9/version relativePath/ !-- lookup parent from repository --/parent 这个项目用的就是spring3,所以父模块继承一个spring3的框架. 入口 @SpringBootApplicationpublic class TecHubApplication { 这是一个入口类 再次测试} 导入了依赖项 进度将项目结构进行完整的梳理自动加载初始化comgithubJakicDongTecHubconfiginitForumDataSourceInitializer.java 这一部分包括很多的知识点,liquibase来管理更改sql,xml解析,jdbc连接数据库等等,然后实现了一个在没有liquibase的情况下的一个初始化器. 实现了分类接口的list功能实现用户名密码登录方式","tags":["项目","TecHub"],"categories":["项目笔记","TecHub"]},{"title":"2025.6.25学习日记","path":"/2025/06/25/学习日记25年6月/2025.6.25学习笔记/","content":"今日学习内容3DGS简单跑跑代码,新部署了mip-gs 力扣每日一题:二分加二分的困难题.已经刷了800道题目了. Mysql学习52 - 54 项目今天准备重新写项目TecHub作为我的主力项目. Unity开发早上新建了一个新项目之后,自动补全就神奇的好了,估计是新建项目之后自动下载了依赖包.晚上把人物移动控制逻辑重构了. 晚上学代码随想录晚上刷了两道回溯题 生活记录晚上健身练的胸肩 膀和二头,强度比较高.","tags":["3DGS","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"技术派项目环境问题","path":"/2025/06/24/项目笔记/技术派环境问题/","content":"重构成spring3 支持jdk17 Mac的jdk多版本切换cd LibraryJavaJavaVirtualMachinesls -al 查看一下都有哪些版本的jdk # 输入cd ~# 打开环境变量配置文件code .bash_profile# 报错：.bash_profile does not exist.# 第一次配置环境变量，先创建文件touch .bash_profile# 再次执行打开环境变量配置文件code .bash_profile # 复制如下内容粘贴到.bash_profile中，# 因为我是安装了三个，所以配置了三个版本# 你自己是安装了几个版本就配置几个export JAVA_8_HOME=$(/usr/libexec/java_home -v1.8)export JAVA_11_HOME=$(/usr/libexec/java_home -v11)export JAVA_17_HOME=$(/usr/libexec/java_home -v17)alias java8=export JAVA_HOME=$JAVA_8_HOMEalias java11=export JAVA_HOME=$JAVA_11_HOMEalias java17=export JAVA_HOME=$JAVA_17_HOMEexport JAVA_HOME=$JAVA_11_HOME# 记得保存，可以用快捷键 cmd + s # 配置文件立即生效source .bash_profile# 查看 JAVA_HOME 目录echo $JAVA_HOME# 查看 JDK 版本信息java -version #切换到JDK8：java8 # 查看 JDK 版本信息java -version#切换到JDK11：java11# 查看 JDK 版本信息java -version#切换到JDK17：java17# 查看 JDK 版本信息java -version 更改Maven版本路径位置: /Users/mac/tools/apache-maven-3.8.9 sudo code ~/.bash_profile export MAVEN_HOME”Usersmactoolsapache-maven-3.8.9”export PATH”$MAVEN_HOMEbin:$PATH” 安装完maven后更改仓库镜像地址即可,更改为了阿里云 admin模块端口问题paicoding-adminvite.config.ts修改代理端口即可","tags":["项目","技术派"],"categories":["项目笔记","技术派"]},{"title":"游戏开发-2D足球Demo","path":"/2025/06/24/游戏开发/游戏开发-2D足球Demo/","content":"游戏介绍2D足球Demo(todo:想个好名字),是一个基于Unity的2.5D横版足球游戏.玩家操纵角色在球场上和AI进行对战. 核心玩法爽感十足的射门,和守门员的博弈.进攻时,选择远射或过人或者传球给队友;防守时,选择保守的防守还是大胆断球. 画风偏向像素风格. TODO LIST优先级 1 到 10 ,越小优先级越高 3 优化射门BUG3 优化守门员逻辑3 设计对手球员状态机3 优化碰撞箱5 添加更多样式的角色美术类 9 实现角色换装功能5 添加界面5 添加UI6 音乐设计一些音乐 完成清单重构角色移动逻辑1 优化带球逻辑现在的带球逻辑很合理","tags":["游戏开发","2D足球Demo"],"categories":["游戏开发"]},{"title":"Unity游戏开发笔记","path":"/2025/06/24/游戏开发/Unitu开发笔记/","content":"核心组件在Unity中，核心组件之间的关系和访问方式可以这样梳理： 基础组件关系图 GameObject (游戏对象)│├── Transform (变换组件) - 控制位置旋转缩放├── Collider2D (碰撞体) - 物理交互基础├── Rigidbody2D (刚体) - 物理模拟└── MonoBehaviour (脚本) - 自定义逻辑 关键组件访问方式 (1) Transform // 获取自身TransformTransform selfTF = GetComponentTransform(); // 访问属性Vector3 position = selfTF.position;Quaternion rotation = selfTF.rotation;Vector3 scale = selfTF.localScale;// 修改属性selfTF.position = new Vector3(1, 0, 0);selfTF.Rotate(0, 90, 0); (2) GameObject // 获取当前对象GameObject selfGO = gameObject; // 通过名称查找子对象Transform childTF = transform.Find(foot);// 激活状态控制gameObject.SetActive(false); (3) Collider2D Apply// 获取碰撞体Collider2D collider = GetComponentCollider2D();// 启用/禁用collider.enabled = false;// 触发检测 (需挂载脚本)void OnTriggerEnter2D(Collider2D other) if(other.CompareTag(Player)) ... 组件间相互调用方式1：直接获取组件 Apply// 获取同一物体上的组件Rigidbody2D rb = GetComponentRigidbody2D();// 获取其他物体的组件（需先获取GameObject）GameObject player = GameObject.Find(Player);charactor_move_v script = player.GetComponentcharactor_move_v(); 方式2：通过公开引用 Apply// 在Inspector面板拖拽赋值public Transform targetTF;void Start() Vector3 pos = targetTF.position; 方式3：父子层级访问 // 获取父对象Transform parent = transform.parent;// 获取子对象Transform child = transform.GetChild(0); 你的代码中的典型调用案例 Apply// 获取Rigidbody2D组件（物理模拟）private Rigidbody2D rb;void Start() rb = GetComponentRigidbody2D();// 通过标签查找对象if(other.CompareTag(FollowBox)) ...// 访问其他脚本属性float power = currentPlayer.GetComponentcharactor_move_v().kick_power;// 修改碰撞体状态playerCollider.enabled = false; 最佳实践建议缓存引用：在Start()中获取常用组件，避免重复调用GetComponent分层查找：使用transform.Find(“路径子对象”)按路径查找空值检查：对GetComponent结果做null检查性能优化：避免在Update中频繁查找对象这种组件系统构成了Unity的”组合优于继承”架构，通过灵活的组合实现复杂游戏逻辑。 多版本.NET切换dotnet –list-sdks 查看已安装的.NET版本","tags":["游戏开发"],"categories":["游戏开发"]},{"title":"2025.6.24学习日记","path":"/2025/06/24/学习日记25年6月/2025.6.24学习笔记/","content":"今日学习内容3DGS实验室汇报,做汇报PPT. 下载mip-splatting工程,该项目引入了3D平滑滤波器和二维Mip滤波器,通过限制3D表示的频率来消除高频伪影,二位Mip滤波器通过模拟物理成像过程的盒式滤波器来缓解混叠效应和拓展问题.使得Mip-splatting可以在各种采样频率下实现无伪影的渲染. 力扣每日一题:一个简单滑窗问题. Mysql学习48 - 52索引篇学习完毕 Spring学习22 - 30 今天实验室汇报项目安装VUE3框架sudo npm install webpack -gsudo npm install -g @vue/clivue ui 可以看到VUE的ui界面 装了spring3框架的paicoding. Unity时隔六个月,把我之前做的2D足球游戏demo部署在mac上了,继续开发. 晚上学代码随想录重新整理一下算法笔记. 生活记录晚上健身练的背和三头,强度比较低.","tags":["3DGS","spring","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"2025.6.23学习日记","path":"/2025/06/23/学习日记25年6月/2025.6.23学习笔记/","content":"今日学习内容杂谈上周末时隔快一年重新做了一次周赛,第一题手速还是可以的,第二题和第三题两道中等有点吃力,第二题是完全背包相关的题目,动态规划题目一直有点苦手,需要狠狠加强一波.第三题图论虽然想到了思路,但是图论具体细节没实现完全,做的太少了.感觉算法遇到瓶颈了,低分题基本可以做出但是复杂的题目差点意思,现在1750分,接下来一个月准备重心放在算法上. 3DGS今天用陈昊师兄的数据跑了原版和DashGaussian进行测试，发现整体渲染质量都比较低，渲染7000次，PSNR都只有20多，使用viewer工具观察后发现主要是草丛处点云数量过少。个人理解可能是在训练过程中，草丛部分纹理过于复杂，导致局部梯度均值相抵消无法触发3DGS的自动增殖模块，导致高斯无法拟合草丛过于复杂的纹理，使得整体渲染质量下降。 力扣每日一题:今天的每日是关于回文串的困难题. Mysql学习41 - 47 Spring学习17 - 21 做明天实验室汇报的PPT准备讲Dash高斯论文. 汇报内容 本周汇报的文章是DashGaussian,主要是对原版3DGS的训练速度进行了大幅度的优化.甚至可以将训练时间压缩到200秒. 这篇文章发现,高斯训练的速度主要就受两个因素的影响,一个是真值图像的分辨率,还有就是高斯球的数量.而DashGaussian通过动态的调节这两个因素,使得训练速度得到大幅度的提高的同时,渲染质量基本保持一致. 这篇文章发现,基本90%的训练时间基本都在前向渲染,反向传播和高斯球参数更新上;并且高分辨率的图像在渲染前期存在浪费的现象.后期的优化会有非常大的边际效应递减.在右上角的表格可以看到,原版3DGS的图像分辨率是一致的,但是DashGaussian的图像分辨率是随时间动态上升的.这样的话可以减少在训练初期使用高分辨率图像的浪费.第二个图标可以看到,原版3DGS的高斯球数量随着迭代,会以一个上凸的形状快速达到数量上限,导致训练进程变慢,而Dash高斯会在前期抑制高斯球的增殖,高斯球数量以一个下凹的形状上升,可以大幅度增加训练速度.这个图是这篇文章的结构图.他的核心就是引入了对图像分辨率和高斯球数量的动态调节机制. 然后就是一个关键的问题就是通过什么机制来决定分辨率的动态变化的.这篇文章引入了频率公式来表示,这个公式是对每张训练的视图做傅里叶变换,得到它的频率表示Fn,然后计算所有频率分量的L2范数平均值,这个值作为频率能量,高频区域也就是细节部分数值更大,低频(也就是平滑部分)数值小.将高斯球训练的过程比成从低频到高频的过程,初期低分辨率时先对物体轮廓和颜色进行拟合,后期高分辨率部分对纹理和细节进行拟合. 然后就是文章定义的分辨率调度器,会预先计算各个分辨率的频率能量,然后计算sr.然后当训练步数达到sr后切换到更高分辨率.这部分是对分辨率的控制 然后是对高斯球数量的控制:这个是设计的增长曲线,它的特点是是一个凹函数,在前期增长慢,在中期随着分辨率提升加速.第二个公式代表高斯球最终数量上限会根据当前新增高斯球数量来动态的预测.这部分就是对高斯球数量的控制. 这是在不同数据集上进行的实验可以看到训练时间部分和其他相比,有了非常大的提升,并且渲染质量在几个指标略有提升. 可以看到在不同数据集和方法下,加入文章的方法后训练速度都有明显的加快,并且训练质量也有一定增强. 左图可以看到随迭代逐渐变大的图像分辨率,中间图实现部分可以看到本文方法下,高斯球数量曲线呈下凹状态,右图可以看到训练耗时最大的三个部分耗时都分别有了不同程度的下降. 这是文章做的消融实验可以两个模块对渲染速度和质量均有一定的提升. 晚上学代码随想录把二叉树的题做完了,把算法笔记二叉树部分收个尾. 生活记录早训踢球颠球 短传 逆足 晚上健身练练胸和三头,强度比较低.","tags":["3DGS","spring","日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"2025.6.22学习日记","path":"/2025/06/22/学习日记25年6月/2025.6.22学习笔记/","content":"今日学习内容力扣每日一题:简单的字符串操作. 力扣周赛回归周赛,准备再次开始一段时间的比赛,今天2 3 题答题思路都有,就差一点点,图论和动态规划还需要加强. 生活记录晚上健身练背和二头","tags":["日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"一次性搞懂B树","path":"/2025/06/21/算法笔记/一次性搞懂B树/","content":"可视化网站算法可视化网站 B-Tree的引入从磁盘查找数据效率低的原因 读写数据越大速度越慢 读写次数越多速度越慢 设计文件查找系统索引可以提供更快的查询. 哈希表优点:等值查询比较快缺点: hash冲突后,数据散列不均匀,产生大量线性查询,效率低 等值查询可以,但是遇到范围查询,需要遍历,hash不合适 树树的种类树 二叉树 BST二叉查找树 AVL平衡二叉树 红黑树 B树 B+树 二叉排序树 BST插入数据的时候得有序,必须保证: 若左子树不为空,则左子树的所有节点的值小于根节点的值 若右子树不为空,则右子树的所有节点的值大于根节点的值 问题:会退化为链表,查询效率降低为O(n). 平衡二叉树 AVL插入数据的时候保持二叉排序树平衡左子树和右子树的高度差不能超过1. 问题:用插入的成本来弥补查询的成本,插入效率降低为O(logn),但是查询效率还是O(logn).一旦出现插入操作比查询操作多的情况就不合适了. 红黑树最长子树不超过最短子树的2倍. 性质1 :根节点是黑色的. 性质2 :每个红色的节点的两个子节点都是黑色.(从每个叶子到根的所有路径上不能有两个连续的红色节点) 性质3 :从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点. 相当于不让AVL做大量的旋转操作.红黑树口诀:左根右 , 根叶黑 . 不红红 , 黑路同. 问题: 当数据特别多的时候,树的深度会很大,就意味着IO的次数会变多,影响读取的效率. B树B树就是一个有序的多路查询树. 满足下列要求的m叉树: 书中每个节点只多有m个孩子节点(至多有m-1个关键字) 每个节节点的结构为:n代表这个节点有几个关键字.P0第一个子树的地址.k1关键字 例子:m4的4阶B树阶数代表单个节点最多有的子节点数量","tags":["基础","算法","B树"],"categories":["算法笔记"]},{"title":"2025.6.21学习日记","path":"/2025/06/21/学习日记25年6月/2025.6.21学习笔记/","content":"今日学习内容力扣每日一题:今天的每日是一个哈希计数＋排序＋贪心的中等题,比较简单. Mysql学习37-41 B树学习记了一篇笔记. 晚上学代码随想录生活记录晚上健身练背和二头","tags":["spring","日记","leetcode","mysql","二叉树"],"categories":["学习日记","2025-06"]},{"title":"2025.6.20学习日记","path":"/2025/06/20/学习日记25年6月/2025.6.20学习笔记/","content":"今日学习内容3DGS换了内存条,可能会对训练速度有影响.重新训练DashGaussian,修改了一下训练的代码,将预先存入cuda的代码改成先存入内存,使用时再动态存入cuda.虽然会变慢,但是可以让显存占用减少.下午用R8设备外出采集数据.继续学习Dash论文. 力扣每日一题:上下左右移动,能修改k次移动方向,求最大曼哈顿距离.比较简单,随着移动更新答案即可. Mysql学习索引部分学习了几条. Spring学习学了几条,并且记了笔记. 项目文档学习异常处理部分,学习了一下. 晚上学代码随想录做了几个递归和回溯的二叉树题.把Morris遍历二叉树又复习了一遍,后序遍历比较有意思,是在向右的前序遍历结果上,翻转一下. 生活记录晚上健身练肩,胸,三头递减组.","tags":["3DGS","spring","日记","leetcode","项目","mysql","二叉树","Morris遍历"],"categories":["学习日记","2025-06"]},{"title":"2025.6.19学习日记","path":"/2025/06/19/学习日记25年6月/2025.6.19学习笔记/","content":"今日学习内容3DGS学习了DashGaussian论文,核心目的就是在尽可能保持渲染质量的同时,加速高斯训练速度.文章提出在训练初始阶段,高斯球是相对比较稀疏的,此时没有必要使用高分辨率图像进行训练,所以在训练初期使用低分辨率图像训练,然后通过计算低分辨率和高分辨率渲染的分数,来判断是否需要切换更高分辨率.然后在高斯球致密化策略上,文章提出的方法是在训练前期,抑制高斯球的增殖,然后在训练中后期再鼓励高斯球的增殖.因为高斯训练的速度主要就取决于高斯球的数量,而高斯球数量达到一定数目后,训练效果变化很小,但训练速度会变得非常慢.所以使高斯球数量随时间上升曲线呈下凹的形状是最兼顾训练速度和质量的. 力扣每日一题:又是一个排序＋贪心的中等题,比较简单. Mysql学习优化篇学完了,明天开始索引篇学习. Spring学习学了四条,并且记了笔记. 项目文档学习图片上传部分.MapStruct,一个用于转换对象的工具. 晚上学代码随想录做了几道二叉树题.学习了一个Morris遍历方式,这种方式可以在O(1)的空间复杂度下完成二叉树的遍历,并且不需要使用栈来存储节点.贴一个自己画的伪代码: 生活记录晚上健身练肩,飞鸟超级组四组.","tags":["3DGS","spring","日记","leetcode","项目","mysql","二叉树","Morris遍历"],"categories":["学习日记","2025-06"]},{"title":"2025.6.18学习日记","path":"/2025/06/18/学习日记25年6月/2025.6.18学习笔记/","content":"今日学习内容3DGS训练DashGaussian.昨天DashGaussian训练过程会出现报错,排查后发现是子模块版本问题导致,重新安装子模块后消除报错.并且在昨天的训练过程中,在对于复杂的场景会出现爆显存的问题:通过修改scenecameras.py中图片预先加载到显存的逻辑,把图片加载到内存,解决了爆显存的问题,虽然会因IO时间增加,训练变慢,但是可以接受. 力扣每日一题:排序加贪心的中等题. 二叉树通过中序和后序构造二叉树.通过对中序做一个索引,找到左右树的分割位置,然后递归. class Solution MapInteger,Integer in_idx = new HashMap(); public TreeNode buildTree(int[] inorder, int[] postorder) for(int i=0 ; iinorder.length ; ++i) in_idx.put(inorder[i],i); int n = inorder.length; return dfs(inorder,postorder,0,n-1,0,n-1); private TreeNode dfs(int[] inorder,int[] postorder , int beg_in , int end_in,int beg_pos , int end_pos) //全部闭区间 if(beg_in end_in ||beg_pos end_pos )return null; int rootIndex = in_idx.get(postorder[end_pos]); TreeNode root = new TreeNode(inorder[rootIndex]); int lenOfLeft = rootIndex - beg_in;//左子树个数 int n=inorder.length; root.left = dfs(inorder,postorder,beg_in,beg_in+lenOfLeft-1,beg_pos,beg_pos+lenOfLeft-1); root.right = dfs(inorder,postorder,rootIndex+1,end_in,beg_pos+lenOfLeft,end_pos-1); return root; 通过前序和后序构造二叉树.对应二叉树不唯一,找前序根节点的下一个位置在后序中的位置,然后找左右子树,剩下做法差不多了. Mysql学习Spring学习晚上学代码随想录生活记录晚上健身今天练的后侧链 背 手臂","tags":["3DGS","spring","日记","leetcode","mysql","二叉树"],"categories":["学习日记","2025-06"]},{"title":"2025.6.17学习日记","path":"/2025/06/17/学习日记25年6月/2025.6.17学习笔记/","content":"今日学习内容3DGS在Ubuntu系统上部署了DashGaussian项目.学习DashGaussian项目的改进点.用DashGaussian训练中. 力扣每日一题:一道很有意思的数学题.正问题比较复杂,但是反过来想分割成为多少块就很简单了.https://leetcode.cn/problems/count-the-number-of-arrays-with-k-matching-adjacent-elements?envType=daily-questionenvId=2025-06-17这道题做完之后,做了灵神模运算的笔记,复习了一下快速幂.求组合数的话,先求n! 然后从后向前求1n!. Mysql学习开始学习SQL优化篇,慢sql优化. Spring学习简单学了几条bean相关的. 晚上学代码随想录二叉树三四道题. 生活记录早上踢球今早七点训练一会儿.颠球,逆足,短传.右膝内侧还是有点不舒服,需要养一养.","tags":["3DGS","spring","日记","leetcode","mysql","模运算","快速幂"],"categories":["学习日记","2025-06"]},{"title":"2025.6.16学习日记","path":"/2025/06/16/学习日记25年6月/2025.6.16学习笔记/","content":"今日学习内容3DGS:今日工作总结配环境.DashGaussian项目部署在Win系统有问题,所以装了一个ubuntu双系统.配置ubuntu环境,基本环境已配置完成. 明日工作计划在Ubuntu系统上部署DashGaussian项目.尝试运行一下. 力扣每日一题:维护最小值遍历的简单题. Mysql学习学完了日志篇 项目学习看了三四篇文档. 晚上学代码随想录二叉树三四道题. 生活记录晚上健身晚上健身 后侧链","tags":["3DGS","日记","leetcode","项目","mysql"],"categories":["学习日记","2025-06"]},{"title":"2025.6.14学习日记","path":"/2025/06/14/学习日记25年6月/2025.6.14学习笔记/","content":"今日学习内容今天一直在配环境……………. 3DGS:今日工作总结配置Effect3DGS环境，安装子模块时出现安装报错: 配置Effect3DGS环境时,子模块一直无法安装成功.尝试重新配置了多个conda环境来安装子模块,但重新安装其他依赖后,安装子模块diff-gaussian-rasterization仍然报错.Effect3DGS项目没有环境依赖文件,也没在github上贴出,只能先暂时搁置.看了一些其他的3DGS项目 明日工作计划准备部署DashGaussian项目:3DGS训练加速方法. 力扣每日一题:二分答案+贪心+相邻相减计数 Spring学习做了一个工厂 晚上学代码随想录生活记录","tags":["3DGS","spring","日记","leetcode","mysql"],"categories":["学习日记","2025-06"]},{"title":"2025.6.13学习日记","path":"/2025/06/13/学习日记25年6月/2025.6.13学习笔记/","content":"今日学习内容3DGS:今日工作总结在drjohnson数据集与playroom数据集跑完了原版3DGS.发现viewer中的帧率显示并不准确,存在锁帧率上限和波动的情况,调试了一个测量3DGS的FPS的脚本,可以准确测出3DGS渲染帧率.配置Effect3DGS环境，尝试运行出现一些版本依赖问题。 明日工作计划部署EfficientGS环境,用改进方法跑数据集. 力扣每日一题:二分答案+贪心+相邻相减计数 看mysql看到29条,日志篇延伸出去看多了,看到下午四点半. Spring学习晚上学代码随想录生活记录晚上健身今天主要练的背.","tags":["3DGS","spring","日记","leetcode","mysql"],"categories":["学习日记","2025-06"]},{"title":"Spring学习笔记","path":"/2025/06/12/基础笔记/Spring学习笔记/","content":"基础1.Spring 是什么？特性？有哪些模块？一句话概括：Spring 是一个轻量级、非入侵式的控制反转 (IoC)和面向切面 (AOP) 的框架。 2003 年，一个音乐家 Rod Johnson 决定发展一个轻量级的 Java 开发框架，Spring作为 Java 战场的龙骑兵渐渐崛起，并淘汰了EJB这个传统的重装骑兵。 到了现在，企业级开发的标配基本就是 Spring5+ Spring Boot 2 + JDK 8 Spring 有哪些特性呢？ 1. IoC 和 DI 的支持Spring 的核心就是一个大的工厂容器，可以维护所有对象的创建和依赖关系，Spring 工厂用于生成 Bean，并且管理 Bean 的生命周期，实现高内聚低耦合的设计理念。 2. AOP 编程的支持Spring 提供了面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等切面功能。 3. 声明式事务的支持支持通过配置就来完成对事务的管理，而不需要通过硬编码的方式，以前重复的一些事务提交、回滚的 JDBC 代码，都可以不用自己写了。 4. 快捷测试的支持Spring 对 Junit 提供支持，可以通过注解快捷地测试 Spring 程序。 5. 快速集成功能方便集成各种优秀框架，Spring 不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz 等）的直接支持。 6. 复杂 API 模板封装Spring 对 JavaEE 开发中非常难用的一些 API（JDBC、JavaMail、远程调用等）都提供了模板化的封装，这些封装 API 的提供使得应用难度大大降低。 简单说一下什么是AOP 和 IoC？AOP：面向切面编程，是一种编程范式，它的主要作用是将那些与核心业务逻辑无关，但是对多个对象产生影响的公共行为封装起来，如日志记录、性能统计、事务等。IoC：控制反转，是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。 Spring源码看过吗？看过一些，主要就是针对 Spring 循环依赖、Bean 声明周期、AOP、事务、IOC 这五部分。详情看笔记,Spring源码笔记. 2.Spring 有哪些模块呢？Spring 框架是分模块存在，除了最核心的Spring Core Container是必要模块之外，其他模块都是可选，大约有 20 多个模块。 最主要的七大模块： Spring Core：Spring 核心，它是框架最基础的部分，提供 IoC 和依赖注入 DI 特性。 Spring Context：Spring 上下文容器，它是 BeanFactory 功能加强的一个子接口。 Spring Web：它提供 Web 应用开发的支持。 Spring MVC：它针对 Web 应用中 MVC 思想的实现。 Spring DAO：提供对 JDBC 抽象层，简化了 JDBC 编码，同时，编码更具有健壮性。 Spring ORM：它支持用于流行的 ORM 框架的整合，比如：Spring + Hibernate、Spring + iBatis、Spring + JDO 的整合等。 Spring AOP：即面向切面编程，它提供了与 AOP 联盟兼容的编程实现。 3.Spring 有哪些常用注解呢？Spring 提供了大量的注解来简化 Java 应用的开发和配置，主要用于 Web 开发、往容器注入 Bean、AOP、事务控制等。 Web 开发方面有哪些注解呢？①、@Controller：用于标注控制层组件。②、@RestController：是@Controller 和 @ResponseBody 的结合体，返回 JSON 数据时使用。③、@RequestMapping：用于映射请求 URL 到具体的方法上，还可以细分为：@GetMapping：只能用于处理 GET 请求@PostMapping：只能用于处理 POST 请求@DeleteMapping：只能用于处理 DELETE 请求④、@ResponseBody：直接将返回的数据放入 HTTP 响应正文中，一般用于返回 JSON 数据。⑤、@RequestBody：表示一个方法参数应该绑定到 Web 请求体。⑥、@PathVariable：用于接收路径参数，比如 @RequestMapping(“hello{name}”)，这里的 name 就是路径参数。⑦、@RequestParam：用于接收请求参数。比如 @RequestParam(name “key”) String key，这里的 key 就是请求参数。 容器类注解有哪些呢？@Component：标识一个类为 Spring 组件，使其能够被 Spring 容器自动扫描和管理。@Service：标识一个业务逻辑组件（服务层）。比如 @Service(“userService”)，这里的 userService 就是 Bean 的名称。@Repository：标识一个数据访问组件（持久层）。@Autowired：按类型自动注入依赖。@Configuration：用于定义配置类，可替换 XML 配置文件。@Value：用于将 Spring Boot 中 application.properties 配置的属性值赋值给变量。 AOP 方面有哪些注解呢？@Aspect 用于声明一个切面，可以配合其他注解一起使用，比如：@After：在方法执行之后执行。@Before：在方法执行之前执行。@Around：方法前后均执行。@PointCut：定义切点，指定需要拦截的方法。事务注解有哪些？主要就是 @Transactional，用于声明一个方法需要事务支持。 4.Spring 中应用了哪些设计模式呢？Spring 框架中用了蛮多设计模式的： ①、比如说工厂模式用于 BeanFactory 和 ApplicationContext，实现 Bean 的创建和管理。 ApplicationContext context = new ClassPathXmlApplicationContext(applicationContext.xml);MyBean myBean = context.getBean(MyBean.class); ②、比如说单例模式，这样可以保证 Bean 的唯一性，减少系统开销。 ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);MyService myService1 = context.getBean(MyService.class);MyService myService2 = context.getBean(MyService.class);// This will print true because both references point to the same instanceSystem.out.println(myService1 == myService2); ③、比如说 AOP 使用了代理模式来实现横切关注点（如事务管理、日志记录、权限控制等）。 @Transactionalpublic void myTransactionalMethod() // 方法实现 Spring如何实现单例模式？Spring 通过 IOC 容器(控制反转)实现单例模式，具体步骤是： 单例 Bean 在容器初始化时创建并使用 DefaultSingletonBeanRegistry 提供的 singletonObjects进行缓存。 // 单例缓存private final MapString, Object singletonObjects = new ConcurrentHashMap();public Object getSingleton(String beanName) return this.singletonObjects.get(beanName);protected void addSingleton(String beanName, Object singletonObject) this.singletonObjects.put(beanName, singletonObject); 在请求 Bean 时，Spring 会先从缓存中获取。 39.Spring 容器、Web 容器之间的区别？（补充）Spring 容器是 Spring 框架的核心部分，负责管理应用程序中的对象生命周期和依赖注入。Web 容器（也称 Servlet 容器），是用于运行 Java Web 应用程序的服务器环境，支持 Servlet、JSP 等 Web 组件。常见的 Web 容器包括 Apache Tomcat、Jetty等。Spring MVC 是 Spring 框架的一部分，专门用于处理 Web 请求，基于 MVC（Model-View-Controller）设计模式。 IoC5.说一说什么是 IoC、DI？所谓的IoC，就是由容器来控制对象的生命周期和对象之间的关系。控制对象生命周期的不再是引用它的对象，而是容器，这就叫控制反转（Inversion of Control）。 没有 IoC 之前： 我需要一个女朋友，刚好大街上突然看到了一个小姐姐，人很好看，于是我就自己主动上去搭讪，要她的微信号，找机会聊天关心她，然后约她出来吃饭，打听她的爱好，三观。。。 有了 IoC 之后： 我需要一个女朋友，于是我就去找婚介所，告诉婚介所，我需要一个长的像赵露思的，会打 Dota2 的，于是婚介所在它的人才库里开始找，找不到它就直接说没有，找到它就直接介绍给我。 婚介所就相当于一个 IoC 容器，我就是一个对象，我需要的女朋友就是另一个对象，我不用关心女朋友是怎么来的，我只需要告诉婚介所我需要什么样的女朋友，婚介所就帮我去找。 Spring 倡导的开发方式就是这样，所有类的创建和销毁都通过 Spring 容器来，不再是开发者去 new，去 null，这样就实现了对象的解耦。 于是，对于某个对象来说，以前是它控制它依赖的对象，现在是所有对象都被 Spring 控制。 说说什么是DI?IOC 是一种思想，DI 是实现 IOC 的具体方式，比如说利用注入机制（如构造器注入、Setter 注入）将依赖传递给目标对象。 2004 年，Martin Fowler 在他的文章《控制反转容器依赖注入模式》首次提出了 DI（依赖注入，Dependency Injection） 这个名词。 打个比方，你现在想吃韭菜馅的饺子，这时候就有人用针管往你吃的饺子里注入韭菜鸡蛋馅。就好像 A 类需要 B 类，以前是 A 类自己 new 一个 B 类，现在是有人把 B 类注入到 A 类里。 为什么要使用 IoC 呢？在平时的 Java 开发中，如果我们要实现某一个功能，可能至少需要两个以上的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new 一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系。 有了 Spring 之后，就不一样了，创建 B 的工作交给了 Spring 来完成，Spring 创建好了 B 对象后就放到容器中，A 告诉 Spring 我需要 B，Spring 就从容器中取出 B 交给 A 来使用。 至于 B 是怎么来的，A 就不再关心了，Spring 容器想通过 newnew 创建 B 还是 new 创建 B，无所谓。 这就是 IoC 的好处，它降低了对象之间的耦合度，使得程序更加灵活，更加易于维护。 6.能简单说一下 Spring IoC 的实现机制吗？Spring 的 IoC 本质就是一个大工厂，我们想想一个工厂是怎么运行的呢？ 生产产品：一个工厂最核心的功能就是生产产品。在 Spring 里，不用 Bean 自己来实例化，而是交给 Spring，应该怎么实现呢？——答案毫无疑问，反射。那么这个厂子的生产管理是怎么做的？你应该也知道——工厂模式。 库存产品：工厂一般都是有库房的，用来库存产品，毕竟生产的产品不能立马就拉走。Spring 我们都知道是一个容器，这个容器里存的就是对象，不能每次来取对象，都得现场来反射创建对象，得把创建出的对象存起来。 订单处理：还有最重要的一点，工厂根据什么来提供产品呢？订单。这些订单可能五花八门，有线上签签的、有到工厂签的、还有工厂销售上门签的……最后经过处理，指导工厂的出货。在 Spring 里，也有这样的订单，它就是我们 bean 的定义和依赖关系，可以是 xml 形式，也可以是我们最熟悉的注解形式。 我们简单地实现一个 mini 版的 Spring IoC： 7.说说 BeanFactory 和 ApplicantContext?可以这么比喻，BeanFactory 是 Spring 的“心脏”，而 ApplicantContext 是 Spring 的完整“身躯”。 BeanFactory 主要负责配置、创建和管理 bean，为 Spring 提供了基本的依赖注入（DI）支持。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上添加了企业级的功能支持。 详细说说 BeanFactoryBeanFactory 位于整个 Spring IoC 容器的顶端，ApplicationContext 算是 BeanFactory 的子接口。 它最主要的方法就是 getBean()，这个方法负责从容器中返回特定名称或者类型的 Bean 实例。 来看一个 XMLBeanFactory（已过时） 获取 bean 的例子： class HelloWorldApp public static void main(String[] args) BeanFactory factory = new XmlBeanFactory (new ClassPathResource(beans.xml)); HelloWorld obj = (HelloWorld) factory.getBean(itwanger); obj.getMessage(); 请详细说说 ApplicationContextApplicationContext 继承了 HierachicalBeanFactory 和 ListableBeanFactory 接口，算是 BeanFactory 的自动挡版本，是 Spring 应用的默认方式。 ApplicationContext 会在启动时预先创建和配置所有的单例 bean，并支持如 JDBC、ORM 框架的集成，内置面向切面编程（AOP）的支持，可以配置声明式事务管理等。 这是 ApplicationContext 的使用例子： class MainApp public static void main(String[] args) // 使用 AppConfig 配置类初始化 ApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); // 从 ApplicationContext 获取 messageService 的 bean MessageService service = context.getBean(MessageService.class); // 使用 bean service.printMessage(); 通过 AnnotationConfigApplicationContext 类，我们可以使用 Java 配置类来初始化 ApplicationContext，这样就可以使用 Java 代码来配置 Spring 容器。 @Configuration@ComponentScan(basePackages = com.github.paicoding.forum.test.javabetter.spring1) // 替换为你的包名public class AppConfig 8.你知道 Spring 容器启动阶段会干什么吗？Spring 的 IoC 容器工作的过程，可以划分为两个阶段：容器启动阶段和Bean 实例化阶段。其中容器启动阶段主要做的工作是加载和解析配置文件，保存到对应的 Bean 定义中。 容器启动开始，首先会通过某种途径加载 Configuration MetaData，在大部分情况下，容器需要依赖某些工具类（BeanDefinitionReader）对加载的 Configuration MetaData 进行解析和分析，并将分析后的信息组为相应的 BeanDefinition。 最后把这些保存了 Bean 定义必要信息的 BeanDefinition，注册到相应的 BeanDefinitionRegistry，这样容器启动就完成了。 说说 Spring 的 Bean 实例化方式Spring 提供了 4 种不同的方式来实例化 Bean，以满足不同场景下的需求。 说说构造方法的方式在类上使用@Component（或@Service、@Repository 等特定于场景的注解）标注类，然后通过构造方法注入依赖。 @Componentpublic class ExampleBean private DependencyBean dependency; @Autowired public ExampleBean(DependencyBean dependency) this.dependency = dependency; 说说静态工厂的方式在这种方式中，Bean 是由一个静态方法创建的，而不是直接通过构造方法。 public class ClientService private static ClientService clientService = new ClientService(); private ClientService() public static ClientService createInstance() return clientService; 说说实例工厂方法实例化的方式与静态工厂方法相比，实例工厂方法依赖于某个类的实例来创建 Bean。这通常用在需要通过工厂对象的非静态方法来创建 Bean 的场景。 public class ServiceLocator public ClientService createClientServiceInstance() return new ClientService(); 说说 FactoryBean 接口实例化方式FactoryBean 是一个特殊的 Bean 类型，可以在 Spring 容器中返回其他对象的实例。通过实现 FactoryBean 接口，可以自定义实例化逻辑，这对于构建复杂的初始化逻辑非常有用。 public class ToolFactoryBean implements FactoryBeanTool private int factoryId; private int toolId; @Override public Tool getObject() throws Exception return new Tool(toolId); @Override public Class? getObjectType() return Tool.class; @Override public boolean isSingleton() return true; // setter and getter methods for factoryId and toolId 9.你是怎么理解 Bean 的？Bean 是指由 Spring 容器管理的对象，它的生命周期由容器控制，包括创建、初始化、使用和销毁。以通过三种方式声明：注解方式、XML 配置、Java 配置。 ①、使用 @Component、@Service、@Repository、@Controller 等注解定义，主流。 ②、基于 XML 配置，Spring Boot 项目已经不怎么用了。 ③、使用 Java 配置类创建 Bean： @Configurationpublic class AppConfig @Bean public UserService userService() return new UserService(); @Component 和 @Bean 的区别@Component 是 Spring 提供的一个类级别注解，由 Spring 自动扫描并注册到 Spring 容器中。@Bean 是一个方法级别的注解，用于显式地声明一个 Bean，当我们需要第三方库或者无法使用 @Component 注解类时，可以使用 @Bean 来将其实例注册到容器中。 10.能说一下 Bean 的生命周期吗？推荐阅读:https://mp.weixin.qq.com/s/zb6eA3Se0gQoqL8PylCPLw Bean的生命周期大致分为五个阶段： 实例化：Spring 首先使用构造方法或者工厂方法创建一个 Bean 的实例。在这个阶段，Bean 只是一个空的 Java 对象，还未设置任何属性。 属性赋值：Spring 将配置文件中的属性值或依赖的 Bean 注入到该 Bean 中。这个过程称为依赖注入，确保 Bean 所需的所有依赖都被注入。 初始化：Spring 调用 afterPropertiesSet 方法，或通过配置文件指定的 init-method 方法，完成初始化。 使用中：Bean 准备好可以使用了。 销毁：在容器关闭时，Spring 会调用 destroy 方法，完成 Bean 的清理工作。 可以从源码角度讲一下吗？ 实例化：Spring 容器根据 Bean 的定义创建 Bean 的实例，相当于执行构造方法，也就是 new 一个对象。 属性赋值：相当于执行 setter 方法为字段赋值。 初始化：初始化阶段允许执行自定义的逻辑，比如设置某些必要的属性值、开启资源、执行预加载操作等，以确保 Bean 在使用之前是完全配置好的。 销毁：相当于执行 null，释放资源。可以在源码 AbstractAutowireCapableBeanFactory 中的 doCreateBean 方法中，看到 Bean 的前三个生命周期：protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) instanceWrapper = (BeanWrapper)this.factoryBeanInstanceCache.remove(beanName); if (instanceWrapper == null) // 实例化阶段 instanceWrapper = this.createBeanInstance(beanName, mbd, args); ... Object exposedObject = bean; try // 属性赋值阶段 this.populateBean(beanName, mbd, instanceWrapper); // 初始化阶段 exposedObject = this.initializeBean(beanName, exposedObject, mbd); catch (Throwable var18) ... ... 源码位置,如下图: 至于销毁，是在容器关闭的时候调用的，详见 ConfigurableApplicationContext 的 close 方法。 请在一个已有的 Spring Boot 项目中通过单元测试的形式来展示 Spring Bean 的生命周期？第一步，创建一个 LifecycleDemoBean 类： public class LifecycleDemoBean implements InitializingBean, DisposableBean // 使用@Value注解注入属性值，这里演示了如何从配置文件中读取值 // 如果配置文件中没有定义lifecycle.demo.bean.name，则使用默认值default name @Value($lifecycle.demo.bean.name:default name) private String name; // 构造方法：在Bean实例化时调用 public LifecycleDemoBean() System.out.println(LifecycleDemoBean: 实例化); // 属性赋值：Spring通过反射调用setter方法为Bean的属性注入值 public void setName(String name) System.out.println(LifecycleDemoBean: 属性赋值); this.name = name; // 使用@PostConstruct注解的方法：在Bean的属性赋值完成后调用，用于执行初始化逻辑 @PostConstruct public void postConstruct() System.out.println(LifecycleDemoBean: @PostConstruct（初始化）); // 实现InitializingBean接口：afterPropertiesSet方法在@PostConstruct注解的方法之后调用 // 用于执行更多的初始化逻辑 @Override public void afterPropertiesSet() throws Exception System.out.println(LifecycleDemoBean: afterPropertiesSet（InitializingBean）); // 自定义初始化方法：在XML配置或Java配置中指定，执行特定的初始化逻辑 public void customInit() System.out.println(LifecycleDemoBean: customInit（自定义初始化方法）); // 使用@PreDestroy注解的方法：在容器销毁Bean之前调用，用于执行清理工作 @PreDestroy public void preDestroy() System.out.println(LifecycleDemoBean: @PreDestroy（销毁前）); // 实现DisposableBean接口：destroy方法在@PreDestroy注解的方法之后调用 // 用于执行清理资源等销毁逻辑 @Override public void destroy() throws Exception System.out.println(LifecycleDemoBean: destroy（DisposableBean）); // 自定义销毁方法：在XML配置或Java配置中指定，执行特定的清理逻辑 public void customDestroy() System.out.println(LifecycleDemoBean: customDestroy（自定义销毁方法）); ①、实例化 实例化是创建 Bean 实例的过程，即在内存中为 Bean 对象分配空间。这一步是通过调用 Bean 的构造方法完成的。 public LifecycleDemoBean() System.out.println(LifecycleDemoBean: 实例化); 在这里，当 Spring 创建 LifecycleDemoBean 的实例时，会调用其无参数的构造方法，这个过程就是实例化。 ②、属性赋值 在实例化之后，Spring 将根据 Bean 定义中的配置信息，通过反射机制为 Bean 的属性赋值。 @Value($lifecycle.demo.bean.name:default name)private String name;public void setName(String name) System.out.println(LifecycleDemoBean: 属性赋值); this.name = name; @Value注解和 setter 方法体现了属性赋值的过程。@Value注解让 Spring 注入配置值（或默认值），setter 方法则是属性赋值的具体操作。 ③、初始化 初始化阶段允许执行自定义的初始化逻辑，比如检查必要的属性是否已经设置、开启资源等。Spring 提供了多种方式来配置初始化逻辑。 1、使用 @PostConstruct 注解的方法 @PostConstructpublic void postConstruct() System.out.println(LifecycleDemoBean: @PostConstruct（初始化）);@PostConstruct注解的方法在 Bean 的所有属性都被赋值后，且用户自定义的初始化方法之前调用。 2、实现 InitializingBean 接口的 afterPropertiesSet 方法 @Overridepublic void afterPropertiesSet() throws Exception System.out.println(LifecycleDemoBean: afterPropertiesSet（InitializingBean）); afterPropertiesSet 方法提供了另一种初始化 Bean 的方式，也是在所有属性赋值后调用。 3、自定义初始化方法 public void customInit() System.out.println(LifecycleDemoBean: customInit（自定义初始化方法）); 需要在配置类中指定初始化方法： @Bean(initMethod = customInit)public LifecycleDemoBean lifecycleDemoBean() return new LifecycleDemoBean(); ④、销毁 销毁阶段允许执行自定义的销毁逻辑，比如释放资源。类似于初始化阶段，Spring 也提供了多种方式来配置销毁逻辑。 1、使用 @PreDestroy 注解的方法 @PreDestroypublic void preDestroy() System.out.println(LifecycleDemoBean: @PreDestroy（销毁前）); @PreDestroy注解的方法在 Bean 被销毁前调用。 2、实现 DisposableBean 接口的 destroy 方法 @Overridepublic void destroy() throws Exception System.out.println(LifecycleDemoBean: destroy（DisposableBean）); destroy 方法提供了另一种销毁 Bean 的方式，也是在 Bean 被销毁前调用。 3、自定义销毁方法 public void customDestroy() System.out.println(LifecycleDemoBean: customDestroy（自定义销毁方法）); 需要在配置类中指定销毁方法： @Bean(destroyMethod = customDestroy)public LifecycleDemoBean lifecycleDemoBean() return new LifecycleDemoBean(); 第二步，注册 Bean 并指定自定义初始化方法和销毁方法： @Configurationpublic class LifecycleDemoConfig @Bean(initMethod = customInit, destroyMethod = customDestroy) public LifecycleDemoBean lifecycleDemoBean() return new LifecycleDemoBean(); 第三步，编写单元测试： @SpringBootTestpublic class LifecycleDemoTest @Autowired private ApplicationContext context; @Test public void testBeanLifecycle() System.out.println(获取LifecycleDemoBean实例...); LifecycleDemoBean bean = context.getBean(LifecycleDemoBean.class); 运行单元测试，查看控制台输出： LifecycleDemoBean: 实例化LifecycleDemoBean: @PostConstruct（初始化）LifecycleDemoBean: afterPropertiesSet（InitializingBean）LifecycleDemoBean: customInit（自定义初始化方法）获取LifecycleDemoBean实例...LifecycleDemoBean: @PreDestroy（销毁前）LifecycleDemoBean: destroy（DisposableBean）LifecycleDemoBean: customDestroy（自定义销毁方法） Aware 类型的接口有什么作用？通过实现 Aware 接口，Bean 可以获取 Spring 容器的相关信息，如 BeanFactory、ApplicationContext 等。常见 Aware 接口有： 如果配置了 init-method 和 destroy-method，Spring 会在什么时候调用其配置的方法？init-method 在 Bean 初始化阶段调用，依赖注入完成后且 postProcessBeforeInitialization 调用之后执行。destroy-method 在 Bean 销毁阶段调用，容器关闭时调用。 11.为什么 IDEA 不推荐使用 @Autowired 注解注入 Bean？当使用 @Autowired 注解注入 Bean 时，IDEA 会提示“Field injection is not recommended”。 这是因为字段注入的方式： 不能像构造方法那样使用 final 注入不可变对象 隐藏了依赖关系，调用者可以看到构造方法注入或者 setter 注入，但无法看到私有字段的注入 在 Spring 4.3 及更高版本中，如果一个类只有一个构造方法，Spring 会自动使用该构造方法进行依赖注入，无需使用 @Autowired 注解。 @Autowired 和 @Resource 注解的区别？ @Autowired 是 Spring 提供的注解，按类型（byType）注入。 @Resource 是 Java EE 提供的注解，按名称（byName）注入。 虽然 IDEA 不推荐使用 @Autowired，但对 @Resource 注解却没有任何提示。这是因为 @Resource 属于 Java EE 标准的注解，如果使用其他 IOC 容器而不是 Spring 也是可以兼容的。 提到了byType，如果两个类型一致的发生了冲突，应该怎么处理当容器中存在多个相同类型的 bean，编译器会提示 Could not autowire. There is more than one bean of UserRepository2 type. @Componentpublic class UserRepository21 implements UserRepository2 @Componentpublic class UserRepository22 implements UserRepository2 @Componentpublic class UserService2 @Autowired private UserRepository2 userRepository; // 冲突 这时候，就可以配合 @Qualifier 注解来指定具体的 bean 名称： @Component(userRepository21)public class UserRepository21 implements UserRepository2 @Component(userRepository22)public class UserRepository22 implements UserRepository2 @Autowired@Qualifier(userRepository22)private UserRepository2 userRepository22; 或者使用 @Resource 注解按名称进行注入，指定 name 属性。 @Resource(name = userRepository21)private UserRepository2 userRepository21; 12.Spring 有哪些自动装配的方式？什么是自动装配？ Spring IoC 容器知道所有 Bean 的配置信息，此外，通过 Java 反射机制还可以获知实现类的结构信息，如构造方法的结构、属性等信息。掌握所有 Bean 的这些信息后，Spring IoC 容器就可以按照某种规则对容器中的 Bean 进行自动装配，而无须通过显式的方式进行依赖配置。 Spring 提供的这种方式，可以按照某些规则进行 Bean 的自动装配，bean元素提供了一个指定自动装配类型的属性：autowire=自动装配类型 Spring 提供了哪几种自动装配类型？ Spring 提供了 4 种自动装配类型： byName：根据名称进行自动匹配，假设 Boss 有一个名为 car 的属性，如果容器中刚好有一个名为 car 的 bean，Spring 就会自动将其装配给 Boss 的 car 属性 byType：根据类型进行自动匹配，假设 Boss 有一个 Car 类型的属性，如果容器中刚好有一个 Car 类型的 Bean，Spring 就会自动将其装配给 Boss 这个属性 constructor：与 byType 类似， 只不过它是针对构造函数注入而言的。如果 Boss 有一个构造函数，构造函数包含一个 Car 类型的入参，如果容器中有一个 Car 类型的 Bean，则 Spring 将自动把这个 Bean 作为 Boss 构造函数的入参；如果容器中没有找到和构造函数入参匹配类型的 Bean，则 Spring 将抛出异常。 autodetect：根据 Bean 的自省机制决定采用 byType 还是 constructor 进行自动装配，如果 Bean 提供了默认的构造函数，则采用 byType，否则采用 constructor。 13.Bean 的作用域有哪些?在 Spring 中，Bean 默认是单例的，即在整个 Spring 容器中，每个 Bean 只有一个实例。可以通过在配置中指定 scope 属性，将 Bean 改为多例（Prototype）模式，这样每次获取的都是新的实例。 @Bean@Scope(prototype) // 每次获取都是新的实例public MyBean myBean() return new MyBean(); 除了单例和多例，Spring 还支持其他作用域，如请求作用域（Request）、会话作用域（Session）等，适合 Web 应用中特定的使用场景。 request：每一次 HTTP 请求都会产生一个新的 Bean，该 Bean 仅在当前 HTTP Request 内有效。 session：同一个 Session 共享一个 Bean，不同的 Session 使用不同的 Bean。 globalSession：同一个全局 Session 共享一个 Bean，只用于基于 Protlet 的 Web 应用，Spring5 中已经移除。 14.Spring 中的单例 Bean 会存在线程安全问题吗？Spring Bean 的默认作用域是单例（Singleton），这意味着 Spring 容器中只会存在一个 Bean 实例，并且该实例会被多个线程共享。 如果单例 Bean 是无状态的，也就是没有成员变量，那么这个单例 Bean 是线程安全的。比如 Spring MVC 中的 Controller、Service、Dao 等，基本上都是无状态的。 但如果 Bean 的内部状态是可变的，且没有进行适当的同步处理，就可能出现线程安全问题。 单例 Bean 线程安全问题怎么解决呢？第一，使用局部变量。局部变量是线程安全的，因为每个线程都有自己的局部变量副本。尽量使用局部变量而不是共享的成员变量。 public class MyService public void process() int localVar = 0; // 使用局部变量进行操作 第二，尽量使用无状态的 Bean，即不在 Bean 中保存任何可变的状态信息。 public class MyStatelessService public void process() // 无状态处理 第三，同步访问。如果 Bean 中确实需要保存可变状态，可以通过 synchronized 关键字或者 Lock 接口来保证线程安全。 public class MyService private int sharedVar; public synchronized void increment() sharedVar++; 或者将 Bean 中的成员变量保存到 ThreadLocal 中，ThreadLocal 可以保证多线程环境下变量的隔离。 public class MyService private ThreadLocalInteger localVar = ThreadLocal.withInitial(() - 0); public void process() localVar.set(localVar.get() + 1); 再或者使用线程安全的工具类，比如说 AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。 public class MyService private ConcurrentHashMapString, String map = new ConcurrentHashMap(); public void putValue(String key, String value) map.put(key, value); 第四，将 Bean 定义为原型作用域（Prototype）。原型作用域的 Bean 每次请求都会创建一个新的实例，因此不存在线程安全问题。 @Component@Scope(prototype)public class MyService // 实例变量 15.说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。 循环依赖只发生在 Singleton 作用域的 Bean 之间，因为如果是 Prototype 作用域的 Bean，Spring 会直接抛出异常。 原因很简单，AB 循环依赖，A 实例化的时候，发现依赖 B，创建 B 实例，创建 B 的时候发现需要 A，创建 A1 实例……无限套娃。。。。 我们来看一个实例，先是 PrototypeBeanA： @Component@Scope(prototype)public class PrototypeBeanA private final PrototypeBeanB prototypeBeanB; @Autowired public PrototypeBeanA(PrototypeBeanB prototypeBeanB) this.prototypeBeanB = prototypeBeanB; 然后是 PrototypeBeanB： @Component@Scope(prototype)public class PrototypeBeanB private final PrototypeBeanA prototypeBeanA; @Autowired public PrototypeBeanB(PrototypeBeanA prototypeBeanA) this.prototypeBeanA = prototypeBeanA; 再然后是测试： @SpringBootApplicationpublic class DemoApplication public static void main(String[] args) SpringApplication.run(DemoApplication.class, args); @Bean CommandLineRunner commandLineRunner(ApplicationContext ctx) return args - // 尝试获取PrototypeBeanA的实例 PrototypeBeanA beanA = ctx.getBean(PrototypeBeanA.class); ; Spring 可以解决哪些情况的循环依赖？看看这几种情形（AB 循环依赖）： 也就是说： AB 均采用构造器注入，不支持 AB 均采用 setter 注入，支持 AB 均采用属性自动注入，支持 A 中注入的 B 为 setter 注入，B 中注入的 A 为构造器注入，支持 B 中注入的 A 为 setter 注入，A 中注入的 B 为构造器注入，不支持第四种可以，第五种不可以的原因是 Spring 在创建 Bean 时默认会根据自然排序进行创建，所以 A 会先于 B 进行创建。 简单总结下，当循环依赖的实例都采用 setter 方法注入时，Spring 支持，都采用构造器注入的时候，不支持；构造器注入和 setter 注入同时存在的时候，看天（😂）。 16.Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖： 一级缓存：存放完全初始化好的单例 Bean。 二级缓存：存放正在创建但未完全初始化的 Bean 实例。 三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。 三级缓存解决循环依赖的过程是什么样的？ 实例化 Bean 时，将其早期引用放入三级缓存。 其他依赖该 Bean 的对象，可以从缓存中获取其引用。 初始化完成后，将 Bean 移入一级缓存。 假如 A、B 两个类发生循环依赖. A 实例的初始化过程：①、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道。②、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。 ③、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。 发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 已经实例化并且初始化完成了，把 B 放入⼀级缓存。 ④、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存 ⑤、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。 17.为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。 因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。 假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean 初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。 如果缺少第二级缓存会有什么问题？如果没有二级缓存，Spring 无法在未完成初始化的情况下暴露 Bean。会导致代理 Bean 的循环依赖问题，因为某些代理逻辑无法在三级缓存中提前暴露。最终可能抛出 BeanCurrentlyInCreationException。 18.@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor 在 Bean 的初始化阶段，会通过 Bean 后置处理器来进行一些前置和后置的处理。 实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是 AutowiredAnnotationBeanPostProcessor。 Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在 doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。 在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的 postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。 /*** 属性赋值**/protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) //………… if (hasInstAwareBpps) if (pvs == null) pvs = mbd.getPropertyValues(); PropertyValues pvsToUse; for(Iterator var9 = this.getBeanPostProcessorCache().instantiationAware.iterator(); var9.hasNext(); pvs = pvsToUse) InstantiationAwareBeanPostProcessor bp = (InstantiationAwareBeanPostProcessor)var9.next(); pvsToUse = bp.postProcessProperties((PropertyValues)pvs, bw.getWrappedInstance(), beanName); if (pvsToUse == null) if (filteredPds == null) filteredPds = this.filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); //执行后处理器，填充属性，完成自动装配 //调用InstantiationAwareBeanPostProcessor的postProcessPropertyValues()方法 pvsToUse = bp.postProcessPropertyValues((PropertyValues)pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvsToUse == null) return; //………… postProcessorPropertyValues()方法的源码如下，在该方法中，会先调用 findAutowiringMetadata()方法解析出 bean 中带有@Autowired 注解、@Inject 和@Value 注解的属性和方法。然后调用 metadata.inject()方法，进行属性填充。 public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) //@Autowired注解、@Inject和@Value注解的属性和方法 InjectionMetadata metadata = this.findAutowiringMetadata(beanName, bean.getClass(), pvs); try //属性填充 metadata.inject(bean, beanName, pvs); return pvs; catch (BeanCreationException var6) throw var6; catch (Throwable var7) throw new BeanCreationException(beanName, Injection of autowired dependencies failed, var7); AOP19.说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。 举个例子，假如我们现在需要在业务代码开始前进行参数校验，在结束后打印日志，该怎么办呢？ 我们可以把日志记录和数据校验这两个功能抽取出来，形成一个切面，然后在业务代码中引入这个切面，这样就可以实现业务逻辑和通用逻辑的分离。 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。 AOP 有哪些核心概念？ 切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象 连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法 切点（Pointcut）：对连接点进行拦截的定位 通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强 目标对象 （Target）：代理的目标对象 引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法 织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。 织入有哪几种方式？①、编译期织入：切面在目标类编译时被织入。 ②、类加载期织入：切面在目标类加载到 JVM 时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。 ③、运行期织入：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP 容器会为目标对象动态地创建一个代理对象。 Spring AOP 采用运行期织入，而 AspectJ 可以在编译期织入和类加载时织入。 AspectJ 是什么？AspectJ 是一个 AOP 框架，它可以做很多 Spring AOP 干不了的事情，比如说支持编译时、编译后和类加载时织入切面。并且提供更复杂的切点表达式和通知类型。 下面是一个简单的 AspectJ 示例： // 定义一个切面@Aspectpublic class LoggingAspect // 定义一个切点，匹配 com.example 包下的所有方法 @Pointcut(execution(* com.example..*(..))) private void selectAll() // 定义一个前置通知，在匹配的方法执行之前执行 @Before(selectAll()) public void beforeAdvice() System.out.println(A method is about to be executed.); AOP 有哪些环绕方式？AOP 一般有 5 种环绕方式： 前置通知 (@Before) 返回通知 (@AfterReturning) 异常通知 (@AfterThrowing) 后置通知 (@After) 环绕通知 (@Around) 多个切面的情况下，可以通过 @Order 指定先后顺序，数字越小，优先级越高。代码示例如下： @Aspect@Componentpublic class WebLogAspect private final static Logger logger = LoggerFactory.getLogger(WebLogAspect.class); @Pointcut(@annotation(cn.fighter3.spring.aop_demo.WebLog)) public void webLog() @Before(webLog()) public void doBefore(JoinPoint joinPoint) throws Throwable // 开始打印请求日志 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 打印请求相关参数 logger.info(========================================== Start ==========================================); // 打印请求 url logger.info(URL : , request.getRequestURL().toString()); // 打印 Http method logger.info(HTTP Method : , request.getMethod()); // 打印调用 controller 的全路径以及执行方法 logger.info(Class Method : ., joinPoint.getSignature().getDeclaringTypeName(), joinPoint.getSignature().getName()); // 打印请求的 IP logger.info(IP : , request.getRemoteAddr()); // 打印请求入参 logger.info(Request Args : ,new ObjectMapper().writeValueAsString(joinPoint.getArgs())); @After(webLog()) public void doAfter() throws Throwable // 结束后打个分隔线，方便查看 logger.info(=========================================== End ===========================================); @Around(webLog()) public Object doAround(ProceedingJoinPoint proceedingJoinPoint) throws Throwable //开始时间 long startTime = System.currentTimeMillis(); Object result = proceedingJoinPoint.proceed(); // 打印出参 logger.info(Response Args : , new ObjectMapper().writeValueAsString(result)); // 执行耗时 logger.info(Time-Consuming : ms, System.currentTimeMillis() - startTime); return result; Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。 在 Spring 容器初始化 Bean 的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代理对象中织入切面逻辑。这一过程发生在 Spring 容器的后处理器（BeanPostProcessor）阶段。 简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。 AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。 ① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像 @Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。 ② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现 AOP。 ③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。 AOP和 OOP 的关系？AOP 和 OOP 是互补的编程思想： OOP 通过类和对象封装数据和行为，专注于核心业务逻辑。 AOP 提供了解决横切关注点（如日志、权限、事务等）的机制，将这些逻辑集中管理。 20.AOP的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。我们在技术派实战项目中主要利用 AOP 来打印接口的入参和出参日志、执行时间，方便后期 bug 溯源和性能调优。 第一步，自定义注解作为切点 @Target(ElementType.METHOD, ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface MdcDot String bizCode() default ; 第二步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间 第三步，在使用的地方加上自定义注解 第四步，当接口被调用时，就可以看到对应的执行日志。 21.说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。 ①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。 使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。 优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身。 示例代码： public interface Service void perform();public class ServiceImpl implements Service public void perform() System.out.println(Performing service...); public class ServiceInvocationHandler implements InvocationHandler private Object target; public ServiceInvocationHandler(Object target) this.target = target; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable System.out.println(Before method); Object result = method.invoke(target, args); System.out.println(After method); return result; public class Main public static void main(String[] args) Service service = new ServiceImpl(); Service proxy = (Service) Proxy.newProxyInstance( service.getClass().getClassLoader(), service.getClass().getInterfaces(), new ServiceInvocationHandler(service) ); proxy.perform(); ②、CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。 使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑。 优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。 public class Service public void perform() System.out.println(Performing service...); public class ServiceInterceptor implements MethodInterceptor @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable System.out.println(Before method); Object result = proxy.invokeSuper(obj, args); System.out.println(After method); return result; public class Main public static void main(String[] args) Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(Service.class); enhancer.setCallback(new ServiceInterceptor()); Service proxy = (Service) enhancer.create(); proxy.perform(); 选择 CGLIB 还是 JDK 动态代理？如果目标对象没有实现任何接口，则只能使用 CGLIB 代理。如果目标对象实现了接口，通常首选 JDK 动态代理。虽然 CGLIB 在代理类的生成过程中可能消耗更多资源，但在运行时具有较高的性能。对于性能敏感且代理对象创建频率不高的场景，可以考虑使用 CGLIB。JDK 动态代理是 Java 原生支持的，不需要额外引入库。而 CGLIB 需要将 CGLIB 库作为依赖加入项目中。 你会用 JDK 动态代理和 CGLIB 吗？假设我们有这样一个小场景，客服中转，解决用户问题： ①、JDK 动态代理实现： 第一步，创建接口public interface ISolver void solve(); 第二步，实现对应接口 public class Solver implements ISolver @Override public void solve() System.out.println(疯狂掉头发解决问题……); 第三步，动态代理工厂:ProxyFactory，直接用反射方式生成一个目标对象的代理，这里用了一个匿名内部类方式重写 InvocationHandler 方法。public class ProxyFactory // 维护一个目标对象 private Object target; public ProxyFactory(Object target) this.target = target; // 为目标对象生成代理对象 public Object getProxyInstance() return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable System.out.println(请问有什么可以帮到您？); // 调用目标对象方法 Object returnValue = method.invoke(target, args); System.out.println(问题已经解决啦！); return null; ); 第四步，客户端：Client，生成一个代理对象实例，通过代理对象调用目标对象方法public class Client public static void main(String[] args) //目标对象:程序员 ISolver developer = new Solver(); //代理：客服小姐姐 ISolver csProxy = (ISolver) new ProxyFactory(developer).getProxyInstance(); //目标方法：解决问题 csProxy.solve(); ②、CGLIB 动态代理实现： 第一步：定义目标类（Solver），目标类 Solver 定义了一个 solve 方法，模拟了解决问题的行为。目标类不需要实现任何接口，这与 JDK 动态代理的要求不同。 public class Solver public void solve() System.out.println(疯狂掉头发解决问题……); 第二步：动态代理工厂（ProxyFactory），ProxyFactory 类实现了 MethodInterceptor 接口，这是 CGLIB 提供的一个方法拦截接口，用于定义方法的拦截逻辑。public class ProxyFactory implements MethodInterceptor //维护一个目标对象 private Object target; public ProxyFactory(Object target) this.target = target; //为目标对象生成代理对象 public Object getProxyInstance() //工具类 Enhancer en = new Enhancer(); //设置父类 en.setSuperclass(target.getClass()); //设置回调函数 en.setCallback(this); //创建子类对象代理 return en.create(); @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable System.out.println(请问有什么可以帮到您？); // 执行目标对象的方法 Object returnValue = method.invoke(target, args); System.out.println(问题已经解决啦！); return null; ProxyFactory 接收一个 Object 类型的 target，即目标对象的实例。 使用 CGLIB 的 Enhancer 类来生成目标类的子类（代理对象）。通过 setSuperclass 设置代理对象的父类为目标对象的类，setCallback 设置方法拦截器为当前对象（this），最后调用 create 方法生成并返回代理对象。 重写 MethodInterceptor 接口的 intercept 方法以提供方法拦截逻辑。在目标方法执行前后添加自定义逻辑，然后通过 method.invoke 调用目标对象的方法。 第三步：客户端使用代理，首先创建目标对象（Solver 的实例），然后使用 ProxyFactory 创建该目标对象的代理。通过代理对象调用 solve 方法时，会先执行 intercept 方法中定义的逻辑，然后执行目标方法，最后再执行 intercept 方法中的后续逻辑。public class Client public static void main(String[] args) //目标对象:程序员 Solver developer = new Solver(); //代理：客服小姐姐 Solver csProxy = (Solver) new ProxyFactory(developer).getProxyInstance(); //目标方法：解决问题 csProxy.solve(); 22.说说 Spring AOP 和 AspectJ AOP 区别?Spring AOP 属于运行时增强，主要具有如下特点： 基于动态代理来实现，默认如果使用接口的，用 JDK 提供的动态代理实现，如果是方法则使用 CGLIB 实现 Spring AOP 需要依赖 IoC 容器来管理，并且只能作用于 Spring 容器，使用纯 Java 代码实现 在性能上，由于 Spring AOP 是基于动态代理来实现的，在容器启动时需要生成代理实例，在方法调用上也会增加栈的深度，使得 Spring AOP 的性能不如 AspectJ 的那么好。 Spring AOP 致力于解决企业级开发中最普遍的 AOP(方法织入)。 AspectJ 是一个易用的功能强大的 AOP 框架，属于编译时增强， 可以单独使用，也可以整合到其它框架中，是 AOP 编程的完全解决方案。AspectJ 需要用到单独的编译器 ajc。 AspectJ 属于静态织入，通过修改代码来实现，在实际运行之前就完成了织入，所以说它生成的类是没有额外运行时开销的，一般有如下几个织入的时机： 编译期织入（Compile-time weaving）：如类 A 使用 AspectJ 添加了一个属性，类 B 引用了它，这个场景就需要编译期的时候就进行织入，否则没法编译类 B。 编译后织入（Post-compile weaving）：也就是已经生成了 .class 文件，或已经打成 jar 包了，这种情况我们需要增强处理的话，就要用到编译后织入。 类加载后织入（Load-time weaving）：指的是在加载类的时候进行织入，要实现这个时期的织入，有几种常见的方法 整体对比如下： 40.说说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java 提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。 事务Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现。 23.Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。 介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager 来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚。 public class AccountService private TransactionTemplate transactionTemplate; public void setTransactionTemplate(TransactionTemplate transactionTemplate) this.transactionTemplate = transactionTemplate; public void transfer(final String out, final String in, final Double money) transactionTemplate.execute(new TransactionCallbackWithoutResult() @Override protected void doInTransactionWithoutResult(TransactionStatus status) // 转出 accountDao.outMoney(out, money); // 转入 accountDao.inMoney(in, money); ); 在上面的代码中，我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法内部实现事务的控制。 介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。 相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。 不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。 说说两者的区别？ 编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。 声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务。 24.说说 Spring 的事务隔离级别？事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 TransactionDefinition 接口中。 ①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 😁），MySQL 默认的是可重复读，Oracle 默认的读已提交。 ②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。 ③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。 ④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。 ⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。 25.Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上下文如何在方法调用链中传播。 Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 事务传播机制是使用 ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。 @Transactionalpublic void parentMethod() new Thread(() - childMethod()).start();public void childMethod() // 这里的操作将不会在 parentMethod 的事务范围内执行 Spring 默认的事务传播行为是 PROPAFATION_REQUIRED，即如果多个 ServiceX#methodX() 都工作在事务环境下，且程序中存在这样的调用链 Service1#method1()-Service2#method2()-Service3#method3()，那么这 3 个服务类的 3 个方法都通过 Spring 的事务传播机制工作在同一个事务中。 protected 和 private 加事务会生效吗在 Spring 中，只有通过 Spring 容器的 AOP 代理调用的公开方法（public method）上的@Transactional注解才会生效。 如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于 CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。 26.声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。 第一步，在 Bean 初始化阶段创建代理对象： Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。 在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属性切面，也就是 @Transactional 注解及其属性值。 然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。 第二步，在执行目标方法时进行事务增强操作： 当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为MethodInterceptor，事务增强对该接口的实现为TransactionInterceptor，类图如下： 事务拦截器TransactionInterceptor在invoke方法中，通过调用父类TransactionAspectSupport的invokeWithinTransaction方法进行事务处理，包括开启事务、事务提交、异常回滚等。 27.声明式事务在哪些情况下会失效？ 1、@Transactional 应用在非 public 修饰的方法上 如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。 是因为在 Spring AOP 代理时，TransactionInterceptor （事务拦截器）在目标方法执行前后进行拦截，DynamicAdvisedInterceptor（CglibAopProxy 的内部类）的 intercept 方法或 JdkDynamicAopProxy 的 invoke 方法会间接调用 AbstractFallbackTransactionAttributeSource 的 computeTransactionAttribute方法，获取 Transactional 注解的事务配置信息。 protected TransactionAttribute computeTransactionAttribute(Method method, Class? targetClass) // Dont allow no-public methods as required. if (allowPublicMethodsOnly() !Modifier.isPublic(method.getModifiers())) return null; 此方法会检查目标方法的修饰符是否为 public，不是 public 则不会获取 @Transactional 的属性配置信息。 2、@Transactional 注解属性 propagation 设置错误 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务方式执行；错误使用场景：在业务逻辑必须运行在事务环境下以确保数据一致性的情况下使用 SUPPORTS。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：总是以非事务方式执行，如果当前存在事务，则挂起该事务。错误使用场景：在需要事务支持的操作中使用 NOT_SUPPORTED。 TransactionDefinition.PROPAGATION_NEVER：总是以非事务方式执行，如果当前存在事务，则抛出异常。错误使用场景：在应该在事务环境下执行的操作中使用 NEVER。 3、@Transactional 注解属性 rollbackFor 设置错误 rollbackFor 用来指定能够触发事务回滚的异常类型。Spring 默认抛出未检查 unchecked 异常（继承自 RuntimeException 的异常）或者 Error 才回滚事务，其他异常不会触发回滚事务。 // 希望自定义的异常可以进行回滚@Transactional(propagation= Propagation.REQUIRED,rollbackFor= MyException.class) 若在目标方法中抛出的异常是 rollbackFor 指定的异常的子类，事务同样会回滚。 4、同一个类中方法调用，导致@Transactional 失效 开发中避免不了会对同一个类里面的方法调用，比如有一个类 Test，它的一个方法 A，A 调用本类的方法 B（不论方法 B 是用 public 还是 private 修饰），但方法 A 没有声明注解事务，而 B 方法有。 则外部调用方法 A 之后，方法 B 的事务是不会起作用的。这也是经常犯错误的一个地方。 那为啥会出现这种情况呢？其实还是由 Spring AOP 代理造成的，因为只有事务方法被当前类以外的代码调用时，才会由 Spring 生成的代理对象来管理。 //@Transactional@GetMapping(/test)private Integer A() throws Exception CityInfoDict cityInfoDict = new CityInfoDict(); cityInfoDict.setCityName(2); /** * B 插入字段为 3的数据 */ this.insertB(); /** * A 插入字段为 2的数据 */ int insert = cityInfoDictMapper.insert(cityInfoDict); return insert;@Transactional()public Integer insertB() throws Exception CityInfoDict cityInfoDict = new CityInfoDict(); cityInfoDict.setCityName(3); cityInfoDict.setParentCityId(3); return cityInfoDictMapper.insert(cityInfoDict); 这种情况是最常见的一种@Transactional 注解失效场景。 @Transactionalprivate Integer A() throws Exception int insert = 0; try CityInfoDict cityInfoDict = new CityInfoDict(); cityInfoDict.setCityName(2); cityInfoDict.setParentCityId(2); /** * A 插入字段为 2的数据 */ insert = cityInfoDictMapper.insert(cityInfoDict); /** * B 插入字段为 3的数据 */ b.insertB(); catch (Exception e) e.printStackTrace(); 如果 B 方法内部抛了异常，而 A 方法此时 try catch 了 B 方法的异常，那这个事务就不能正常回滚了，会抛出异常： org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only MVC28.Spring MVC 的核心组件？ DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。 Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。 HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping 将不同的请求映射到不同的 Handler。 HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。 HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和 HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。 HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。 ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。 ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。 29.Spring MVC 的工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。 DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。 然后 DispatcherServlet 通过 ViewResolver 解析视图。 最后，DispatcherServlet 渲染视图并将响应返回给客户端。 ①、发起请求：客户端通过 HTTP 协议向服务器发起请求。 ②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。 ③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个 Controller 应该处理这个请求。通常会根据请求的 URL 来确定。 ④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用 HandlerAdapter 来调用 Controller 的处理方法。 ⑤、执行处理器：Controller 处理请求，处理完后返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。 ⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用 ViewResolver 来解析视图名称，找到具体的视图页面。 ⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。 ⑧、响应结果：DispatcherServlet 将视图结果返回给客户端。 Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。 在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。 这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。 Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller注解的类，还可以是实现了特定接口的其他类（如 HttpRequestHandler 或 SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。 HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。 30.SpringMVC Restful 风格的接口的流程是什么样的呢？PS:这是一道全新的八股，毕竟 ModelAndView 这种方式应该没人用了吧？现在都是前后端分离接口，八股也该更新换代了。我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody @GetMapping(/user)@ResponseBodypublic User user() return new User(1,张三); 加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同： 1. 客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServlet 2. DispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理 3. DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪个 Controller 4. Controller 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理 5. HandlerAdapter 执行完对 Controller 的请求，会调用 HandlerMethodReturnValueHandler 去处理返回值，主要的过程： 5.1. 调用 RequestResponseBodyMethodProcessor，创建 ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例 5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入 ServletServerHttpResponse 的 OutputStream 输出流中 5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化 6. 执行完请求后，返回的 ModealAndView 为 null，ServletServerHttpResponse 里也已经写入了响应，所以不用关心 View 的处理 SpringBoot31.介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架。 以前的 Spring 开发需要配置大量的 xml 文件，并且需要引入大量的第三方 jar 包，还需要手动放到 classpath 下。现在只需要引入一个 Starter，或者一个注解，就可以轻松搞定。 Spring Boot 的优点非常多，比如说： Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。 Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和 Spring MVC。 Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控。 Spring Boot常用注解有哪些？ @SpringBootApplication：Spring Boot 应用的入口，用在启动类上。 还有一些 Spring 框架本身的注解，比如 @Component、**@RestController、@Service、@ConfigurationProperties、@Transactional** 等。 32.SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。 在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。 Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。 main 类启动的时候，Spring Boot 会通过底层的AutoConfigurationImportSelector 类加载自动装配类。 @AutoConfigurationPackage //将main同级的包下的所有组件注册到容器中@Import(AutoConfigurationImportSelector.class) //加载自动装配类 xxxAutoconfigurationpublic @interface EnableAutoConfiguration String ENABLED_OVERRIDE_PROPERTY = spring.boot.enableautoconfiguration; Class?[] exclude() default ; String[] excludeName() default ; AutoConfigurationImportSelector实现了ImportSelector接口，该接口的作用是收集需要导入的配置类，配合 @Import() 将相应的类导入到 Spring 容器中。 获取注入类的方法是 selectImports()，它实际调用的是getAutoConfigurationEntry()，这个方法是获取自动装配类的关键。 protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) // 检查自动配置是否启用。如果@ConditionalOnClass等条件注解使得自动配置不适用于当前环境，则返回一个空的配置条目。 if (!isEnabled(annotationMetadata)) return EMPTY_ENTRY; // 获取启动类上的@EnableAutoConfiguration注解的属性，这可能包括对特定自动配置类的排除。 AnnotationAttributes attributes = getAttributes(annotationMetadata); // 从spring.factories中获取所有候选的自动配置类。这是通过加载META-INF/spring.factories文件中对应的条目来实现的。 ListString configurations = getCandidateConfigurations(annotationMetadata, attributes); // 移除配置列表中的重复项，确保每个自动配置类只被考虑一次。 configurations = removeDuplicates(configurations); // 根据注解属性解析出需要排除的自动配置类。 SetString exclusions = getExclusions(annotationMetadata, attributes); // 检查排除的类是否存在于候选配置中，如果存在，则抛出异常。 checkExcludedClasses(configurations, exclusions); // 从候选配置中移除排除的类。 configurations.removeAll(exclusions); // 应用过滤器进一步筛选自动配置类。过滤器可能基于条件注解如@ConditionalOnBean等来排除特定的配置类。 configurations = getConfigurationClassFilter().filter(configurations); // 触发自动配置导入事件，允许监听器对自动配置过程进行干预。 fireAutoConfigurationImportEvents(configurations, exclusions); // 创建并返回一个包含最终确定的自动配置类和排除的配置类的AutoConfigurationEntry对象。 return new AutoConfigurationEntry(configurations, exclusions); 总结：Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。 33.如何自定义一个 SpringBoot Starter?创建一个自定义的 Spring Boot Starter，需要这几步： 第一步，创建一个新的 Maven 项目，例如命名为 my-spring-boot-starter。在 pom.xml 文件中添加必要的依赖和配置： properties spring.boot.version2.3.1.RELEASE/spring.boot.version/propertiesdependencies dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-autoconfigure/artifactId version$spring.boot.version/version /dependency dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter/artifactId version$spring.boot.version/version /dependency/dependencies 第二步，在 srcmainjava 下创建一个自动配置类，比如 MyServiceAutoConfiguration.java：（通常是 autoconfigure 包下）。 @Configuration@EnableConfigurationProperties(MyStarterProperties.class)public class MyServiceAutoConfiguration @Bean @ConditionalOnMissingBean public MyService myService(MyStarterProperties properties) return new MyService(properties.getMessage()); 第三步，创建一个配置属性类 MyStarterProperties.java： @ConfigurationProperties(prefix = mystarter)public class MyStarterProperties private String message = 二哥的 Java 进阶之路不错啊!; public String getMessage() return message; public void setMessage(String message) this.message = message; 第四步，创建一个简单的服务类 MyService.java： public class MyService private final String message; public MyService(String message) this.message = message; public String getMessage() return message; 第五步，配置 spring.factories，在 srcmainresourcesMETA-INF 目录下创建 spring.factories 文件，并添加：org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.itwanger.mystarter.autoconfigure.MyServiceAutoConfiguration 第六步，使用 Maven 打包这个项目：mvn clean install 第七步，在其他的 Spring Boot 项目中，通过 Maven 来添加这个自定义的 Starter 依赖，并通过 application.properties 配置欢迎消息：mystarter.message=javabetter.cn 然后就可以在 Spring Boot 项目中注入 MyStarterProperties 来使用它。 -启动项目，然后在浏览器中输入 localhost:8081hello，就可以看到欢迎消息了。 Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。 起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat 和 Jackson 等依赖。 自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。 比如有了 spring-boot-starter-web，我们开发者就不需要再手动配置 Tomcat、Spring MVC 等，Spring Boot 会自动帮我们完成这些工作。 34.Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责： 第一步，创建 SpringApplication 实例，负责应用的启动和初始化； 第二步，从 application.yml 中加载配置文件和环境变量； 第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入； 第四步，启动内嵌的 Web 容器。 第五步，发布启动完成事件 ApplicationReadyEvent，并调用 ApplicationRunner 的 run 方法完成启动后的逻辑。 关键的代码逻辑如下： public ConfigurableApplicationContext run(String... args) // 1. 创建启动时的监听器并触发启动事件 SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); // 2. 准备运行环境 ConfigurableEnvironment environment = prepareEnvironment(listeners); configureIgnoreBeanInfo(environment); // 3. 创建上下文 ConfigurableApplicationContext context = createApplicationContext(); try // 4. 准备上下文 prepareContext(context, environment, listeners, args); // 5. 刷新上下文，完成 Bean 初始化和装配 refreshContext(context); // 6. 调用运行器 afterRefresh(context, args); // 7. 触发启动完成事件 listeners.started(context); catch (Exception ex) handleRunFailure(context, ex, listeners); return context; 以TecHub实战项目为例。在启动类 QuickForumApplication 中，main 方法会调用 SpringApplication.run() 启动项目。 该方法负责 Spring 应用的上下文环境（ApplicationContext）准备，包括： 扫描配置文件，添加依赖项 初始化和加载 Bean 定义 启动内嵌的 Web 容器等 发布启动完成事件 了解@SpringBootApplication 注解吗？@SpringBootApplication是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解： @SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。 @EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。 @ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean。@SpringBootApplicationpublic class Application public static void main(String[] args) SpringApplication.run(Application.class, args); 为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架内部的一系列处理流程。 当运行一个 Spring Boot 程序时，通常会调用主类中的main方法，这个方法会执行SpringApplication.run()，比如： @SpringBootApplicationpublic class MyApplication public static void main(String[] args) SpringApplication.run(MyApplication.class, args); SpringApplication.run(Class? primarySource, String... args)方法接收两个参数：第一个是主应用类（即包含main方法的类），第二个是命令行参数。primarySource参数提供了一个起点，Spring Boot 通过它来加载应用上下文。 Spring Boot 利用 Java 反射机制来读取传递给run方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication。 Spring Boot 默认的包扫描路径是什么？Spring Boot 的默认包扫描路径是以启动类 @SpringBootApplication 注解所在的包为根目录的，即默认情况下，Spring Boot 会扫描启动类所在包及其子包下的所有组件。 比如说在技术派实战项目中，启动类QuickForumApplication所在的包是com.github.paicoding.forum.web，那么 Spring Boot 默认会扫描com.github.paicoding.forum.web包及其子包下的所有组件。 @SpringBootApplication 是一个组合注解，它里面的@ComponentScan注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。 @ComponentScan(excludeFilters = @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) )public @interface SpringBootApplication 比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。 如果需要自定义包扫描路径，可以在@SpringBootApplication注解上添加@ComponentScan注解，指定要扫描的包路径。 @SpringBootApplication@ComponentScan(basePackages = com.github.paicoding.forum)public class QuickForumApplication public static void main(String[] args) SpringApplication.run(QuickForumApplication.class, args); 这种方式会覆盖默认的包扫描路径，只扫描com.github.paicoding.forum包及其子包下的所有组件。 36.SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种 Model-View-Controller（模型-视图-控制器）的开发模式。 Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个 SpringMVC 的 Web 项目。 38.Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。 特性: Spring Cloud35.对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。 什么是微服务？ 2014 年 Martin Fowler 提出的一种新的架构形式。微服务架构是一种架构模式，提倡将单一应用程序划分成一组小的服务，服务之间相互协调，互相配合，为用户提供最终价值。每个服务运行在其独立的进程中，服务与服务之间采用轻量级的通信机制(如 HTTP 或 Dubbo)互相协作，每个服务都围绕着具体的业务进行构建，并且能够被独立的部署到生产环境中，另外，应尽量避免统一的，集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具(如 Maven)对其进行构建。 微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。 微服务架构主要要解决哪些问题？ 服务很多，客户端怎么访问，如何提供对外网关? 这么多服务，服务之间如何通信? HTTP 还是 RPC? 这么多服务，如何治理? 服务的注册和发现。 服务挂了怎么办？熔断机制。 有哪些主流微服务框架？ Spring Cloud Netflix Spring Cloud Alibaba SpringBoot + Dubbo + ZooKeeper SpringCloud 有哪些核心组件？ 补充37.SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务。 ①、@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml： @Scheduled(cron = 0 15 5 * * ?)public void autoRefreshCache() log.info(开始刷新sitemap.xml的url地址，避免出现数据不一致问题!); refreshSitemap(); log.info(刷新完成！); @Scheduled 注解支持多种调度选项，如 fixedRate、fixedDelay 和 cron 表达式。 ②、@EnableScheduling：用于开启定时任务的支持。 用SpringTask资源占用太高，有什么其他的方式解决？（补充） 第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。 ①、在订单创建时，将订单超时检查任务放入消息队列，并设置延迟时间（即订单超时时间）。 @Servicepublic class OrderService @Autowired private RabbitTemplate rabbitTemplate; public void createOrder(Order order) // 创建订单逻辑 // ... // 发送延迟消息 rabbitTemplate.convertAndSend(orderExchange, orderTimeoutQueue, order, message - message.getMessageProperties().setExpiration(600000); // 设置延迟时间（10分钟） return message; ); ②、使用消费者从队列中消费消息，当消费到超时任务时，执行订单超时处理逻辑。 @Servicepublic class OrderTimeoutConsumer @RabbitListener(queues = orderTimeoutQueue) public void handleOrderTimeout(Order order) // 处理订单超时逻辑 // ... 第二，使用数据库调度器（如 Quartz）。 ①、创建一个 Quartz 任务类，处理订单超时逻辑。 public class OrderTimeoutJob implements Job @Override public void execute(JobExecutionContext context) throws JobExecutionException // 获取订单信息 Order order = (Order) context.getJobDetail().getJobDataMap().get(order); // 处理订单超时逻辑 // ... ②、在订单创建时，调度一个 Quartz 任务，设置任务的触发时间为订单超时时间。 @Servicepublic class OrderService @Autowired private Scheduler scheduler; public void createOrder(Order order) // 创建订单逻辑 // ... // 调度 Quartz 任务 JobDetail jobDetail = JobBuilder.newJob(OrderTimeoutJob.class) .usingJobData(order, order) .build(); Trigger trigger = TriggerBuilder.newTrigger() .startAt(new Date(System.currentTimeMillis() + 600000)) // 设置触发时间（10分钟后） .build(); try scheduler.scheduleJob(jobDetail, trigger); catch (SchedulerException e) e.printStackTrace(); 41.Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。 它通过注解（如 @Cacheable、@CachePut、@CacheEvict）来实现缓存管理，极大简化了代码实现。 @Cacheable：缓存方法的返回值。 @CachePut：用于更新缓存，每次调用方法都会将结果重新写入缓存。 @CacheEvict：用于删除缓存。使用示例： Spring Cache 和 Redis 有什么区别？ Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。 Redis 是一个分布式的缓存中间件，支持多种数据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。 Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。 Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。 在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。 有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。 Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。 说说Spring Cache 的底层原理？Spring Cache 是基于 AOP 和缓存抽象层实现的。它通过 AOP 拦截被 @Cacheable、@CachePut 和 @CacheEvict 注解的方法，在方法调用前后自动执行缓存逻辑。 其提供的 CacheManager 和 Cache 等接口，不依赖具体的缓存实现，因此可以灵活地集成 Redis、Caffeine 等多种缓存。 ConcurrentMapCacheManager：基于 Java ConcurrentMap 的本地缓存实现。 RedisCacheManager：基于 Redis 的分布式缓存实现。 CaffeineCacheManager：基于 Caffeine 的缓存实现。","tags":["基础","spring"],"categories":["基础笔记"]},{"title":"2025.6.12学习日记","path":"/2025/06/12/学习日记25年6月/2025.6.12学习笔记/","content":"今日学习内容3DGS:今日工作总结今天对开源数据集train,truck,drjohnson进行训练和测试.结果主要用于之后跑不同改进版本3DGS的参考。结果在飞书文档中在训练drjohnson数据集过程中,训练速度明显变慢.分析原因：可能是drjohnson数据集中，图像分辨率相比train和truck数据集更高,点云数量更大，并且在train.py训练过程中，高斯球克隆更加激进，导致显存交换更加频繁，IO时间变长或者缓存碎片化，导致训练时间成倍增加。 明日工作计划继续将drjohnson数据集与playroom数据集进行训练和测试作为参考.部署EfficientGS方法进行测试,该方法主要针对室外大场景,对原版3DGS进行性能优化. 力扣每日一题:循环数组简单题. 上午看mysql看到24题. 下午看项目看了一篇Servlet的文章. Spring学习写了四条,开个头. 晚上学代码随想录继续二叉树. 做了一道数组模拟环形链表找入口的题,挺有意思的. https://leetcode.cn/problems/find-the-duplicate-number 生活记录晚上健身今天主要练的背.","tags":["3DGS","spring","日记","leetcode","mysql"],"categories":["学习日记","2025-06"]},{"title":"3DGS-Ubuntu环境.md","path":"/2025/06/11/3DGS/3DGS-Ubuntu环境/","content":"环境:Ubuntu20.04和Win11双系统CUDA11.8Anacoda3 安装clash for linux 快捷命令 clashtun on off anaconda:source conda activate 虚拟环境名","tags":["3DGS","公司"],"categories":["3DGS"]},{"title":"3DGS学习笔记","path":"/2025/06/11/3DGS/3DGS学习笔记/","content":"环境配置https://www.youtube.com/watch?v=UXtuigy_wYc youtube复现视频https://github.com/graphdeco-inria/gaussian-splatting 3DGS的github源码地址https://dl.acm.org/doi/10.1145/3592433 论文地址https://arxiv.org/abs/2308.04079 下载依赖和环境视频老哥的github地址https://github.com/jonstephens85/gaussian-splatting-Windows 下载git(跳过)测试是否下载git –version 下载anaconda(跳过)下载CUDA :nvcc –version测试版本nvidia-smi确定电脑最高支持的CUDA版本 ,我的最高支持12.8 准备下载CUDA 11.8版本安装vs2019官方下载地址：https://visualstudio.microsoft.com/zh-hans/vs/older-downloads/ 下载colmap准备编译打开anaconda prompt D: 切换D盘D:\\user\\desktop\\workplace\\3DGS\\userdesktopworkplace3DGS 要将 Anaconda 创建的虚拟环境设置在 D 盘，可以按照以下步骤操作： 修改 Anaconda 配置首先，需要修改 Anaconda 的配置，使其将环境创建在指定路径。在 Anaconda Prompt 中执行以下命令：conda config –add envs_dirs D:\\Anaconda\\envs（如果需要，将 D:\\Anaconda\\envs 替换为你想要的路径） 确保路径存在确保你指定的路径已经存在。如果不存在，请手动创建该文件夹。 创建虚拟环境现在你就可以创建虚拟环境，新的环境将被创建在 D 盘的指定路径下：conda create –name env_name python3.x 检查环境位置可以使用以下命令查看虚拟环境的位置：conda info –envs经过以上步骤后，你的虚拟环境将会在 D 盘创建。conda create -n gaussian_splatting python3.7conda activate gaussian_splattingconda install -c conda-forge vs2019_win-64pip install torch1.13.1+cu117 torchvision0.14.1+cu117 torchaudio0.13.1 –extra-index-url https://download.pytorch.org/whl/cu117pip install submodulesdiff-gaussian-rasterizationpip install submodulessimple-knnpip install submodulesfused-ssim 训练3dgs的运行训练下述所有命令都是在终端里运行的，运行时保持是从gaussian-splatting目录下开始输入的。省流版本:conda activate gaussian_splattingd:cd D:\\sys\\Desktop\\Workplace\\3DGS新的路径训练:python train.py -s data_train -m data_train/output (参数为输出地址)python render.py -m data_train/output把train文件夹的method复制到test文件夹python metrics.py -m data_train/output 1.视频截取帧这里可以用自己手机拍摄的一段视频，一两分钟即可，可以参考一下作者的训练时间，作者用自己的笔记本（4060 8G），大概训练了两个小时左右，跑完了所有的迭代。 在gaussian-splatting目录下新建一个data文件夹，将你拍摄的视频移动到该data文件夹下，并将你的视频改名为input，后缀.mp4不用改。然后在data文件夹里再建一个与视频同名的文件夹，名字也是input。然后就可以输入命令啦（终端里从gaussian-splattingdata目录下开始输入） cd data ffmpeg -i input.mp4 -vf setpts=0.2*PTS input\\input_%04d.jpg #推荐运行这个指令 ffmpeg -i input.mp4 -vf setpts=0.2*PTS,fps=1 input\\input_%04d.jpg #如果需要调整抽帧频率可以参考这个指令。选择一个运行即可 这里简单的说一下各个参数的含义。setpts0.2*PTS 将视频播放速度加快到原来的 5 倍。这意味着原视频的每秒帧数增加到 5 倍。如果原始视频是 30 FPS，加速后的视频将以 150 FPS 播放。尽管视频播放速度加快了，ffmpeg fps1 会以每秒一帧的频率提取图片。 这样就可以把你的视频截取为帧并保存在input文件夹里，在input文件夹里应该可以看到许多张照片。 2.产生点云在终端gaussian-splatting目录下输入cd ..python convert.py -s data 这个就是利用安装的colmap产生点云，会花费一些时间，等待完成即可。 3.查看点云终端里输入 colmap 调出来colmap后，选择file-import model然后选择gaussian-splattingdatasparse0文件夹，选择确定，即可打开生成的点云，遇到弹窗×掉即可。可以看到生成的点云还有相机路径。 4.开始训练同样，在终端里gaussian-splatting目录下，输入 python train.py -s data -m dataoutput (参数为输出地址)python train.py -s data -m dataimages 成功开始会出现如下图所示然后耐心等待训练完成以后即可。 5.查看结果同样，在终端里gaussian-splatting目录下，输入 .\\viewers\\bin\\SIBR_gaussianViewer_app -m dataoutput 即可打开viewer窗口，可以把你的场景拖大，下面是一些快捷按键 w uio asd jkl 就是可以控制视角的变化，大家自己按一下就知道是干啥的了，这里就不一一列举对应的功能了（作者已经累了），注意切换输入法为英文输入。 至此，就全部结束啦，完结撒花！ 1 . convert.py将input数据集转换成为点云通过sfm算法将输入的图片集转换成点云,这种方式的具体流程如下: 特征提取：从输入的图像集中，对每一张图像提取特征点及其描述子，常用的特征提取算法有 SIFT、SURF、ORB 等。 全局特征匹配：在所有图像的特征点之间进行匹配，找出不同图像中表示同一物理点的特征点对。由于是全局匹配，可能会处理大量的特征点对，计算量较大。 相机位姿估计：根据匹配的特征点对，使用诸如对极几何、PnP 等算法来估计相机的相对位姿。 三角测量：利用已知的相机位姿和匹配的特征点，通过三角测量计算出三维点的坐标，从而生成点云。 全局优化：使用束调整（Bundle Adjustment）等方法对相机位姿和三维点的坐标进行全局优化，提高点云的精度。SfM 侧重于对静态图像集进行全局处理，通过全局优化来生成高精度的点云,SfM相比ORB方法 更侧重于离线的高精度三维重建. 渲染辐射场的几种方法建立了最近的数据集。革命性地合成了用多张照片或视频捕获的场景。然而，实现高视觉质量仍然需要训练和渲染成本高昂的神经网络，而最近更快的方法不可避免地要牺牲速度来换取质量。对于无界和完整的场景(而不是孤立的对象)和1080p分辨率的渲染，目前没有一种方法可以实现实时显示速率。我们介绍了三个关键要素，使我们能够在保持有竞争力的训练时间的同时实现最先进的视觉质量，并且重要的是允许在1080p分辨率下实现高质量的实时(≥30 fps)新视图合成。首先，从相机校准过程中产生的稀疏点开始，我们用3D高斯分布表示场景，该分布保留了用于场景优化的连续体辐射场的理想属性，同时避免了在空白空间中不必要的计算;其次，我们对3D高斯分布进行交错优化密度控制，特别是优化各向异性协方差以实现场景的准确表示;第三，我们开发了一种支持各向异性喷溅的快速可视性感知渲染算法，既加速了训练，又允许实时渲染。我们展示了最先进的视觉质量和实时性 3dgs流程3DGS流程：（1）通过colmap等工具从多视角图像获取SfM点云（SfM是一种三维重建算法，通过两个或多个场景图片恢复相机位姿，并重建三维坐标点），对 SfM 点云进行了初始化。 （2）点云中的每一个点代表着一个三维的高斯分布，除了点的位置（均值）外，还有协方差、不透明度、颜色（球谐函数）–3D 高斯球云。 （3）将这些椭球体沿着特定角度投影到对应位姿所在平面（Splatting）。一个椭球体投影到平面会得到一个椭圆；然后通过计算待求解像素和椭圆中心的距离，我们得到不透明度（离的近，说明越不透明）；每个椭球又代表各自的颜色，进行alpha composting来合成颜色，然后快速的对所有像素做“可微光栅化”，渲染得到图像。 （4）得到渲染图像Image后，再与gt图像比较，得到损失loss，并沿蓝色箭头反向传播，随机梯度下降；向下送入自适应密度控制中（增密或修剪），更新点云优化。 代码运行流程1.Runningpython train.py -s 示例： python train.py -s data360_extra_scenestreehill 运行完在output下得到相应的文件夹outputtreehill， 将得到的结果路径添加至SIBR_viewer.py（model_path r’D:\\gaussian-splatting\\output\\treehill’），运行即可获得可视化。 densify_and_prune操作会改变高斯数量。结合在一起，允许模型根据当前两个的训练状态动态地调整高斯的数量，从而实现更好的表示能力和计算效率。因此，在这个过程中，高斯的数量会变化，所以需要在执行后打印出当前的高斯数量。在训练的时候添加高斯数量打印： print(f”Iteration {iteration}: Number of Gaussians after densification and pruning: {gaussians.get_xyz.shape[0]}”) 2. Evaluationpython train.py -s –eval # Train with traintest splitpython render.py -m # Generate renderingspython metrics.py -m # Compute error metrics on renderings 训练模型-渲染图像-计算指标 示例：Evaluation运行，输入命令行(python train.py -s + 数据集的路径) python train.py -s data360_extra_scenestreehill –evalpython render.py -m outputtreehillpython metrics.py -m outputtreehill 2. Processing your own Scenes按照README.me进行，选择的mill19building-pixsfm进行简单测试 图像目录结构： location|---input |---image 0 |---image 1 |---...python convert.py -s location [--resize] #If not resizing, ImageMagick is not needed示例：data/mill19/building-pixsfm|---input |---image 0 |---image 1 |---... 然后运行： python convert.py -s datamill19building-pixsfm 两句代码优化3DGS显存问题做实验的时候发现，对于同一个场景，图片越多显存占用越大。但是训练的时候是一张图片做一次梯度下降，说明batch_size一直是1。那数据集变大，batch_size不变的情况下，显存占用却变大了，这明显很不合理。当训练的图片比较多的时候，显存就不够了。 图片越多，显存占用越大，这就暗示了3DGS源码里一定是训练前预先将所有的图片都加载显存中的。事实也确实如此，在训练前的预处理中，为每张图片都构造了一个对应的Camera对象，Camera的初始化函数中就将图片加载到了显存中，如下。 源码scenecameras.py中39行，修改前self.original_image image.clamp(0.0, 1.0).to(self.data_device) 修改后self.original_image image.clamp(0.0, 1.0)这优化很简单，不要提前载入显存就好了，直接把“.to(self.data_device)”删掉。训练的时候，具体训练哪张图片，载入哪张图片就好了。这个在3DGS源码中，已经做了，如下。经过所述的代码修改后，确实显存减少了，但是训练速度变慢了。因为相比修改前，多出来的耗时在每次训练迭代中都有一次图片加载到显存中的操作。在cuda中将数据从内存载入显存是可以异步进行的，pytorch也提供了接口，所以对代码做出以下修改。 源码train.py中90行，修改前gt_image viewpoint_cam.original_image.cuda() 修改后，需移动到render_pkg render(…)之前gt_image viewpoint_cam.original_image.cuda(non_blockingTrue)不过仅仅只是将载入方式设置为non_blocking还不够，因为可能在要使用gt_image之前其还没有完成载入显存的操作，此时也会发生阻塞。所以需要将这句代码往前放，可以放到“render_pkg render(viewpoint_cam, gaussians, pipe, bg)”这句之前。这样就几乎不会影响训练速度。","tags":["3DGS","公司"],"categories":["3DGS"]},{"title":"nvm更改node版本","path":"/2025/06/11/杂项笔记/nvm更改node版本/","content":"安装brew首先安装 Brew。这个就不详细说了 没有的话自己去搜一下 第一步：进行nvm 安装操作brew install nvm 执行后：== Pouring nvm-0.39.1_1.all.bottle.tar.gz== CaveatsPlease note that upstream has asked us to make explicit managingnvm via Homebrew is unsupported by them and you should check anyproblems against the standard nvm install method prior to reporting.You should create NVMs working directory if it doesnt exist: // 这里就是提示你创建一个 nvm文件 mkdir ~/.nvmAdd the following to ~/.zshrc or your desired shellconfiguration file: // 这里就是想让你进行一些配置 export NVM_DIR=$HOME/.nvm [ -s /opt/homebrew/opt/nvm/nvm.sh ] \\. /opt/homebrew/opt/nvm/nvm.sh # This loads nvm [ -s /opt/homebrew/opt/nvm/etc/bash_completion.d/nvm ] \\. /opt/homebrew/opt/nvm/etc/bash_completion.d/nvm # This loads nvm bash_completionYou can set $NVM_DIR to any location, but leaving it unchanged from/opt/homebrew/opt/nvm will destroy any nvm-installed Node installationsupon upgrade/reinstall.Type `nvm help` for further information.== Summary🍺 /opt/homebrew/Cellar/nvm/0.39.1_1: 9 files, 184KB 执行 nvm –versionnvm --version //出现问题。去进行配置 zsh: command not found: nvm 第二步：nvm配置1.vim ~/.bash_profile点击 i 进行插入操作 插入下面配置 export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh 插入完成后 点击Esc 然后 使用 :wq. 保存并退出 执行：source ~/.bash_profile 如果出问题先不管接着往下走 2.vim ~/.zshrc点击 i 进行插入操作 插入下面配置 export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh 插入完成后 点击Esc 然后 使用 :wq. 保存并退出 执行：source ~/.zshrc 3.vim ~/.profile点击 i 进行插入操作 插入下面配置 export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh 插入完成后 点击Esc 然后 使用 :wq. 保存并退出 执行：source ~/.profile 最后开始进行测试执行：nvm --version 显示版本号就说明配置成功：0.39.1 通过nvm 进行node 版本控制 版本号根据自己的需求定义 nvm install 12.6.0 查看版本：node -v nvm 常用命令：以下用8.9.2版本为例nvm ls ：打印出所有的版本 install stable：安装最稳定的版本nvm install v8.9.2 ： 安装node的8.9.2的版本（删除用uninstall）nvm current ：当前使用的node版本nvm use v8.9.2 ：将node改为8.9.2版本nvm alias default 0.12.7：设置默认 node 版本为 0.12.7nvm alias default ：设置系统默认的node版本nvm alias ：给不同的版本号添加别名nvm unalias ： 删除已定义的别名nvm reinstall-packages ：在当前版本node环境下，重新全局安装指定版本号的npm包npm install -g mz-fis：安装 mz-fis 模块至全局目录，安装的路径：/Users/你的用户名/.nvm/versions/node/v0.12.7/lib/mz-fisnvm use 4：切换至 4.2.2 版本（支持模糊查询）npm install -g react-native-cli：安装 react-native-cli 模块至全局目录，安装的路径：/Users/你的用户名/.nvm/versions/node/v4.2.2/lib/react-native-cli node npm 版本对照链接: 版本对照 末尾 npm 降级执行：sudo npm install npm@6.9.0 -g 问题是不可控的 如果未能解决你的问题 就祝你顺利","tags":["nvm","node"],"categories":["杂项笔记"]},{"title":"2025.6.11学习日记","path":"/2025/06/11/学习日记25年6月/2025.6.11学习笔记/","content":"今日学习内容3DGS:今日工作总结:​一. 代码运行与结构梳理​​:学习了原版3DGS的项目结构和训练流程: train.py : 训练脚本,主要负责对高斯球参数进行训练. render.py : 渲染脚本,用于将训练得到的高斯球参数渲染成图像. metrics.py : 评估脚本,对比真值图像和渲染图像,用于评估渲染结果的质量. 二.训练结果:火车数据集301张图像. train.py :ITER 7000:L1损失 : 0.06603520661592484PSNR :20.096060180664065训练时间 : 13分30秒ITER 30000:L1损失 : 0.038734884932637215PSNR : 24.450721740722656s训练时间:77分25秒 render.py : metrics.py :SSIM: 0.87444342PSNR: 25.8431702LPIPS: 0.1703709 ​​明日工作计划:​​今天对火车的数据集进行了实验,明天准备开始对其他数据集进行实验,得出一组原版3DGS的训练结果作为参考,方便对后续3DGS改进算法进行实验对比. 下午回学校开会来着晚上继续做力扣二叉树篇明日计划3dgs明天尽量让时间一直在跑代码,然后我可以学java. mysql学习力扣项目文档生活记录1. 早上足球训练早上7点,颠球短传训练.","tags":["3DGS","日记"],"categories":["学习日记","2025-06"]},{"title":"Java集合框架笔记","path":"/2025/06/10/基础笔记/Java集合框架笔记/","content":"框架图先贴一个Java集合框架图 可以看出,集合主要分成两大部分: Collection:主要由 List、Set、Queue 组成，List 代表有序、可复的集合，典型代表就是封装了动态数组的 ArrayList 和封装了链表的 LinkedList；Set 代表⽆序、不可复的集合，典型代表就是 HashSet 和 TreeSet；Queue 代表队列，典型代表就是双端队列ArrayDeque，以及优先级队列 PriorityQueue。 Map:代表键值对的集合，典型代表就是 HashMap。 CollectionListList 的特点是存取有序，可以存放复的元素，可以⽤下标对元素进⾏操作。 ArrayListArrayList的增删改查: // 创建⼀个集合ArrayListString list = new ArrayListString();// 添加元素list.add(王⼆);list.add(沉默);list.add(陈清扬);// 遍历集合 for 循环for (int i = 0; i list.size(); i++) String s = list.get(i); System.out.println(s);// 遍历集合 for eachfor (String s : list) stem.out.println(s);// 删除元素list.remove(1);// 遍历集合for (String s : list) System.out.println(s);// 修改元素list.set(1,王⼆狗);// 遍历集合for (String s : list) System.out.println(s);","tags":["基础","Java","集合"],"categories":["基础笔记"]},{"title":"Mysql学习笔记","path":"/2025/06/10/基础笔记/Mysql学习笔记/","content":"Mysql基础🌟0.什么是MYSQLMySQL 是⼀个开源的关系型数据库，现在⾪属于 Oracle 公司。 删除创建一张表DROP TABLE 删除表CREATE TABLE 创建表创建表的时候，可以通过 PRIMARY KEY 设定主键。 CREATE TABLE users ( id INT AUTO_INCREMENT, name VARCHAR(100) NOT NULL, email VARCHAR(100), PRIMARY KEY (id)); 写一个升序降序的SQL语句可以使用ORDER BY字句对查询结果进行排序.默认情况下是升序排序.如需要降序,使用关键字DESC例子: SELECT id, name, salaryFROM employeesORDER BY salary DESC; 如若对多个字段进行排序: SELECT id, name, salaryFROM employeesORDER BY salary DESC, name ASC; 优先级从左到右,相当于先按工资降序,工资相同再按照姓名升序. MYSQL出现性能差的原因可能是 SQL 查询使⽤了全表扫描，也可能是查询语句过于复杂，如多表JOIN或嵌套⼦查询。也有可能是单表数据量过⼤。 通常情况下,增加索引就可以解决大部分的性能问题.对于热点数据,增加redis缓存,减轻对数据库的压力. 1.两张表怎么进行连接可以通过内连接inner join、外连接 outer join 、交叉连接 cross join 等方式来进行连接. 什么是内连接内连接⽤于返回两个表中有匹配关系的⾏。假设有两张表，⽤户表和订单表，想查询有订单的⽤户，就可以使⽤内连接 users INNER JOIN orders，按照⽤户 ID 关联就⾏了。 SELECT users.name, orders.order_idFROM usersINNER JOIN orders ON users.id = orders.user_id; 两表匹配的行才会输出. 什么是外连接和内连接不同，外连接不仅返回两个表中匹配的⾏，还返回没有匹配的⾏，⽤ null 来填充。外连接⼜分为左外连接 left join 和右外连接 right join。 left join 会保留左表中符合条件的所有记录，如果右表中有匹配的记录，就返回匹配的记录，否则就⽤null 填充，常⽤于某表中有，但另外⼀张表中可能没有的数据的查询场景。假设要查询所有⽤户及他们的订单，即使⽤户没有下单，就可以使⽤左连接： SELECT users.id, users.name, orders.order_idFROM usersLEFT JOIN orders ON users.id = orders.user_id; (这里面左表就是users,users所有行都会输出) 右连接就是左连接的镜像，right join 会保留右表中符合条件的所有记录，如果左表中有匹配的记录，就返回匹配的记录，否则就⽤ null 填充。 什么是交叉连接交叉连接会返回两张表的笛卡尔积，也就是将左表的每⼀⾏与右表的每⼀⾏进⾏组合，返回的⾏数是两张表⾏数的乘积。假设有 A 表和 B 表，A 表有 2 ⾏数据，B 表有 3 ⾏数据，那么交叉连接的结果就是 2 * 3 6 ⾏。笛卡尔积是数学中的⼀个概念，例如集合 A{a,b}，集合 B{0,1,2} ，那么 A x B {a,0,a,1,a,2,b,0,b,1,b,2,}。 SELECT A.id, B.idFROM ACROSS JOIN B; 2.内连接 左连接 右连接有什么区别左连接 FROM 表a join 表b 相当于 a在左 b在右MySQL 的连接主要分为内连接和外连接，外连接⼜可以分为左连接和右连接。 内连接相当于找两表的交集.左连接和右连接可以⽤来找出两个表中不同的记录，相当于两个数据集的并集。两者的区别是，左连接会保留左表中符合条件的所有记录，右连接则刚好相反。 例子:有三张表，⼀张⽂章表 article，主要存⽂章标题 title.⼀张⽂章详情表 article_detail，主要存⽂章的内容 content.⼀张⽂章评论表 comment，主要存评论 content.三个表通过⽂章 id关联。内连接:返回至少有评论的文章标题和评论内容 SELECT LEFT(a.title, 20) AS ArticleTitle, LEFT(c.content, 20) AS CommentContent //返回LEFT左数前20字符 AS为列或表起临时别名FROM article a //article起别名 aINNER JOIN comment c ON a.id = c.article_id //内连接 id相同 起别名cLIMIT 2; //只返回两条 左连接:返回所有⽂章的标题和⽂章评论，即使某些⽂章没有评论（填充为 NULL）。 SELECT LEFT(a.title, 20) AS ArticleTitle, LEFT(c.content, 20) ASCommentContentFROM article aLEFT JOIN comment c ON a.id = c.article_idLIMIT 2; 右连接:调换了位置 SELECT LEFT(a.title, 20) AS ArticleTitle, LEFT(c.content, 20) ASCommentContentFROM comment cRIGHT JOIN article a ON a.id = c.article_idLIMIT 2; 3.数据库的三大范式 - 第⼀范式:确保表的每⼀列都是不可分割的基本数据单元.⽐如说⽤户地址，应该拆分成省、市、区、详细地址等 4 个字段。相当于保持列的原子性. - 第⼆范式:要求表中的每⼀列都和主键直接相关。⽐如在订单表中，商品名称、单位、商品价格等字段应该拆分到商品表中。然后再创建一个订单商品关联表.不能出现部分依赖的情况.相当于让解决复合主键的部分依赖问题.订单明细(订单ID, 产品ID, 产品名称, 数量)↑ 复合主键是(订单ID, 产品ID)，但”产品名称”只依赖”产品ID”（部分依赖）修改为:订单明细(订单ID, 产品ID, 数量)产品(产品ID, 产品名称) - 第三范式:⾮主键列应该只依赖于主键列。⽐如说在设计订单信息表的时候，可以把客户名称、所属公司、联系⽅式等信息拆分到客户信息表中，然后在订单信息表中⽤客户编号进⾏关联。**相当于消除传递依赖.**将​​传递依赖​​的字段（A→B→C，其中A是主键）拆分到新表中。学生(学号, 姓名, 宿舍号, 宿舍费用)↑ “宿舍费用”依赖”宿舍号”，而”宿舍号”依赖”学号”（传递依赖）修改为:学生(学号, 姓名, 宿舍号)宿舍(宿舍号, 宿舍费用) 建表时需要考虑哪些问题⾸先需要考虑表是否符合数据库的三⼤范式，确保字段不可再分，消除⾮主键依赖，确保字段仅依赖于主键等。然后在选择字段类型时，应该尽量选择合适的数据类型。在字符集上，尽量选择 utf8mb4，这样不仅可以⽀持中⽂和英⽂，还可以⽀持表情符号等。当数据量较⼤时，⽐如上千万⾏数据，需要考虑分表。⽐如订单表，可以采⽤⽔平分表的⽅式来分散单表存储压⼒。 水平分表(按行拆分):水平分表是将​​同一张表中的数据按行拆分到多个结构相同的表中 按照id范围分表 按照时间范围分表 按照hash分表 按照地域分表 垂直分表(按列拆分):不同分表包含不同字段,表字段过多或冷热数据分离时使用. 4.varchar 与 char 的区别latin1 字符集，且列属性定义为 NOT NULL。 varchar 是可变⻓度的字符类型，原则上最多可以容纳 65535 个字符，但考虑字符集，以及MySQL 需要 1 到 2 个字节来表示字符串⻓度，所以实际上最⼤可以设置到 65533。char 是固定⻓度的字符类型，当定义⼀个 CHAR(10) 字段时，不管实际存储的字符⻓度是多少，都只会占⽤ 10 个字符的空间。如果插⼊的数据⼩于 10 个字符，剩余的部分会⽤空格填充。 varchar在输入过长时也会截断. 5.blob 和 text 有什么区别blob ⽤于存储⼆进制数据，⽐如图⽚、⾳频、视频、⽂件等；但实际开发中，我们都会把这些⽂件存储到 OSS 或者⽂件服务器上，然后在数据库中存储⽂件的 URL。 text ⽤于存储⽂本数据，⽐如⽂章、评论、⽇志等。 6.DATETIME 和 TIMESTAMP 有什么区别DATETIME 直接存储⽇期和时间的完整值，与时区⽆关。TIMESTAMP 存储的是 Unix 时间戳，1970-01-01 00:00:01 UTC 以来的秒数，受时区影响。 另外，DATETIME 的默认值为 null，占⽤ 8 个字节；TIMESTAMP 的默认值为当前时间——CURRENT_TIMESTAMP，占 4 个字节，实际开发中更常⽤，因为可以⾃动更新。 7. in 和 exists 的区别当使⽤ IN 时，MySQL 会⾸先执⾏⼦查询，然后将⼦查询的结果集⽤于外部查询的条件。这意味着⼦查询的结果集需要全部加载到内存中。 ⽽ EXISTS 会对外部查询的每⼀⾏，执⾏⼀次⼦查询。如果⼦查询返回任何⾏，则EXISTS条件为真。EXISTS 关注的是⼦查询是否返回⾏，⽽不是返回的具体值。 -- IN 的临时表可能成为性能瓶颈SELECT * FROM usersWHERE id IN (SELECT user_id FROM orders WHERE amount 100);//查询所有下过单笔订单金额超过100的用户（完整信息）-- EXISTS 可以利⽤关联索引SELECT * FROM users uWHERE EXISTS (SELECT 1 FROM orders oWHERE o.user_id = u.id AND o.amount 100); IN 适⽤于⼦查询结果集较⼩的情况。如果⼦查询返回⼤量数据,IN的查询效率会下降,因为他会把整个结果存到内存当中. ⽽ EXISTS 适⽤于⼦查询结果集可能很⼤的情况。由于 EXISTS 只需要判断⼦查询是否返回⾏，⽽不需要加载整个结果集，因此在某些情况下性能更好，特别是当⼦查询可以使⽤索引时。 NULL值IN的返回结果中如果有NULL值,可能会出现意料外的情况:比如WHERE column IN ((subquery))，如果subquery为NULL,这个条件永远不会为真,除非column也为NULL. EXISTS如果有NULL值的话,因为EXISTS只关心是否有⾏,所以不会出现NULL值的影响. 8.记录货币⽤什么类型⽐较好?如果是电商、交易、账单等涉及货币的场景，建议使⽤ DECIMAL 类型，因为 DECIMAL 类型是精确数值类型，不会出现浮点数计算误差。 如果是银⾏,涉及到⽀付的场景，建议使⽤ BIGINT 类型。可以将货币⾦额乘以⼀个固定因⼦，⽐如 100，表示以“分”为单位，然后存储为 BIGINT 。这种⽅式既避免了浮点数问题，同时也提供了不错的性能。但在展示的时候需要除以相应的因⼦。 为什么不推荐使⽤ FLOAT 或 DOUBLE？因为 FLOAT 和 DOUBLE 都是浮点数类型，会存在精度问题。在许多编程语⾔中， 0.1 + 0.2 的结果会是类似 0.30000000000000004 的值，⽽不是预期的 0.3 。 9.如何存储emoji?因为 emoji是 4 个字节的 UTF-8 字符，⽽ MySQL 的 utf8 字符集只⽀持最多 3 个字节的 UTF-8 字符，所以在 MySQL 中存储 emoji 时，需要使⽤ utf8mb4 字符集。 MySQL 8.0 已经默认⽀持 utf8mb4 字符集，可以通过 SHOW VARIABLES WHERE Variable_name LIKE character\\_set\\_% OR Variable_name LIKE collation%; 查看。 10.drop、delete 与 truncate 的区别？ DROP 是物理删除，⽤来删除整张表，包括表结构，且不能回滚。 DELETE ⽀持⾏级删除，可以带 WHERE 条件，可以回滚。 TRUNCATE ⽤于清空表中的所有数据，但会保留表结构，不能回滚。 11.UNION 与 UNION ALL 的区别？UNION 会⾃动去除合并后结果集中的重复⾏。UNION ALL 不会去重，会将所有结果集合并起来。 12.count(1)、count(*) 与 count(列名) 的区别？在InnoDB引擎里面,count(1)和count(*)没有任何区别,都用来统计所有行,包括NULL.如果有索引,那么count(1)和count(*)都会走索引,而count(列名)会走主键索引. COUNT(列名) 只统计列名不为 NULL 的⾏数。 13.SQL 查询语句的执⾏顺序了解吗？了解,先执行FROM确定主表,，再执⾏JOIN连接，然后 WHERE 进⾏过滤，接着 GROUP BY 进⾏分组，HAVING 过滤聚合结果，SELECT 选择最终列，ORDER BY 排序，最后 LIMIT 限制返回⾏数。 WHERE 先执⾏是为了减少数据量，HAVING 只能过滤聚合数据，ORDER BY 必须在SELECT 之后排序最终结果，LIMIT 最后执⾏以减少数据传输。 这个执⾏顺序与编写 SQL 语句的顺序不同，这也是为什么有时候在 SELECT ⼦句中定义的别名不能在 WHERE ⼦句中使⽤得原因，因为 WHERE 是在 SELECT 之前执⾏的。 LIMIT 为什么在最后执⾏？因为 LIMIT 是在最终结果集上执⾏的，如果在 WHERE 之前执⾏ LIMIT，那么就会先返回所有⾏，然后再进⾏ LIMIT 限制，这样会增加数据传输的开销。 ORDER BY 为什么在 SELECT 之后执⾏？因为排序需要基于最终返回的列，如果 ORDER BY 早于 SELECT 执⾏，计算 类的聚合函数就会出问题。比如说如果要按照所选的平均值排序,order by先执行,还没有计算平均值. 14.介绍⼀下 MySQL 的常⽤命令MySQL 的常⽤命令主要包括数据库操作命令、表操作命令、⾏数据 CRUD 命令、索引和约束的创建修改命令、⽤户和权限管理的命令、事务控制的命令等。 -- 数据库操作CREATE DATABASE db_name; -- 创建数据库DROP DATABASE db_name; -- 删除数据库SHOW DATABASES; -- 查看所有数据库USE db_name; -- 切换数据库-- 表操作CREATE TABLE table_name ( -- 创建表 id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(50) NOT NULL);DROP TABLE table_name; -- 删除表ALTER TABLE table_name ADD COLUMN col_name INT; -- 添加列ALTER TABLE table_name DROP COLUMN col_name; -- 删除列SHOW TABLES; -- 查看所有表DESC table_name; -- 查看表结构-- 数据CRUDINSERT INTO table_name VALUES (1, test); -- 插入数据SELECT * FROM table_name; -- 查询数据UPDATE table_name SET name=new WHERE id=1;-- 更新数据DELETE FROM table_name WHERE id=1; -- 删除数据-- 索引和约束CREATE INDEX idx_name ON table_name(col); -- 创建索引ALTER TABLE table_name ADD PRIMARY KEY(id); -- 添加主键ALTER TABLE table_name ADD UNIQUE(col_name); -- 添加唯一约束ALTER TABLE table_name ADD FOREIGN KEY(col) REFERENCES other_table(col); -- 外键-- 用户权限CREATE USER user@host IDENTIFIED BY pwd; -- 创建用户GRANT ALL ON db_name.* TO user@host; -- 授权REVOKE ALL ON db_name.* FROM user@host; -- 撤销权限DROP USER user@host; -- 删除用户-- 事务控制START TRANSACTION; -- 开始事务COMMIT; -- 提交事务ROLLBACK; -- 回滚事务SET autocommit=0; -- 关闭自动提交 说说数据库操作命令?CREATE DATABASE database_name; ⽤于创建数据库;USE database_name;⽤于显示所有数据库;DROP DATABASE database_name;⽤于删除数据库;SHOW DATABASES; 换数据库。 说说表操作命令？CREATE TABLE table_name (列名1 数据类型1, 列名2 数据类型2,...); 用于创建表；DROP TABLE table_name; 用于删除表；SHOW TABLES; 用于显示所有表；DESCRIBE table_name; 用于查看表结构；ALTER TABLE table_name ADD column_name datatype; 用于修改表。 说说行数据的 CRUD 命令？INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...); 用于插入数据；SELECT column_names FROM table_name WHERE condition; 用于查询数据；UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; 用于更新数据；DELETE FROM table_name WHERE condition; 用于删除数据。 说说索引和约束的创建修改命令？CREATE INDEX index_name ON table_name (column_name); 用于创建索引；ALTER TABLE table_name ADD PRIMARY KEY (column_name); 用于添加主键；ALTER TABLE table_name ADD CONSTRAINT fk_name FOREIGN KEY (column_name) REFERENCES parent_table (parent_column_name); 用于添加外键。 说说用户和权限管理的命令？CREATE USER username@host IDENTIFIED BY password; 用于创建用户；GRANT ALL PRIVILEGES ON database_name.table_name TO username@host; 用于授予权限；REVOKE ALL PRIVILEGES ON database_name.table_name FROM username@host; 用于撤销权限；DROP USER username@host; 用于删除用户。 说说事务控制的命令？START TRANSACTION; 用于开始事务；COMMIT; 用于提交事务；ROLLBACK; 用于回滚事务。 15.MySQL bin 目录下的可执行文件了解吗MySQL 的 bin 目录下有很多可执行文件，主要用于管理 MySQL 服务器、数据库、表、数据等。比如说：mysql：用于连接 MySQL 服务器mysqldump：用于数据库备份，对数据备份、迁移或恢复时非常有用mysqladmin：用来执行一些管理操作，比如说创建数据库、删除数据库、查看 MySQL 服务器的状态等。mysqlcheck：用于检查、修复、分析和优化数据库表，对数据库的维护和性能优化非常有用。mysqlimport：用于从文本文件中导入数据到数据库表中，适合批量数据导入。mysqlshow：用于显示 MySQL 数据库服务器中的数据库、表、列等信息。mysqlbinlog：用于查看 MySQL 二进制日志文件的内容，可以用于恢复数据、查看数据变更等。 16.MySQL 第 3-10 条记录怎么查？可以使用 limit 语句，结合偏移量和行数来实现。 SELECT * FROM table_name LIMIT 2, 8; limit 语句用于限制查询结果的数量，偏移量表示从哪条记录开始，行数表示返回的记录数量。2：偏移量，表示跳过前两条记录，从第三条记录开始。8：行数，表示从偏移量开始，返回 8 条记录。偏移量是从 0 开始的，即第一条记录的偏移量是 0；如果想从第 3 条记录开始，偏移量就应该是 2。 17.用过哪些 MySQL 函数？用过挺多的，比如说处理字符串的函数：CONCAT(): 用于连接两个或多个字符串。LENGTH(): 用于返回字符串的长度。SUBSTRING(): 从字符串中提取子字符串。REPLACE(): 替换字符串中的某部分。TRIM(): 去除字符串两侧的空格或其他指定字符。 处理数字的函数：ABS(): 返回一个数的绝对值。ROUND(): 四舍五入到指定的小数位数。MOD(): 返回除法操作的余数。 日期和时间处理函数：NOW(): 返回当前的日期和时间。CURDATE(): 返回当前的日期。 汇总函数：SUM(): 计算数值列的总和。AVG(): 计算数值列的平均值。COUNT(): 计算某列的行数。 逻辑函数：IF(): 如果条件为真，则返回一个值；否则返回另一个值。CASE: 根据一系列条件返回值。 18.说说 SQL 的隐式数据类型转换？当一个整数和一个浮点数相加时，整数会被转换为浮点数。SELECT 1 + 1.0; – 结果为 2.0当一个字符串和一个整数相加时，字符串会被转换为整数。SELECT 1 + 1; – 结果为 2隐式转换会导致意想不到的结果，最好通过显式转换来规避。SELECT CAST(1 AS SIGNED INTEGER) + 1; – 结果为 2 19. 说说 SQL 的语法树解析？SQL 语法树解析是将 SQL 查询语句转换成抽象语法树 —— AST 的过程，是数据库引擎处理查询的第一步，也是防止 SQL 注入的重要手段。通常分为 3 个阶段。 第一个阶段，词法分析：拆解 SQL 语句，识别关键字、表名、列名等。 —start—比如说：SELECT id, name FROM users WHERE age 18;将会被拆解为：[SELECT] [id] [,] [name] [FROM] [users] [WHERE] [age] [] [18] [;]—end— 第二个阶段，语法分析：检查 SQL 是否符合语法规则，并构建抽象语法树。 —start—比如说上面的语句会被构建成如下的语法树： SELECT / \\ Columns FROM / \\ | id name users | WHERE | age 18或者这样表示：SELECT ├── COLUMNS: id, name ├── FROM: users ├── WHERE │ ├── CONDITION: age 18—end— 第三个阶段，语义分析：检查表、列是否存在，进行权限验证等。 —start—比如说执行：SELECT id, name FROM users WHERE age ‘eighteen’;会报错：ERROR: Column ‘age’ is INT, but ‘eighteen’ is STRING.—end— 数据库架构20.说说 MySQL 的基础架构？MySQL 采用分层架构，主要包括连接层、服务层、和存储引擎层。①、连接层主要负责客户端连接的管理，包括验证用户身份、权限校验、连接管理等。可以通过数据库连接池来提升连接的处理效率。②、服务层是 MySQL 的核心，主要负责查询解析、优化、执行等操作。在这一层，SQL 语句会经过解析、优化器优化，然后转发到存储引擎执行，并返回结果。这一层包含查询解析器、优化器、执行计划生成器、日志模块等。③、存储引擎层负责数据的实际存储和提取。MySQL 支持多种存储引擎，如 InnoDB、MyISAM、Memory 等。binlog写入在哪一层？binlog 在服务层，负责记录 SQL 语句的变化。它记录了所有对数据库进行更改的操作，用于数据恢复、主从复制等。 21.一条查询语句SELECT是如何执行的？当我们执行一条 SELECT 语句时，MySQL 并不会直接去磁盘读取数据，而是经过 6 个步骤来解析、优化、执行，然后再返回结果。第一步，客户端发送 SQL 查询语句到 MySQL 服务器。 第二步，MySQL 服务器的连接器开始处理这个请求，跟客户端建立连接、获取权限、管理连接。 第三步，解析器对 SQL 语句进行解析，检查语句是否符合 SQL 语法规则，确保数据库、表和列都是存在的，并处理 SQL 语句中的名称解析和权限验证。 第四步，优化器负责确定 SQL 语句的执行计划，这包括选择使用哪些索引，以及决定表之间的连接顺序等。 第五步，执行器会调用存储引擎的 API来进行数据的读写。 第六步，存储引擎负责查询数据，并将执行结果返回给客户端。客户端接收到查询结果，完成这次查询请求。 22.一条更新语句UPDATE是如何执行的？undo log 回滚日志: 用于事务的回滚操作.redo log 重做日志: 用于实现事务的持久性,保持数据一致性.总的来说，一条 UPDATE 语句的执行过程包括读取数据页、加锁解锁、事务提交、日志记录等多个步骤。拿 update test set a=1 where id=2 举例来说：在事务开始前，MySQL 需要记录undo log，用于事务回滚。 操作 id 旧值 新值 update 2 N 1 除了记录 undo log，存储引擎还会将更新操作写入 redo log，状态标记为 prepare，并确保 redo log 持久化到磁盘。这一步可以保证即使系统崩溃，数据也能通过 redo log 恢复到一致状态。 写完redo log 后，MySQL 会获取行锁，将 a 的值修改为 1，标记为脏页，此时数据仍然在内存的 buffer pool 中，不会立即写入磁盘。后台线程会在适当的时候将脏页刷盘，以提高性能。 最后提交事务，redo log 中的记录被标记为 committed，行锁释放。 如果 MySQL 开启了 binlog，还会将更新操作记录到 binlog 中，主要用于主从复制。 以及数据恢复，可以结合 redo log 进行点对点的恢复。binlog 的写入通常发生在事务提交时，与 redo log 共同构成“两阶段提交”，确保两者的一致性。 注意，redo log 的写入有两个阶段的提交，一是 binlog 写入之前prepare 状态的写入，二是binlog写入之后 commit 状态的写入。 23.说说 MySQL 的段区页行MySQL 是以表的形式存储数据的，而表空间的结构则由段、区、页、行组成。 ①、段：表空间由多个段组成，常见的段有数据段、索引段、回滚段等。创建索引时会创建两个段，数据段和索引段，数据段用来存储叶子节点中的数据；索引段用来存储非叶子节点的数据。回滚段包含了事务执行过程中用于数据回滚的旧数据。 ②、区：段由一个或多个区组成，区是一组连续的页，通常包含 64 个连续的页，也就是 1M 的数据。使用区而非单独的页进行数据分配可以优化磁盘操作，减少磁盘寻道时间，特别是在大量数据进行读写时。 ③、页：页是 InnoDB 存储数据的基本单元，标准大小为 16 KB，索引树上的一个节点就是一个页。也就意味着数据库每次读写都是以 16 KB 为单位的，一次最少从磁盘中读取 16KB 的数据到内存，一次最少写入 16KB 的数据到磁盘。 ④、行：InnoDB 采用行存储方式，意味着数据按照行进行组织和管理，行数据可能有多个格式，比如说 COMPACT、REDUNDANT、DYNAMIC 等。MySQL 8.0 默认的行格式是 DYNAMIC，由COMPACT 演变而来，意味着这些数据如果超过了页内联存储的限制，则会被存储在溢出页中。可以通过 show table status like ‘%article%’ 查看行格式。 了解 MySQL的数据行、行溢出机制吗？InnoDB从磁盘中读取数据的最小单位是数据页。 一 行有哪些格式Mysql的数据行有两种格式:Compact格式和Redundant格式。Compact是一种紧凑的行格式，设计的初衷就是为了让一个数据页中可以存放更多的数据行。 你品一品，让一个数据页中可以存放更多的数据行是一个多么激动人心的事，MySQL以数据页为单位从磁盘中读数据，如果能做到让一个数据页中有更多的行，那岂不是使用的空间变少了，且整体的效率直线飙升？ 官网介绍：Compact能比Redundant格式节约20%的存储。 Compact从MySQL5.0引入，MySQL5.1之后，行格式默认设置成 Compact 。所以本文描述的也是Compact格式。 二、紧凑的行格式长啥样？表中有的列允许为null，有的列是变长的varchar类型。那Compact行格式是如何组织描述这些信息的呢？如下图： 每部分包含的数据可能要比我上面标注的1、2、3还要多。 为了给大家更直观的感受和理解我只是挑了一部分展示给大家看。 三、MySQL单行能存多大体量的数据？在MySQL的设定中，单行数据最大能存储65535byte的数据（注意是byte，而不是字符）MySQL不允许创建一个长度为65535byte的列，因为数据页中每一行中都有我们上图提到的隐藏列。所以将varchar的长度降低到65532byte即可成功创建该表. 所以如果你将charset换成utf8这种编码格式，那varchar(N)中的N其实指的N个字符，而不是N个byte。假如encodeutf8时三个byte表示一个字符。那么65535 3 21845个字符。 四、Compact格式是如何做到紧凑的？MySQL每次进行随机的IO读默认情况下，数据页的大小为16KB。数据页中存储着数行。那就意味着一个数据页中能存储越多的数据行，MySQL整体的进行的IO次数就越少？性能就越快？Compact格式的实现思路是：当列的类型为VARCHAR、 VARBINARY、 BLOB、TEXT时，该列超过768byte的数据放到其他数据页中去。如下图：MySQL这样做，有效的防止了单个varchar列或者Text列太大导致单个数据页中存放的行记录过少而让IO飙升的窘境且占内存的。 五、什么是行溢出？如果数据页默认大小为16KB，换算成byte： 16*1024 16384 byte那你有没有发现，单页能存储的16384byte和单行最大能存储的 65535byte 差了好几倍呢 也就是说，假如你要存储的数据行很大超过了65532byte那么你是写入不进去的。假如你要存储的单行数据小于65535byte但是大于16384byte，这时你可以成功insert，但是一个数据页又存储不了你插入的数据。这时肯定会行溢出！ 其实在MySQL的设定中，发生行溢出并不是达到16384byte边缘才会发生。对于varchar、text等类型的行。当这种列存储的长度达到几百byte时就会发生行溢。 六、行 如何溢出？还是看这张图：在MySQL设定中，当varchar列长度达到768byte后，会将该列的前768byte当作当作prefix存放在行中，多出来的数据溢出存放到溢出页中，然后通过一个偏移量指针将两者关联起来，这就是行溢出机制。 七、思考一个问题不知道你有没有想过这样一个问题：首先你肯定知道，MySQL使用的是B+Tree的聚簇索引，在这棵B+Tree中非叶子节点是只存索引不存数据，叶子节点中存储着真实的数据。同时叶子结点指向数据页。那当单行存不下的时候，为啥不存储在两个数据页中呢？就像下图这样～。单个节点存储下，我用多个节点存总行吧！说不定这样我的B+Tee还能变大长高（这其实是错误的想法）这个错误的描述对应的脑图如下： 那MySQL不这样做的原因如下：MySQL想让一个数据页中能存放更多的数据行，至少也得要存放两行数据。否则就失去了B+Tree的意义。B+Tree也退化成一个低效的链表。你可以品一下这句蓝色的话，他说的每个数据页至少要存放两行数据的意思不是说 数据页不能只存一行。你确确实实可以只往里面写一行数据，然后去吃个饭，干点别的。一直让这个数据页中只有一行数据。 这句话的意思是，当你往这个数据页中写入一行数据时，即使它很大将达到了数据页的极限，但是通过行溢出机制。依然能保证你的下一条数据还能写入到这个数据页中。 存储引擎24.MySQL 有哪些常见存储引擎？MySQL 支持多种存储引擎，常见的有 MyISAM、InnoDB、MEMORY 等。—这部分是帮助理解 start，面试中可不背—我来做一个表格对比：—这部分是帮助理解 end，面试中可不背—除此之外，我还了解到：①、MySQL 5.5 之前，默认存储引擎是 MyISAM，5.5 之后是 InnoDB。②、InnoDB 支持的哈希索引是自适应的，不能人为干预。③、InnoDB 从 MySQL 5.6 开始，支持全文索引。④、InnoDB 的最小表空间略小于 10M，最大表空间取决于页面大小。如何切换 MySQL 的数据引擎？可以通过 alter table 语句来切换 MySQL 的数据引擎。ALTER TABLE your_table_name ENGINE=InnoDB;不过不建议，应该提前设计好到底用哪一种存储引擎。 25.存储引擎应该怎么选择？大多数情况下，使用默认的 InnoDB 就可以了，InnoDB 可以提供事务、行级锁、外键、B+ 树索引等能力。MyISAM 适合读多写少的场景。MEMORY 适合临时表，数据量不大的情况。因为数据都存放在内存，所以速度非常快。 26.InnoDB 和 MyISAM 主要有什么区别？InnoDB 和 MyISAM 的最大区别在于事务支持和锁机制。InnoDB 支持事务、行级锁，适合大多数业务系统；而 MyISAM 不支持事务，用的是表锁，查询快但写入性能差，适合读多写少的场景。 另外，从存储结构上来说，MyISAM 用三种格式的文件来存储，.frm 文件存储表的定义；.MYD 存储数据；.MYI 存储索引；而 InnoDB 用两种格式的文件来存储，.frm 文件存储表的定义；.ibd 存储数据和索引。 从索引类型上来说，MyISAM 为非聚簇索引，索引和数据分开存储，索引保存的是数据文件的指针。 InnoDB 为聚簇索引，索引和数据不分开。 更细微的层面上来讲，MyISAM 不支持外键，可以没有主键，表的具体行数存储在表的属性中，查询时可以直接返回；InnoDB 支持外键，必须有主键，具体行数需要扫描整个表才能返回，有索引的情况下会扫描索引。 InnoDB的内存结构了解吗？InnoDB 的内存区域主要有两块，buffer pool 和 log buffer。 buffer pool 用于缓存数据页和索引页，提升读写性能；log buffer 用于缓存 redo log，提升写入性能。InnoDB引擎框架图如下: 数据页的结构了解过吗InnoDB 的数据页由 7 部分组成，其中文件头、页头和文件尾的大小是固定的，分别为 38、56 和 8 个字节，用来标记该页的一些信息。行记录、空闲空间和页目录的大小是动态的，为实际的行记录存储空间。 真实的记录会按照指定的行格式存储到 User Records 中。 每个数据页的 File Header 都有一个上一页和下一页的编号，所有的数据页会形成一个双向链表。 在 InnoDB 中，默认的页大小是 16KB。可以通过 show variables like innodb_page_size; 查看。 27. InnoDB 的 Buffer Pool了解吗？Buffer Pool 是 InnoDB 存储引擎中的一个内存缓冲区，它会将经常使用的数据页、索引页加载进内存，读的时候先查询 Buffer Pool，如果命中就不用访问磁盘了。 如果没有命中，就从磁盘读取，并加载到 Buffer Pool，此时可能会触发页淘汰，将不常用的页移出 Buffer Pool。 写操作时不会直接写入磁盘，而是先修改内存中的页，此时页被标记为脏页，后台线程会定期将脏页刷新到磁盘。 Buffer Pool 可以显著减少磁盘的读写次数，从而提升 MySQL 的读写性能。 Buffer Pool 的默认大小是多少？本机上 InnoDB 的 Buffer Pool 默认大小是 128MB。 SHOW VARIABLES LIKE innodb_buffer_pool_size; 另外，在具有 1GB-4GB RAM 的系统上，默认值为系统 RAM 的 25%；在具有超过 4GB RAM 的系统上，默认值为系统 RAM 的 50%，但不超过 4GB。 InnoDB 对 LRU 算法的优化了解吗？LRU (least resently used)：近期最少使用 LFU (least freqently used)：频数最少使用 了解，InnoDB 对 LRU 算法进行了改良，最近访问的数据并不直接放到 LRU 链表的头部，而是放在一个叫 midpoiont 的位置。默认情况下，midpoint 位于 LRU 列表的 58 处。 比如 Buffer Pool 有 100 页，新页插入的位置大概是在第 80 页；当页数据被频繁访问后，再将其移动到 young 区，这样做的好处是热点页能长时间保留在内存中，不容易被挤出去。 —-这部分是帮助理解 start，面试中可不背—- 可以通过 innodb_old_blocks_pct 参数来调整 Buffer Pool 中 old 和 young 区的比例；通过 innodb_old_blocks_time 参数来调整页在 young 区的停留时间。 默认情况下，LRU 链表中 old 区占 37%；同一页再次访问提升的最小时间间隔是 1000 毫秒。也就是说，如果某页在 1 秒内被多次访问，只会计算一次，不会立刻升级为热点页，防止短时间批量访问导致缓存污染。 —-这部分是帮助理解 end，面试中可不背—- 日志28.MySQL 日志文件有哪些？有 6 大类，其中错误日志用于问题诊断，慢查询日志用于 SQL 性能分析，general log 用于记录所有的 SQL 语句，binlog 用于主从复制和数据恢复，redo log 用于保证事务持久性，undo log 用于事务回滚和 MVCC。 —-这部分是帮助理解 start，面试中可不背—- ①、错误日志（Error Log）：记录 MySQL 服务器启动、运行或停止时出现的问题。②、慢查询日志（Slow Query Log）：记录执行时间超过 long_query_time 值的所有 SQL 语句。这个时间值是可配置的，默认情况下，慢查询日志功能是关闭的。③、一般查询日志（General Query Log）：记录 MySQL 服务器的启动关闭信息，客户端的连接信息，以及更新、查询的 SQL 语句等。④、二进制日志（Binary Log）：记录所有修改数据库状态的 SQL 语句，以及每个语句的执行时间，如 INSERT、UPDATE、DELETE 等，但不包括 SELECT 和 SHOW 这类的操作。⑤、重做日志（Redo Log）：记录对于 InnoDB 表的每个写操作，不是 SQL 级别的，而是物理级别的，主要用于崩溃恢复。⑥、回滚日志（Undo Log，或者叫事务日志）：记录数据被修改前的值，用于事务的回滚。 —-这部分是帮助理解 end，面试中可不背—- 请重点说说 binlog？binlog 是一种物理日志，会在磁盘上记录数据库的所有修改操作。如果误删了数据，就可以使用 binlog 进行回退到误删之前的状态。 # 步骤1：恢复全量备份mysql -u root -p full_backup.sql# 步骤2：应用Binlog到指定时间点mysqlbinlog --start-datetime=2025-03-13 14:00:00 --stop-datetime=2025-03-13 15:00:00 binlog.000001 | mysql -u root -p 如果要搭建主从复制，就可以让从库定时读取主库的 binlog。MySQL 提供了三种格式的 binlog：Statement、Row 和 Mixed，分别对应 SQL 语句级别、行级别和混合级别，默认为行级别。从后缀名上来看，binlog 文件分为两类：以 .index 结尾的索引文件，以 .00000* 结尾的二进制日志文件。binlog 默认是没有启用的。 生产环境中是一定要启用的，可以通过在 my.cnf 文件中配置 log_bin 参数，以启用 binlog。 log_bin = mysql-bin #开启binlog#mysql-bin.*日志文件最大字节（单位：字节）#设置最大100MBmax_binlog_size=104857600#设置了只保留7天BINLOG（单位：天）expire_logs_days = 7#binlog日志只记录指定库的更新#binlog-do-db=db_name#binlog日志不记录指定库的更新#binlog-ignore-db=db_name#写缓冲多少次，刷一次磁盘，默认0sync_binlog=0 binlog 的配置参数都了解哪些？log_bin = mysql-bin 用于启用 binlog，这样就可以在 MySQL 的数据目录中找到 db-bin.000001、db-bin.000002 等日志文件。max_binlog_size=104857600 用于设置每个 binlog 文件的大小，不建议设置太大，网络传送起来比较麻烦。当 binlog 文件达到 max_binlog_size 时，MySQL 会关闭当前文件并创建一个新的 binlog 文件。expire_logs_days = 7 用于设置 binlog 文件的自动过期时间为 7 天。过期的 binlog 文件会被自动删除。防止长时间累积的 binlog 文件占用过多存储空间，所以这个配置很重要。binlog-do-db=db_name，指定哪些数据库表的更新应该被记录。binlog-ignore-db=db_name，指定忽略哪些数据库表的更新。sync_binlog=0，设置每多少次 binlog 写操作会触发一次磁盘同步操作。默认值为 0，表示 MySQL 不会主动触发同步操作，而是依赖操作系统的磁盘缓存策略。即当执行写操作时，数据会先写入缓存，当缓存区满了再由操作系统将数据一次性刷入磁盘。如果设置为 1，表示每次 binlog 写操作后都会同步到磁盘，虽然可以保证数据能够及时写入磁盘，但会降低性能。可以通过 show variables like %log_bin%; 查看 binlog 是否开启。 有了binlog为什么还要undolog redolog？binlog 属于 Server 层，与存储引擎无关，无法直接操作物理数据页。而 redo log 和 undo log 是 InnoDB 存储引擎实现 ACID的基石。————–ps————-ACID: **原子性(Atomicity)**：事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败回滚通过undo log实现，记录事务开始前的状态，用于回滚 **一致性(Consistency)**：事务执行前后，数据库从一个一致状态转变为另一个一致状态通过其他三个特性(AID)共同保证 **隔离性(Isolation)**：多个并发事务执行时，一个事务的执行不应影响其他事务通过锁机制和MVCC(多版本并发控制)实现 **持久性(Durability)**：事务一旦提交，其结果就是永久性的通过redo log实现，即使系统崩溃也能恢复数据 ————–ps————- binlog 关注的是逻辑变更的全局记录；redo log 用于确保物理变更的持久性，确保事务最终能够刷盘成功；undo log 是逻辑逆向操作日志，记录的是旧值，方便恢复到事务开始前的状态。 另外一种回答方式。 binlog 会记录整个 SQL 或行变化；redo log 是为了恢复已提交但未刷盘的数据，undo log 是为了撤销未提交的事务。 以一次事务更新为例： # 开启事务BEGIN;# 更新数据UPDATE users SET age = age + 1 WHERE id = 1;# 提交事务COMMIT; 事务开始的时候会生成 undo log，记录更新前的数据，比如原值是 18： undo log: id=1, age=18 修改数据的时候，会将数据写入到 redo log。 比如数据页 page_id=123 上，id1 的用户被更新为 age26： redo log (prepare): page_id=123, offset=0x40, before=18, after=26 等事务提交的时候，redo log 刷盘，binlog 刷盘。 binlog 写完之后，redo log 的状态会变为 commit： redo log (commit): page_id=123, offset=0x40, before=18, after=26 binlog 如果是 Statement 格式，会记录一条 SQL 语句： UPDATE users SET age age + 1 WHERE id 1;binlog 如果是 Row 格式，会记录： 表：usersbefore: id=1, age=18after: id=1, age=26 随后，后台线程会将 redo log 中的变更异步刷新到磁盘。 详细探究一下binlog(长文警告⚠️):MySQL 的 Binlog 日志是一种二进制格式的日志，Binlog 记录所有的 DDL 和 DML 语句(除了数据查询语句SELECT、SHOW等)，以 Event 的形式记录，同时记录语句执行时间。 Binlog 的主要作用有两个：1. 数据恢复:因为 Binlog 详细记录了所有修改数据的 SQL，当某一时刻的数据误操作而导致出问题，或者数据库宕机数据丢失，那么可以根据 Binlog 来回放历史数据。2. 主从复制:想要做多机备份的业务，可以去监听当前写库的 Binlog 日志，同步写库的所有更改。 Binlog 包括两类文件：二进制日志索引文件(.index)：记录所有的二进制文件。二进制日志文件(.00000*)：记录所有 DDL 和 DML 语句事件。 Binlog 日志功能默认是开启的，线上情况下 Binlog 日志的增长速度是很快的，在 MySQL 的配置文件 my.cnf 中提供一些参数来对 Binlog 进行设置。 #设置此参数表示启用binlog功能，并制定二进制日志的存储目录log-bin=/home/mysql/binlog/#mysql-bin.*日志文件最大字节（单位：字节）#设置最大100MBmax_binlog_size=104857600#设置了只保留7天BINLOG（单位：天）expire_logs_days = 7#binlog日志只记录指定库的更新#binlog-do-db=db_name#binlog日志不记录指定库的更新#binlog-ignore-db=db_name#写缓冲多少次，刷一次磁盘，默认0sync_binlog=0 需要注意的是：max_binlog_size ：Binlog 最大和默认值是 1G，该设置并不能严格控制 Binlog 的大小，尤其是 Binlog 比较靠近最大值而又遇到一个比较大事务时，为了保证事务的完整性不可能做切换日志的动作，只能将该事务的所有 SQL 都记录进当前日志直到事务结束。所以真实文件有时候会大于 max_binlog_size 设定值。expire_logs_days ：Binlog 过期删除不是服务定时执行，是需要借助事件触发才执行，事件包括： 服务器重启 服务器被更新 日志达到了最大日志长度 max_binlog_size 日志被刷新 二进制日志由配置文件的 log-bin 选项负责启用，MySQL 服务器将在数据根目录创建两个新文件mysql-bin.000001 和 mysql-bin.index，若配置选项没有给出文件名，MySQL 将使用主机名称命名这两个文件，其中 .index 文件包含一份全体日志文件的清单。 sync_binlog：这个参数决定了 Binlog 日志的更新频率。默认 0 ，表示该操作由操作系统根据自身负载自行决定多久写一次磁盘。 sync_binlog = 1 表示每一条事务提交都会立刻写盘。sync_binlog=n 表示 n 个事务提交才会写盘。 根据 MySQL 文档，写 Binlog 的时机是：SQL transaction 执行完，但任何相关的 Locks 还未释放或事务还未最终 commit 前。这样保证了 Binlog 记录的操作时序与数据库实际的数据变更顺序一致。 检查 Binlog 文件是否已开启： mysql show variables like %log_bin%;+---------------------------------+------------------------------------+| Variable_name | Value |+---------------------------------+------------------------------------+| log_bin | ON || log_bin_basename | /usr/local/mysql/data/binlog || log_bin_index | /usr/local/mysql/data/binlog.index || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF || sql_log_bin | ON |+---------------------------------+------------------------------------+6 rows in set (0.00 sec) MySQL 会把用户对所有数据库的内容和结构的修改情况记入 mysql-bin.n 文件，而不会记录 SELECT 和没有实际更新的 UPDATE 语句。 如果你不知道现在有哪些 Binlog 文件，可以使用如下命令： show binary logs; #查看binlog列表show master status; #查看最新的binlogmysql show binary logs;+------------------+-----------+-----------+| Log_name | File_size | Encrypted |+------------------+-----------+-----------+| mysql-bin.000001 | 179 | No || mysql-bin.000002 | 156 | No |+------------------+-----------+-----------+2 rows in set (0.00 sec) Binlog 文件是二进制文件，强行打开看到的必然是乱码，MySQL 提供了命令行的方式来展示 Binlog 日志： mysqlbinlog mysql-bin.000002 | more mysqlbinlog 命令即可查看。虽然看起来凌乱其实也有迹可循。Binlog 通过事件的方式来管理日志信息，可以通过 show binlog events in 的语法来查看当前 Binlog 文件对应的详细事件信息。 mysql show binlog events in mysql-bin.000001;+------------------+-----+----------------+-----------+-------------+-----------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-----------------------------------+| mysql-bin.000001 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000001 | 125 | Previous_gtids | 1 | 156 | || mysql-bin.000001 | 156 | Stop | 1 | 179 | |+------------------+-----+----------------+-----------+-------------+-----------------------------------+3 rows in set (0.01 sec) 这是一份没有任何写入数据的 Binlog 日志文件。Binlog 的版本是V4，可以看到日志的结束时间为 Stop。出现 Stop event 有两种情况： 是 master shut down 的时候会在 Binlog 文件结尾出现 是备机在关闭的时候会写入 relay log 结尾，或者执行 RESET SLAVE 命令执行 本文出现的原因是我有手动停止过 MySQL 服务。一般来说一份正常的 Binlog 日志文件会以 Rotate event 结束。当 Binlog 文件超过指定大小，Rotate event 会写在文件最后，指向下一个 Binlog 文件。我们来看看有过数据操作的 Binlog 日志文件是什么样子的。 mysql show binlog events in mysql-bin.000002;+------------------+-----+----------------+-----------+-------------+-----------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-----------------------------------+| mysql-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000002 | 125 | Previous_gtids | 1 | 156 | |+------------------+-----+----------------+-----------+-------------+-----------------------------------+2 rows in set (0.00 sec) 上面是没有任何数据操作且没有被截断的 Binlog。接下来我们插入一条数据，再看看 Binlog 事件。 mysql show binlog events in mysql-bin.000002;+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| mysql-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000002 | 125 | Previous_gtids | 1 | 156 | || mysql-bin.000002 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= ANONYMOUS || mysql-bin.000002 | 235 | Query | 1 | 323 | BEGIN || mysql-bin.000002 | 323 | Intvar | 1 | 355 | INSERT_ID=13 || mysql-bin.000002 | 355 | Query | 1 | 494 | use `test_db`; INSERT INTO `test_db`.`test_db`(`name`) VALUES (xdfdf) || mysql-bin.000002 | 494 | Xid | 1 | 525 | COMMIT /* xid=192 */ |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+7 rows in set (0.00 sec) 这是加入一条数据之后的 Binlog 事件。 我们对 event 查询的数据行关键字段来解释一下： Pos：当前事件的开始位置，每个事件都占用固定的字节大小，结束位置(End_log_position)减去Pos，就是这个事件占用的字节数。上面的日志中我们能看到，第一个事件位置并不是从 0 开始，而是从 4。MySQL 通过文件中的前 4 个字节，来判断这是不是一个 Binlog 文件。这种方式很常见，很多格式的文件，如 pdf、doc、jpg等，都会通常前几个特定字符判断是否是合法文件。 Event_type：表示事件的类型 Server_id：表示产生这个事件的 MySQL server_id，通过设置 my.cnf 中的 server-id 选项进行配置 End_log_position：下一个事件的开始位置 Info：包含事件的具体信息 Binlog 日志格式:针对不同的使用场景，Binlog 也提供了可定制化的服务，提供了三种模式来提供不同详细程度的日志内容。 Statement 模式：基于 SQL 语句的复制(statement-based replication-SBR) Row 模式：基于行的复制(row-based replication-RBR) Mixed 模式：混合模式复制(mixed-based replication-MBR) Statement 模式保存每一条修改数据的SQL。该模式只保存一条普通的SQL语句，不涉及到执行的上下文信息。因为每台 MySQL 数据库的本地环境可能不一样，那么对于依赖到本地环境的函数或者上下文处理的逻辑 SQL 去处理的时候可能同样的语句在不同的机器上执行出来的效果不一致。比如像 sleep()函数，last_insert_id()函数，等等，这些都跟特定时间的本地环境有关。 Row 模式MySQL V5.1.5 版本开始支持Row模式的 Binlog，它与 Statement 模式的区别在于它不保存具体的 SQL 语句，而是记录具体被修改的信息。比如一条 update 语句更新10条数据，如果是 Statement 模式那就保存一条 SQL 就够，但是 Row 模式会保存每一行分别更新了什么，有10条数据。Row 模式的优缺点就很明显了。保存每一个更改的详细信息必然会带来存储空间的快速膨胀，换来的是事件操作的详细记录。所以要求越高代价越高。 Mixed 模式Mixed 模式即以上两种模式的综合体。既然上面两种模式分别走了极简和一丝不苟的极端，那是否可以区分使用场景的情况下将这两种模式综合起来呢？在 Mixed 模式中，一般的更新语句使用 Statement 模式来保存 Binlog，但是遇到一些函数操作，可能会影响数据准确性的操作则使用 Row 模式来保存。这种方式需要根据每一条具体的 SQL 语句来区分选择哪种模式。MySQL 从 V5.1.8 开始提供 Mixed 模式，V5.7.7 之前的版本默认是Statement 模式，之后默认使用Row模式， 但是在 8.0 以上版本已经默认使用 Mixed 模式了。 查询当前 Binlog 日志使用格式： mysql show global variables like %binlog_format%;+---------------------------------+---------+| Variable_name | Value |+---------------------------------+---------+| binlog_format | MIXED || default_week_format | 0 || information_schema_stats_expiry | 86400 || innodb_default_row_format | dynamic || require_row_format | OFF |+---------------------------------+---------+5 rows in set (0.01 sec) 如何通过 mysqlbinlog 命令手动恢复数据上面说过每一条 event 都有位点信息，如果我们当前的 MySQL 库被无操作或者误删除了，那么该如何通过 Binlog 来恢复到删除之前的数据状态呢？首先发现误操作之后，先停止 MySQL 服务，防止继续更新。接着通过 mysqlbinlog命令对二进制文件进行分析，查看误操作之前的位点信息在哪里。接下来肯定就是恢复数据，当前数据库的数据已经是错的，那么就从开始位置到误操作之前位点的数据肯定的都是正确的；如果误操作之后也有正常的数据进来，这一段时间的位点数据也要备份。比如说：误操作的位点开始值为 501，误操作结束的位置为705，之后到800的位点都是正确数据。那么从 0 - 500 ，706 - 800 都是有效数据，接着我们就可以进行数据恢复了。先将数据库备份并清空。接着使用 mysqlbinlog 来恢复数据：0 - 500 的数据： mysqlbinlog --start-position=0 --stop-position=500 bin-log.000003 /root/back.sql; 上面命令的作用就是将 0 -500 位点的数据恢复到自定义的 SQL 文件中。同理 706 - 800 的数据也是一样操作。之后我们执行这两个 SQL 文件就行了。 Binlog 事件类型上面我们说到了 Binlog 日志中的事件，不同的操作会对应着不同的事件类型，且不同的 Binlog 日志模式同一个操作的事件类型也不同，下面我们一起看看常见的事件类型。首先我们看看源码中的事件类型定义：源码位置：libbinlogeventsincludebinlog_event.h enum Log_event_type /** Every time you update this enum (when you add a type), you have to fix Format_description_event::Format_description_event(). */ UNKNOWN_EVENT= 0, START_EVENT_V3= 1, QUERY_EVENT= 2, STOP_EVENT= 3, ROTATE_EVENT= 4, INTVAR_EVENT= 5, LOAD_EVENT= 6, SLAVE_EVENT= 7, CREATE_FILE_EVENT= 8, APPEND_BLOCK_EVENT= 9, EXEC_LOAD_EVENT= 10, DELETE_FILE_EVENT= 11, /** NEW_LOAD_EVENT is like LOAD_EVENT except that it has a longer sql_ex, allowing multibyte TERMINATED BY etc; both types share the same class (Load_event) */ NEW_LOAD_EVENT= 12, RAND_EVENT= 13, USER_VAR_EVENT= 14, FORMAT_DESCRIPTION_EVENT= 15, XID_EVENT= 16, BEGIN_LOAD_QUERY_EVENT= 17, EXECUTE_LOAD_QUERY_EVENT= 18, TABLE_MAP_EVENT = 19, /** The PRE_GA event numbers were used for 5.1.0 to 5.1.15 and are therefore obsolete. */ PRE_GA_WRITE_ROWS_EVENT = 20, PRE_GA_UPDATE_ROWS_EVENT = 21, PRE_GA_DELETE_ROWS_EVENT = 22, /** The V1 event numbers are used from 5.1.16 until mysql-trunk-xx */ WRITE_ROWS_EVENT_V1 = 23, UPDATE_ROWS_EVENT_V1 = 24, DELETE_ROWS_EVENT_V1 = 25, /** Something out of the ordinary happened on the master */ INCIDENT_EVENT= 26, /** Heartbeat event to be send by master at its idle time to ensure masters online status to slave */ HEARTBEAT_LOG_EVENT= 27, /** In some situations, it is necessary to send over ignorable data to the slave: data that a slave can handle in case there is code for handling it, but which can be ignored if it is not recognized. */ IGNORABLE_LOG_EVENT= 28, ROWS_QUERY_LOG_EVENT= 29, /** Version 2 of the Row events */ WRITE_ROWS_EVENT = 30, UPDATE_ROWS_EVENT = 31, DELETE_ROWS_EVENT = 32, GTID_LOG_EVENT= 33, ANONYMOUS_GTID_LOG_EVENT= 34, PREVIOUS_GTIDS_LOG_EVENT= 35, TRANSACTION_CONTEXT_EVENT= 36, VIEW_CHANGE_EVENT= 37, /* Prepared XA transaction terminal event similar to Xid */ XA_PREPARE_LOG_EVENT= 38, /** Add new events here - right above this comment! Existing events (except ENUM_END_EVENT) should never change their numbers */ ENUM_END_EVENT /* end marker */; 这么多的事件类型我们就不一一介绍，挑出来一些常用的来看看。FORMAT_DESCRIPTION_EVENTFORMAT_DESCRIPTION_EVENT 是 Binlog V4 中为了取代之前版本中的 START_EVENT_V3 事件而引入的。它是 Binlog 文件中的第一个事件，而且，该事件只会在 Binlog 中出现一次。MySQL 根据 FORMAT_DESCRIPTION_EVENT 的定义来解析其它事件。它通常指定了 MySQL 的版本，Binlog 的版本，该 Binlog 文件的创建时间。 QUERY_EVENT QUERY_EVENT 类型的事件通常在以下几种情况下使用： 事务开始时，执行的 BEGIN 操作STATEMENT 格式中的 DML 操作ROW 格式中的 DDL 操作比如上文我们插入一条数据之后的 Binlog 日志： mysql show binlog events in mysql-bin.000002;+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| mysql-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000002 | 125 | Previous_gtids | 1 | 156 | || mysql-bin.000002 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= ANONYMOUS || mysql-bin.000002 | 235 | Query | 1 | 323 | BEGIN || mysql-bin.000002 | 323 | Intvar | 1 | 355 | INSERT_ID=13 || mysql-bin.000002 | 355 | Query | 1 | 494 | use `test_db`; INSERT INTO `test_db`.`test_db`(`name`) VALUES (xdfdf) || mysql-bin.000002 | 494 | Xid | 1 | 525 | COMMIT /* xid=192 */ |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+7 rows in set (0.00 sec) XID_EVENT在事务提交时，不管是 STATEMENT 还 是ROW 格式的 Binlog，都会在末尾添加一个 XID_EVENT 事件代表事务的结束。该事件记录了该事务的 ID，在 MySQL 进行崩溃恢复时，根据事务在 Binlog 中的提交情况来决定是否提交存储引擎中状态为 prepared 的事务。ROWS_EVENT对于 ROW 格式的 Binlog，所有的 DML 语句都是记录在 ROWS_EVENT 中。ROWS_EVENT分为三种：WRITE_ROWS_EVENTUPDATE_ROWS_EVENTDELETE_ROWS_EVENT分别对应 insert，update 和 delete 操作。对于 insert 操作，WRITE_ROWS_EVENT 包含了要插入的数据。对于 update 操作，UPDATE_ROWS_EVENT 不仅包含了修改后的数据，还包含了修改前的值。对于 delete 操作，仅仅需要指定删除的主键（在没有主键的情况下，会给定所有列）。 对比 QUERY_EVENT 事件，是以文本形式记录 DML 操作的。而对于 ROWS_EVENT 事件，并不是文本形式，所以在通过 mysqlbinlog 查看基于 ROW 格式的 Binlog 时，需要指定 -vv –base64-outputdecode-rows。 我们来测试一下，首先将日志格式改为 Rows： mysql set binlog_format=row;Query OK, 0 rows affected (0.00 sec)mysqlmysql flush logs;Query OK, 0 rows affected (0.01 sec) 然后刷新一下日志文件，重新开始一个 Binlog 日志。我们插入一条数据之后看一下日志： mysql show binlog events in binlog.000008;+---------------+-----+----------------+-----------+-------------+--------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+---------------+-----+----------------+-----------+-------------+--------------------------------------+| binlog.000008 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || binlog.000008 | 125 | Previous_gtids | 1 | 156 | || binlog.000008 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= ANONYMOUS || binlog.000008 | 235 | Query | 1 | 313 | BEGIN || binlog.000008 | 313 | Table_map | 1 | 377 | table_id: 85 (test_db.test_db) || binlog.000008 | 377 | Write_rows | 1 | 423 | table_id: 85 flags: STMT_END_F || binlog.000008 | 423 | Xid | 1 | 454 | COMMIT /* xid=44 */ |+---------------+-----+----------------+-----------+-------------+--------------------------------------+7 rows in set (0.01 sec) 说说 redo log 的工作机制？当事务启动时，MySQL 会为该事务分配一个唯一标识符。在事务执行过程中，每次对数据进行修改，MySQL 都会生成一条 Redo Log，记录修改前后的数据状态。这些 Redo Log 首先会被写入内存中的 Redo Log Buffer。当事务提交时，MySQL 再将 Redo Log Buffer 中的记录刷新到磁盘上的 Redo Log 文件中。只有当 Redo Log 成功写入磁盘，事务才算真正提交成功。当 MySQL 崩溃重启时，会先检查 Redo Log。对于已提交的事务，MySQL 会重放 Redo Log 中的记录。对于未提交的事务，MySQL 会通过 Undo Log 回滚这些修改，确保数据恢复到崩溃前的一致性状态。Redo Log 是循环使用的，当文件写满后会覆盖最早的记录。为避免覆盖未持久化的记录，MySQL 会定期执行 CheckPoint 操作，将内存中的数据页刷新到磁盘，并记录 CheckPoint 点。 重启时，MySQL 只会重放 CheckPoint 之后的 Redo Log，从而提高恢复效率。 省流版: 事务开始 记录undo log（旧数据） 修改Buffer Pool中的数据 写入redo log（prepare状态） 写入binlog 提交事务（redo log标记为commit） 后台异步刷脏页到磁盘 redo log 文件的大小是固定的吗？redo log 文件是固定大小的，通常配置为一组文件，使用环形方式写入，旧的日志会在空间需要时被覆盖。 命名方式为 ib_logfile0、iblogfile1、、、iblogfilen。默认 2 个文件，每个文件大小为 48MB。可以通过 show variables like innodb_log_file_size; 查看 redo log 文件的大小；通过 show variables like innodb_log_files_in_group; 查看 redo log 文件的数量。 说一说WAL?WAL——Write-Ahead Logging。 预写日志是 InnoDB 实现事务持久化的核心机制，它的思想是：先写日志再刷磁盘。即在修改数据页之前，先将修改记录写入 Redo Log。这样的话，即使数据页尚未写入磁盘，系统崩溃时也能通过 Redo Log 恢复数据。—-这部分是帮助理解 start，面试中可不背—-解释一下为什么需要 WAL：数据最终是要写入磁盘的，但磁盘 IO 很慢；如果每次更新都立刻把数据页刷盘，性能很差；如果还没写入磁盘就宕机，事务会丢失。WAL 的好处是更新时不直接写数据页，而是先写一份变更记录到 redo log，后台再慢慢把真正的数据页刷盘，一举多得。—-这部分是帮助理解 end，面试中可不背—- 29.binlog 和 redo log 有什么区别？binlog 由 MySQL 的 Server 层实现，与存储引擎无关；redo log 由 InnoDB 存储引擎实现。 binlog 记录的是逻辑日志，包括原始的 SQL 语句或者行数据变化，例如“将 id2 这行数据的 age 字段+1”。redo log 记录物理日志，即数据页的具体修改，例如“将 page_id123 上 offset0x40 的数据从 18 修改为 26”。binlog 是追加写入的，文件写满后会新建文件继续写入，不会覆盖历史日志，保存的是全量操作记录；redo log 是循环写入的，空间是固定的，写满后会覆盖旧的日志，仅保存未刷盘的脏页日志，已持久化的数据会被清除。另外，为保证两种日志的一致性，innodb 采用了两阶段提交策略，redo log 在事务执行过程中持续写入，并在事务提交前进入 prepare 状态；binlog 在事务提交的最后阶段写入，之后 redo log 会被标记为 commit 状态。可以通过回放 binlog 实现数据同步或者恢复到指定时间点；redo log 用来确保事务提交后即使系统宕机，数据仍然可以通过重放 redo log 恢复。 30.为什么要两阶段提交?为了保证 redo log 和 binlog 中的数据一致性，防止主从复制和事务状态不一致。 为什么 2PC 能保证 redo log 和 binlog 的强⼀致性？假如 MySQL 在预写 redo log 之后、写入 binlog 之前崩溃。那么 MySQL 重启后 InnoDB 会回滚该事务，因为 redo log 不是提交状态。并且由于 binlog 中没有写入数据，所以从库也不会有该事务的数据。 假如 MySQL 在写入 binlog 之后、redo log 提交之前崩溃。那么 MySQL 重启后 InnoDB 会提交该事务，因为 redo log 是提交状态。并且由于 binlog 中有写入数据，所以从库也会同步到该事务的数据。伪代码如下: // 事务开始begin;// try // 执行 SQL execute SQL; // 写入 redo log 并标记为 prepare write redo log prepare xid; // 写入 binlog write binlog xid sql; // 提交 redo log commit redo log xid;// catch // 回滚 redo log innodb rollback redo log xid;// 事务结束end; XID 了解吗？XID 是 binlog 中用来标识事务提交的唯一标识符。 在事务提交时，会写入一个 XID_EVENT 到 binlog，表示这个事务真正完成了。 Log_name | Pos | Event_type | Server_id | End_log_pos | Info | mysql-bin.000003 | 2005 | Gtid | 1013307 | 2070 | SET @@SESSION.GTID_NEXT= f971d5f1-d450-11ec-9e7b-5254000a56df:11 || mysql-bin.000003 | 2070 | Query | 1013307 | 2142 | BEGIN || mysql-bin.000003 | 2142 | Table_map | 1013307 | 2187 | table_id: 109 (test.t1) || mysql-bin.000003 | 2187 | Write_rows | 1013307 | 2227 | table_id: 109 flags: STMT_END_F || mysql-bin.000003 | 2227 | Xid | 1013307 | 2258 | COMMIT /* xid=121 */ 它不仅用于主从复制中事务完整性的判断，也在崩溃恢复中对 redo log 和 binlog 的一致性校验起到关键作用。 XID 可以帮助 MySQL 判断哪些 redo log 是已提交的，哪些是未提交需要回滚的，是两阶段提交机制中非常关键的一环。 31.redo log 的写入过程了解吗？InnoDB 会先将 Redo Log 写入内存中的 Redo Log Buffer，之后再以一定的频率刷入到磁盘的 Redo Log File 中。 哪些场景会触发 redo log 的刷盘动作？比如说 Redo Log Buffer 的空间不足时，事务提交时，触发 Checkpoint 时，后台线程定期刷盘时。不过，Redo Log Buffer 刷盘到 Redo Log File 还会涉及到操作系统的磁盘缓存策略，可能不会立即刷盘，而是等待一定时间后才刷盘。 innodb_flush_log_at_trx_commit 参数你了解多少？innodb_flush_log_at_trx_commit 参数是用来控制事务提交时，Redo Log 的刷盘策略，一共有三种。 0 表示事务提交时不刷盘，而是交给后台线程每隔 1 秒执行一次。这种方式性能最好，但是在 MySQL 宕机时可能会丢失一秒内的事务。 1 表示事务提交时会立即刷盘，确保事务提交后数据就持久化到磁盘。这种方式是最安全的，也是 InnoDB 的默认值。 2 表示事务提交时只把 Redo Log Buffer 写入到 Page Cache，由操作系统决定什么时候刷盘。操作系统宕机时，可能会丢失一部分数据。 一个没有提交事务的 redo log，会不会刷盘？InnoDB 有一个后台线程，每隔 1 秒会把Redo Log Buffer中的日志写入到文件系统的缓存中，然后调用刷盘操作。 因此，一个没有提交事务的 Redo Log 也可能会被刷新到磁盘中。另外，如果当 Redo Log Buffer 占用的空间即将达到 innodb_log_buffer_size 的一半时，也会触发刷盘操作。 Redo Log Buffer 是顺序写还是随机写？MySQL 在启动后会向操作系统申请一块连续的内存空间作为 Redo Log Buffer，并将其分为若干个连续的 Redo Log Block。那为了提高写入效率，Redo Log Buffer 采用了顺序写入的方式，会先往前面的 Redo Log Block 中写入，当写满后再往后面的 Block 中写入。于此同时，InnoDB 还提供了一个全局变量 buf_free，来控制后续的 redo log 记录应该写入到 block 中的哪个位置。 buf_next_to_write 了解吗？buf_next_to_write 指向 Redo Log Buffer 中下一次需要写入硬盘的起始位置。 而 buf_free 指向的是 Redo Log Buffer 中空闲区域的起始位置。 了解 MTR 吗？Mini Transaction 是 InnoDB 内部用于操作数据页的原子操作单元。 mtr_t mtr;mtr_start(mtr);// 1. 加锁// 对待访问的index加锁mtr_s_lock(rw_lock_t, mtr);mtr_x_lock(rw_lock_t, mtr);// 对待读写的page加锁mtr_memo_push(mtr, buf_block_t, MTR_MEMO_PAGE_S_FIX);mtr_memo_push(mtr, buf_block_t, MTR_MEMO_PAGE_X_FIX);// 2. 访问或修改pagebtr_cur_search_to_nth_levelbtr_cur_optimistic_insert// 3. 为修改操作生成redomlog_openmlog_write_initial_log_record_fastmlog_close// 4. 持久化redo，解锁mtr_commit(mtr); 多个事务的 Redo Log 会以 MTR 为单位交替写入到 Redo Log Buffer 中，假如事务 1 和事务 2 均有两个 MTR，一旦某个 MTR 结束，就会将其生成的若干条 Redo Log 记录顺序写入到 Redo Log Buffer 中。 也就是说，一个 MTR 会包含一组 Redo Log 记录，是 MySQL 崩溃后恢复事务的最小执行单元。 Redo Log Block 的结构了解吗？Redo Log Block 由日志头、日志体和日志尾组成，一共占用 512 个字节，其中日志头占用 12 个字节，日志尾占用 4 个字节，剩余的 496 个字节用于存储日志体。日志头包含了当前 Block 的序列号、第一条日志的序列号、类型等信息。日志尾主要存储的是 LOG_BLOCK_CHECKSUM，也就是 Block 的校验和，主要用于判断 Block 是否完整。 Redo Log Block 为什么设计成 512 字节？因为机械硬盘的物理扇区大小通常为 512 字节，Redo Log Block 也设计为同样的大小，就可以确保每次写入都是整数个扇区，减少对齐开销。 比如说操作系统的页缓存默认为 4KB，8 个 Redo Log Block 就可以组合成一个页缓存单元，从而提升 Redo Log Buffer 的写入效率。 LSN 了解吗？Log Sequence Number 是一个 8 字节的单调递增整数，用来标识事务写入 redo log 的字节总量，存在于 redo log、数据页头部和 checkpoint 中。 —-这部分是帮助理解 start，面试中可不背—-MySQL 在第一次启动时，LSN 的初始值并不为 0，而是 8704；当 MySQL 再次启动时，会继续使用上一次服务停止时的 LSN。 在计算 LSN 的增量时，不仅需要考虑 log block body 的大小，还需要考虑 log block header 和 log block tail 中部分字节数。 比如说在上图中，事务 3 的 MTR 总量为 300 字节，那么写入到 Redo Log Buffer 中的 LSN 会增长为 8704 + 300 + 12 9016。 假如事务 4 的 MTR 总量为 900 字节，那么再次写入到 Redo Log Buffer 中的 LSN 会增长为 9016 + 900 + 122 + 42 9948。 2 个 12 字节的 log block header + 2 个 4 字节的 log block tail。 —-这部分是帮助理解 end，面试中可不背—- 核心作用有三个： 第一，redo log 按照 LSN 递增顺序记录所有数据的修改操作。LSN 的递增量等于每次写入日志的字节数。 第二，InnoDB 的每个数据页头部中，都会记录该页最后一次刷新到磁盘时的 LSN。如果数据页的 LSN 小于 redo log 的 LSN，说明该页需要从日志中恢复；否则说明该页已更新。 第三，checkpoint 通过 LSN 记录已刷新到磁盘的数据页位置，减少恢复时需要处理的日志。 —-这部分是帮助理解 start，面试中可不背—- 可以通过 show engine innodb status; 查看当前的 LSN 信息。 Log sequence number：当前系统最大 LSN（已生成的日志总量）。 Log flushed up to：已写入磁盘的 redo log LSN。 Pages flushed up to：已刷新到数据页的 LSN。 Last checkpoint at：最后一次检查点的 LSN，表示已持久化的数据状态。 —-这部分是帮助理解 end，面试中可不背—- Checkpoint 了解多少？Checkpoint 是 InnoDB 为了保证事务持久性和回收 redo log 空间的一种机制。 它的作用是在合适的时机将部分脏页刷入磁盘，比如说 buffer pool 的容量不足时。并记录当前 LSN 为 Checkpoint LSN，表示这个位置之前的 redo log file 已经安全，可以被覆盖了。 MySQL 崩溃恢复时只需要从 Checkpoint 之后开始恢复 redo log 就可以了，这样可以最大程度减少恢复所花费的时间。 redo log file 的写入是循环的，其中有两个标记位置非常重要，也就是 Checkpoint 和 write pos。 write pos 是 redo log 当前写入的位置，Checkpoint 是可以被覆盖的位置。 当 write pos 追上 Checkpoint 时，表示 redo log 日志已经写满。这时候就要暂停写入并强制刷盘，释放可覆写的日志空间。 关于redo log 的调优参数了解多少？如果是高并发写入的电商系统，可以最大化写入吞吐量，容忍秒级数据丢失的风险。 innodb_flush_log_at_trx_commit = 2sync_binlog = 1000innodb_redo_log_capacity = 64Ginnodb_io_capacity = 5000innodb_lru_scan_depth = 512innodb_log_buffer_size = 256M 如果是金融交易系统，需要保证数据零丢失，接受较低的吞吐量。 innodb_flush_log_at_trx_commit = 1sync_binlog = 1innodb_redo_log_capacity = 32Ginnodb_io_capacity = 2000innodb_lru_scan_depth = 1024 核心参数一览表: 总结 对数据一致性要求高的场景，如金融交易使用innodb_flush_log_at_trx_commit1，对写入吞吐量敏感的场景，如日志采集可以使用 2 或 0，需要结合 sync_binlog 参数 sync_binlog 参数控制 binlog 的刷盘策略，可以设置为 0、1、N，0 表示依赖系统刷盘，1 表示每次事务提交都刷盘（推荐与 innodb_flush_log_at_trx_commit1 搭配），N1000 表示累计 1000 次事务后刷盘 innodb_redo_log_capacity 动态调整 Redo Log 总容量，可以根据业务负载情况调整，建议设置为 1 小时写入量的峰值（如每秒 10MB 写入则设为 36GB） innodb_io_capacity 定义 InnoDB 后台线程的每秒 IO 操作上限，直接影响脏页刷新速率；机械硬盘建议 200-500，SSD 建议 1000-2000，NVMe SSD 可设为 5000+ innodb_lru_scan_depth 控制每个缓冲池实例中 LRU 列表的扫描深度，决定每秒可刷新的脏页数量，默认值 1024 适用于多数场景，IO 密集型负载可适当降低（如 512），减少 CPU 开销。 SQL优化🌟32.什么是慢 SQL？拓展阅读: https://juejin.cn/post/7048974570228809741MySQL 中有一个叫long_query_time的参数，原则上执行时间超过该参数值的 SQL 就是慢 SQL，会被记录到慢查询日志中。 —-这部分是帮助理解 start，面试中可不背—- 可通过 show variables like ‘long_query_time’; 查看当前的 long_query_time 的参数值。—-这部分是帮助理解 end，面试中可不背—- SQL 的执行过程了解吗？SQL 的执行过程大致可以分为六个阶段：连接管理、语法解析、语义分析、查询优化、执行器调度、存储引擎读写等。Server 层负责理解和规划 SQL 怎么执行，存储引擎层负责数据的真正读写。 —-这部分是帮助理解 start，面试中可不背—- 来详细拆解一下： 客户端发送 SQL 语句给 MySQL 服务器。 如果查询缓存打开则会优先查询缓存，缓存中有对应的结果就直接返回。不过，MySQL 8.0 已经移除了查询缓存。这部分的功能正在被 Redis 等缓存中间件取代。 分析器对 SQL 语句进行语法分析，判断是否有语法错误。 搞清楚 SQL 语句要干嘛后，MySQL 会通过优化器生成执行计划。 执行器调用存储引擎的接口，执行 SQL 语句。 SQL 执行过程中，优化器通过成本计算预估出执行效率最高的方式，基本的预估维度为： IO 成本：从磁盘读取数据到内存的开销。 CPU 成本：CPU 处理内存中数据的开销。 基于这两个维度，可以得出影响 SQL 执行效率的因素有： ①、IO 成本，数据量越大，IO 成本越高。所以要尽量查询必要的字段；尽量分页查询；尽量通过索引加快查询。 ②、CPU 成本，尽量避免复杂的查询条件，如有必要，考虑对子查询结果进行过滤。 —-这部分是帮助理解 end，面试中可不背—- 如何优化慢SQL?首先，需要找到那些比较慢的 SQL，可以通过启用慢查询日志，记录那些超过指定执行时间的 SQL 查询。 也可以使用 show processlist; 命令查看当前正在执行的 SQL 语句，找出执行时间较长的 SQL。 或者在业务基建中加入对慢 SQL 的监控，常见的方案有字节码插桩、连接池扩展、ORM 框架扩展等。 然后，使用 EXPLAIN 查看慢 SQL 的执行计划，看看有没有用索引，大部分情况下，慢 SQL 的原因都是因为没有用到索引。 EXPLAIN SELECT * FROM your_table WHERE conditions;最后，根据分析结果，通过添加索引、优化查询条件、减少返回字段等方式进行优化。 慢sql日志怎么开启？编辑 MySQL 的配置文件 my.cnf，设置 slow_query_log 参数为 1。 slow_query_log = 1slow_query_log_file = /var/log/mysql/slow.loglong_query_time = 2 # 记录执行时间超过2秒的查询 然后重启 MySQL 就好了。 也可以通过 set global 命令动态设置。 SET GLOBAL slow_query_log = ON;SET GLOBAL slow_query_log_file = /var/log/mysql/slow.log;SET GLOBAL long_query_time = 2; 🌟33.你知道哪些方法来优化 SQL？SQL 优化的方法非常多，但本质上就一句话：尽可能少地扫描、尽快地返回结果。最常见的做法就是加索引、改写 SQL 让它用上索引，比如说使用覆盖索引、让联合索引遵守最左前缀原则等。 如何利用覆盖索引？覆盖索引的核心是“查询所需的字段都在同一个索引里”，这样 MySQL 就不需要回表，直接从索引中返回结果。 实际使用中，我会优先考虑把 WHERE 和 SELECT 涉及的字段一起建联合索引，并通过 EXPLAIN 观察结果是否有 Using index，确认命中索引。 —-这部分是帮助理解 start，面试中可不背—- 举个例子，现在要从 test 表中查询 city 为上海的 name 字段。 select name from test where city=上海 如果仅在 city 字段上添加索引，那么这条查询语句会先通过索引找到 city 为上海的行，然后再回表查询 name 字段。 为了避免回表查询，可以在 city 和 name 字段上建立联合索引，这样查询结果就可以直接从索引中获取。 alter table test add index index1(city,name); 相当于利用空间换时间,把查询结果都放到了索引里,不需要回表查询。—-这部分是帮助理解 end，面试中可不背—- 如何正确使用联合索引？使用联合索引最重要的一条是遵守最左前缀原则，也就是查询条件需要从索引的左侧字段开始。 —-这部分是帮助理解 start，面试中可不背—-比如说我们创建了一个三列的联合索引。 CREATE INDEX idx_name_age_sex ON user(name, age, sex); 我们来看一下什么样的查询条件可以用到这个索引：—-这部分是帮助理解 end，面试中可不背—- 如何进行分页优化？分页优化的核心是避免深度偏移(Deep Offset)带来的全表扫描，可以通过两种方式来优化：延迟关联和添加书签。 延迟关联适用于需要从多个表中获取数据且主表行数较多的情况。它首先从索引表中检索出需要的行 ID，然后再根据这些 ID 去关联其他的表获取详细信息。 SELECT e.id, e.name, d.detailsFROM employees eJOIN department d ON e.department_id = d.idORDER BY e.idLIMIT 1000, 20; 延迟关联后，第一步只查主键，速度快，第二步只处理 20 条数据，效率高。 SELECT e.id, e.name, d.detailsFROM ( SELECT id FROM employees ORDER BY id LIMIT 1000, 20) AS subJOIN employees e ON sub.id = e.idJOIN department d ON e.department_id = d.id; 添加书签的方式是通过记住上一次查询返回的最后一行主键值，然后在下一次查询的时候从这个值开始，从而跳过偏移量计算，仅扫描目标数据，适合翻页、资讯流等场景。 假设需要对用户表进行分页。 SELECT id, nameFROM usersORDER BY idLIMIT 1000, 20; 通过添加书签来优化后，查询不再使用OFFSET，而是从上一页最后一个用户的 ID 开始查询。这种方法可以有效避免不必要的数据扫描，提高了分页查询的效率。 SELECT id, nameFROM usersWHERE id last_max_id -- 假设last_max_id是上一页最后一行的IDORDER BY idLIMIT 20; 为什么分页会变慢？分页查询的效率问题主要是由于 OFFSET 的存在，OFFSET 会导致 MySQL 必须扫描和跳过 offset + limit 条数据，这个过程是非常耗时的。 比如说，我们要查询第 100000 条数据，那么 MySQL 就必须扫描 100000 条数据，然后再返回 10 条数据。 SELECT * FROM user ORDER BY id LIMIT 100000, 10; 数据越多、偏移越大，就越慢！ JOIN 代替子查询有什么好处？第一，JOIN 的 ON 条件能更直接地触发索引，而子查询可能因嵌套导致索引失效。第二，JOIN 的一次连接操作替代了子查询的多次重复执行，尤其在大数据量的情况下性能差异明显。 —-这部分是帮助理解 start，面试中可不背—- 比如说我们有两个表 orders 和 customers。 CREATE TABLE orders ( order_id INT PRIMARY KEY, customer_id INT, amount DECIMAL(10,2), INDEX idx_customer_id (customer_id) -- customer_id字段有索引);CREATE TABLE customers ( customer_id INT PRIMARY KEY, name VARCHAR(100)); 子查询的写法： SELECT o.order_id, o.amount, (SELECT c.name FROM customers c WHERE c.customer_id = o.customer_id) AS customer_nameFROM orders o; JOIN 的写法： SELECT o.order_id, o.amount, c.name AS customer_nameFROM orders oJOIN customers c ON o.customer_id = c.customer_id; 对于子查询，执行流程是这样的： 外层 orders 表的每一行都会触发一次子查询。 如果 orders 表有 1000 条记录，则子查询会执行 1000 次。 每次子查询都需要单独查询 customers 表（即使 customer_id 相同）。 而 JOIN 的执行流程是这样的： 数据库优化器会将两张表的连接操作合并为一次执行。 通过索引（如 orders.customer_id 和 customers.customer_id）快速关联数据。 仅执行一次关联操作，而非多次子查询。来看一下子查询的执行计划：EXPLAIN SELECT o.order_id, (SELECT c.name FROM customers c WHERE c.customer_id = o.customer_id) FROM orders o; 子查询（DEPENDENT SUBQUERY）类型表明其依赖外层查询的每一行，导致重复执行。 再对比看一下 JOIN 的执行计划： EXPLAIN SELECT o.order_id, (SELECT c.name FROM customers c WHERE c.customer_id = o.customer_id) FROM orders o; JOIN 通过 eq_ref 类型直接利用主键（customers.customer_id）快速关联，减少扫描次数。 JOIN操作为什么要小表驱动大表？ 第一，如果大表的 JOIN 字段有索引，那么小表的每一行都可以通过索引快速匹配大表。时间复杂度为**小表行数 N 乘以大表索引查找复杂度 log(大表行数 M)**，总复杂度为 N*log(M)。显然小表做驱动表比大表做驱动表的时间复杂度 M*log(N) 更低。 第二，如果大表没有索引，需要将小表的数据加载到内存，再全表扫描大表进行匹配。时间复杂度为小表分段数 K 乘以大表行数 M，其中 K 小表行数 N 内存大小 join_buffer_size。显然小表做驱动表的时候 K 的值更小，大表做驱动表的时候需要多次分段。 -- 小表驱动（高效）SELECT * FROM small_table sJOIN large_table l ON s.id = l.id; -- l.id有索引-- 大表驱动（低效）SELECT * FROM large_table lJOIN small_table s ON l.id = s.id; -- s.id无索引 当使用 left join 时，左表是驱动表，右表是被驱动表。 当使用 right join 时，刚好相反。 当使用 join 时，MySQL 会选择数据量比较小的表作为驱动表，大表作为被驱动表。 这里的小表指实际参与 JOIN 的数据量，而不是表的总行数。大表经过 where 条件过滤后也可能成为逻辑小表。– 实际参与JOIN的数据量决定小表 SELECT * FROM large_table lJOIN small_table s ON l.id = s.idWHERE l.created_at 2025-01-01; -- l经过过滤后可能成为小表 也可以强制通过 STRAIGHT_JOIN 提示 MySQL 使用指定的驱动表。 explain select table_1.col1, table_2.col2, table_3.col2from table_1straight_join table_2 on table_1.col1=table_2.col1straight_join table_3 on table_1.col1 = table_3.col1;explain select straight_join table_1.col1, table_2.col2, table_3.col2from table_1join table_2 on table_1.col1=table_2.col1join table_3 on table_1.col1 = table_3.col1; 为什么要避免使用 JOIN 关联太多的表？第一，多表 JOIN 的执行路径会随着表的数量呈现指数级增长，优化器需要估算所有路径的成本，有可能会导致出现大表驱动小表的情况。 SELECT * FROM AJOIN B ON A.id = B.a_idJOIN C ON B.id = C.b_idJOIN D ON C.id = D.c_idJOIN E ON D.id = E.d_id; -- 5 个表，优化器需评估 5! = 120 种顺序 第二，多表 JOIN 需要缓存中间结果集，可能超出 join_buffer_size，这种情况下内存临时表就会转为磁盘临时表，性能也会急剧下降。《阿里巴巴 Java 开发手册》上就规定，不要使用 join 关联太多的表，最多不要超过 3 张表。 如何进行排序优化？第⼀，对 ORDER BY 涉及的字段创建索引，避免 filesort。 -- 优化前（可能触发 filesort）SELECT * FROM users ORDER BY age DESC;-- 优化后（添加索引）ALTER TABLE users ADD INDEX idx_age (age); 如果是多个字段，联合索引需要保证ORDER BY 的列是索引的最左前缀。 -- 联合索引需与 ORDER BY 顺序⼀致（age 在前，name 在后）ALTER TABLE users ADD INDEX idx_age_name (age, name);-- 有效利⽤索引的查询SELECT * FROM users ORDER BY age, name;-- ⽆效案例（索引失效，因 name 在索引中排在 age 之后）SELECT * FROM users ORDER BY name, age; 第⼆，可以适当调整排序参数，如增⼤ sort_buffer_size、max_length_for_sort_data 等，让排序在内存中完成。—-这部分是帮助理解 start，⾯试中可不背—- sort_buffer_size：用于控制排序缓冲区的大小，默认为 256KB。也就是说，如果排序的数据量小于 256KB，MySQL 会在内存中直接排序；否则就要在磁盘上进行 filesort。 max_length_for_sort_data：单行数据的最大长度，会影响排序算法选择。如果单行数据超过该值，MySQL 会使用双路排序，否则使用单路排序。 max_sort_length：限制字符串排序时比较的前缀长度。当 MySQL 不得不对 text、blob 字段进行排序时，会截取前 max_sort_length 个字符进行比较。—-这部分是帮助理解 end，面试中可不背—-第三，可以通过 where 和 limit 限制待排序的数据量，减少排序的开销。-- 优化前SELECT * FROM users ORDER BY age LIMIT 100;-- 优化后（减少数据传输和排序开销）SELECT id, name, age FROM users ORDER BY age LIMIT 100;-- 深度分页优化（避免 OFFSET 扫描全表）SELECT * FROM users ORDER BY age LIMIT 10000, 20; -- 低效SELECT * FROM users WHERE age last_age ORDER BY age LIMIT 20; -- 高效（记录上一页最后一条的 age 值） 什么是 filesort？Mysql如何执行ORDER BY 当不能使用索引生成排序结果的时候，MySQL 需要自己进行排序，如果数据量比较小，会在内存中进行；如果数据量比较大就需要写临时文件到磁盘再排序，我们将这个过程称为文件排序。 —-这部分是帮助理解 start，面试中可不背—-让我们来验证一下 filesort 的情况 能够看得出来，当 order by id 也就是主键的时候，没有触发 filesort；当 order by age 的时候，由于没有索引，就触发了 filesort。—-这部分是帮助理解 end，面试中可不背—- 全字段排序和 rowid 排序了解多少？当排序字段是索引字段且满足最左前缀原则时，MySQL 可以直接利用索引的有序性完成排序。 当无法使用索引排序时，MySQL 需要在内存或磁盘中进行排序操作，分为全字段排序和 rowid 排序两种算法。 全字段排序会一次性取出满足条件行的所有字段，然后在 sort buffer 中进行排序，排序后直接返回结果，无需回表。 以 SELECT * FROM user WHERE name = 王二 ORDER BY age 为例： 从 name 索引中找到第一个满足 name’张三’ 的主键 id；根据主键 id 取出整行所有的字段，存入 sort buffer；重复上述过程直到处理完所有满足条件的行对 sort buffer 中的数据按 age 排序，返回结果。 优点是仅需要一次磁盘 IO 缺点是内存占用大，如果数量超过 sort buffer 的话，需要分片读取并借助临时文件合并排序，IO 次数反而会增加。也无法处理包含 text 和 blob 类型的字段。 rowid 排序分为两个阶段： 第一阶段：根据查询条件取出排序字段和主键 ID，存入sort buffer进行排序； 第二阶段：根据排序后的主键 ID 回表取出其他需要的字段。 同样以 SELECT * FROM user WHERE name = 王二 ORDER BY age 为例： 从 name 索引中找到第一个满足 name’张三’ 的主键 id； 根据主键 id 取出排序字段 age，连同主键 id 一起存入 sort buffer； 重复上述过程直到处理完所有满足条件的行 对 sort buffer 中的数据按 age 排序； 遍历排序后的主键 id，回表取出其他所需字段，返回结果。 优点是内存占用较少，适合字段多或者数据量大的场景，缺点是需要两次磁盘 IO。 MySQL 会根据系统变量 max_length_for_sort_data 和查询字段的总大小来决定使用全字段排序还是 rowid 排序。 如果查询字段总长度 max_length_for_sort_data，MySQL 会使用全字段排序；否则会使用 rowid 排序。 你对 Sort_merge_passes 参数了解吗？深入了解 MySQL Order By 文件排序Sort_merge_passes 是一个状态变量，用于统计 MySQL 在执行排序操作时进行归并排序的次数。当 MySQL 需要进行排序但排序数据无法完全放入 sort_buffer_size 定义的内存缓冲区时，就会使用临时文件进行外部排序，这时就会产生 Sort_merge_passes。 如果 Sort_merge_passes 在短时间内快速激增，说明排序操作的数据量较大，需要调整 sort_buffer_size 或者优化查询语句。 MySQL 在执行排序操作时，会经历两个过程： 内存排序阶段，MySQL 首先尝试在 sort buffer 中进行排序。如果数据量小于 sort_buffer_size 缓冲区大小，会完全在内存中完成快速排序。 外部排序阶段，如果数据量超过 sort_buffer_size，MySQL 会将数据分成多个块，每块单独排序后写入临时文件，然后对这些已排序的块进行归并排序。每次归并操作都会增加 Sort_merge_passes 的计数。 条件下推你了解多少？条件下推的核心思想是将外层的过滤条件，比如说 where、join 等，尽可能地下推到查询计划的更底层，比如说子查询、连接操作之前，从而减少中间结果的数据量。 比如说原始查询是： SELECT * FROM ( SELECT * FROM orders WHERE total 100) AS subqueryWHERE subquery.status = shipped; 就可以将条件下推到子查询： SELECT * FROM ( SELECT * FROM orders WHERE total 100 AND status = shipped) AS subquery; 这样就可以减少查询返回的数据量，避免外层再过滤。 再比如说 union 中的原始查询是： (SELECT * FROM t1) UNION ALL (SELECT * FROM t2)ORDER BY col LIMIT 10; 就可以将条件下推到每个子查询： (SELECT * FROM t1 ORDER BY col LIMIT 10)UNION ALL (SELECT * FROM t2 ORDER BY col LIMIT 10); 每个子查询仅返回前 10 条数据，减少临时表的数据量。 再比如说连接查询 join 中的原始查询是： SELECT * FROM ordersJOIN customers ON orders.customer_id = customers.idWHERE customers.country = china; 就可以将条件下推到表扫描的时候： SELECT * FROM ordersJOIN ( SELECT * FROM customers WHERE country = china) AS filtered_customersON orders.customer_id = filtered_customers.id; 先过滤 customers 表，减少 join 时的数据量。 为什么要尽量避免使用 select *？SELECT * 会强制 MySQL 读取表中所有字段的数据，包括应用程序可能并不需要的，比如 text、blob 类型的大字段。加载冗余数据会占用更多的缓存空间，从而挤占其他重要数据的缓存资源，降低整体系统的吞吐量。也会增加网络传输的开销，尤其是在大字段的情况下。最重要的是，SELECT * 可能会导致覆盖索引失效，本来可以走索引的查询最后变成了全表扫描。 -- 使用覆盖索引（假设索引为 idx_country）SELECT id, country FROM users WHERE country = china; -- 可能仅扫描索引-- 使用 SELECT *SELECT * FROM users WHERE country = china; -- 需回表读取所有列 你还知道哪些 SQL 优化方法？①、避免使用 != 或者 操作符 ! 或者 操作符会导致 MySQL 无法使用索引，从而导致全表扫描。 可以把column’aaa’，改成column’aaa’ or column’aaa’。 ②、使用前缀索引 比如，邮箱的后缀一般都是固定的@xxx.com，那么类似这种后面几位为固定值的字段就非常适合定义为前缀索引：alter table test add index index2(email(6));需要注意的是，MySQL 无法利用前缀索引做 order by 和 group by 操作。 ③、避免在列上使用函数 在 where 子句中直接对列使用函数会导致索引失效，因为 MySQL 需要对每行的列应用函数后再进行比较。select name from test where date_format(create_time,%Y-%m-%d)=2021-01-01;可以改成： select name from test where create_time=2021-01-01 00:00:00 and create_time2021-01-02 00:00:00; 通过日期的范围查询，而不是在列上使用函数，可以利用 create_time 上的索引。 34.🌟explain平常有用过吗？经常用，explain 是 MySQL 提供的一个用于查看 SQL 执行计划的工具，可以帮助我们分析查询语句的性能问题。 一共有 10 来个输出参数。 比如说 typeALL,keyNULL 表示 SQL 正在全表扫描，可以考虑为 where 字段添加索引进行优化；ExtraUsing filesort 表示 SQL 正在文件排序，可以考虑为 order by 字段添加索引。 使用方式也非常简单，直接在 select 前加上 explain 关键字就可以了。 explain select * from students where name=王二;更高级的用法可以配合 formatjson 参数，将 explain 的输出结果以 JSON 格式返回。explain format=json select * from students where name=王二; explain 输出结果中常见的字段含义理解吗？在 EXPLAIN 输出结果中我最关注的字段是 type、key、rows 和 Extra。 我会通过它们判断 SQL 有没有走索引、是否全表扫描、预估扫描行数是否太大，以及是否触发了 filesort 或临时表。一旦发现问题，比如 typeALL 或者 ExtraUsing filesort，我会考虑建索引、改写 SQL 或控制查询结果集来做优化。 —-这部分是帮助理解 start，面试中可不背—-以 EXPLAIN SELECT * FROM orders WHERE user_id = 100 的输出为例： 非表格版本：①、id 列：查询的执行顺序编号。id 相同：同一执行层级，按 table 列从上到下顺序执行（如多表 JOIN）；id 递增：嵌套子查询，数值越大优先级越高，越先执行。 EXPLAIN SELECT * FROM t1 JOIN (SELECT * FROM t2 WHERE id = 1) AS sub; t2 子查询的 id2，优先执行。 ②、select_type 列：查询的类型。常见的类型有： SIMPLE：简单查询，不包含子查询或者 UNION。 PRIMARY：查询中如果包含子查询，则最外层查询被标记为 PRIMARY。需要关注子查询或派生表性能。 SUBQUERY：子查询；需要避免多层嵌套，尽量改写为 JOIN。 DERIVED：派生表（FROM 子句中的子查询）。需要减少派生表数据量，或物化为临时表。 ③、table 列：查的哪个表。 derivedN：表示派生表（N 对应 id）。 unionNM,N：表示 UNION 合并的结果（M、N 为参与 UNION 的 id）。 ④、type 列：表示 MySQL 在表中找到所需行的方式。 system，表仅有一行（系统表或衍生表），无需优化。 const：通过主键或唯一索引找到一行（如 WHERE id 1）。理想情况。 eq_ref：对主键唯一索引 JOIN 匹配（如 A JOIN B ON A.id B.id）。确保 JOIN 字段有索引。 ref：非唯一索引匹配（如 WHERE name ‘王二’，name 有普通索引）。 range：只检索给定范围的行，使用索引来检索。在where语句中使用 bettween…and、、、、in 等条件查询 type 都是 range。 index：全索引扫描，如果不需要回表，可接受；否则考虑覆盖索引。 ALL：全表扫描，效率最低。⑤、possible_keys 列：可能会用到的索引，但并不一定实际被使用。 ⑥、key 列：实际使用的索引。如果为 NULL，则没有使用索引。如果为 PRIMARY，则使用了主键索引。 ⑦、key_len 列：使用的索引字节数，反映索引列的利用率。使用联合索引 (a, b)，key_len 是 a 和 b 的字节总和（仅当查询条件用到 a 或 a+b 时有效）。 – 表结构：CREATE TABLE t (a INT, b VARCHAR(20), INDEX idx_a_b (a, b));EXPLAIN SELECT * FROM t WHERE a 1 AND b ‘test’;key_len 4（INT） + 20*3（utf8） + 2 66 字节。 ⑧、ref 列：与索引列比较的值或列。 const：常量。例如 WHERE column ‘value’。 func：函数。例如 WHERE column func(column)。 ⑨、rows 列：优化器估算的需要扫描的行数。数值越小越好，若与实际差距大，可能统计信息过期（需 ANALYZE TABLE）。结合 filtered 字段可以计算最终返回行数（rows × filtered）。 ⑩、Extra 列：附加信息。 Using index：覆盖索引，无需回表。 Using where：存储引擎返回结果后，Server 层需要再次过滤（条件未完全下推）。 Using temporary ：使用临时表（常见于 GROUP BY、DISTINCT）。 Using filesort：文件排序（常见于 ORDER BY）。考虑为 ORDER BY 字段添加索引。 Select tables optimized away：优化器已优化（如 COUNT(*) 通过索引直接统计）。 Using join buffer：使用连接缓冲区（Block Nested Loop 或 Hash Join）。考虑增大 join_buffer_size。 —-这部分是帮助理解 end，面试中可不背—- type的执行效率等级，达到什么级别比较合适？从高到低的效率排序是 system、const、eq_ref、ref、range、index 和 ALL。 一般情况下，建议 type 值达到 const、eq_ref 或 ref，因为这些类型表明查询使用了索引，效率较高。如果是范围查询，range 类型也是可以接受的。ALL 类型表示全表扫描，性能最差，往往不可接受，需要优化。 索引35.🌟索引为什么能提高MySQL查询效率？索引就像一本书的目录，能让 MySQL 快速定位数据，避免全表扫描。 它一般是 B+ 树结构，查找效率是 O(log n)，比从头到尾扫一遍数据要快得多。 除了查得快，索引还能加速排序、分组、连接等操作。可以通过 create index 创建索引，比如：create index idx_name on students(name); 36.🌟能简单说一下索引的分类吗？从功能上分类的话，有主键索引、唯一索引、全文索引；从数据结构上分类的话，有 B+ 树索引、哈希索引；从存储内容上分类的话，有聚簇索引、非聚簇索引。 你对主键索引了解多少？主键索引用于唯一标识表中的每条记录，其列值必须唯一且非空。创建主键时，MySQL 会自动生成对应的唯一索引。 每个表只能有一个主键索引，一般是表中的自增 id 字段。 CREATE TABLE emp6 (emp_id INT PRIMARY KEY, name VARCHAR(50)); -- 单列主键CREATE TABLE CountryLanguage ( CountryCode CHAR(3), Language VARCHAR(30), PRIMARY KEY (CountryCode, Language) -- 复合主键); —- 这部分是帮助理解 start，面试中可不背 —- 如果创建表的时候没有指定主键，MySQL 的 InnoDB 存储引擎会优先选择一个非空的唯一索引作为主键；如果没有符合条件的索引，MySQL 会自动生成一个隐藏的 _rowid 列作为主键。 可以通过 show index from table_name 查看索引信息： Table 当前索引所属的表名。 Non_unique 是否唯一索引，0 表示唯一索引（如主键），1 表示非唯一。 Key_name 主键索引默认叫 PRIMARY；普通索引为自定义名。 Seq_in_index 索引中的列顺序，在联合索引中这个字段表示第几列（第 1 个）。 Column_name 当前索引中包含的字段名。 Collation A 表示升序（Ascend）；D 表示降序。 Cardinality 索引的基数，即不重复的索引值的数量。越高说明区分度越好（影响优化器是否用此索引）。 Sub_part 前缀索引的长度。 Packed 是否压缩存储索引；一般不用，默认为 NULL。 Null 字段是否允许为 NULL；主键字段不允许为 NULL。 Index_type 索引底层结构，InnoDB 默认是 B+ 树（BTREE）。 Comment 索引的注释。 Visible 是否可见；MySQL 8.0+ 可隐藏索引。—- 这部分是帮助理解 end，面试中可不背 —- 唯一索引和主键索引有什么区别？主键索引=唯一索引+非空。每个表只能有一个主键索引，但可以有多个唯一索引。 -- 在 email 列上添加唯一索引CREATE TABLE users ( id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(50) NOT NULL, email VARCHAR(100) NOT NULL, UNIQUE KEY uk_email (email) -- 唯一索引);-- 复合唯一索引（保证 user_id 和 role 组合唯一）CREATE TABLE user_roles ( user_id INT NOT NULL, role VARCHAR(20) NOT NULL, UNIQUE KEY uk_user_role (user_id, role)); 主键索引不允许插入 NULL 值，尝试插入 NULL 会报错；唯一索引允许插入多个 NULL 值。 unique key 和 unique index 有什么区别？创建唯一键时，MySQL 会自动生成一个同名的唯一索引；反之，创建唯一索引也会隐式添加唯一性约束。 可通过 UNIQUE KEY uk_name 定义或者 CONSTRAINT uk_name UNIQUE 定义唯一键。 CREATE TABLE users ( id INT PRIMARY KEY, email VARCHAR(100), -- 显式命名唯一键 CONSTRAINT uk_email UNIQUE (email));CREATE TABLE users3 ( id INT PRIMARY KEY, email VARCHAR(100), UNIQUE KEY uk_email (email) -- 唯一索引); 可通过 CREATE UNIQUE INDEX 创建唯一索引。 CREATE TABLE users ( id INT PRIMARY KEY, email VARCHAR(100));-- 手动创建唯一索引CREATE UNIQUE INDEX uk_email ON users(email); 通过 SHOW CREATE TABLE table_name 查看表结构时，结果都是一样的。 普通索引和唯一索引有什么区别？普通索引仅用于加速查询，不限制字段值的唯一性；适用于高频写入的字段、范围查询的字段。 -- 日志时间戳允许重复，无需唯一性检查CREATE INDEX idx_log_time ON access_logs(access_time);-- 订单状态允许重复，但需频繁按状态过滤数据CREATE INDEX idx_order_status ON orders(status); 唯一索引强制字段值的唯一性，插入或更新时会触发唯一性检查；适用于业务唯一性约束的字段、防止数据重复插入的字段。 -- 用户邮箱必须唯一CREATE UNIQUE INDEX uk_email ON users(email);-- 确保同一用户对同一商品只能有一条未支付订单CREATE UNIQUE INDEX uk_user_product ON orders(user_id, product_id) WHERE status = unpaid; 你对全文索引了解多少？全文索引是 MySQL 一种优化文本数据检索的特殊类型索引，适用于 CHAR、VARCHAR 和 TEXT 等字段。 MySQL 5.7 及以上版本内置了 ngram 解析器，可处理中文、日文和韩文等分词。 建表时通过 FULLTEXT (title, body) 来定义。通过 MATCH(col1, col2) AGAINST(keyword) 进行检索，默认按照降序返回结果，支持布尔模式查询。 + 表示必须包含； - 表示排除； * 表示通配符；-- 建表时创建全文索引（支持中文）CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY, title VARCHAR(200), content TEXT, FULLTEXT(title, content) WITH PARSER ngram) ENGINE=InnoDB;-- 使用布尔模式查询SELECT * FROM articles WHERE MATCH(title, content) AGAINST(+MySQL -Oracle IN BOOLEAN MODE); 底层使用倒排索引将字段中的文本内容进行分词，然后建立一个倒排表。性能比 LIKE ‘%keyword%’ 高很多。 —- 这部分是帮助理解 start，面试中可不背 —- 倒排索引通过一个辅助表存储单词与单词自身在一个或多个文档中所在位置之间的映射，通常采用关联数组实现。 有两种表现形式：inverted file index（{单词，单词所在文档的ID}）和full inverted index（{单词，(单词所在文档的ID，在具体文档中的位置)}） 比如有这样一个文档： DocumentId Text 1 Pease porridge hot, pease porridge cold 2 Pease porridge in the pot 3 Nine days old 4 Some like it hot, some like it cold 5 Some like it in the pot 6 Nine days old inverted file index 的关联数组存储形式为： days → 3,6 old → 3,6 pease → 1,2 porridge → 1,2 ... full inverted index 更加详细： days → (3:5),(6:5) old → (3:11),(6:11) pease → (1:1),(1:7),(2:1) porridge → (1:7),(2:7) ... full inverted index 不仅存储了文档 ID，还存储了单词在文档中的具体位置。 InnoDB 采用的是 full inverted index 的方式实现全文索引。 如果需要处理中文分词的话，一定要记得加上 WITH PARSER ngram，否则可能查不出来数据。 不过，对于复杂的中文场景，建议使用 Elasticsearch 等专业搜索引擎替代，技术派项目中就用了这种方案。 —- 这部分是帮助理解 end，面试中可不背 —- 37.🌟创建索引有哪些注意点？第一，选择合适的字段 比如说频繁出现在 WHERE、JOIN、ORDER BY、GROUP BY 中的字段。 优先选择区分度高的字段，比如用户 ID、手机号等唯一值多的，而不是性别、状态等区分度极低的字段，如果真的需要，可以考虑联合索引。 第二，要控制索引的数量，避免过度索引，每个索引都要占用存储空间，单表的索引数量不建议超过 5 个。 要定期通过 SHOW INDEX FROM table_name 查看索引的使用情况，删除不必要的索引。比如说已经有联合索引 (a, b)，单索引（a）就是冗余的。 第三，联合索引的时候要遵循最左前缀原则，即在查询条件中使用联合索引的第一个字段，才能充分利用索引。 比如说联合索引 (A, B, C) 可支持 A、A+B、A+B+C 的查询，但无法支持 B 或 C 的单独查询。 区分度高的字段放在左侧，等值查询的字段优先于范围查询的字段。例如 WHERE A1 AND B10 AND C2，优先 (A, C, B)。 如果联合索引包含查询的所需字段，还可以避免回表，提高查询效率。 38.🌟索引哪些情况下会失效呢？简版：比如索引列使用了函数、使用了通配符开头的模糊查询、联合索引不满足最左前缀原则，或者使用 or 的时候部分字段无索引等。 第一，对索引列使用函数或表达式会导致索引失效。 -- 索引失效SELECT * FROM users WHERE YEAR(create_time) = 2023;SELECT * FROM products WHERE price*2 100;-- 优化方案（使用范围查询）SELECT * FROM users WHERE create_time BETWEEN 2023-01-01 AND 2023-12-31;SELECT * FROM products WHERE price 50; 第二，LIKE 模糊查询以通配符开头会导致索引失效。 -- 索引失效SELECT * FROM articles WHERE title LIKE %数据库%;-- 可以使用索引（但范围有限）SELECT * FROM articles WHERE title LIKE 数据库%;-- 解决方案：考虑全文索引或搜索引擎SELECT * FROM articles WHERE MATCH(title) AGAINST(数据库); 第三，联合索引违反了最左前缀原则，索引会失效。 -- 假设有联合索引 (a, b, c)SELECT * FROM table WHERE b = 2 AND c = 3; -- 索引失效SELECT * FROM table WHERE a = 1 AND c = 3; -- 只使用a列索引-- 正确使用联合索引SELECT * FROM table WHERE a = 1 AND b = 2 AND c = 3; 联合索引，但 WHERE 不满足最左前缀原则，索引无法起效。例如：SELECT * FROM table WHERE column2 = 2，联合索引为 (column1, column2)。 —- 这部分是帮助理解 start，面试中可不背 —- 第四，使用 OR 连接非索引列条件，会导致索引失效。 -- 假设name有索引但age没有SELECT * FROM users WHERE name = 张三 OR age = 25; -- 全表扫描-- 优化方案1：使用UNION ALLSELECT * FROM users WHERE name = 张三UNION ALLSELECT * FROM users WHERE age = 25 AND name != 张三;-- 优化方案2：考虑为age添加索引 第五，使用 ! 或 不等值查询会导致索引失效。 SELECT * FROM user WHERE status != 1; -- 若大部分行 `status=1`，可能全表扫描-- 优化方案：使用范围查询SELECT * FROM user WHERE status 1 OR status 1; —- 这部分是帮助理解 end，面试中可不背 —- 什么情况下模糊查询不走索引？模糊查询主要使用 LIKE 语句，结合通配符来实现。%（代表任意多个字符）和 _（代表单个字符） SELECT * FROM table WHERE column LIKE %xxx%; 这个查询会返回所有 column 列中包含 xxx 的记录。但是，如果模糊查询的通配符** % 出现在搜索字符串的开始位置，如 LIKE ‘%xxx’，MySQL 将无法使用索引，因为数据库必须扫描全表以匹配任意位置的字符串**。 39.索引不适合哪些场景呢？第一，区分度低的列，可以和其他高区分度的列组成联合索引。 第二，频繁更新的列，索引会增加更新的成本。 第三，TEXT、BLOB 等大对象类型的字段，可以使用前缀索引、全文索引替代。 第四，当表的数据量很小的时候，不超过 1000 行，全表扫描可能比使用索引更快。 —- 这部分是帮助理解 start，面试中可不背 —- 原因时当数据量很小时，全表扫描的成本很低，因为所有的数据可能都加载到内存中了，使用索引反而需要先查找索引，再通过索引去找到实际的数据行，增加了额外的 IO 寻址时间。 —- 这部分是帮助理解 end，面试中可不背 —- 性别字段要建立索引吗？性别字段不适合建立单独索引。因为性别字段的区分度很低。如果性别字段确实经常用于查询条件，数据规模也比较大，可以将性别字段作为联合索引的一部分，与区分度高的字段一起，效果会好很多。 什么是区分度？区分度是衡量一个字段在 MySQL 表中唯一值的比例。 区分度 = 字段的唯一值数量 / 字段的总记录数；越接近 1，就越适合作为索引。因为索引可以更有效地缩小查询范围。 例如，一个表中有 1000 条记录，其中性别字段只有两个值（男、女），那么性别字段的区分度只有 0.002，就不适合建立索引。 可以通过COUNT(DISTINCT column_name)和COUNT(*)的比值来计算字段的区分度。例如： SELECT COUNT(DISTINCT gender) / COUNT(*) AS gender_selectivityFROM users; 什么样的字段适合加索引？一句话回答：一般来说，主键、唯一键、以及经常作为查询条件的字段最适合加索引。除此之外，字段的区分度要高，这样索引才能起到过滤作用；如果字段经常用于表连接、排序或分组，也建议加索引。同时如果多个字段经常一起出现在查询条件中，也可以建立联合索引来提升性能。 —- 这部分是帮助理解 start，面试中可不背 —- 查询条件中的高频字段，比如说WHERE子句中频繁用于等值查询、范围查询或者 IN 列表的字段。 SELECT * FROM orders WHERE status = PAID AND create_time 2023-01-01;-- 若`status`和`create_time`常组合查询，建联合索引`(status, create_time)` 多表连接时的关联字段，比如说 user.id 和 order.user_id。 SELECT * FROM user u JOIN order o ON u.id = o.user_id; -- `user_id`需索引 参与排序或者分组的字段，可以直接利用索引的有序性，避免文件排序。 SELECT * FROM product ORDER BY price DESC; -- 单字段排序SELECT category, COUNT(*) FROM product GROUP BY category; -- 分组统计 需要利用覆盖索引的字段，可以避免回表操作。 -- 创建联合索引`(user_id, create_time)`SELECT user_id, create_time FROM orders WHERE user_id = 100; -- 覆盖索引生效 —- 这部分是帮助理解 end，面试中可不背 —- 40.索引是不是建的越多越好？索引不是越多越好。虽然索引可以加快查询，但也会带来写入变慢、占用更多存储空间、甚至让优化器选错索引的风险。 —- 这部分是帮助理解 start，面试中可不背 —- 每次数据写入（INSERTUPDATEDELETE）时，MySQL 都需同步更新所有相关索引，索引越多，维护成本越高。 假如某表有 10 个索引，插入一行数据需更新 10 个 B+树结构，导致写入延迟增加 5~10 倍。 假如某表数据量 100GB，若建 5 个索引，总存储可能达到 200GB+。 索引过多时，优化器需评估更多可能的执行路径，可能导致选择困难症，优化器也会选错索引。 再比如说，已有联合索引 (A, B, C)，再单独建 (A) 或 (A, B) 索引即为冗余。 单表索引数量建议不超过 5 个，MySQL 官方建议单表索引总字段数 ≤ 表字段数的 30%。 —- 这部分是帮助理解 end，面试中可不背 —- 说说索引优化的思路？一句话回答：先通过慢查询日志找出性能瓶颈，然后用 EXPLAIN 分析执行计划，判断是否走了索引、是否回表、是否排序。接着根据字段特性设计合适的索引，如选择区分度高的字段，使用联合索引和覆盖索引，避免索引失效的写法，最后通过实测来验证优化效果。 41.🌟为什么 InnoDB 要使用 B+树作为索引？一句话总结：因为 B+ 树是一种高度平衡的多路查找树，能有效降低磁盘的 IO 次数，并且支持有序遍历和范围查询。 查询性能非常高，其结构也适合 MySQL 按照页为单位在磁盘上存储。 像其他选项，比如说哈希表不支持范围查询，二叉树层级太深，B 树又不方便范围扫描，所以最终选择了 B+ 树。 再换一种回答： 相比哈希表：B+ 树支持范围查询和排序相比二叉树和红黑树：B+ 树更“矮胖”，层级更少，磁盘 IO 次数更少相比 B 树：B+ 树的非叶子节点只存储键值，叶子节点存储数据并通过链表连接，支持范围查询另外一种回答版本： B+树是一种自平衡的多路查找树，和红黑树、二叉平衡树不同，B+树的每个节点可以有 m 个子节点，而红黑树和二叉平衡树都只有 2 个。 另外，和 B 树不同，B+树的非叶子节点只存储键值，不存储数据，而叶子节点存储了所有的数据，并且构成了一个有序链表。 这样做的好处是，非叶子节点上由于没有存储数据，就可以存储更多的键值对，再加上叶子节点构成了一个有序链表，范围查询时就可以直接通过叶子节点间的指针顺序访问整个查询范围内的所有记录，而无需对树进行多次遍历。查询的效率比 B 树更高。 先说说 B 树。B 树是一种自平衡的多路查找树，和红黑树、二叉平衡树不同，B 树的每个节点可以有 m 个子节点，而红黑树和二叉平衡树都只有 2 个。换句话说，红黑树、二叉平衡树是细高个，而 B 树是矮胖子。 再来说说内存和磁盘的 IO 读写。 为了提高读写效率，从磁盘往内存中读数据的时候，一次会读取至少一页的数据，如果不满一页，会再多读点。 比如说查询只需要读取 2KB 的数据，但 MySQL 实际上会读取 4KB 的数据，以装满整页。页是 MySQL 进行内存和磁盘交互的最小逻辑单元。 再比如说需要读取 5KB 的数据，实际上 MySQL 会读取 8KB 的数据，刚好两页。 因为读的次数越多，效率就越低。就好比我们在工地上搬砖，一次搬 10 块砖肯定比一次搬 1 块砖的效率要高，反正我每次都搬 10 块（😁）。 对于红黑树、二叉平衡树这种细高个来说，每次搬的砖少，因为力气不够嘛，那来回跑的次数就越多。 通常 B+ 树高度为 3-4 层即可支持 TB 级数据，而每次查询只需 2-4 次磁盘 IO，远低于二叉树或红黑树的 O(log2N) 复杂度 树越高，意味着查找数据时就需要更多的磁盘 IO，因为每一层都可能需要从磁盘加载新的节点。 B 树的节点通常与页的大小对齐，这样每次从磁盘加载一个节点时，正好就是一页的大小。 B 树的一个节点通常包括三个部分： 键值：即表中的主键 指针：存储子节点的信息 数据：除主键外的行数据正所谓“祸兮福所倚，福兮祸所伏”，因为 B 树的每个节点上都存储了数据，就导致每个节点能存储的键值和指针变少了，因为每一个节点的大小是固定的，对吧？于是 B+树就来了，B+树的非叶子节点只存储键值，不存储数据，而叶子节点会存储所有的行数据，并且构成一个有序链表。 这样做的好处是，非叶子节点由于没有存储数据，就可以存储更多的键值对，树就变得更加矮胖了，于是就更有劲了，每次搬的砖也就更多了（😂）。 相比 B 树，B+ 树的非叶子节点可容纳的键值更多，一个 16KB 的节点可存储约 1200 个键值，大幅降低树的高度。 由此一来，查找数据进行的磁盘 IO 就更少了，查询的效率也就更高了。 再加上叶子节点构成了一个有序链表，范围查询时就可以直接通过叶子节点间的指针顺序访问整个查询范围内的所有记录，而无需对树进行多次遍历。 B 树就做不到这一点。 —- 这部分是帮助理解 end，面试中可不背 —- B+树的叶子节点是单向链表还是双向链表？如果从大值向小值检索，如何操作？B+树的叶子节点是通过双向链表连接的，这样可以方便范围查询和反向遍历。 当执行范围查询时，可以从范围的开始点或结束点开始，向前或向后遍历。在需要对数据进行逆序处理时，双向链表非常有用。如果需要在 B+树中从大值向小值进行检索，可以先定位到最右侧节点，找到包含最大值的叶子节点。从根节点开始向右遍历树的方式实现。 定位到最右侧的叶子节点后，再利用叶节点间的双向链表向左遍历就好了。 为什么 MongoDB 的索引用 B树，而 MySQL 用 B+ 树？MongoDB 通常以 JSON 格式存储文档，查询以单键查询（如 find({_id: 123})）为主。B 树的“节点既存键又存数据”的特性允许查询在非叶子节点提前终止，从而减少 IO 次数。 MySQL 的查询通常涉及范围（WHERE id 100）、排序（ORDER BY）、连接（JOIN）等操作。B+ 树的叶子节点是链表结构，天然支持顺序遍历，无需回溯至根节点或中序遍历，效率远高于 B 树。 42.🌟一棵B+树能存储多少条数据呢？一句话回复：一棵 B+ 树能存多少数据，取决于它的分支因子和高度。在 InnoDB 中，页的默认大小为 16KB，当主键为 bigint 时，3 层 B+ 树通常可以存储约 2000 万条数据。 —- 这部分是帮助理解 start，面试中可不背 —- 先来看一下计算公式： 最大记录数 = (分支因子)^(树高度-1) × 叶子节点容量 再来看一下关键参数：①、页大小，默认 16KB②、主键大小，假设是 bigint 类型，那么它的大小就是 8 个字节。③、页指针大小，InnoDB 源码中设置为 6 字节，4 字节页号 + 2 字节页内偏移。 所以非叶子节点可以存储 1638414(键值+指针)1170 个这样的单元。 当层高为 2 时，根节点可以存储 1170 个指针，指向 1170 个叶子节点，所以总数据量为 1170×16 18720 条。 当层高为 3 时，根节点指向 1170 个非叶子节点，每个非叶子节点再指向 1170 个叶子节点，所以总数据量为 1170×1170×16≈21,902,400 条（约2,190万条）记录。 推荐阅读：清幽之地：InnoDB 一棵 B+树可以存放多少行数据？ —- 这部分是帮助理解 end，面试中可不背 —- 现在有一张表 2kw 数据，我这个 b+树的高度有几层？对于 2KW 条数据来说，B+树的高度为 3 层就够了。 每个叶子节点能存放多少条数据？如果单行数据大小为 1KB，那么每页可存储约 16 行（16KB1KB）数据。 —- 这部分是帮助理解 start，面试中可不背 —- 假设有这样一个表结构： CREATE TABLE `user` ( `id` BIGINT PRIMARY KEY, -- 8字节 `name` VARCHAR(255) NOT NULL, -- 实际长度50字节（UTF8MB4，每个字符最多4字节） `age` TINYINT, -- 1字节 `email` VARCHAR(255) -- 实际长度30字节，可为NULL) ROW_FORMAT=COMPACT; 那么一行数据的大小为：8 + 50 + 1 + 30 89 字节。 行格式的开销为：行头 5 字节+指针 6 字节+可变长度字段开销 2 字节（name 和 email 各占 1 字节）+ NULL 位图 1 字节 14 字节。 所以每行数据的实际大小为：89 + 14 103 字节。 每页大小默认为 16KB，那么每页最多可以存储 16384 103 ≈ 158 行数据。 —- 这部分是帮助理解 end，面试中可不背 —- 43.索引为什么用 B+树不用普通二叉树？普通二叉树的每个节点最多有两个子节点。当数据按顺序递增插入时，二叉树会退化成链表，导致树的高度等于数据量。 此时查找 id7 就需要 7 次 IO 操作，相当于全表扫描。而 B+ 树作为多叉平衡树，能将数亿级的数据量控制在 3-4 层的树高，能极大减少磁盘的 IO 次数。 为什么不用平衡二叉树呢？平衡二叉树虽然解决了普通二叉树的退化问题，但每个节点最多只有两个子节点的问题依然存在。并且平衡二叉树的插入和删除操作也会导致频繁的旋转操作，影响性能。 44.🌟为什么用 B+ 树而不用 B 树呢？B+ 树相比 B 树有 3 个显著优势： 第一，B 树的每个节点既存储键值，又存储数据和指针，导致单节点存储的键值数量较少。 一个 16KB 的 InnoDB 页，如果数据较大，B 树的非叶子节点只能容纳几十个键值，而 B+ 树的非叶子节点可以容纳上千个键值。 第二，B 树的范围查询需要通过中序遍历逐层回溯；而 B+ 树的叶子节点通过双向链表顺序连接，范围查询只需定位起始点后顺序遍历链表即可，没有回溯开销。 第三，B 树的数据可能存储在任意节点，假如目标数据恰好位于根节点或上层节点，查询仅需 1-2 次 IO；但如果数据位于底层节点，则需多次 IO，导致查询时间波动较大。 而 B+ 树的所有数据都存储在叶子节点，查询路径的长度是固定的，**时间稳定为 O(logN)**，对 MySQL 在高并发场景下的稳定性至关重要。 B+树的时间复杂度是多少？O(logN)。 为什么用 B+树不用跳表呢？跳表本质上还是链表结构，只不过把某些节点抽到上层做了索引。 一条数据一个节点，如果需要存放 2000 万条数据，且每次查询都要能达到二分查找的效果，那么跳表的高度大约为 24 层（2 的 24 次方）。 在最坏的情况下，这 24 层数据分散在不同的数据页，查找一次数据就需要 24 次磁盘 IO。 而 2000 万条数据在 B+树中只需要 3 层就可以了。 B+树的范围查找怎么做的？一句话回答： 先通过索引路径定位到第一个满足条件的叶子节点，然后顺着叶子节点之间的链表向右向左扫描，直到超过范围。 详细版： B+ 树索引的范围查找主要依赖叶子节点之间的双向链表来完成。 第一步，从 B+ 树的根节点开始，通过索引键值逐层向下，找到第一个满足条件的叶子节点。 第二步，利用叶子节点之间的双向链表，从起始节点开始，依次向后遍历每个节点。当索引值超过查询范围，或者遍历到链表末尾时，终止查询。 了解快排吗快速排序使用分治法将一个序列分为较小和较大的 2 个子序列，然后递归排序两个子序列，由东尼·霍尔在 1960 年提出。 其核心思想是： 选择一个基准值。 将数组分为两部分，左边小于基准值，右边大于或等于基准值。 对左右两部分递归排序，最终合并。public static void quickSort(int[] arr, int low, int high) if (low high) int pivotIndex = partition(arr, low, high); quickSort(arr, low, pivotIndex - 1); quickSort(arr, pivotIndex + 1, high); private static int partition(int[] arr, int low, int high) int pivot = arr[high]; int i = low - 1; for (int j = low; j high; j++) if (arr[j] = pivot) i++; swap(arr, i, j); swap(arr, i + 1, high); return i + 1;private static void swap(int[] arr, int i, int j) int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; 45.B+树索引和 Hash 索引有什么区别？简版回答：B+ 树索引支持范围查询、有序扫描，是 InnoDB 的默认索引结构。 Hash 索引只支持等值查找，速度快但功能弱，常见于 Memory 引擎。 稍微详细一点的回答： B+ 树索引是一种平衡多路搜索树，所有数据存储在叶子节点上，非叶子节点仅存储索引键。叶子节点通过指针连接形成有序链表，天然支持排序。并且支持范围查询、模糊查询，是 InnoDB 默认的索引结构。 Hash 索引基于哈希函数将键值映射到固定长度的哈希值，通过哈希值定位数据存储的位置。完全无序，只支持等值查询，常见于 Memory 引擎。 —- 这部分是帮助理解 start，面试中可不背 —- 因为 B+ 树是 InnoDB 的默认索引类型，所以创建 B+ 树的时候不需要指定索引类型。 CREATE TABLE example_btree ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255), INDEX name_index (name)) ENGINE=InnoDB; 可以通过UNIQUE HASH创建哈希索引： CREATE TABLE example_hash ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255), UNIQUE HASH (name)) ENGINE=MEMORY; InnoDB 并不提供直接创建哈希索引的选项，因为 B+ 树索引能够很好地支持范围查询和等值查询，满足了大多数数据库操作的需要。 不过，InnoDB 内部使用了一种名为“自适应哈希索引”（Adaptive Hash Index, AHI）的技术，当某些索引值频繁访问时，InnoDB 会在 B+ 树基础上自动创建哈希索引，兼具两者的优点。 可通过 SHOW VARIABLES LIKE innodb_adaptive_hash_index; 查看自适应哈希索引的状态。 如果返回的值是 ON，说明自适应哈希索引是开启的。—- 这部分是帮助理解 end，面试中可不背 —- 46.🌟聚族索引和非聚族索引有什么区别？聚簇索引的叶子节点存储了完整的数据行，数据和索引是在一起的。InnoDB 的主键索引就是聚簇索引，叶子节点不仅存储了主键值，还存储了其他列的值，因此按照主键进行查询的速度会非常快。 每个表只能有一个聚簇索引，通常由主键定义。如果没有显式指定主键，InnoDB 会隐式创建一个隐藏的主键索引 row_id。非聚簇索引的叶子节点只包含了主键值，需要通过回表按照主键去聚簇索引查找其他列的值，唯一索引、普通索引等非主键索引都是非聚簇索引。 每个表都可以创建多个非聚簇索引，如果不想回表的话，可以通过覆盖索引把要查询的字段也放到索引中。 —- 这部分是帮助大家理解 start，面试中可不背 —- 一张表只能有一个聚簇索引。 CREATE TABLE user ( id INT PRIMARY KEY, name VARCHAR(100), age INT);主键 id 是聚簇索引，B+ 树的叶子节点直接存储了 (id, name, age)。 一张表可以有多个非聚簇索引。 CREATE INDEX idx_name ON user(name);CREATE INDEX idx_age ON user(age);idx_name 是非聚簇索引，叶子节点存的是 name - id，查整行数据要回表。 idx_age 也是非聚簇索引，叶子节点存的是 age - id，查整行数据也要回表。 想要了解更多聚簇索引和非聚簇索引，推荐阅读：https://www.cnblogs.com/vipstone/p/16370305.htmlhttps://learnku.com/articles/50096https://blog.csdn.net/m0_52226803/article/details/135494499https://mp.weixin.qq.com/s/F0cEzIqecF4sWg7ZRmHKRQ—- 这部分是帮助理解 end，面试中可不背 —- 47.🌟回表了解吗？当使用非聚簇索引进行查询时，MySQL 需要先通过非聚簇索引找到主键值，然后再根据主键值回到聚簇索引中查找完整数据行，这个过程称为回表。 假设现在有一张用户表 users： CREATE TABLE users ( id INT PRIMARY KEY, name VARCHAR(50), age INT, email VARCHAR(50), INDEX (name)); 执行查询： SELECT * FROM users WHERE name = 王二; 查询过程如下： 第一步，MySQL 使用 name 列上的非聚簇索引查找所有 name ‘王二’ 的主键 id。 第二步，使用主键 id 到聚簇索引中查找完整记录。 回表的代价是什么？回表通常需要访问额外的数据页，如果数据不在内存中，还需要从磁盘读取，增加 IO 开销。 可通过覆盖索引或者联合索引来避免回表。 -- 原表结构CREATE TABLE users ( id INT PRIMARY KEY, name VARCHAR(50), age INT, INDEX idx_name (name));-- 需要查询name和ageSELECT name, age FROM users WHERE name = 张三;-- 这会回表，因为age不在idx_name索引中-- 优化方案1：创建包含age的联合索引ALTER TABLE users ADD INDEX idx_name_age (name, age);-- 现在同样的查询不需要回表 什么情况下会触发回表？ 第一，当查询字段不在非聚簇索引中时，必须回表到主键索引获取数据。第二，查询字段包含非索引列（如 SELECT *），必然触发回表。 回表记录越多好吗？回表记录越多，通常代表性能越差，因为每条记录都需要通过主键再查询一次完整数据。这个过程涉及内存访问或磁盘 IO，尤其当缓存命中率不高时，回表会严重影响查询效率。 了解 MRR 吗？MRR 是 InnoDB 为了解决回表带来的大量随机 IO 问题而引入的一种优化策略。 它会先把非聚簇索引查到的主键值列表进行排序，再按顺序去主键索引中批量回表，将随机 IO 转换为顺序 IO，以减少磁盘寻道时间。 —- 这部分是帮助理解 start，面试中可不背 —- 可通过 SHOW VARIABLES LIKE optimizer_switch; 查看 MRR 是否启用。 其中 mrr=on 表示启用 MRR，mrr_cost_based=on 表示基于成本决定使用 MRR。 另外可以通过 show variables like read_rnd_buffer_size; 查看 MRR 的缓冲区大小，默认是 256KB。 我们来创建一个表，插入一些数据，然后执行一个查询来演示 MRR 的效果。 CREATE DATABASE IF NOT EXISTS mrr_test; USE mrr_test; CREATE TABLE IF NOT EXISTS orders (id INT AUTO_INCREMENT PRIMARY KEY, user_id INT, order_date DATE, amount DECIMAL(10,2), status VARCHAR(20), INDEX idx_user_date(user_id, order_date));DELIMITER //CREATE PROCEDURE generate_test_data()BEGIN DECLARE i INT DEFAULT 1; WHILE i = 100000 DO INSERT INTO orders (user_id, order_date, amount, status) VALUES ( FLOOR(1 + RAND() * 1000), -- Random user_id between 1 and 1000 DATE_ADD(2023-01-01, INTERVAL FLOOR(RAND() * 365) DAY), -- Random date in 2023 ROUND(10 + RAND() * 990, 2), -- Random amount between 10 and 1000 ELT(1 + FLOOR(RAND() * 3), completed, pending, cancelled) -- Random status ); SET i = i + 1; END WHILE;END //DELIMITER ;CALL generate_test_data();DROP PROCEDURE generate_test_data; 查看 MRR 开启和关闭时的性能数据： -- 确保MRR开启并设置足够大的缓冲区SET SESSION optimizer_switch=mrr=on,mrr_cost_based=off;SET SESSION read_rnd_buffer_size = 16*1024*1024;-- 清理缓存和状态FLUSH STATUS;FLUSH TABLES;-- 强制使用二级索引并回表查询（通过选择未被索引的列）SELECT Raw data access pattern with MRR ON as test_case;SELECT /*+ MRR(orders_mrr_test) */ id, shipping_address, customer_nameFROM orders_mrr_test FORCE INDEX(idx_user_date)WHERE user_id IN (100,200,300,400,500,600,700,800,900,1000)AND order_date BETWEEN 2023-03-01 AND 2023-04-01LIMIT 15;-- 显示处理器状态SHOW STATUS LIKE Handler_%;SHOW STATUS LIKE %mrr%;-- 对比：关闭MRRSET SESSION optimizer_switch=mrr=off,mrr_cost_based=off;FLUSH STATUS;FLUSH TABLES;SELECT Raw data access pattern with MRR OFF as test_case;SELECT id, shipping_address, customer_nameFROM orders_mrr_test FORCE INDEX(idx_user_date)WHERE user_id IN (100,200,300,400,500,600,700,800,900,1000)AND order_date BETWEEN 2023-03-01 AND 2023-04-01LIMIT 15;-- 显示处理器状态SHOW STATUS LIKE Handler_%;SHOW STATUS LIKE %mrr%;-- 显示详细的执行计划EXPLAIN FORMAT=TREESELECT /*+ MRR(orders_mrr_test) */ id, shipping_address, customer_nameFROM orders_mrr_test FORCE INDEX(idx_user_date)WHERE user_id IN (100,200,300,400,500,600,700,800,900,1000)AND order_date BETWEEN 2023-03-01 AND 2023-04-01; 可以看到 MRR 开启时的结果对比： Wrap 也给出了对应的结果说明：也可以在 explain 中确认 MRR 的使用情况。—- 这部分是帮助理解 end，面试中可不背 —- 48.🌟联合索引了解吗？（补充）联合索引就是把多个字段放在一个索引里，但必须遵守“最左前缀”原则，只有从第一个字段开始连续使用，索引才会生效。 联合索引会按字段顺序构建B+树。例如（age, name）索引会先按照 age 排序，age 相同则按照 name 排序，若两者都相同则按主键排序，确保叶子节点无重复索引项。 创建(A,B,C)联合索引相当于同时创建了(A)、(A,B)和(A,B,C)三个索引。 -- 创建联合索引CREATE INDEX idx_order_user_product ON orders(user_id, product_id, create_time)-- 高效查询SELECT * FROM orders WHERE user_id=1001 AND product_id=2002ORDER BY create_time DESC 联合索引底层的存储结构是怎样的？联合索引在底层采用 B+ 树结构进行存储，这一点与单列索引相同。 与单列索引不同的是，联合索引的每个节点会存储所有索引列的值，而不仅仅是第一列的值。例如，对于联合索引(a,b,c)，每个节点都包含 a、b、c 三列的值。 非叶子节点示例： [(a=1, b=2, c=3) → 子节点1, (a=5, b=3, c=1) → 子节点2]叶子节点示例（InnoDB）： (a=1, b=2, c=3) → PK=100 | (a=1, b=2, c=4) → PK=101 （通过指针连接形成双向链表） 联合索引的叶子节点存的什么内容?联合索引属于非聚簇索引，叶子节点存储的是联合索引各列的值和对应行的主键值，而不是完整的数据行。查询非索引字段时，需要通过主键值回表到聚簇索引获取完整数据。 例如索引(a, b)的叶子节点会完整存储(a, b)的值，并按字段顺序排序（如 a 优先，a 相同则按 b 排序）。如果主键是 id，叶子节点会存储 (a, b, id) 的组合。 49.🌟覆盖索引了解吗？覆盖索引指的是：查询所需的字段全部都在索引中，不需要回表，从索引页就能直接返回结果。 empname 和 job 两个字段是一个联合索引，而查询也恰好是这两个字段，这时候单次查询就可以达到目的，不需要回表。 可以将高频查询的字段（如 WHERE 条件和 SELECT 列）组合为联合索引，实现覆盖索引。 例如： CREATE INDEX idx_empname_job ON employee(empname, job); 这样查询的时候就可以走索引： SELECT empname, job FROM employee WHERE empname = 王二 AND job = 程序员; 普通索引只用于加速查询条件的匹配，而覆盖索引还能直接提供查询结果。 一个表（name, sex,age,id），select age,id,name from tblname where name’paicoding’;怎么建索引由于查询条件有 name 字段，所以最少应该为 name 字段添加一个索引。、 CREATE INDEX idx_name ON tblname(name); 查询结果中还需要 age、id 字段，可以为这三个字段创建一个联合索引，利用覆盖索引，直接从索引中获取数据，减少回表。 CREATE INDEX idx_name_age_id ON tblname (name, age, id); 50.🌟什么是最左前缀原则？最左前缀原则指的是：MySQL 使用联合索引时，必须从最左边的字段开始匹配，才能命中索引。 假设有一个联合索引 (A, B, C)，其生效条件如下： 如果排序或分组的列是最左前缀的一部分，索引还可以加速操作。 -- 索引(a,b)SELECT * FROM table WHERE a = 1 ORDER BY b; -- 可以利用索引排序 范围查询后的列还能用索引吗？范围查询只能应用于最左前缀的最后一列。范围查询之后的列无法使用索引。 -- 索引(a,b,c)SELECT * FROM table WHERE a = 1 AND b 2 AND c = 3; -- 只能使用a和b，c无法使用索引 为什么不从最左开始查，就无法匹配呢？一句话回答： 因为联合索引在 B+ 树中是按照最左字段优先排序构建的，如果跳过最左字段，MySQL 无法判断查找范围从哪里开始，自然也就无法使用索引。 比如有一个 user 表，我们给 name 和 age 建立了一个联合索引 (name, age)。 ALTER TABLE user add INDEX comidx_name_phone (name,age); 联合索引在 B+ 树中按照从左到右的顺序依次建立搜索树，name 在左，age 在右。 当我们使用 where name ‘王二’ and age ‘20’ 去查询的时候， B+ 树会优先比较 name 来确定下一步应该搜索的方向，往左还是往右。 如果 name 相同的时候再比较 age。 但如果查询条件没有 name，就不知道应该怎么查了，因为 name 是 B+树中的前置条件，没有 name，索引就派不上用场了。 联合索引 (a, b)，where a 1 和 where b 1，效果是一样的吗不一样。 WHERE a = 1 能命中联合索引，因为 a 是联合索引的第一个字段，符合最左前缀匹配原则。而 WHERE b = 1 无法命中联合索引，因为缺少 a 的匹配条件，MySQL 会全表扫描。 —- 这部分是帮助理解 start，面试中可不背 —- 我们来验证一下，假设有一个 ab 表，建立了联合索引 (a, b)： CREATE TABLE ab ( a INT, b INT, INDEX ab_index (a, b)); 插入数据： INSERT INTO ab (a, b) VALUES (1, 2), (1, 3), (2, 1), (3, 3), (2, 2); 执行查询： 通过 explain 可以看到，WHERE a 1 使用了联合索引，而 WHERE b 1 需要全表扫描，依次检查每一行。 —- 这部分是帮助理解 end，面试中可不背 —- 假如有联合索引 abc，下面的 sql 怎么走的联合索引？select * from t where a = 2 and b = 2;select * from t where b = 2 and c = 2;select * from t where a 2 and b = 2; 第一条 SQL 语句包含条件 a 2 和 b 2，刚好符合联合索引的前两列。 第二条 SQL 语句由于未使用最左前缀中的 a，会触发全表扫描。 第三条 SQL 语句在范围条件 a 2 之后，索引后会停止匹配，b 2 的条件需要额外过滤。 (A,B,C) 联合索引 select * from tbn where a? and b in (?,?) and c? 会走索引吗？这个查询会命中联合索引，因为 a 是等值匹配，b 是 IN 等值多匹配，c 是 b 之后的范围条件，符合最左前缀原则。 对于 a?：这是一个精确匹配，并且是联合索引的第一个字段，所以一定会命中索引。 对于 b IN (?, ?)：等价于 b? OR b?，属于多值匹配，并且是联合索引的第二个字段，所以也会命中索引。 对于 c?：这是一个范围条件，属于联合索引的第三个字段，也会命中索引。 —- 这部分是帮助理解 start，面试中可不背 —- 来验证一下。 第一步，建表。 CREATE TABLE tbn (A INT, B INT, C INT, D TEXT); 第二步，创建索引。 CREATE INDEX idx_abc ON tbn (A, B, C); 第三步，插入数据。 INSERT INTO tbn VALUES (1, 2, 3, First);INSERT INTO tbn VALUES (1, 2, 4, Second);INSERT INTO tbn VALUES (1, 3, 5, Third);INSERT INTO tbn VALUES (2, 2, 3, Fourth);INSERT INTO tbn VALUES (2, 3, 4, Fifth); 第四步，执行查询。 EXPLAIN SELECT * FROM tbn WHERE A=1 AND B IN (2, 3) AND C3\\G 从 EXPLAIN 输出结果来看，我们可以得到 MySQL 是如何执行查询的一些关键信息： type: 查询类型，这里是 range，表示 MySQL 使用了范围查找，这是因为查询条件包含了 操作符。 possible_keys: 可能被用来执行查询的索引，这里是 idx_abc，表示 MySQL 认为 idx_abc 索引会用于查询优化。 key: 实际用来执行查询的索引，也是 idx_abc，这确定这条查询命中了联合索引。 Extra: 提供了关于查询执行的额外信息。Using index condition 表示 MySQL 使用了索引下推（Index Condition Pushdown，ICP），这是 MySQL 的一个优化方式，它允许在索引层面过滤数据。 —- 这部分是帮助理解 end，面试中可不背 —- 联合索引的一个场景题：(a,b,c)联合索引，(b,c)是否会走索引吗？根据最左前缀原则，(b,c) 查询不会走索引。 因为联合索引 (a,b,c) 中，a 是最左边的列，联合索引在创建索引树的时候需要先有 a，然后才会有 b 和 c。而查询条件中没有包含 a，所以 MySQL 无法利用这个索引。 EXPLAIN SELECT * FROM tbn WHERE B=1 AND C=1\\G 建立联合索引(a,b,c)，where c 5 是否会用到索引？为什么？不会。只有索引的第三列 c 被用作查询条件，而前两列 a 和 b 都没有被使用。这不符合最左前缀原则。 EXPLAIN SELECT * FROM tbn WHERE C=5\\G sql中使用like，如果遵循最左前缀匹配，查询是不是一定会用到索引？如果查询模式是后缀通配符 LIKE prefix%，且该字段有索引，优化器通常会使用索引。否则即便是遵循最左前缀匹配，LIKE 字段也无法命中索引。 如 age 18 and name LIKE ‘%xxx’，MySQL 会先使用联合索引 age_name 找到 age 符合条件的所有行，然后再全表扫描进行 name 字段的过滤。 type: ref 表示使用索引查找匹配某个值的所有行。 如果是后缀通配符，如 age = 18 and name LIKE xxx%，MySQL 会直接使用联合索引 age_name 找到所有符合条件的行。 type 为 range，表示 MySQL 使用了索引范围扫描，filtered 为 100.00%，表示在扫描的行中，所有的行都满足 WHERE 条件。 51.🌟什么是索引下推？索引下推是指：MySQL 把 WHERE 条件尽可能“下推”到索引扫描阶段，在存储引擎层提前过滤掉不符合条件的记录。 当查询条件包含索引列但未完全匹配时，ICP 会在存储引擎层过滤非索引列条件，以减少回表次数。 传统的查询流程是，存储引擎通过联合索引定位到符合最左前缀条件的主键 ID；回表读取完整数据行并返回给 Server 层；Server 层对所有返回的行进行 WHERE 条件过滤。 有了 ICP 后，存储引擎在索引层直接过滤可下推的条件，仅对符合索引条件的记录回表读取数据，再返回给 Server 层进行剩余条件过滤。 —- 这部分是帮助理解 start，面试中可不背 —- 例如有一张 user 表，建了一个联合索引（name, age），查询语句：select * from user where name like 张% and age=10;，没有索引下推优化的情况下： MySQL 会使用索引 name 找到所有 name like 张% 的主键，根据这些主键，一条条回表查询整行数据，并在 Server 层过滤掉不符合 age=10 的数据行。 启用 ICP 后，InnoDB 会通过联合索引直接筛选出符合条件的主键 ID（name like 张% and age=10），然后再回表查询整行数据。 换句话说，假设 name like ‘张%’ 找到 10000 行数据，age10 只有其中 10 行，没有索引下推的情况下，MySQL 会回表 10000 次，读取 10000 行数据，然后在 Server 层过滤掉 9990 行。 而有了索引下推后，MySQL 只会回表 10 次，读取 10 行数据。 我们来验证一下。 从结果中我们可以清楚地看到 ICP 的效果。ICP 开启时，Extra 列显示”Using index condition”，表明过滤条件被下推到存储引擎层。 ICP关闭时，Extra 列仅显示”Using where”，表明过滤条件在服务器层执行。 -- 开启ICPSET optimizer_switch=index_condition_pushdown=on;-- 清理状态FLUSH STATUS;SELECT Performance test with ICP ON as test_case;-- 执行查询并分析性能EXPLAIN ANALYZESELECT /*+ ICP_ON */ *FROM orders_mrr_testWHERE user_id BETWEEN 100 AND 200 AND order_date = 2023-01-01 AND order_date 2023-02-01 AND order_date NOT LIKE 2023-01-15%;-- 显示处理器状态SHOW STATUS LIKE Handler_read%;-- 关闭ICPSET optimizer_switch=index_condition_pushdown=off;-- 清理状态FLUSH STATUS;SELECT Performance test with ICP OFF as test_case;-- 执行相同的查询EXPLAIN ANALYZESELECT *FROM orders_mrr_testWHERE user_id BETWEEN 100 AND 200 AND order_date = 2023-01-01 AND order_date 2023-02-01 AND order_date NOT LIKE 2023-01-15%;-- 显示处理器状态SHOW STATUS LIKE Handler_read%; 实际的性能差距也很大。ICP 开启时，实际扫描行数：1,649 行，执行时间：约12.3 毫秒。关闭时，实际扫描行数：19,959 行，执行时间：约 32.1 毫秒。Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现。 52.如何查看是否用到了索引？（补充）可以通过 EXPLAIN 关键字来查看是否使用了索引。 EXPLAIN SELECT * FROM table WHERE column = value; 如果使用了索引，结果中的 key 值会显示索引的名称。 联合索引 abc，a1,c1b1,c1a1,c1,b1 走不走索引？ac 能用上索引，条件 a1 符合最左前缀原则，触发索引的第一列 a；由于跳过了中间列 b，c1 无法直接利用索引的有序性优化，但可通过索引下推在存储引擎层过滤 c 的条件，减少回表次数。 bc 无法使用索引，只能全表扫描，因为不符合最左前缀原则；acb 虽然顺序是乱的，但 MySQL 优化器会自动重排为 abc，所以能命中索引。 —- 这部分是帮助理解 start，面试中可不背 —- 我们通过实际的 SQL 来验证一下。 示例 1（a1,c1）： EXPLAIN SELECT * FROM tbn WHERE A=1 AND C=1\\G key 是 idx_abc，表明 a1,c1 会使用联合索引。Extra: Using index condition 表示 ICP 生效。 示例 2（b1,c1）： EXPLAIN SELECT * FROM tbn WHERE B=1 AND C=1\\G key 是 NULL，表明 b1,c1 不会使用联合索引。这是因为查询条件没有遵循最左前缀原则。 示例 3（a1,c1,b1）： EXPLAIN SELECT * FROM tbn WHERE A=1 AND C=1 AND B=1\\G 优化器会自动调整条件顺序为 a1 AND b1 AND c1。 key 是 idx_abc，表明 a1,c1,b1 会使用联合索引。 并且 rows1，因为 MySQL 优化器会自动重排查询条件，以满足最左前缀原则，直接使用联合索引找出 a1 AND b1 AND c1 的行。 锁53.🌟MySQL 中有哪几种锁？MySQL 中有多种类型的锁，可以从不同维度来分类，按锁粒度划分的话，有表锁、行锁。 按照加锁机制划分的话，有乐观锁和悲观锁。按照兼容性划分的话，有共享锁和排他锁。 —- 这部分是帮助理解 start，面试中可不背 —- 表锁：锁定整个表，资源开销小，加锁快，但并发度低，不会出现死锁；适合查询为主、少量更新的场景（如 MyISAM 引擎）。 再细分的话，有表共享读锁（S锁）：允许多个事务同时读，但阻塞写操作；表独占写锁（X锁）：独占表，阻塞其他事务的读写。 行锁：锁定单行或多行，开销大、加锁慢，可能出现死锁，但并发度高（InnoDB 默认支持）。 再细分的话，有记录锁（Record Lock）：锁定索引中的具体记录；间隙锁（Gap Lock）：锁定索引记录之间的间隙，防止幻读；临键锁（Next-Key Lock）：结合记录锁和间隙锁，锁定一个左开右闭的区间（如 (5, 10]）。 共享锁（S锁读锁），允许多个事务同时读取数据，但阻塞写操作。语法：SELECT ... LOCK IN SHARE MODE 排他锁（X锁写锁），独占数据，阻塞其他事务的读写。语法：SELECT ... FOR UPDATE。 乐观锁假设冲突少，通过版本号或 CAS 机制检测冲突（如 UPDATE SET version=version+1 WHERE version=old_version）。 悲观锁假设并发冲突频繁，先加锁再操作SELECT FOR UPDATE。—- 这部分是帮助理解 end，面试中可不背 —- 54.全局锁了解吗？（补充）全局锁就是对整个数据库实例进行加锁，当执行全局锁定操作时，整个数据库将会处于只读状态，所有写操作都会被阻塞，直到全局锁被释放。 在进行全库备份，或者数据迁移时，可以使用全局锁来保证数据的一致性。 在 MySQL 中，可以使用 FLUSH TABLES WITH READ LOCK 命令来获取全局锁。 执行该命令后，所有表将被锁定为只读状态。记得在完成备份或迁移后，使用 UNLOCK TABLES 命令释放全局锁。 -- 锁定整个数据库FLUSH TABLES WITH READ LOCK;-- 执行备份操作-- 例如使用 mysqldump 进行备份! mysqldump -u username -p database_name backup.sql-- 释放全局锁定UNLOCK TABLES; 表锁了解吗？了解。表锁常见于 MyISAM 引擎，InnoDB 也可以手动通过 LOCK TABLES 加锁。 适合读多写少、全表扫描或者表结构变更的场景用。 表锁又可以细分为共享锁和排他锁。共享锁允许多个事务同时读表，但不允许写操作。 LOCK TABLES table_name READ; -- 显式加读锁SELECT * FROM table_name; -- 其他会话可读，不可写UNLOCK TABLES; -- 释放锁 排他锁只允许一个事务进行写操作，其他事务不能读也不能写。 LOCK TABLES table_name WRITE; -- 显式加写锁INSERT/UPDATE/DELETE table_name; -- 其他会话读写均阻塞UNLOCK TABLES; MyISAM 在执行 SELECT 时会自动加读锁，执行 INSERTUPDATEDELETE 时会加写锁。 对于 InnoDB 引擎，无索引的 UPDATE/DELETE 可能会导致锁升级为表锁。 UPDATE innodb_table SET name=new WHERE name=old; -- 全表扫描，退化为表锁 执行 ALTER TABLE 时会自动加表锁，阻塞所有读写操作。 55.🌟说说 MySQL 的行锁？行锁是 InnoDB 存储引擎中最细粒度的锁，它锁定表中的一行记录，允许其他事务访问表中的其他行。 底层是通过给索引加锁实现的，这就意味着只有通过索引条件检索数据时，InnoDB 才能使用行级锁，否则会退化为表锁。 行锁又可以细分为记录锁、间隙锁和临键锁三种形式。通过 SELECT ... FOR UPDATE 可以加排他锁。 START TRANSACTION;-- 加排他锁，锁定某一行SELECT * FROM your_table WHERE id = 1 FOR UPDATE;-- 对该行进行操作UPDATE your_table SET column1 = new_value WHERE id = 1;COMMIT; 通过 SELECT ...LOCK IN SHARE MODE 可以加共享锁。 START TRANSACTION;-- 加共享锁，锁定某一行SELECT * FROM your_table WHERE id = 1 LOCK IN SHARE MODE;-- 只能读取该行，不能修改COMMIT; select for update 有什么需要注意的？第一，必须在事务中使用，否则锁会立即释放。 START TRANSACTION;SELECT * FROM your_table WHERE id = 1 FOR UPDATE;-- 对该行进行操作COMMIT; 第二，使用时必须注意是否命中索引，否则可能锁全表。 -- name 没有索引，会退化为表锁SELECT * FROM user WHERE name = 王二 FOR UPDATE; —- 这部分是帮助理解 start，面试中可不背 —- 假设有一张名为 orders 的表，包含以下数据： CREATE TABLE orders ( id INT PRIMARY KEY, order_no VARCHAR(255), amount DECIMAL(10,2), status VARCHAR(50), INDEX (order_no) -- order_no 上有索引); 表中的数据是这样的： 如果我们通过主键索引执行 SELECT FOR UPDATE，确实只会锁定特定的行： START TRANSACTION;SELECT * FROM orders WHERE id = 1 FOR UPDATE;-- 对 id=1 的行进行操作COMMIT; 由于 id 是主键，所以只会锁定 id1 这行，不会影响其他行的操作。其他事务依然可以对 id 2, 3, 4, 5 等行执行更新操作，因为它们没有被锁定。如果使用 order_no 这个普通索引执行 SELECT FOR UPDATE，也只会锁定特定的行： START TRANSACTION;SELECT * FROM orders WHERE order_no = 10001 FOR UPDATE;-- 对 order_no=10001 的行进行操作COMMIT; 因为 order_no 是唯一索引，所以只会锁定 order_no10001 这行，不会影响其他行的操作。 但如果 WHERE 条件是 status’pending’，而 status 上没有索引： START TRANSACTION;SELECT * FROM orders WHERE status = pending FOR UPDATE;-- 对 status=pending 的行进行操作COMMIT; 就会退化为表锁，因为在这种情况下，MySQL 需要全表扫描检查每一行的 status。 —- 这部分是帮助理解 end，面试中可不背 —- 说说记录锁吧？记录锁是行锁最基本的表现形式，当我们使用唯一索引或者主键索引进行等值查询时，MySQL 会为该记录自动添加排他锁，禁止其他事务读取或者修改锁定记录。 例如： SELECT * FROM table WHERE id = 1 FOR UPDATE; -- 加X锁UPDATE table SET name = 王二 WHERE id = 1; -- 隐式加X锁 间隙锁了解吗？间隙锁用于在范围查询时锁定记录之间的“间隙”，防止其他事务在该范围内插入新记录。仅在可重复读及以上的隔离级别下生效，主要用于防止幻读。 —- 这部分是帮助大家理解 start，面试中可不背 —- 例如事务 A 锁定了 (1000,2000) 区间，会阻止事务 B 在此区间插入新记录： -- 事务ABEGIN;SELECT * FROM orders WHERE amount BETWEEN 1000 AND 2000 FOR UPDATE;-- 事务B尝试插入会被阻塞INSERT INTO orders VALUES(null,1500,pending); -- 阻塞/code 假设表 test_gaplock 有 id、age、name 三个字段，其中 id 是主键，age 上有索引，并插入了 4 条数据。 CREATE TABLE `test_gaplock` ( `id` int(11) NOT NULL, `age` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`), KEY `age` (`age`)) ENGINE=InnoDB;insert into test_gaplock values(1,1,张三),(6,6,吴老二),(8,8,赵四),(12,12,熊大); 间隙锁会锁住： (−∞, 1)：最小记录之前的间隙。 (1, 6)、(6, 8)、(8, 12)：记录之间的间隙。 (12, +∞)：最大记录之后的间隙。 假设有两个事务，T1 执行以下语句： START TRANSACTION;SELECT * FROM test_gaplock WHERE age 5 FOR UPDATE; T2 执行以下语句： START TRANSACTION;INSERT INTO test_gaplock VALUES (7, 7, 王五); T1 会锁住 (6, 8) 的间隙，防止其他事务在这个范围内插入新记录。 T2 在插入 (7, 7, 王五) 时，会被阻塞，可以在另外一个会话中执行 SHOW ENGINE INNODB STATUS 查看到间隙锁的信息。 执行什么命令会加上间隙锁？在可重复读隔离级别下，执行FOR UPDATE / LOCK IN SHARE MODE等加锁语句，且查询条件是范围查询时，就会自动加上间隙锁。 -- SELECT ... FOR UPDATE + 范围查询SELECT * FROM user WHERE score 100 FOR UPDATE;-- SELECT ... LOCK IN SHARE MODE + 范围查询SELECT * FROM user WHERE id BETWEEN 10 AND 20 LOCK IN SHARE MODE;-- UPDATE/DELETE + 范围查询DELETE FROM user WHERE score 50;","tags":["基础","Mysql"],"categories":["基础笔记"]},{"title":"2025.6.10学习日记","path":"/2025/06/10/学习日记25年6月/2025.6.10学习笔记/","content":"学习内容最近学习重心想转到算法相关,把随想录和题单刷一遍之后做力扣周赛,太长时间没做需要复健一下. 1. 打卡力扣每日简单的字符串计数找最大最小. 2. 看Mysql基础笔记同步在了Mysql学习笔记中. 3. 代码随想录二叉树的三种遍历和迭代遍历.今天踩了一个小坑:在实现迭代调用的统一写法时,需要在Deque中插入一个null元素作为是否遍历过的标志,但是实现Deque时用的ArrayDeque,这个实现方式不能插入null元素,需要使用LinkedList实现才可以.感觉这几个集合框架还是有必要学的深入一些的,不管是算法还是写项目都有很大帮助. 明天开始跑项目ply生活记录1. 健身今天健身房练上肢 肩 胸 核心","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-06"]},{"title":"JVM学习笔记","path":"/2025/06/09/基础笔记/JVM学习笔记/","content":"运行时数据区域 程序计数器记录正在执行的虚拟机字节码指令的地址,如果是本地方法则为空. Java虚拟机栈每个 Java ⽅法在执⾏的同时会创建⼀个栈帧⽤于存储局部变量表、操作数栈、常量池引⽤等信息。从⽅法调⽤直⾄执⾏完成的过程，对应着⼀个栈帧在 Java 虚拟机栈中⼊栈和出栈的过程。 该区域可能抛出的异常: 当线程请求的栈深度超过最⼤值，会抛出 StackOverflowError 异常； 栈进⾏动态扩展时如果⽆法申请到⾜够内存，会抛出 OutOfMemoryError 异常。 本地方法栈本地方法栈和Java虚拟机栈类似,区别在于本地方法栈为虚拟机使用到的 Native ⽅法服务. 堆所有对象都在这⾥分配内存，是垃圾收集的主要区域（”GC 堆”）。现代垃圾收集器基本都是采⽤分代收集算法，其主要的思想是针对不同类型的对象采取不同的垃圾回收算法。可以将堆分成两块： 新⽣代（Young Generation） ⽼年代（Old Generation）堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 ⽅法区⽤于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。和堆⼀样不需要连续的内存，并且可以动态扩展，动态扩展失败⼀样会抛出 OutOfMemoryError 异常。在 JDK 1.8 之后，原来永久代的数据被分到了堆和元空间中。元空间存储类的元信息，静态变量和常量池等放⼊堆中。 运行时常量池是⽅法区的⼀部分。Class ⽂件中的常量池（编译器⽣成的字⾯量和符号引⽤）会在类加载后被放⼊这个区域。除了在编译期⽣成的常量，还允许动态⽣成，例如 String 类的 intern()。 直接内存JDK 1.4 新引⼊了 NIO 类，它可以使⽤ Native 函数库直接分配堆外内存，然后通过 Java 堆⾥的 DirectByteBuffer对象作为这块内存的引⽤进⾏操作。这样能在⼀些场景中显著提⾼性能，因为避免了在堆内存和堆外内存来回拷⻉数据。 理解运行时的数据区 垃圾收集垃圾收集主要是针对堆和⽅法区进⾏。程序计数器、虚拟机栈和本地⽅法栈这三个区域属于线程私有的，只存在于线程的⽣命周期内，线程结束之后就会消失，因此不需要对这三个区域进⾏垃圾回收。 判断一个对象是否可被回收1. 引⽤计数算法为对象添加⼀个引⽤计数器，当对象增加⼀个引⽤时计数器加 1，引⽤失效时计数器减 1。引⽤计数为 0 的对象可被回收。 但由于对象之间循环引用的存在，引⽤计数器也会失效。 2. 可达性分析算法以 GC Roots 为起始点进⾏搜索，可达的对象都是存活的，不可达的对象可被回收。Java 虚拟机使⽤该算法来判断对象是否可被回收，GC Roots ⼀般包含以下内容： 虚拟机栈中局部变量表中引⽤的对象 本地⽅法栈中 JNI 中引⽤的对象 ⽅法区中类静态属性引⽤的对象 ⽅法区中的常量引⽤的对象 3. 方法区的回收因为⽅法区主要存放永久代对象，⽽永久代对象的回收率⽐新⽣代低很多，所以在⽅法区上进⾏回收性价⽐不⾼。主要是对常量池的回收和对类的卸载。为了避免内存溢出，在⼤量使⽤反射和动态代理的场景都需要虚拟机具备类卸载功能。类的卸载条件很多，需要满⾜以下三个条件，并且满⾜了条件也不⼀定会被卸载： 该类所有的实例都已经被回收，此时堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地⽅被引⽤，也就⽆法在任何地⽅通过反射访问该类⽅法。 4. finalize()类似 C++ 的析构函数，⽤于关闭外部资源。但是⽤ try-finally 可以做得更好，并且 finalize ⽅法运⾏代价很⾼，不确定性⼤，⽆法保证各个对象的调⽤顺序，因此最好不要使⽤。 引用类型⽆论是通过引⽤计数算法判断对象的引⽤数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引⽤有关。Java 提供了四种强度不同的引⽤类型。 1. 强引⽤被强引⽤关联的对象不会被回收。使⽤ new ⼀个新对象的⽅式来创建强引⽤。Object obj = new Object(); 2. 软引⽤被软引⽤关联的对象只有在内存不够的情况下才会被回收。使⽤ SoftReference 类来创建软引⽤。 Object obj = new Object();SoftReferenceObject sf = new SoftReferenceObject(obj);obj = null; // 使对象只被软引⽤关联 3. 弱引⽤被弱引⽤关联的对象⼀定会被回收，也就是说它只能存活到下⼀次垃圾回收发⽣之前。使⽤ WeakReference 类来创建弱引⽤。 Object obj = new Object();WeakReferenceObject wf = new WeakReferenceObject(obj);obj = null; 4. 虚引⽤⼜称为幽灵引⽤或者幻影引⽤，⼀个对象是否有虚引⽤的存在，不会对其⽣存时间造成影响，也⽆法通过虚引⽤得到⼀个对象。为⼀个对象设置虚引⽤的唯⼀⽬的是能在这个对象被回收时收到⼀个系统通知。使⽤ PhantomReference 来创建虚引⽤。 Object obj = new Object();PhantomReferenceObject pf = new PhantomReferenceObject(obj, null);obj = null; 垃圾收集算法1. 标记-清除算法最基础的收集算法，分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。不足: 效率问题，标记和清除两个过程的效率都不⾼； 空间问题，会产⽣⼤量不连续的内存碎⽚，导致⽆法给⼤对象分配内存。 2. 标记-整理算法让所有存活的对象都向⼀端移动，然后直接清理掉端边界以外的内存。 优点: 不会产⽣内存碎⽚ 不⾜: 需要移动⼤量对象，处理效率⽐较低。 3. 复制算法将内存划分为⼤⼩相等的两块，每次只使⽤其中⼀块，当这⼀块内存⽤完了就将还存活的对象复制到另⼀块上⾯，然后再把使⽤过的内存空间进⾏⼀次清理。不⾜是只使⽤了内存的⼀半。 4. 分代收集算法现在的商业虚拟机采⽤分代收集算法，它根据对象存活周期将内存划分为⼏块，不同块采⽤适当的收集算法。⼀般将堆分为新⽣代和⽼年代。 新⽣代使⽤：复制算法 ⽼年代使⽤：标记 - 清除 或者 标记 - 整理 算法 垃圾收集器以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使⽤。 单线程与多线程：单线程指的是垃圾收集器只使⽤⼀个线程，⽽多线程使⽤多个线程； 串⾏与并⾏：串⾏指的是垃圾收集器与⽤户程序交替执⾏，这意味着在执⾏垃圾收集的时候需要停顿⽤户程序；并⾏指的是垃圾收集器和⽤户程序同时执⾏。除了 CMS 和 G1 之外，其它垃圾收集器都是以串⾏的⽅式执⾏。 1. Serial 收集器串行的垃圾收集器，是最基本、历史最悠久的垃圾收集器。单CPU环境下，Serial 收集器由于没有线程交互的开销，可以获得最高的单线程收集效率。 优点: 简单、容易实现 缺点: 单线程、停顿时间⻓ 2. ParNew 收集器ParNew 收集器是 Serial 收集器的多线程版本，它是 Server 场景下默认的新⽣代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合使⽤。 3. Parallel Scavenge 收集器与 ParNew ⼀样是多线程收集器。其它收集器⽬标是尽可能缩短垃圾收集时⽤户线程的停顿时间，⽽它的⽬标是达到⼀个可控制的吞吐量，因此它被称为“吞吐量优先”收集器。这⾥的吞吐量指 CPU ⽤于运⾏⽤户程序的时间占总时间的⽐值。 4. Serial Old 收集器是 Serial 收集器的⽼年代版本，也是给 Client 场景下的虚拟机使⽤。如果⽤在 Server 场景下，它有两⼤⽤途： 在 JDK 1.5 以及之前版本（Parallel Old 诞⽣以前）中与 Parallel Scavenge 收集器搭配使⽤。 作为 CMS 收集器的后备预案，在并发收集发⽣ Concurrent Mode Failure 时使⽤。 5. Parallel Old 收集器是 Parallel Scavenge 收集器的⽼年代版本。在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。分为以下四个流程： 初始标记：仅仅只是标记⼀下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进⾏ GC Roots Tracing 的过程，它在整个回收过程中耗时最⻓，不需要停顿。 重新标记：为了修正并发标记期间因⽤户程序继续运作⽽导致标记产⽣变动的那⼀部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最⻓的并发标记和并发清除过程中，收集器线程都可以与⽤户线程⼀起⼯作，不需要进⾏停顿。具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利⽤率不够⾼。 ⽆法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于⽤户线程继续运⾏⽽产⽣的垃圾，这部分垃圾只能到下⼀次 GC 时才能进⾏回收。由于浮动垃圾的存在，因此需要预留出⼀部分内存，意味着 CMS 收集不能像其它收集器那样等待⽼年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启⽤ Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎⽚，往往出现⽼年代空间剩余，但⽆法找到⾜够⼤连续空间来分配当前对象，不得不提前触发⼀次 Full GC。 7. G1 收集器G1（Garbage-First），它是⼀款⾯向服务端应⽤的垃圾收集器，在多 CPU 和⼤内存的场景下有很好的性能。堆被分为新⽣代和⽼年代，其它收集器进⾏收集的范围都是整个新⽣代或者⽼年代，⽽ G1 可以直接对新⽣代和⽼年代⼀起回收。G1 把堆划分成多个⼤⼩相等的独⽴区域（Region），新⽣代和⽼年代不再物理隔离。通过引⼊ Region 的概念，从⽽将原来的⼀整块内存空间划分成多个的⼩空间，使得每个⼩空间可以单独进⾏垃圾回收。这种划分⽅法带来了很⼤的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护⼀个优先列表，每次根据允许的收集时间，优先回收价值最⼤的 Region。每个 Region 都有⼀个 Remembered Set，⽤来记录该 Region 对象的引⽤对象所在的 Region。通过使⽤Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作⼤致可划分为以下⼏个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因⽤户程序继续运作⽽导致标记产⽣变动的那⼀部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs ⾥⾯，最终标记阶段需要把 Remembered Set Logs的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并⾏执⾏。 筛选回收：⾸先对各个 Region 中的回收价值和成本进⾏排序，根据⽤户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与⽤户程序⼀起并发执⾏，但是因为只回收⼀部分 Region，时间是⽤户可控制的，⽽且停顿⽤户线程将⼤幅度提⾼收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运⾏期间不会产⽣内存空间碎⽚。 可预测的停顿：能让使⽤者明确指定在⼀个⻓度为 M 毫秒的时间⽚段内，消耗在 GC 上的时间不得超过 N 毫秒。 内存分配与回收策略’Minor GC 和 Full GC Minor GC：回收新⽣代，因为新⽣代对象存活时间很短，因此 Minor GC 会频繁执⾏，执⾏的速度⼀般也会⽐较快。 Full GC：回收⽼年代和新⽣代，⽼年代对象存活时间⻓，因此 Full GC 很少执⾏，执⾏速度会⽐ Minor GC 慢很多。 内存分配策略 对象优先在 Eden 分配⼤多数情况下，对象在新⽣代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 ⼤对象直接进⼊⽼年代⼤对象是指需要连续内存空间的对象，最典型的⼤对象是那种很⻓的字符串以及数组。经常出现⼤对象会提前触发垃圾收集以获取⾜够的连续空间分配给⼤对象。-XX:PretenureSizeThreshold，⼤于此值的对象直接在⽼年代分配，避免在 Eden 和 Survivor之间的⼤量内存复制。 ⻓期存活的对象进⼊⽼年代为对象定义年龄计数器，对象在 Eden 出⽣并经过 Minor GC 依然存活，将移动到 Survivor中，年龄就增加 1 岁，增加到⼀定年龄则移动到⽼年代中。-XX:MaxTenuringThreshold ⽤来定义年龄的阈值。 动态对象年龄判定虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升⽼年代，如果在Survivor 中相同年龄所有对象⼤⼩的总和⼤于 Survivor 空间的⼀半，则年龄⼤于或等于该年龄的对象可以直接进⼊⽼年代，⽆需等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保在发⽣ Minor GC 之前，虚拟机先检查⽼年代最⼤可⽤的连续空间是否⼤于新⽣代所有对象总空间，如果条件成⽴的话，那么 Minor GC 可以确认是安全的。如果不成⽴的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查⽼年代最⼤可⽤的连续空间是否⼤于历次晋升到⽼年代对象的平均⼤⼩，如果⼤于，将尝试着进⾏⼀次 Minor GC；如果⼩于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进⾏⼀次 Full GC。 Full GC 的触发条件对于 Minor GC，其触发条件⾮常简单，当 Eden 空间满时，就将触发⼀次 Minor GC。⽽ FullGC 则相对复杂，有以下条件： 调⽤ System.gc()只是建议虚拟机执⾏ Full GC，但是虚拟机不⼀定真正去执⾏。不建议使⽤这种⽅式，⽽是让虚拟机管理内存。 ⽼年代空间不⾜⽼年代空间不⾜的常⻅场景为前⽂所讲的⼤对象直接进⼊⽼年代、⻓期存活的对象进⼊⽼年代等。为了避免以上原因引起的 Full GC，应当尽量不要创建过⼤的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调⼤新⽣代的⼤⼩，让对象尽量在新⽣代被回收掉，不进⼊⽼年代。还可以通过 -XX:MaxTenuringThreshold 调⼤对象进⼊⽼年代的年龄，让对象在新⽣代多存活⼀段时间。 空间分配担保失败使⽤复制算法的 Minor GC 需要⽼年代的内存空间作担保，如果担保失败会执⾏⼀次 FullGC。具体内容请参考上⾯的第 5 ⼩节。 JDK 1.7 及以前的永久代空间不⾜在 JDK 1.7 及以前，HotSpot 虚拟机中的⽅法区是⽤永久代实现的，永久代中存放的为⼀些Class 的信息、常量、静态变量等数据。当系统中要加载的类、反射的类和调⽤的⽅法较多时，永久代可能会被占满，在未配置为采⽤CMS GC 的情况下也会执⾏ Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出java.lang.OutOfMemoryError。为避免以上原因引起的 Full GC，可采⽤的⽅法为增⼤永久代空间或转为使⽤ CMS GC。 Concurrent Mode Failure执⾏ CMS GC 的过程中同时有对象要放⼊⽼年代，⽽此时⽼年代空间不⾜（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不⾜），便会报 Concurrent Mode Failure 错误，并触发Full GC。 类加载机制类是在运⾏期间第⼀次使⽤时动态加载的，⽽不是⼀次性加载所有类。因为如果⼀次性加载，会占⽤很多的内存。 类的生命周期 包括以下 7 个阶段： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使⽤（Using） 卸载（Unloading） 类的加载过程包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载加载是类加载的⼀个阶段，注意不要混淆。加载过程完成以下三件事： 通过类的完全限定名称获取定义该类的⼆进制字节流。 将该字节流表示的静态存储结构转换为⽅法区的运⾏时存储结构。 在内存中⽣成⼀个代表该类的 Class 对象，作为⽅法区中该类各种数据的访问⼊⼝。 其中⼆进制字节流可以从以下⽅式中获取： 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础。 从⽹络中获取，最典型的应⽤是 Applet。 运⾏时计算⽣成，例如动态代理技术，在 java.lang.reflect.Proxy 使⽤ProxyGenerator.generateProxyClass 的代理类的⼆进制字节流。由其他⽂件⽣成，例如由 JSP ⽂件⽣成对应的 Class 类。 2. 验证确保 Class ⽂件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机⾃身的安全。 3. 准备类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使⽤的是⽅法区的内存。应该注意到，实例化不是类加载的⼀个过程，类加载发⽣在所有实例化操作之前，并且类加载只进⾏⼀次，实例化可以进⾏多次。初始值⼀般为 0 值，例如下⾯的类变量 value 被初始化为 0 ⽽不是 123。public static int value = 123;如果类变量是常量，那么它将初始化为表达式所定义的值⽽不是 0。例如下⾯的常量 value 被初始化为 123 ⽽不是 0。public static final int value = 123; 4. 解析将常量池的符号引⽤替换为直接引⽤的过程。其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了⽀持 Java 的动态绑定。 5. 初始化初始化阶段才真正开始执⾏类中定义的 Java 程序代码。 初始化阶段是虚拟机执⾏类构造器clinit() ⽅法的过程。在准备阶段，类变量已经赋过⼀次系统要求的初始值，⽽在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 clinit() 是由编译器⾃动收集类中所有类变量的赋值动作和静态语句块中的语句合并产⽣的，编译器收集的顺序由语句在源⽂件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： public class Test static i = 0; // 给变量赋值可以正常编译通过System.out.print(i); // 这句编译器会提示“⾮法向前引⽤”static int i = 1; 由于⽗类的 () ⽅法先执⾏，也就意味着⽗类中定义的静态语句块的执⾏要优先于⼦类。例如以下代码： static class Parent public static int A = 1;static A = 2;static class Sub extends Parent public static int B = A;public static void main(String[] args) System.out.println(Sub.B); // 2 静态代码块和赋值动作是按照代码的顺序执行的。接⼝中不可以使⽤静态语句块，但仍然有类变量初始化的赋值操作，因此接⼝与类⼀样都会⽣成 clinit() ⽅法。 但接⼝与类不同的是，执⾏接⼝的 clinit() ⽅法不需要先执⾏⽗接⼝的 clinit() ⽅法。只有当⽗接⼝中定义的变量使⽤时，⽗接⼝才会初始化。另外，接⼝的实现类在初始化时也⼀样不会执⾏接⼝的 clinit() ⽅法。虚拟机会保证⼀个类的 clinit() ⽅法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化⼀个类，只会有⼀个线程执⾏这个类的 clinit() ⽅法，其它线程都会阻塞等待，直到活动线程执⾏ clinit() ⽅法完毕。 如果在⼀个类的clinit()⽅法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 类的初始化时机1.主动引用虚拟机规范中并没有强制约束何时进⾏加载，但是规范严格规定了有且只有下列五种情况必须对类进⾏初始化（加载、验证、准备都会随之发⽣）: 遇到 new、getstatic、putstatic 或 invokestatic 这 4 条字节码指令时，如果类没有进⾏过初始化，则需要先触发其初始化。 使⽤ java.lang.reflect 包的方法对类进⾏反射调⽤的时候，如果类没有进⾏过初始化，则需要先触发其初始化。 当初始化⼀个类时，如果发现其父类还没有进⾏过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，⽤于执⾏主类（包含 main() ⽅法的那个类）的初始化。 使⽤ JDK 7 新加入的动态语言⽀持时，如果⼀个 java.lang.invoke.MethodHandle 实例最后的解析结果 REF_getStatic、REF_putStatic、REF_invokeStatic 的⽅法句柄，并且这个⽅法句柄所对应的类没有进⾏过初始化，则需要先出触发其初始化。 2.被动引用以上 5 种场景中的⾏为称为对⼀个类进⾏主动引⽤。除此之外，所有引⽤类的⽅式都不会触发初始化，称为被动引⽤。被动引⽤的常⻅例⼦包括：通过⼦类引⽤⽗类的静态字段，不会导致⼦类初始化。 System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引⽤类，不会触发此类的初始化。该过程会对数组类进⾏初始化，数组类是⼀个由虚拟机⾃动⽣成的、直接继承⾃ Object 的⼦类，其中包含了数组的属性和⽅法。 SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存⼊调⽤类的常量池中，本质上并没有直接引⽤到定义常量的类，因此不会触发定义常量的类的初始化。 System.out.println(ConstClass.HELLOWORLD); 类加载器分类从 Java 虚拟机的⻆度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），使⽤ C++ 实现，是虚拟机⾃身的⼀部分； 所有其它类的加载器，使⽤ Java 实现，独⽴于虚拟机，继承⾃抽象类java.lang.ClassLoader。 从 Java 开发⼯程的⻆度来看，类加载器可以划分得更细致一些： 启动类加载器（Bootstrap ClassLoader）：这个类加载器负责将存放在 JAVA_HOME\\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引⽤，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器去处理，那么直接⽤ null 代替即可。 扩展类加载器（Extension ClassLoader）：这个类加载器是在类 java.lang.ClassLoader 的构造函数中被调⽤的。它负责将 libext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使⽤扩展类加载器。 应用程序类加载器（Application ClassLoader）：这个类加载器是在类 java.lang.ClassLoader 的构造函数中被调⽤的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，所以也被称为系统类加载器。它负责将⽤户类路径（ClassPath）上所指定的类库加载到内存中。开发者可以直接使⽤这个类加载器，如果应⽤程序中没有定义过⾃定义的类加载器，一般情况下这个就是程序中默认的类加载器。 ⼯作过程⼀个类加载器⾸先将类加载请求转发到⽗类加载器，只有当⽗类加载器⽆法完成时才尝试⾃⼰加载。 好处使得 Java 类随着它的类加载器⼀起具有⼀种带有优先级的层次关系，从⽽使得基础类得到统⼀。例如 java.lang.Object 存放在 rt.jar 中，如果编写另外⼀个 java.lang.Object 并放到 ClassPath中，程序可以编译通过。由于双亲委派模型的存在，所以在 rt.jar 中的 Object ⽐在 ClassPath 中的 Object 优先级更⾼，这是因为 rt.jar 中的 Object 使⽤的是启动类加载器，⽽ ClassPath 中的 Object 使⽤的是应⽤程序类加载器。rt.jar 中的 Object 优先级更⾼，那么程序中所有的 Object 都是这个Object。","tags":["JVM","基础"],"categories":["基础笔记"]},{"title":"2025.6.9学习日记","path":"/2025/06/09/学习日记25年6月/2025.6.9学习笔记/","content":"学习内容1. 力扣每日一题 ＋ 昨天的每日一题昨天又忘打卡力扣了,还欠了13天的,得五个月之后才能全续上了.今天的每日是字典序第K数字.昨天的也是字典序相关.做了两道字典序感觉理解了. 2. 学习JVM相关基础学习了JVM相关,并且记了一篇笔记. 3. 把KMP算法实践一下手画梳理了一下流程,然后构造了两遍之后感觉理解很多了.个人理解KMP的核心思想就是让主串索引不后退,匹配串索引通过next数组快速找到前缀相同的下标位置继续匹配.核心就是next数组的构造.这个算法理解后实现也比较复杂,有一堆的边界条件,一个比较好的思路是在主串和匹配串前面加一个空格作为哨兵,然后边界问题会少很多,只需要比较j+1和主串i位置即可. 4. 学习项目文档生活记录1. 足球训练今天七点起床,下楼练球,练颠球和逆足.10","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-06"]},{"title":"Java并发篇","path":"/2025/06/07/基础笔记/Java并发篇/","content":"使用线程三种方法 继承Thread类需要重写run方法，然后调用start方法启动线程。 实现Runnable接口需要重写run方法，然后调用start方法启动线程。 实现Callable接口需要重写call方法，然后调用start方法启动线程。有返回值，通过 FutureTask 进⾏封装。 实现接口还是继承类？实现接口更好一些，java不支持多继承，但可以实现多个接口。 基础线程机制ExecutorExecutor用来管理多个异步操作（多个任务互不干扰，不需要同步操作）。 主要有三种Executor：CachedThreadPool：⼀个任务创建⼀个线程；FixedThreadPool：所有任务只能使⽤固定⼤⼩的线程；SingleThreadExecutor：相当于⼤⼩为 1 的 FixedThreadPool。 DaemonDaemon守护线程是⼀个服务线程，⽤于为其他线程提供服务。所有非守护线程都执⾏完毕后，无论有没有守护线程，程序都会退出。线程启动前可以通过setDaemon() 方法来设置该线程是否为守护线程。 sleep()Thread.sleep() 使当前线程暂停执⾏指定的时间，暂停期间，其他线程可以继续运⾏，不会受到阻塞。sleep()可能会抛出InterruptedException异常，异常不会传回main()，所以必须在Thread类中捕获并处理。 yield()代表线程已经走完了重要的部分，可以让其他线程有机会执行。 中断线程完成会自动关闭，但是如果线程异常也会提前关闭。 InterruptedException 异常线程在 sleep() 或 wait() 时被中断，会抛出 InterruptedException 异常。 interrupted()如果一个线程处于无限循环中，并且没有执行 sleep() 或 wait()，那么可以通过 interrupted() 来判断线程是否被中断。如果线程被中断，interrupted() 会返回 true。如果线程没有被中断，interrupted() 会返回 false。 Executor的中断操作调用shutdown()方法会等待所有任务执行完毕后关闭Executor。调用shutdownNow()方法会中断所有任务，相当于调用每个任务的interrupt()方法，然后关闭Executor。 如果只想中断某一个线程，可以通过submit() 方法提交一个Callable任务，然后调用Future的cancel()方法来中断任务。 互斥同步Java 提供了两种锁机制来控制多个线程对共享资源的互斥访问，第⼀个是 JVM 实现的synchronized，另⼀个是 JDK 实现的 ReentrantLock。 synchronizedsynchronized 是 Java 中的关键字，它可以修饰方法和代码块。修饰方法时，锁的是当前对象。修饰代码块时，锁的是括号中的对象。 ReentrantLockReentrantLock 是 java.util.concurrent（J.U.C）包中的锁。 比较synchronized和ReentrantLock 锁的实现synchronized 是 JVM 实现的，⽽ ReentrantLock 是 JDK 实现的。 性能新版本 Java 对 synchronized 进⾏了很多优化，例如⾃旋锁，synchronized 与ReentrantLock 的性能⼤致相同。 等待可中断当持有锁的线程⻓期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。ReentrantLock 可中断，⽽ synchronized 不⾏。 公平锁公平锁是指多个线程在等待同⼀个锁时，必须按照申请锁的时间顺序来依次获得锁。synchronized 中的锁是⾮公平的，ReentrantLock 默认情况下也是⾮公平的，但是也可以是公平的。 锁绑定多个条件⼀个 ReentrantLock 可以同时绑定多个 Condition 对象。 使用选择除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。因为synchronized 是 JVM 实现的，可以保证⾃⼰的线程安全，⽽ ReentrantLock 需要程序员手动释放锁， 线程之间的协作join()在一个线程中调用另一个线程的 join() 方法，会将当前线程挂起，直到被调用的线程执行完毕。 wait() notify() notifyAll()wait() 使当前线程等待，直到其他线程调⽤ notify() 或 notifyAll() 方法。notify() 随机唤醒⼀个等待线程，notifyAll() 唤醒所有等待线程。wait()必须在 synchronized 块中调⽤,否则会抛出IllegalMonitorStateException异常。 wait()和sleep()的区别 wait() 是 Object 的⽅法，sleep() 是 Thread 的⽅法。 wait() 会释放锁，sleep() 不会释放锁。 wait() 可以被 notify() 或 notifyAll() 唤醒，sleep() 只能被中断。 wait() 必须在 synchronized 块中调⽤，sleep() 可以在任何位置调⽤。 await() signal() signalAll()java.util.concurrent 提供的 Condition 类，可以再Condition 上调⽤ await() 使线程等待. 相比wait()，await() 可以指定时间，超过时间会⾃动唤醒。 线程状态一个线程通常只有一种状态,并且这里特指jvm线程状态,而不是操作系统线程状态.线程状态有6种: 新建(New)创建后尚未启用. 可运行(Runnable)正在Java虚拟机中执行,但是它可能正在等待操作系统分配处理器资源. 阻塞(Blocked)线程被阻塞,等待其他线程完成操作. 等待(Waiting)线程等待其他线程执行特定操作. 计时等待(Timed Waiting)线程等待指定的时间. 终止(Terminated)线程已经完成执行. 状态转换 J.U.C - AQSjava.util.concurrent （J.U.C）⼤⼤提⾼了并发性能，AQS 被认为是 J.U.C 的核⼼。AQS是AbstractQueuedSynchronizer的缩写,是Java并发包中用来实现锁的基础框架. CountDownLatchCountDownLatch用来控制一个或多个线程等待其他线程完成操作. CyclicBarrier用来控制多个线程互相等待，直到到达某个公共屏障点（common barrier point）。和CountDownLatch不同的是，CyclicBarrier的计数器可以被重置后使用，所以它被称为循环屏障。 SemaphoreSemaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 J.U.C - 其它组件FutureTask实现了Future接口和Runnable接口,可以作为Runnable被线程执行,也可以用来获取异步执行的结果.适用于需要异步执行任务,并且需要获取结果的场景. BlockingQueue阻塞队列,可以用来实现生产者-消费者模式. java.util.concurrent.BlockingQueue 接⼝有以下阻塞队列的实现：FIFO 队列 ：LinkedBlockingQueue、ArrayBlockingQueue（固定⻓度）优先级队列 ：PriorityBlockingQueue ForkJoin和MapReduce类似,可以将⼤量的数据拆分成⼩量的数据，然后分⽴计算，最后将结果合并。 Java 内存模型Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到⼀致的内存访问效果。 主内存和工作内存主内存是所有线程共享的内存区域，工作内存是每个线程独有的内存区域。 所有的变量都存储在主内存中，每个线程还有⾃⼰的⼯作内存，⼯作内存存储在⾼速缓存或者寄存器中，保存了该线程使⽤的变量的主内存副本拷⻉。线程只能直接读写⾃⼰的⼯作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。 内存间的交互操作 java内存模型规定了8种操作来完成主内存和工作内存之间的交互操作：read：把⼀个变量的值从主内存传输到⼯作内存中load：在 read 之后执⾏，把 read 得到的值放⼊⼯作内存的变量副本中use：把⼯作内存中⼀个变量的值传递给执⾏引擎assign：把⼀个从执⾏引擎接收到的值赋给⼯作内存的变量store：把⼯作内存的⼀个变量的值传送到主内存中write：在 store 之后执⾏，把 store 得到的值放⼊主内存的变量中lock：作⽤于主内存的变量unlock:作⽤于主内存的变量 内存模型的三大特性原子性java内存模型保证了read、load、use、assign、store、write这6个操作是具有原子性的。但是不保证这6个操作的组合是具有原子性的。AtomicInteger 是⼀个提供原子操作的 Integer 类。除了使用原子类外，还可以通过 synchronized 关键字来保证操作的原子性。 可见性可⻅性指当⼀个线程修改了共享变量的值，其它线程能够⽴即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可⻅性的。主要有三种可见性的实现方式: volatile synchronized，对⼀个变量执⾏ unlock 操作之前，必须把变量值同步回主内存。 final，被 final 关键字修饰的字段在构造⽅法中⼀旦初始化完成，并且没有发⽣ this 逃逸（其它线程通过 this 引⽤访问到初始化了⼀半的对象），那么其它线程就能看⻅ final 字段的值。 有序性有序性指的是在本线程内观察，所有操作都是有序的；如果在⼀个线程观察另⼀个线程，所有操作都是无序的。Java 内存模型是通过禁止指令重排序来保证有序性的。主要有两种有序性的实现方式: volatile synchronized，对⼀个变量执⾏ unlock 操作之前，必须把变量值同步回主内存。 先⾏发⽣原则先⾏发⽣原则是指如果在程序中两个操作的先后顺序与代码中的顺序相同，那么这两个操作就会先⾏发⽣。 1. 单⼀线程原则在单⼀线程中，在程序前⾯的操作先⾏发⽣于后⾯的操作。 2. 管程锁定规则⼀个 unlock 操作先⾏发⽣于后⾯对同⼀个锁的 lock 操作。 3. volatile 变量规则对⼀个 volatile 变量的写操作先⾏发⽣于后⾯对这个变量的读操作。 4. 线程启动规则Thread 对象的 start() ⽅法调⽤先⾏发⽣于此线程的每⼀个动作。 5. 线程加⼊规则Thread 对象的结束先⾏发⽣于 join() ⽅法返回。 6. 线程中断规则对线程interrupt()⽅法的调⽤先⾏发⽣于被中断线程的代码检测到中断事件的发⽣，可以通过 interrupted() ⽅法检测到是否有中断发⽣。 7. 对象终结规则⼀个对象的初始化完成（构造⽅法执⾏结束）先⾏发⽣于它的 finalize() ⽅法的开始。 8. 传递性如果操作 A 先⾏发⽣于操作 B，操作 B 先⾏发⽣于操作 C，那么操作 A 先⾏发⽣于操作 C。 线程安全多个线程不管以何种⽅式访问某个类，并且在主调代码中不需要任何额外的同步或协调，这个类都能表现出正确的⾏为，那么就称这个类是线程安全的。 线程安全有以下几种实现方法:不可变不可变（Immutable）的对象⼀定是线程安全的，不需要再采取任何的线程安全保障措施。只要⼀个不可变的对象被正确地构建出来，永远也不会看到它在多个线程之中处于不⼀致的状态。多线程环境下，应当尽量使对象成为不可变，来满⾜线程安全。 不可变的类型：final 关键字修饰的基本数据类型String枚举类型Number 部分⼦类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等⼤数据类型。但同为 Number 的原⼦类 AtomicInteger 和 AtomicLong 则是可变的。对于集合类型，我们可以使⽤ Collections.unmodifiableXXX() 方法来获取⼀个不可变的集合。 互斥同步synchronized 和 ReentrantLock。 ⾮阻塞同步阻塞同步是一种悲观的并发策略，即认为只要不去做正确的同步措施，那就肯定会出现问题。而非阻塞是一种基于冲突检测的乐观并发策略，即不加锁，但是如果存在冲突，就重试当前操作直到成功。 CAS硬件⽀持的原⼦性操作最典型的是：⽐较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执⾏操作时，只有当 V的值等于 A，才将 V 的值更新为 B。 乐观锁需要操作、冲突检测这两个步骤具备原⼦性，这⾥就不能再使⽤互斥同步来保证了，只能靠硬件来完成。 AtomicIntegerJ.U.C 包⾥⾯的整数原⼦类 AtomicInteger 的⽅法调⽤了 Unsafe 类的 CAS 操作。 ABA如果⼀个变量初次读取的时候是 A 值，它的值被改成了 B，后来⼜被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。J.U.C 包提供了⼀个带有标记的原⼦引⽤类 AtomicStampedReference 来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。⼤部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改⽤传统的互斥同步可能会⽐原⼦类更⾼效。 无同步方案要保证线程安全，并不是⼀定就要进⾏同步。如果⼀个⽅法本来就不涉及共享数据，那它⾃然就⽆须任何同步措施去保证正确性。 栈封闭多个线程访问同⼀个⽅法的局部变量时，不会出现线程安全问题，因为局部变量存储在虚拟机栈中，属于线程私有的。 线程本地存储（Thread Local Storage）如果⼀个变量在线程的⼀个⽅法中被TLS变量存储，并被其他⽅法读取和修改，那么即使两个线程执⾏的是同⼀个代码，它们也会访问到不同的数据。 锁优化主要是针对synchronized关键字的优化。 ⾃旋锁⾃旋锁虽然能避免进⼊阻塞状态从⽽减少开销，但是它需要进⾏忙循环操作占⽤ CPU 时间，它只适⽤于共享数据的锁定状态很短的场景。 锁消除锁消除是指虚拟机即时编译器在运⾏时，对代码进⾏扫描，去除不可能存在共享数据竞争的锁，通过锁消除，可以节省毫无意义的请求锁时间。 锁粗化锁粗化是指虚拟机即时编译器在运⾏时，对代码进⾏扫描，将多个相邻的加锁操作合并为⼀个加锁操作，通过锁粗化，可以节省加锁和释放锁的时间。 轻量级锁JDK 1.6 引⼊了偏向锁和轻量级锁，从⽽让锁拥有了四个状态：⽆锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 轻量级锁相比传统的重量级锁，它使用CAS操作来避免线程阻塞和唤醒的开销，同时也避免了操作系统层面的线程调度。如果轻量级锁自旋或检测到有线程冲突，会升级为重量级锁。 偏向锁偏向锁是指在没有线程竞争的情况下，锁对象会偏向于使⽤它的线程，这样就不需要进⾏额外的加锁和解锁操作。 多线程开发良好的实践 给线程起个有意义的名字，这样可以⽅便找 Bug。 缩⼩同步范围，从⽽减少锁争⽤。例如对于 synchronized，应该尽量使⽤同步块⽽不是同步⽅法。 多⽤同步⼯具少⽤ wait()和 notify() 。⾸先，CountDownLatch, CyclicBarrier, Semaphore 和 Exchanger 这些同步类简化了编码操作，⽽⽤ wait()和 notify() 很难实现复杂控制流；其次，这些同步类是由最好的企业编写和维护，在后续的 JDK 中还会不断优化和完善。 使⽤ BlockingQueue 实现⽣产者消费者问题。 多⽤并发集合少⽤同步集合，例如应该使⽤ ConcurrentHashMap ⽽不是 Hashtable。 使⽤本地变量和不可变类来保证线程安全。 使⽤线程池⽽不是直接创建线程，这是因为创建线程代价很⾼，线程池可以有效地利⽤有限的线程来启动任务。","tags":["基础","Java","并发"],"categories":["基础笔记"]},{"title":"2025.6.7学习日记","path":"/2025/06/07/学习日记25年6月/2025.6.7学习笔记/","content":"学习内容1. 力扣每日一题https://leetcode.cn/problems/lexicographically-minimum-string-after-removing-stars?envType=daily-questionenvId=2025-06-07 先是尝试优先队列，最小堆找前面的最小元素，然后遇到*就删除，但时间复杂度为On2，特殊用例超时了。然后发现可以用栈模拟二十六个字母，栈中存储下标，遇到*出栈最小的字符。优化点：可以使用二进制掩码来表示最小的非空栈在哪里。 2. 学习完java并发篇笔记记录在java并发篇. 3. 把昨天的题用堆来完善一下维护小顶堆求前k大的元素。 4. 学习KMP算法.KMP算法是一种字符串匹配算法，它的时间复杂度为O(n+m)，其中n是文本串的长度，m是模式串的长度。贴一个模板,原理已经理解了，但是代码还需要再写一遍： class Solution // KMP 算法 // ss: 原串(string) pp: 匹配串(pattern) public int strStr(String ss, String pp) if (pp.isEmpty()) return 0; // 分别读取原串和匹配串的长度 int n = ss.length(), m = pp.length(); // 原串和匹配串前面都加空格，使其下标从 1 开始 ss = + ss; pp = + pp; char[] s = ss.toCharArray(); char[] p = pp.toCharArray(); // 构建 next 数组，数组长度为匹配串的长度（next 数组是和匹配串相关的） int[] next = new int[m + 1]; // 构造过程 i = 2，j = 0 开始，i 小于等于匹配串长度 【构造 i 从 2 开始】 for (int i = 2, j = 0; i = m; i++) // 匹配不成功的话，j = next(j) while (j 0 p[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++ if (p[i] == p[j + 1]) j++; // 更新 next[i]，结束本次循环，i++ next[i] = j; // 匹配过程，i = 1，j = 0 开始，i 小于等于原串长度 【匹配 i 从 1 开始】 for (int i = 1, j = 0; i = n; i++) // 匹配不成功 j = next(j) while (j 0 s[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++，结束本次循环后 i++ if (s[i] == p[j + 1]) j++; // 整一段匹配成功，直接返回下标 if (j == m) return i - m; return -1;","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-06"]},{"title":"java30天学习笔记","path":"/2025/06/06/基础笔记/java30天笔记/","content":"杂项最好不使用 clone()来进行复制,可以使用拷贝构造函数或拷贝工厂来复制对象. 抽象类提供了⼀种 IS-A 的关系接⼝更像是⼀种 LIKE-A 关系 Fail-Fast 机制Fail-Fast 机制是 Java 集合(Collection)中的⼀种错误机制。当多个线程对同一个集合的内容进⾏操作时，就可能产⽣线程安全问题。Fail-Fast 机制会⾃动检测到线程安全问题,在操作前后比较集合的结构变化次数是否相同，并抛出 ConcurrentModificationException 异常。 tips:禁⽌在foreach⾥执⾏元素的删除操作 容器的一些解析vector和arraylist的比较vector是同步的，所以开销更大vector每次扩容请求2倍，而arraylist是1.5倍 如果对线程安全有要求的话，可以选择Collections.synchronizedList() 或者使用CopyOnWriteArrayList保证线程安全。 CopyOnWriteArrayList写操作在拷贝的数组上进行，而读操作在原数组上进行。写操作需要加锁，防止并发写操作。适用于读多写少的场景。问题:内存占用约占原数组的两倍。数据一致性问题。 所以CopyOnWriteArrayList更加适合对内存不敏感以及实时性要求很高的场景。 LinkedListArrayList是基于数组实现的，而LinkedList是基于链表实现的。数组支持随机访问，而链表不支持随机访问。数组的插入和删除操作需要移动元素，而链表的插入和删除操作只需要修改指针。数组的空间利用率高，而链表的空间利用率低。LinkedList适用于需要频繁插入和删除元素的场景。 HashMap相当于分成了很多个桶，每个桶里面是一个链表，链表的每个节点是一个键值对，使用头插法插入节点。 ConcurrentHashMapConcurrentHashMap是线程安全的HashMap，它的实现方式是使用分段锁。ConcurrentHashMap将整个HashMap分成了多个段，每个段都是一个HashMap。每个段都有一个锁，当一个线程访问一个段时，其他线程也可以访问其他段。 LinkedHashMapLinkedHashMap是HashMap的子类，它的实现方式是使用双向链表。LinkedHashMap的迭代顺序是插入顺序或者访问顺序。可以通过LinkedHashMap实现LRU缓存。 WeakHashMap主要是用来实现缓存的。WeakHashMap的键是弱引用，当键不再被引用时，键值对会被自动移除。WeakHashMap的迭代器是弱引用的，所以在迭代时可能会出现空指针异常。 ConcurrentCacheConcurrentCache是一个线程安全的缓存，它的实现方式是使用ConcurrentHashMap。","tags":["java"],"categories":["基础笔记"]},{"title":"好用的快捷键和操作","path":"/2025/06/06/杂项笔记/好用的快捷键或操作/","content":"MAC系统操作截图到剪切板mac:shift+control+command+4 最小化窗口mac:command+m 关闭窗口mac:command+w 全屏mac:command+control+f VSCODE单行上下移动mac:shift+option+up/down 多行上下移动mac:shift+option+command+up/down 删除本行mac:command+shift+k 开关左侧项目树mac:command+1 重命名文件mac:fn+shift+f6 VSCODE picgo插件上传到图床mac:option+command+u 关闭终端窗口mac:command+j 终端Warp到hexo根目录cd /Users/mac/Blog/JakicDong.github.io hexo清理并且重新上传hexo clean hexo g hexo d 本地部署hexo s Edge浏览器标签页左右移动mac:command+option+left/right 关闭标签页mac:command+w 开发者工具mac:command+option+i fn+f12","tags":["快捷键"],"categories":["杂项笔记"]},{"title":"2025.6.6学习日记","path":"/2025/06/06/学习日记25年6月/2025.6.6学习笔记/","content":"学习内容1. 优化了一下个人博客的图片上传工作流由于大部分笔记在语雀中,导致图片上传后外链无法访问,所以今天通过picgo➕github搭建了一个图床.具体流程如下: 首先需要在github中创建一个仓库,用来存放图片 然后在picgo中配置github的仓库,将图片上传到github中操作如下:mac:shift+control+command+4 截图 然后在vscode中option+command+u上传到图床后会自动生成markdown的链接. 2. 优化了一下自己记笔记发博客的工作流语雀写笔记还是太麻烦了。所以准备以后还是用vscode写笔记,然后直接运行一行命令同步到仓库,而且还有补全工具。 3. 力扣两道每日一题：这题思路很清晰，贪心加一个栈的模拟题，实现有一些小细节需要注意。前K个高频元素：map记录频率，然后小顶堆维护前K个元素。（第一遍做用了单调栈，忘记用小顶堆维护了）做了关于最大最小堆的笔记。 4. 看了java容器相关的八股文记在java30天笔记里面了。 5. 项目部分看了session登录拦截器相关的部分6. 杂谈感觉需要增加基础的学习，项目可以先读一遍文档。","tags":["基础","日记","leetcode","项目","博客","picgo"],"categories":["学习日记","2025-06"]},{"title":"Hello World","path":"/2025/06/05/杂项笔记/hello-world/","content":"hexo的基本使用和模板常用模版 ---title: 个人博客搭建tags: [博客, hexo]date: 2025-06-01--- 快捷部署: hexo clean hexo g hexo d Quick Start创建一篇文章$ hexo new My New Post 更多信息: Writing 运行服务$ hexo server 更多信息: Server 生成静态文件$ hexo generate 更多信息: Generating 部署到远程站点$ hexo deploy 更多信息: Deployment 文章模版---# 基本信息title: title date: date tags: []categories: []description: # excerpt 也可 # 封面cover: banner: poster: # 海报（可选，全图封面卡片） topic: 标题上方的小字 # 可选 headline: 大标题 # 必选 caption: 标题下方的小字 # 可选 color: 标题颜色 # 可选# 插件sticky: # 数字越大越靠前mermaid:katex: mathjax: # 可选topic: # 专栏 idauthor: references:comments: # 设置 false 禁止评论indexing: # 设置 false 避免被搜索breadcrumb: # 设置 false 隐藏面包屑导航leftbar: rightbar:h1: # 设置为 隐藏标题type: # tech/story---","tags":["欢迎页"],"categories":["杂项笔记"]},{"title":"2025.6.5学习日记","path":"/2025/06/05/学习日记25年6月/2025.6.5学习日记/","content":"学习内容1. hexo轻量化框架搭建个人博客搭建了个人博客网站.简化了一下笔记的流程:直接本地写markdown笔记然后直接运行一行命令同步太仓库,比较方便.todo:后期可以考虑加一个打卡墙. 2. leetcode每日一题并查集的题,太久没做图论有点忘了 class Solution public String smallestEquivalentString(String s1, String s2, String baseStr) int[] fa = new int[26]; for (int i = 0; i 26; i++) fa[i] = i; for (int i = 0; i s1.length(); i++) merge(fa, s1.charAt(i) - a, s2.charAt(i) - a); char[] s = baseStr.toCharArray(); for (int i = 0; i s.length; i++) s[i] = (char) (find(fa, s[i] - a) + a); return new String(s); private int find(int[] fa, int x) if (fa[x] != x) fa[x] = find(fa, fa[x]); return fa[x]; private void merge(int[] fa, int x, int y) int fx = find(fa, x); int fy = find(fa, y); // 把大的代表元指向小的代表元 if (fx fy) fa[fy] = fx; else fa[fx] = fy; 3. 健身练胸日","tags":["日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"技术派项目笔记","path":"/2025/06/04/项目笔记/技术派项目笔记/","content":"开启新项目要考虑的事情业务模块拆解首先对业务模块进行拆解，除了业务属性纬度以外，还有一个很重要的属性就是参与者角色。 首先对功能、模块划分、概要设计，详细设计有初步的了解。 主要就是功能模块设计 + DB 的设计。 启动项目mysql 启动redis 启动：路径 ： D:\\workTools\\Redis-x64-3.2.100 redis-server.exe redis.windows.conf 启动成功，点击进入首页: http://127.0.0.1:8080 跑环境D:\\sys\\Desktop\\Workplace\\IDEA_Projects\\paicoding下载位置 git clone git@github.com:itwanger/paicoding.git D:\\sys\\Desktop\\Workplace\\IDEA_Projects\\paicodinggit 下载 git clone 出现报错 ， 原因：github ssh 秘钥没有配置 1 打开运行，输入services.msc，确定 找到 OpenSSH Authentication Agent 服务，需开启它. ssh-keygen -t rsa -C “你的邮箱地址” 我用的是Administrator用户，执行完后，可以在 C:\\Users\\Administrator.ssh 目录下生成 id_rsa 和 id_rsa.pub 这两个文件。如果你没有用Administrator用户，也是在类似的目录下 项目结构该项目主要有五个模块，各模块功能如下： paicoding-api： 用于定义一些通用的枚举、实体类，包含 DO（数据对象）、DTO（数据传输对象）、VO（视图对象）等，为不同层之间的数据交互提供统一的格式和规范。 paicoding-core： 核心工具和组件相关的模块，像工具包 util 以及通用的组件都放在这里。按照包路径对模块功能进行拆分，例如搜索、缓存、推荐等功能组件。 paicoding-service： 服务模块，负责业务相关的主要逻辑，数据库（DB）的操作都在这个模块中进行。 paicoding-ui： 存放 HTML 前端资源，包括 JavaScript、CSS、Thymeleaf 等，主要用于构建用户界面。 paicoding-web： Web 模块，是 HTTP 请求的入口，也是项目启动的入口，同时包含权限身份校验、全局异常处理等功能。 web 模块：admin：admin 目录存放和管理后台相关的代码，主要处理管理员对系统的管理操作。 common：common 一般用来存放项目通用的代码，提高代码的复用性和可维护性。 comoinent：TemplateEngineHelper Thymeleaf模版渲染引擎，通过末班引擎进行服务端渲染（SSR），在初次渲染速度方面有显著优势。 configForumDataSourceInitializer 用来进行数据库表的初始化，首次启动时候执行： DbChangeSetLoader 杂项笔记请求参数解析如果一个请求不会引起服务器上任何资源的状态变化，那就可以使用 GET 请求 AOP技术派中的 AOP 记录接口访问日志是放在 paicoding-core 模块下的 mdc 包下。 三层架构为什么要使用微服务而不是单体项目呢？用不用微服务取决于业务量，能用单体的绝对不用微服务，毕竟单体的好处显而易见，当业务简单的时候，部署非常简单，技术架构也简单，不用考虑微服务间的调用什么的，但是随着业务的复杂，单体的缺点也就暴露出来了，例如修改一个模块上线，就要整个服务下线，这在某些业务中是不被允许的，其次单体复杂度高了，部署就缓慢了，出现问题排查也很困难，这些的前提就是业务复杂度提高了。 所以微服务的出现在我看来最初就是为了解决业务复杂单体所出现的问题的，将业务拆分到不同的模块，不同的模块单独部署开发，提高了开发效率，节省了维护时间成本，问题排查也方便了很多，微服务也并不是没有缺点，只不过是维护一个平衡，例如需要引入注册中心，为了方便配置的修改，还需要引入配置中心，不可能修改一个配置重新打包发布，服务间的调用组件，很多都是为了使用微服务而引入的。 所以没有哪种技术更好，只有哪种技术更符合当下的业务，抛开业务谈技术，在我看来并不是那么可靠。 工厂模式创建型设计模式，定义一个创建对象的接口，但让实现这个接口的类决定实例化哪个类。 工厂方法把类的实例化延迟到子类中进行。 @Bean 就是一个工厂方法 各种注解@Slf4j@Slf4j：借助 Lombok 自动添加 SLF4J 日志记录器，简化日志记录代码。 @Value@Configuration@Configuration：把类标记为 Spring 配置类，允许在类里使用 @Bean 注解定义 Spring Bean。 在 Spring 应用启动时，Spring 会扫描带有 @Configuration 注解的类，将其作为配置类来处理，把类里使用 @Bean 注解定义的方法返回的对象注册到 Spring 容器中。 @Component@Component：通用的组件注解，用于标记一个类为 Spring 组件，Spring 会自动扫描并将其注册到容器中。 @Service@Service：@Component 的特殊化注解，通常用于标记服务层类。 @Repository@Repository：@Component 的特殊化注解，通常用于标记数据访问层类。 @Controller@Controller：@Component 的特殊化注解，通常用于标记 Web 控制器类。 @Bean@Bean注册一个实体类 @册一个实体类 实体对象 用 GET 还是 POSTGET - 从指定的资源请求数据。POST - 向指定的资源提交要被处理的数据。 GET 请求是 HTTP 协议中的一种请求方法，通常用于请求访问指定的资源。如果一个请求不会导致服务器上任何资源的状态变化，那你就可以使用 GET 请求。 Filter过滤器 首先进入 filter，执行相关业务逻辑 若判定通行，则进入 Servlet 逻辑，Servlet 执行完毕之后，又返回 Filter，最后在返回给请求方 判定失败，直接返回，不需要将请求发给 Servlet 过滤器的使用如果要使用过滤器，实现 Filter 接口，需要重写 doFilter 方法，在方法中编写过滤逻辑。init: 初始化时执行destory: 销毁时执行doFilter: 重点关注这个，filter 规则命中的请求，都会走进来三个参数，注意第三个 FilterChain，这里是经典的责任链设计模式执行 filterChain.doFilter(servletRequest, servletResponse) 表示会继续将请求执行下去；若不执行这一句，表示这一次的 http 请求到此为止了，后面的走不下去了 过滤器在项目中的应用 身份识别,并保存身份到ReqInfoContext上下文中 记录请求记录 添加跨域支持 跨域问题:跨域问题（CORS）的本质是浏览器的安全限制，而代理服务器是解决该问题的关键方案之一。以下通过​​场景化解析​​帮你彻底理解代理机制. 假如说:前端​​：运行在 http://localhost:5700​​后端​​：运行在 http://localhost:8080​​问题​​：前端直接请求后端接口时，浏览器会拦截并报错： 代理与服务器间的通信属于服务器之间的通信,不受浏览器同源规则的约束. ServletServlet的使用姿势，以及注册自定义的Servelt的四种姿势 ● @WebServlet 注解:在自定义的servlet上添加Servlet3+的注解@WebServlet，来声明这个类是一个Servlet和Fitler的注册方式一样，使用这个注解，需要配合Spring Boot的@ServletComponentScan，否则单纯的添加上面的注解并不会生效 /** * 使用注解的方式来定义并注册一个自定义Servlet */@WebServlet(urlPatterns = /annotation)public class AnnotationServlet extends HttpServlet @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException String name = req.getParameter(name); PrintWriter writer = resp.getWriter(); writer.write([AnnotationServlet] welcome + name); writer.flush(); writer.close(); 还需要配置启动类 @ServletComponentScan@SpringBootApplicationpublic class Application public static void main(String[] args) SpringApplication.run(Application.class); ● ServletRegistrationBean bean定义在Filter的注册中，我们知道有一种方式是定义一个Spring的BeanFilterRegistrationBean来包装我们的自定义Filter，从而让Spring容器来管理我们的过滤器；同样的在Servlet中，也有类似的包装bean: ServletRegistrationBean自定义的bean如下，注意类上没有任何注解: public class RegisterBeanServlet extends HttpServlet @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException String name = req.getParameter(name); PrintWriter writer = resp.getWriter(); writer.write([RegisterBeanServlet] welcome + name); writer.flush(); writer.close(); 接下来我们需要定义一个ServletRegistrationBean，让它持有RegisterBeanServlet的实例 @Beanpublic ServletRegistrationBean servletBean() ServletRegistrationBean registrationBean = new ServletRegistrationBean(); registrationBean.addUrlMappings(/register); registrationBean.setServlet(new RegisterBeanServlet()); return registrationBean; ● ServletContext 动态添加这种姿势，在实际的Servlet注册中，其实用得并不太多，主要思路是在ServletContext初始化后，借助javax.servlet.ServletContext#addServlet(java.lang.String, java.lang.Class? extends javax.servlet.Servlet)方法来主动添加一个Servlet 所以我们需要找一个合适的时机，获取ServletContext实例，并注册Servlet，在SpringBoot生态下，可以借助ServletContextInitializer public class ContextServlet extends HttpServlet @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException String name = req.getParameter(name); PrintWriter writer = resp.getWriter(); writer.write([ContextServlet] welcome + name); writer.flush(); writer.close(); @Componentpublic class SelfServletConfig implements ServletContextInitializer @Override public void onStartup(ServletContext servletContext) throws ServletException ServletRegistration initServlet = servletContext.addServlet(contextServlet, ContextServlet.class); initServlet.addMapping(/context);","tags":["项目","技术派"],"categories":["项目笔记","技术派"]},{"title":"个人博客搭建","path":"/2025/06/02/杂项笔记/个人博客搭建/","content":"环境配置: node -v #查看node版本npm -v #查看npm版本 可以使用nvm来管理node版本 安装 hexo: npm install hexo-cli -g 出现权限问题 ,修改: sudo chown -R $(whoami) $(npm config get prefix)/lib/node_modules,bin,share #修改权限范围 初始化个人博客文件夹: hexo init /Users/mac/Blog/JakicDong.github.io # 博客初始化，这里是创建的本地博客文件夹，执行后会自动创建，我这里图简洁，直接用了我网站的地址 hihulu.github.io 为文件名cd /Users/mac/Blog/JakicDong.github.io # 进入本地的博客文件夹hexo server # 打开本地服务器预览 UsersmacJakicDong.github.io 4.Github部署为了部署到Github上，需要安装hexo-deployer-git插件，命令如下： sudo npm install hexo-deployer-git --save 然后找到自己的本地博客文件夹，修改博客根目录下的_config.yml文件中的deploy，修改成： deploy: type: git repo: git@github.com:JakicDong/JakicDong.github.io.git #这个地址是从github仓库复制过来的ssh branch: main （⚠️注意，这里有一个很容易犯错的点，我们在创建“hihulu.github.io”这个仓库的时候，一定要创建和你github用户名相同的仓库，后面加.http://github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是http://xxxx.github.io，其中xxx就是你注册GitHub的用户名。所以上图我的仓库显示的是“hihuluhihulu.github.io”，如果仓库名和用户名不一致，后面是根本打不开这个网站的～） 然后就可以通过以下命令行上传github了 hexo g #hexo generate的简写，即把刚刚做的改动生成更新一下hexo d #hexo deploy，上传到github网站 还有一些常用的命令行： hexo clean #清空一下缓存，有时候博客页面显示不正常也可以试试这个命令行hexo server # 在本地服务器运行，网址默认https://localhost:4000 5.更改主题很多前端大牛博主设计了很多好看的主题，网址https://hexo.io/themes/ ，可以预览并选择你喜爱的主题进行应用。 这里浅以一个蛮火的主题butterfly主题来走一个安装主题的步骤～ 执行以下代码： git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly 运行成功之后，在项目文件夹根目录中可以查看到新的主题themes文件夹：butterfly 在博客的项目文件夹下，修改_config.yml配置文件如下: # theme: landscape 默认主题theme: butterfly 此时主题还不能正常配置使用，需要安装pug 以及stylus 的渲染器： npm install hexo-renderer-pug hexo-renderer-stylus --save 最后推送到github # 清除缓存b.json 和已生成的静态文件 publichexo clean# 生成静态页面到默认设置的 public 文件夹hexo g# 部署到设定的仓库或上传部署至服务端hexo d 至此主题安装end，可访问https://hihulu.github.io/ 查看～ https://JakicDong.github.io/ # 7.4 常用命令hexo new name # 新建文章hexo new page name # 新建页面hexo g # 生成页面hexo d # 部署hexo g -d # 生成页面并部署hexo s # 本地预览hexo clean # 清除缓存和已生成的静态文件hexo help # 帮助","tags":["博客","hexo"],"categories":["杂项笔记"]},{"title":"算法学习笔记","path":"/2025/06/01/算法笔记/算法题刷题笔记/","content":"算法笔记一些好的逻辑思路反问题如果一个问题正问题很复杂,可以尝试想一下反问题,比如求有恰好k个相邻数的数组个数,正问题很复杂,但是反问题只需要找到n-k-1个分割线,把数组分割成n-k块.3405. 统计恰好有 K 个相等相邻元素的数组数目 子问题如果一个大的问题可以分解成多个子问题,可以尝试先解决子问题,再解决大问题.这就可以用递归或动态规划的方法来解决. ACM 格式import java.util.*;import java.io.*;public class Main public static void main(String[] args) throws IOException BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); String line; while ((line = br.readLine()) != null) // 逐行读取 int n = Integer.parseInt(line.trim()); int[] nums = new int[n]; String[] parts = br.readLine().split( ); for (int i = 0; i n; i++) nums[i] = Integer.parseInt(parts[i]); // 处理逻辑 // 输出结果 System.out.println(result); public static void main(String[] args) Scanner scanner = new Scanner(System.in); String s = scanner.next(); System.out.println(replaceNumber(s)); scanner.close(); 第二种: import java.util.*;import java.io.*;public static void main(String[] args) Scanner scanner = new Scanner(System.in); String s = scanner.next(); System.out.println(replaceNumber(s)); scanner.close(); 杂项操作绝对值:Math.abs();Int最大值:Integer.MAX_VALUE前导零:Integer.numberOfLeadingZeros(k); 取随机值： import java.util.Random;import java.util.concurrent.ThreadLocalRandom;public class RandomDemo public static void main(String[] args) int n = 10; //0~9 // 方法1: Math.random() int random1 = (int) (Math.random() * n); System.out.println(Math.random(): + random1); // 方法2: Random类 Random rand = new Random(); int random2 = rand.nextInt(n); System.out.println(Random: + random2); // 方法3: ThreadLocalRandom（多线程安全） int random3 = ThreadLocalRandom.current().nextInt(n); System.out.println(ThreadLocalRandom: + random3); Array 常用操作// ========== 数组声明与初始化 ==========int[] arr = new int[5]; // 创建空数组 [0,0,0,0,0]int[] nums = 3,1,4,5,2; // 直接初始化String[] strArr = A, B, C; // 字符串数组int[][] matrix = 1,2, 3,4,5; // 二维数组// ========== 基本操作 ==========// 访问元素int first = nums[0]; nums[2] = 9; // 获取长度int len = nums.length; // 一维数组长度 → 5int cols = matrix[1].length; // 二维数组第二维长度 → 3// ========== 遍历操作 ==========// 普通for循环for(int i=0; inums.length; i++) System.out.print(nums[i] + ); // 3 1 9 5 2// 增强for循环for(int num : nums) System.out.print(num + );// 二维数组遍历for(int[] row : matrix) for(int n : row) System.out.print(n + ); // 1 2 / 3 4 5 // ========== 数组工具操作 ==========// ========== 排序操作 ==========//升序排序Arrays.sort(array);//降序排序Arrays.sort(Collections.reverseOrder());// 复制int[] copy1 = Arrays.copyOf(nums, 3); // 复制前3元素 → [1,2,3]int[] copy2 = Arrays.copyOfRange(nums, 1, 4); // 复制1-3索引 → [2,3,5]// 比较boolean isEqual = Arrays.equals(nums, copy1); // false// 填充Arrays.fill(nums, 0); // 全部填充0 → [0,0,0,0,0]Arrays.fill(nums, 1, 3, 5); // 索引1-2填充5 → [0,5,5,0,0]// ========== 类型转换 ==========// 数组转List（固定大小）ListInteger list = Arrays.asList(1,2,3); // 数组转StreamIntStream stream = Arrays.stream(nums);// ========== 其他操作 ==========// 二分查找（需先排序）int index = Arrays.binarySearch(nums, 5); // 返回元素索引// 多维数组操作int[][] matrixCopy = Arrays.copyOf(matrix, matrix.length); // 浅拷贝String deepStr = Arrays.deepToString(matrix); // [[1, 2], [3, 4, 5]]// 反转数组for(int i=0; inums.length/2; i++) int temp = nums[i]; nums[i] = nums[nums.length-1-i]; nums[nums.length-1-i] = temp; 数组带着索引排序例题 public int[] maxSubsequence(int[] nums, int k) int n=nums.length; int ans[]=new int[k]; Integer idx[]=new Integer[n]; Arrays.setAll(idx, i-i); Arrays.sort(idx,(a,b)-nums[a]-nums[b]); int j=0; for(int i=n-k;in;i++) ans[j++]=idx[i]; Arrays.sort(ans); for(int i=0;ik;i++) ans[i]=nums[ans[i]]; return ans; 和 int 数组相互转换： Integer[] arr = 1,2,3;ArrayListInteger list = new ArrayList(Arrays.asList(arr));ArrayListInteger list = new ArrayList();Integer[] arr = list.toArray(new Integer[0]);//但是如何转成int[]数组呢//方法1 arr = list.stream().mapToInt(Integer::valueOf).toArray(); List 常用操作// ====================== 1. 初始化 List ======================ListString arrayList = new ArrayList(); // 可修改列表ListInteger linkedList = new LinkedList(); ListString immutableList = List.of(A, B, C); // Java9+ 不可变列表// ====================== 2. 添加元素 ======================arrayList.add(Apple); // 末尾添加arrayList.add(0, Banana); // 索引0插入arrayList.addAll(List.of(Orange, Grape)); // 批量添加// ====================== 3. 访问元素 ======================String firstElement = arrayList.get(0); // Bananaboolean containsApple = arrayList.contains(Apple); // trueint indexOfOrange = arrayList.indexOf(Orange); // 2// ====================== 4. 删除元素 ======================arrayList.remove(Banana); // 按值删除arrayList.remove(0); // 按索引删除arrayList.clear(); // 清空列表// ====================== 5. 修改元素 ======================arrayList.add(Mango);arrayList.set(0, Pineapple); // 修改索引0的元素// ====================== 6. 遍历 List ======================Collections.sort(path); // 直接修改原List// 方式1: 普通for循环for (int i = 0; i arrayList.size(); i++) System.out.println(arrayList.get(i));// 方式2: 增强for循环for (String fruit : arrayList) System.out.println(fruit);// 方式3: 迭代器IteratorString it = arrayList.iterator();while (it.hasNext()) System.out.println(it.next());// 方式4: forEach + LambdaarrayList.forEach(System.out::println);// ====================== 7. 其他操作 ======================int size = arrayList.size(); // 列表长度boolean isEmpty = arrayList.isEmpty(); // 是否为空ListString subList = arrayList.subList(0, 2); // 截取子列表Object[] array = arrayList.toArray(); // 转为数组// ====================== 8. 注意事项 ======================/*1. ArrayList初始容量10，扩容1.5倍 2. LinkedList用节点链接实现3. 线程不安全，多线程环境使用：ListString syncList = Collections.synchronizedList(new ArrayList());4. 快速失败机制(fast-fail)：遍历时修改会抛出ConcurrentModificationException*/ 二维List例子 import java.util.ArrayList;import java.util.Arrays;import java.util.List;public class TwoDListExample public static void main(String[] args) // ====================== // 1. 创建二维List // ====================== ListInteger[] ls = new ArrayList[26]; Arrays.setAll(ls,i-new ArrayList()); // 方法1: 使用Arrays.asList()初始化 ListListInteger matrix1 = new ArrayList(); matrix1.add(Arrays.asList(1, 2, 3)); matrix1.add(Arrays.asList(4, 5, 6)); matrix1.add(Arrays.asList(7, 8, 9)); // 方法2: 动态创建空二维List ListListString matrix2 = new ArrayList(); // 方法3: 使用嵌套循环初始化 ListListCharacter matrix3 = new ArrayList(); for (int i = 0; i 3; i++) ListCharacter row = new ArrayList(); for (int j = 0; j 4; j++) row.add((char) (A + i + j)); matrix3.add(row); // ====================== // 2. 添加元素 // ====================== // 添加新行 matrix2.add(new ArrayList(Arrays.asList(Java, Python))); matrix2.add(new ArrayList(Arrays.asList(C++, JavaScript))); // 在指定行添加元素 matrix2.get(0).add(Ruby); // 第一行添加元素 matrix2.get(1).add(0, Go); // 第二行开头插入元素 // 添加新行（空行） matrix2.add(new ArrayList()); matrix2.get(2).add(Swift); // 给新行添加元素 // ====================== // 3. 访问元素 // ====================== // 访问单个元素 int element = matrix1.get(1).get(2); // 获取第二行第三列元素 → 6 String lang = matrix2.get(0).get(1); // 获取第一行第二列元素 → Python // 获取行数 int rows = matrix1.size(); // 获取列数（特定行） int colsRow0 = matrix1.get(0).size(); int colsRow2 = matrix2.get(2).size(); // ====================== // 4. 修改元素 // ====================== matrix1.get(0).set(0, 100); // 修改第一行第一列: 1 → 100 matrix2.get(1).set(2, TypeScript); // 修改第二行第三列 // ====================== // 5. 删除元素 // ====================== // 删除指定位置的元素 matrix1.get(2).remove(1); // 删除第三行第二列元素(8) // 删除整行 matrix2.remove(2); // 删除第三行 // ====================== // 6. 遍历二维List // ====================== System.out.println( 遍历matrix1:); // 方法1: 索引遍历 for (int i = 0; i matrix1.size(); i++) for (int j = 0; j matrix1.get(i).size(); j++) System.out.print(matrix1.get(i).get(j) + ); System.out.println(); System.out.println( 遍历matrix2:); // 方法2: 增强for循环 for (ListString row : matrix2) for (String item : row) System.out.print(item + ); System.out.println(); System.out.println( 遍历matrix3:); // 方法3: 使用forEach + lambda matrix3.forEach(row - row.forEach(item - System.out.print(item + )); System.out.println(); ); // ====================== // 7. 其他常用操作 // ====================== // 检查是否为空 boolean isEmpty = matrix2.isEmpty(); // 检查是否包含元素 boolean containsPython = matrix2.get(0).contains(Python); // 查找元素位置 int rowIndex = -1, colIndex = -1; for (int i = 0; i matrix1.size(); i++) int index = matrix1.get(i).indexOf(5); if (index != -1) rowIndex = i; colIndex = index; break; // 转换为二维数组 String[][] array2D = new String[matrix2.size()][]; for (int i = 0; i matrix2.size(); i++) ListString row = matrix2.get(i); array2D[i] = row.toArray(new String[0]); ////翻转行 //方法1 使用工具类 ListListInteger matrix = new ArrayList(); // 假设 matrix 已经填充数据 // 翻转行（首行变末行，次行变次末行，依此类推） Collections.reverse(matrix); //方法2 ListListInteger matrix = new ArrayList(); // 假设 matrix 已经填充数据 int left = 0; int right = matrix.size() - 1; while (left right) // 使用临时行 temp 交换左右两行 ListInteger temp = matrix.get(left); matrix.set(left, matrix.get(right)); matrix.set(right, temp); left++; right--; //方法3 Stream ListListInteger matrix = new ArrayList(); // 假设 matrix 已经填充数据 // 翻转行顺序 ListListInteger reversed = IntStream.range(0, matrix.size()) .mapToObj(i - matrix.get(matrix.size() - 1 - i)) .collect(Collectors.toList()); matrix = reversed; // 如果需要原地翻转，需重新赋值 // 打印结果 System.out.println( matrix1: + matrix1); System.out.println(matrix2: + matrix2); System.out.println(matrix3: + matrix3); System.out.println(找到数字5的位置: [ + rowIndex + ][ + colIndex + ]); HashSet 常用操作// =================== HashSet 基础操作 ===================import java.util.Collections;import java.util.HashSet;import java.util.Iterator;import java.util.Set;// 创建对象SetString set = new HashSet(); // 空集合（默认初始容量16，负载因子0.75）SetInteger initSet = new HashSet(32); // 指定初始容量SetString prefilled = new HashSet(Arrays.asList(A, B, C)); // 通过集合初始化// 元素操作boolean added = set.add(Apple); // 添加元素 → true（首次添加）boolean dupAdd = set.add(Apple); // 添加重复元素 → falseboolean hasBanana = set.contains(Apple); // 检查存在 → trueboolean removed = set.remove(Apple); // 删除元素 → true（存在时）set.clear(); // 清空集合// 批量操作SetString fruits = new HashSet(Arrays.asList(Orange, Mango));boolean addedAll = set.addAll(fruits); // 合并集合 → true（集合改变时）boolean retainAll = set.retainAll(Arrays.asList(Mango)); // 保留交集 → true（集合改变时）boolean removeAll = set.removeAll(fruits); // 删除所有匹配元素 → true（集合改变时）// 遍历操作set.add(Apple);set.add(Banana);for (String item : set) // 增强for循环（无序） System.out.print(item + ); // 输出顺序不确定（如 Banana Apple）IteratorString it = set.iterator(); // 迭代器遍历while (it.hasNext()) System.out.print(it.next() + );set.forEach(item - System.out.print(item)); // Java8+ Lambda遍历// 集合信息int size = set.size(); // 元素数量（如 2）boolean isEmpty = set.isEmpty(); // 是否空集合 → falseObject[] array = set.toArray(); // 转Object数组String[] strArray = set.toArray(new String[0]); // 转指定类型数组// 特殊操作SetString cloneSet = (HashSetString) ((HashSetString) set).clone(); // 浅拷贝SetString syncSet = Collections.synchronizedSet(set); // 线程安全包装// 集合运算示例SetString set1 = new HashSet(Arrays.asList(A, B));SetString set2 = new HashSet(Arrays.asList(B, C));SetString union = new HashSet(set1); // 并集 → [A, B, C]union.addAll(set2);SetString intersection = new HashSet(set1); // 交集 → [B]intersection.retainAll(set2);SetString difference = new HashSet(set1); // 差集 → [A]difference.removeAll(set2);/* 核心特性：1. 唯一性：基于 hashCode() 和 equals() 判断重复2. 无序性：遍历顺序不保证与插入顺序一致3. 允许 null 元素（但只能有一个 null）4. 基础操作时间复杂度：add/remove/contains → 平均 O(1)5. 非线程安全：需通过 Collections.synchronizedSet 包装实现线程安全*/ HashMap 常用操作// =================== HashMap 基础操作 ===================import java.util.Collections;import java.util.HashMap;import java.util.Map;import java.util.Set;// 创建对象MapString, Integer map = new HashMap(); // 默认容量16，负载因子0.75MapString, String initMap = new HashMap(32); // 指定初始容量MapString, Integer prefilled = new HashMap(Map.of(A, 1, B, 2)); // Java9+快速初始化// 增删改操作map.put(Apple, 10); // 添加键值对 → Apple=10map.put(Banana, 20); // → Apple=10, Banana=20map.putIfAbsent(Apple, 50); // 仅当键不存在时添加 → 原值10保持不变map.replace(Apple, 15); // 替换已有键的值 → Apple=15, Banana=20map.remove(Banana); // 删除键 → Apple=15map.replace(Apple, 15, 20); // 键值匹配时替换 → Apple=20map.clear(); // 清空映射// 查询操作int count = map.get(Apple); // 获取值（需确保键存在）Integer countSafe = map.get(Orange); // 键不存在时返回nullboolean existsKey = map.containsKey(Apple); // 检查键是否存在 → trueboolean existsValue = map.containsValue(20); // 检查值是否存在 → trueint size = map.size(); // 键值对数量boolean isEmpty = map.isEmpty(); // 是否为空映射// 遍历操作（4种方式）// 1. Entry遍历（推荐）for (Map.EntryString, Integer entry : map.entrySet()) System.out.println(entry.getKey() + : + entry.getValue());// 2. Key遍历for (String key : map.keySet()) System.out.println(key + - + map.get(key));// 3. Value遍历for (Integer value : map.values()) System.out.println(Value: + value);// 4. Java8+ Lambda遍历map.forEach((k, v) - System.out.println(k + = + v));// 批量操作MapString, Integer newItems = Map.of(Cherry, 5, Durian, 8);map.putAll(newItems); // 合并映射 → Apple=20, Cherry=5, Durian=8// 特殊值处理map.put(null, 0); // 允许null键 → null=0map.put(Mango, null); // 允许null值 → null=0, Mango=null// 高级操作（Java8+）map.computeIfAbsent(Orange, k - 3); // 不存在时计算 → 添加 Orange=3map.computeIfPresent(Apple, (k, v) - v + 5); // 存在时更新 → Apple=25map.merge(Apple, 10, (oldVal, newVal) - oldVal + newVal); // 合并值 → Apple=35// 线程安全包装MapString, Integer syncMap = Collections.synchronizedMap(map);// 不可变映射（Java9+）MapString, Integer immutableMap = Map.ofEntries( Map.entry(A, 1), Map.entry(B, 2));/* 核心特性：1. 键唯一性：基于 hashCode() 和 equals() 判断重复2. 无序存储：迭代顺序不保证与插入顺序一致3. 允许一个null键和多个null值4. 基础操作时间复杂度：get/put → 平均 O(1)5. 扩容机制：当元素数量超过（容量*负载因子）时自动翻倍扩容6. 树化优化：当链表长度超过8时转红黑树（Java8+）*/ HashMapCharacter, Integer hm_s = new HashMap();//遍历字符串的所有字符（更清晰的逻辑）for (char c : s.toCharArray()) hm_s.merge(c, 1, Integer::sum);// 使用getOrDefault避免NullPointerExceptionint countS = hm_s.getOrDefault(c, 0); 一个 Key 只能对应一个值，所以如果要实现类似 Multimap 的结构需要如下方式： //初始化HashMapString, ListInteger multiValueMap = new HashMap();/////添加元素if (!multiValueMap.containsKey(scores)) multiValueMap.put(scores, new ArrayList());multiValueMap.get(scores).add(90);// 若键不存在，自动创建空列表multiValueMap.computeIfAbsent(scores, k - new ArrayList()).add(90);multiValueMap.computeIfAbsent(scores, k - new ArrayList()).add(85);////访问元素//获取某个键的所有值ListInteger scores = multiValueMap.get(scores);if (scores != null) for (int num : scores) System.out.println(num); //输出 90, 85 //避免 NullPointerException的情况ListInteger scores = multiValueMap.getOrDefault(scores, new ArrayList());for (int num : scores) System.out.println(num);////删除元素//删除整个键值对multiValueMap.remove(scores);// 删除某个键的特定值ListInteger scores = multiValueMap.get(scores);if (scores != null) scores.remove(Integer.valueOf(90)); // 删除值为 90 的元素 // 如果列表为空，可选删除键 if (scores.isEmpty()) multiValueMap.remove(scores); ////遍历哈希表// 遍历所有键值对for (Map.EntryString, ListInteger entry : multiValueMap.entrySet()) String key = entry.getKey(); ListInteger values = entry.getValue(); System.out.println(key + : + values);//遍历所有键for (String key : multiValueMap.keySet()) System.out.println(Key: + key);//遍历所有值for (ListInteger values : multiValueMap.values()) System.out.println(Values: + values); String的常用操作// =================== String 常用操作 ===================// 创建对象String str1 = Hello; // 直接量创建（字符串常量池）String str2 = new String(World); // 堆内存新对象char[] chars = J,a,v,a;String str3 = new String(chars); // 通过字符数组创建 → Java// 基本操作int len = str1.length(); // 获取长度 → 5char c = str1.charAt(1); // 获取索引1字符 → eString substr1 = str1.substring(2); // 从索引2截取 → lloString substr2 = str1.substring(1,4); // 截取1-3索引 → ellString concatStr = str1.concat( World);// 拼接 → Hello WorldString upper = str1.toUpperCase(); // 转大写 → HELLOString lower = HELLO.toLowerCase(); // 转小写 → helloString trimStr = text .trim(); // 去首尾空格 → textString replaced = str1.replace(l, L);// 替换字符 → HeLLoString replacedAll = a1b2c3.replaceAll(\\\\d, #); // 正则替换 → a#b#c#// 转换与比较char[] arr = str1.toCharArray(); // 转字符数组 → [H,e,l,l,o]byte[] bytes = str1.getBytes(); // 按默认编码转字节数组boolean eq1 = str1.equals(Hello); // 值比较 → trueboolean eq2 = str1.equalsIgnoreCase(hElLo); // 忽略大小写 → trueint cmp = str1.compareTo(Hella); // 字典序比较 → 正数（o a）// 正则处理boolean matches = 123.matches(\\\\d+); // 正则匹配 → trueString[] parts = a,b,c.split(,); // 分割字符串 → [a,b,c]String[] regexParts = a1b2c3.split(\\\\d); // 按数字分割 → [a,b,c]// 格式化处理String format1 = String.format(%s-%d, ID, 100); // → ID-100String format2 = String.join(|, A, B, C); // → A|B|C// 特殊判断boolean isEmpty = .isEmpty(); // 空字符串 → trueboolean isBlank = .isBlank(); // 全空白字符（Java 11+） → trueboolean starts = str1.startsWith(He); // 开头判断 → trueboolean ends = str1.endsWith(lo); // 结尾判断 → true/* 核心特性：1. 不可变性：所有操作返回新字符串，原对象不变2. 字符串常量池复用机制：直接量赋值优先使用常量池3. 支持正则表达式操作（split/matches/replaceAll等）4. 包含丰富的格式化方法（format/join等）*/ int转StringString s = String.valueOf(i);String s = Integer.toString(i);String 转 intString str = 123;int num = Integer.parseInt(str); StringBuffer 和 StringBuilderStringBuilder 类在 Java 5 中被提出，它和 StringBuffer 之间的最大不同在于 StringBuilder 的方法不是线程安全的（不能同步访问）。由于 StringBuilder 相较于 StringBuffer 有速度优势，所以多数情况下建议使用 StringBuilder 类。 // =================== StringBuilder ===================// 创建对象StringBuilder sb = new StringBuilder(); // 默认容量16StringBuilder sb2 = new StringBuilder(Hello); // 指定初始内容// 基本操作sb.append( World); // 追加内容 → Hello Worldsb.insert(5, Java); // 指定位置插入 → Hello Java Worldsb.delete(5, 10); // 删除5-9位置 → Hello Worldsb.replace(6, 11, Earth); // 替换指定区间 → Hello Earthsb.reverse(); // 反转 → htraE olleH sb.setLength(5); // 截断保留前5字符 → htraEchar c = sb.charAt(2); // 获取索引2的字符 → rsb.setCharAt(0, H); // 修改索引0字符 → HtraEint len = sb.length(); // 获取当前长度 → 5String s = sb.toString(); // 转换为String// 正向遍历for (int i = 0; i sb.length(); i++) char c = sb.charAt(i); System.out.println(c);// 链式调用StringBuilder sb3 = new StringBuilder() .append(123) // 支持多类型 .append(3.14) .append(true);sb.setLength(prevLen); // 直接回退到添加前的长度// =================== StringBuffer ===================// 创建对象（操作方法与StringBuilder完全一致）StringBuffer sbf = new StringBuffer(); sbf.append(100); // 追加数值sbf.insert(3, new char[]A,B); // 插入字符数组sbf.deleteCharAt(4); // 删除单个字符sbf.setLength(0); // 清空缓冲区（复用对象）// 线程安全示例sbf.append(ThreadSafe); // 所有方法都有synchronized修饰符/* 共性特征：1. 初始容量16，自动扩容（每次扩容2n+2）2. append() 支持所有基础类型/Object类型3. 修改后对象地址不变（与String的不可变性对比）4. 主要方法：append/insert/delete/replace/reverse*/String s1 = buffer.toString(); // StringBuffer → StringString s2 = builder.toString(); // StringBuilder → String PriorityQueue的常用操作小顶堆(默认，前 K 大，升序) // 创建小顶堆（默认）PriorityQueueInteger minHeap = new PriorityQueue();// 添加元素minHeap.offer(5);minHeap.add(3); // offer和add功能相同// 查看堆顶元素（不删除）int min = minHeap.peek();// 取出堆顶元素（删除）int removedMin = minHeap.poll();// 获取堆大小int size = minHeap.size();// 检查是否为空boolean isEmpty = minHeap.isEmpty();// 删除指定元素（非堆顶）minHeap.remove(2);// 清空堆minHeap.clear(); 大顶堆(前 K 小，降序) // 创建大顶堆（使用自定义比较器）PriorityQueueInteger maxHeap = new PriorityQueue((a, b) - b - a);// 或 PriorityQueue(Comparator.reverseOrder());// 添加元素maxHeap.offer(8);maxHeap.add(4);// 查看堆顶元素（不删除）int max = maxHeap.peek();// 取出堆顶元素（删除）int removedMax = maxHeap.poll();// 检查元素是否存在boolean contains = maxHeap.contains(5);// 遍历并输出元素for (Integer num : minHeap) System.out.println(num);// 构造按照字典序排列的优先队列PriorityQueueInteger pq = new PriorityQueue((a, b) - // 将 int 转换为 String 后比较字典序 String strA = String.valueOf(a); String strB = String.valueOf(b); return strA.compareTo(strB);); 堆求前k大,然后存储索引(需要保存下标的写法)堆求前k大,然后存储索引 //小根堆，如果当前元素大于堆顶，则加入堆中，堆中的元素还要保存其下标public int[] maxSubsequence(int[] nums, int k) PriorityQueueint[] heap = new PriorityQueue((o1,o2)-o1[0]-o2[0]); int n = nums.length; for(int i = 0; i n; i++) if(heap.size() k) heap.offer(new int[]nums[i], i); else if(nums[i] heap.peek()[0]) heap.poll(); heap.offer(new int[]nums[i], i); int[] idx = new int[k]; for(int i = 0; i k; i++) idx[i] = heap.poll()[1]; Arrays.sort(idx); for(int i = 0; i k; i++) idx[i] = nums[idx[i]]; return idx; 自定义对象堆 // 自定义类class Student String name; int score; // 构造方法等...// 按分数的小顶堆PriorityQueueStudent studentMinHeap = new PriorityQueue( (s1, s2) - s1.score - s2.score);// 按分数的大顶堆 PriorityQueueStudent studentMaxHeap = new PriorityQueue( (s1, s2) - s2.score - s1.score);// 添加自定义对象studentMinHeap.offer(new Student(Alice, 85)); 堆序性质堆类型\t比较条件\t数学表达\tJava比较器实现小顶堆\t父 ≤ 子\ta ≤ b\ta - b 或 a.compareTo(b)大顶堆\t父 ≥ 子\ta ≥ b\tb - a 或 b.compareTo(a)比较器的本质a在堆里代表父节点 b是子节点 a-b 要小于0 a优先级才高 ab 所以是最小堆 比较器定义的是”优先级”关系： 返回负数：第一个参数（a）应该排在前面（更高优先级） 返回正数：第二个参数（b）应该排在前面 返回零：两者优先级相同 使用场景前 K 大的元素：使用最小堆，因为堆顶是存储的最小的元素，如果新增元素比堆顶大，那只需要替换掉堆顶即可。前 K 小的元素：使用最大堆，因为堆顶是存储的最大的元素，如果新增元素比堆顶小，那只需要替换掉堆顶即可。默认小顶堆：升序排序。默认大顶堆：降序排序。 class Solution public int maxEvents(int[][] events) int mx = 0; for (int[] e : events) mx = Math.max(mx, e[1]); // 按照开始时间分组 ListInteger[] groups = new ArrayList[mx + 1]; Arrays.setAll(groups, i - new ArrayList()); for (int[] e : events) groups[e[0]].add(e[1]); int ans = 0; PriorityQueueInteger pq = new PriorityQueue(); for (int i = 0; i = mx; i++) // 删除过期会议 while (!pq.isEmpty() pq.peek() i) pq.poll(); // 新增可以参加的会议 for (int endDay : groups[i]) pq.offer(endDay); // 参加一个结束时间最早的会议 if (!pq.isEmpty()) ans++; pq.poll(); return ans; Deque 的常用操作import java.util.Deque;import java.util.ArrayDeque;import java.util.Iterator;// 初始化 Deque（以 ArrayDeque 为例）DequeString deque = new ArrayDeque(); //不能插入null元素DequeString deque = new LinkedList();// ================== 插入操作 ==================// 队头插入deque.addFirst(A); // 插入元素到队头（容量满时抛出 IllegalStateException）deque.offerFirst(B); // 插入元素到队头（容量满时返回 false）// 队尾插入deque.addLast(C); // 插入元素到队尾（容量满时抛出 IllegalStateException）deque.offerLast(D); // 插入元素到队尾（容量满时返回 false）// 批量插入（从 Collection 继承）deque.addAll(List.of(E, F)); // 依次插入队尾// ================== 删除操作 ==================// 队头删除String first1 = deque.removeFirst(); // 删除并返回队头元素（空队列抛 NoSuchElementException）String first2 = deque.pollFirst(); // 删除并返回队头元素（空队列返回 null）// 队尾删除String last1 = deque.removeLast(); // 删除并返回队尾元素（空队列抛异常）String last2 = deque.pollLast(); // 删除并返回队尾元素（空队列返回 null）// 删除指定元素（从队头开始搜索）boolean removed1 = deque.remove(E); // 删除第一个出现的 Eboolean removed2 = deque.removeFirstOccurrence(F); // 删除队头方向第一个 Fboolean removed3 = deque.removeLastOccurrence(G); // 删除队尾方向第一个 G// ================== 查看元素 ==================// 查看队头String head1 = deque.getFirst(); // 返回队头元素（空队列抛异常）String head2 = deque.peekFirst(); // 返回队头元素（空队列返回 null）// 查看队尾String tail1 = deque.getLast(); // 返回队尾元素（空队列抛异常）String tail2 = deque.peekLast(); // 返回队尾元素（空队列返回 null）// ================== 队列状态 ==================boolean isEmpty = deque.isEmpty(); // 判断队列是否为空int size = deque.size(); // 返回元素数量boolean exists = deque.contains(A); // 判断是否包含元素 A// ================== 其他操作 ==================// 清空队列deque.clear();// 转换为数组Object[] array1 = deque.toArray(); // 返回 Object[]String[] array2 = deque.toArray(new String[0]); // 指定类型数组// 迭代器（正向：队头 → 队尾）IteratorString iterator = deque.iterator();while (iterator.hasNext()) String element = iterator.next();// 反向迭代器（队尾 → 队头）IteratorString descendingIterator = deque.descendingIterator();while (descendingIterator.hasNext()) String element = descendingIterator.next();// ================== 栈操作（Deque 兼容的额外方法）==================deque.push(X); // 等效于 addFirst()String popped = deque.pop(); // 等效于 removeFirst()// ================== 容量限制队列（如 LinkedBlockingDeque）==================// 阻塞操作示例（需使用线程安全 Deque，此处仅展示方法）/*deque.offerFirst(W, 1, TimeUnit.SECONDS); // 等待1秒尝试插入队头deque.offerLast(Z, 1, TimeUnit.SECONDS); // 等待1秒尝试插入队尾String item = deque.pollFirst(1, TimeUnit.SECONDS); // 等待1秒尝试取出队头*/ 模运算恒等式费马小定理组合数灵神的模运算帖子.respect.https://leetcode.cn/discuss/post/3584387/fen-xiang-gun-mo-yun-suan-de-shi-jie-dan-7xgu/ Java的取模（mod）和取余（% rem），发现我们常用的基本都是正数取余或取模，那带有负数的要怎么计算呢。当x和y的正负相同，取余和取模结果相同，当x和y正负不同，取余结果的符号和x相同，取模结果的符号和y的符号相同。假设：被除数 a 除数 b 商c 余数d 公式 abc…d 可以变形为 da-b*c那么关键就在于这个c取什么值。举个栗子：a5，b-2 ，那么 5÷(-2)-2.5 取模的时候，因为mod 函数采用了 floor 函数，floor函数是向下取整的，所以-2.5向下取整就是-3，那么d5-(-2)*(-3)5-6-1。 取余(%)的时候，因为rem 函数采用 fix 函数，fix函数是向0取整的，所以-2.5向0取整就是-2，那么d5-(-2)*(-2)5-41。 前言某些题目，由于要计算的答案非常大（超出 64 位整数的范围），会要求把答案对 10e9+7 取模。如果没有处理得当的话，会 WA（错误）或者 TLE（超时）。例如计算一堆数字的乘积，如果没有及时取模，乘法会溢出（例如计算结果超出 C++ 中 long long 的最大值），从而得到和预期不符的答案。对于 Python 来说，虽然没有溢出的问题，但大整数（big integer）之间的运算并不是 O(1) 的，可能会导致 TLE。 如何正确的取模呢? 加法和乘法的取模如果让你计算 1234×6789 的个位数，你会如何计算？ 由于只有个位数会影响到乘积的个位数，那么 4×936 的个位数 6 就是答案。 对于 1234+6789 的个位数，同理，4+913 的个位数 3 就是答案。 你能把这个结论抽象成数学等式吗？ 一般涉及到取模的题目，会用到如下两个恒等式，其中 mod 表示取模运算（modulo），即编程语言中的 %。上面计算的是 m10 的情况。 根据这两个恒等式，我们可以在计算过程中（例如循环），对加法和乘法的结果取模，而不是在循环结束后再取模。注：如果涉及到幂运算，指数是不能随意取模的。如果指数在 64 位整数的范围内，可以用快速幂计算，原理见一张图秒懂快速幂；如果指数超出 64 位整数的范围，见欧拉降幂。 如果计算过程中有减法，可能会产生负数，处理不当也会导致 WA。如何正确处理这种情况呢？同余 同余式的移项同余式中的加减法可以移项负数和减法的取模 除法的取模证明: 求模运算总结 代码实现时，上面的加减乘除通常是这样写的： MOD = 1_000_000_007// 加(a + b) % MOD// 减，b 在 [0,MOD-1] 中(a - b + MOD) % MOD// 把任意整数 a 取模到 [0,MOD-1] 中，无论 a 是正是负(a % MOD + MOD) % MOD// 乘（注意使用 64 位整数）a * b % MOD// 多个数相乘，要步步取模，防止溢出a * b % MOD * c % MOD// 除（MOD 是质数且 b 不是 MOD 的倍数）a * qpow(b, MOD - 2, MOD) % MOD 其中 qpow 为快速幂. 总之，如果发现解答错误，可以检查下代码，看看是不是哪里漏掉取模了。附:组合数计算模板代码如下: class Solution private static final int MOD = 1_000_000_007; private static final int MX = 100_001; // 根据题目数据范围修改 private static final long[] F = new long[MX]; // F[i] = i! private static final long[] INV_F = new long[MX]; // INV_F[i] = i!^-1 = pow(i!, MOD-2) static F[0] = 1; for (int i = 1; i MX; i++) F[i] = F[i - 1] * i % MOD; INV_F[MX - 1] = pow(F[MX - 1], MOD - 2); for (int i = MX - 1; i 0; i--) INV_F[i - 1] = INV_F[i] * i % MOD; private static long pow(long x, int n) long res = 1; for (; n 0; n /= 2) if (n % 2 0) res = res * x % MOD; x = x * x % MOD; return res; // 从 n 个数中选 m 个数的方案数 private long comb(int n, int m) return m 0 || m n ? 0 : F[n] * INV_F[m] % MOD * INV_F[n - m] % MOD; public int solve(int[] nums) // 预处理的逻辑写在 static 块中，这样只会初始化一次 快速幂一图流(灵神): 代码实现时，注意 n−2^31的情况，取反后 n2^31超出 int 最大值。可以转成 64 位 int 解决。模版: class Solution public double myPow(double x, int N) double ans = 1; long n = N; if (n 0) // x^-n = (1/x)^n n = -n; x = 1 / x; while (n != 0) // 从低到高枚举 n 的每个比特位 if ((n 1) == 1) // 这个比特位是 1 ans *= x; // 把 x 乘到 ans 中 x *= x; // x 自身平方 n = 1; // 继续枚举下一个比特位 return ans; 二进制从集合论到位运算，常见位运算技巧分类总结 s = 101100 s-1 = 101011 // 最低位的 1 变成 0，同时 1 右边的 0 都取反，变成 1s(s-1) = 101000 特别地，如果 s 是 2 的幂，那么 s(s−1)0。 此外，编程语言提供了一些和二进制有关的库函数，例如： 计算二进制中的 1 的个数，也就是集合大小； 计算二进制长度，减一后得到集合最大元素； 计算二进制尾零个数，也就是集合最小元素。 调用这些函数的时间复杂度都是 O(1)。 s = 101100 ~s = 010011(~s)+1 = 010100 // 根据补码的定义，这就是 -s = s 的最低 1 左侧取反，右侧不变s -s = 000100 // lowbit 遍历集合 for (int i = 0; i n; i++) if (((s i) 1) == 1) // i 在 s 中 // 处理 i 的逻辑 for (int t = s; t 0; t = t - 1) int i = Integer.numberOfTrailingZeros(t); // 处理 i 的逻辑 四、枚举集合§4.1 枚举所有集合设元素范围从 0 到 n−1，从空集 ∅ 枚举到全集 U： for (int s = 0; s (1 n); s++) // 处理 s 的逻辑 §4.2 枚举非空子集设集合为 s，从大到小枚举 s 的所有非空子集 sub： for (int sub = s; sub 0; sub = (sub - 1) s) // 处理 sub 的逻辑 为什么要写成 sub (sub - 1) s 呢？ 暴力做法是从 s 出发，不断减一，直到 0。但这样做，中途会遇到很多并不是 s 的子集的情况。例如 s10101 时，减一得到 10100，这是 s 的子集。但再减一就得到 10011 了，这并不是 s 的子集，下一个子集应该是 10001。 把所有的合法子集按顺序列出来，会发现我们做的相当于「压缩版」的二进制减法，例如 10101→10100→10001→10000→00101→⋯如果忽略掉 10101 中的两个 0，数字的变化和二进制减法是一样的，即 111→110→101→100→011→⋯如何快速跳到下一个子集呢？比如，怎么从 10100 跳到 10001？ 普通的二进制减法，是 10100−110011，也就是把最低位的 1 变成 0，同时把最低位的 1 右边的 0 都变成 1。压缩版的二进制减法也是类似的，对于 10100→10001，也会把最低位的 1 变成 0，对于最低位的 1 右边的 0，并不是都变成 1，只有在 s10101 中的 1 才会变成 1。怎么做到？减一后 10101 就行，也就是 (10100−1) 1010110001。§4.3 枚举子集（包含空集）如果要从大到小枚举 s 的所有子集 sub（从 s 枚举到空集 ∅），可以这样写： int sub = s;do // 处理 sub 的逻辑 sub = (sub - 1) s; while (sub != s); 其中 Java 和 C++ 的原理是，当 sub0 时（空集），再减一就得到 −1，对应的二进制为 111⋯1，再 s 就得到了 s。所以当循环到 subs 时，说明最后一次循环的 sub0（空集），s 的所有子集都枚举到了，退出循环。 注：还可以枚举全集 U 的所有大小恰好为 k 的子集，这一技巧叫做 Gosper’s Hack，具体请看 视频讲解。 §4.4 枚举超集如果 T 是 S 的子集，那么称 S 是 T 的超集（superset）。 枚举超集的原理和上文枚举子集是类似的，这里通过或运算保证枚举的集合 S 一定包含集合 T 中的所有元素。 枚举 S，满足 S 是 T 的超集，也是全集 U{0,1,2,…,n−1} 的子集。 for (int s = t; s (1 n); s = (s + 1) | t) // 处理 s 的逻辑 数组 二分查找https://leetcode.cn/problems/binary-search/通用模板： class Solution public int search(int[] nums, int target) int i = lowerBound(nums, target); // 选择其中一种写法即可 return i nums.length nums[i] == target ? i : -1; // 【下面列了三种写法，选一种自己喜欢的就行】 // lowerBound 返回最小的满足 nums[i] = target 的 i // 如果数组为空，或者所有数都 target，则返回 nums.length // 要求 nums 是非递减的，即 nums[i] = nums[i + 1] // 闭区间写法 private int lowerBound(int[] nums, int target) int left = 0, right = nums.length - 1; // 闭区间 [left, right] while (left = right) // 区间不为空 // 循环不变量： // nums[left-1] target // nums[right+1] = target int mid = left + (right - left) / 2; if (nums[mid] target) left = mid + 1; // 范围缩小到 [mid+1, right] else right = mid - 1; // 范围缩小到 [left, mid-1] return left; // 或者 right+1 // 左闭右开区间写法 private int lowerBound2(int[] nums, int target) int left = 0, right = nums.length; // 左闭右开区间 [left, right) while (left right) // 区间不为空 // 循环不变量： // nums[left-1] target // nums[right] = target int mid = left + (right - left) / 2; if (nums[mid] target) left = mid + 1; // 范围缩小到 [mid+1, right) else right = mid; // 范围缩小到 [left, mid) return left; // 或者 right // 开区间写法 private int lowerBound3(int[] nums, int target) int left = -1, right = nums.length; // 开区间 (left, right) while (left + 1 right) // 区间不为空 // 循环不变量： // nums[left] target // nums[right] = target int mid = left + (right - left) / 2; if (nums[mid] target) left = mid; // 范围缩小到 (mid, right) else right = mid; // 范围缩小到 (left, mid) return right; // 或者 left+1 二分法数组：每次遇到二分法，都是一看就会，一写就废 暴力解法时间复杂度：O(n) 二分法时间复杂度：O(logn) 双指针双指针法 数组：就移除个元素很难么？(opens new window) 双指针法（快慢指针法）：通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。 暴力解法时间复杂度：O(n^2) 双指针时间复杂度：O(n) 滑动窗口滑动窗口 数组：滑动窗口拯救了你(opens new window) 滑动窗口的精妙之处在于根据当前子序列和大小的情况，不断调节子序列的起始位置。从而将O(n^2)的暴力解法降为O(n)。 模拟行为 数组：这个循环可以转懵很多人 在这道题目中，我们再一次介绍到了循环不变量原则，其实这也是写程序中的重要原则。 相信大家有遇到过这种情况： 感觉题目的边界调节超多，一波接着一波的判断，找边界，拆了东墙补西墙，好不容易运行通过了，代码写的十分冗余，毫无章法，其实真正解决题目的代码都是简洁的，或者有原则性的，大家可以在这道题目中体会到这一点。 前缀和链表JAVA版本 public class ListNode // 结点的值 int val; // 下一个结点 ListNode next; // 节点的构造函数(无参) public ListNode() // 节点的构造函数(有一个参数) public ListNode(int val) this.val = val; // 节点的构造函数(有两个参数) public ListNode(int val, ListNode next) this.val = val; this.next = next; 移除链表元素设计链表反转链表原地反转，只需要把每个节点间的指向反转就可以尝试递归调用两两交换链表中的节点删除链表的倒数第N个节点链表相交环形链表II环形链表快慢指针一同走 相遇后差一圈 结论a c从起点和相遇点同时走 就是入口点相似题: https://leetcode.cn/problems/find-the-duplicate-number这个是数组模拟环形链表,非常有意思 class Solution public int findDuplicate(int[] nums) int slow = 0; int fast = 0; slow = nums[slow]; fast = nums[nums[fast]]; while(slow != fast) slow = nums[slow]; fast = nums[nums[fast]]; int pre1 = 0; int pre2 = slow; while(pre1 != pre2) pre1 = nums[pre1]; pre2 = nums[pre2]; return pre1; 哈希表题目242.有效的字母异位词力扣题目链接349.两个数组的交集力扣题目链接第202题. 快乐数力扣题目链接两数之和力扣题目链接第454题.四数相加II力扣题目链接赎金信力扣题目链接第15题. 三数之和力扣题目链接第18题. 四数之和力扣题目链接 字符串KMP算法介绍:KMP 算法是一个快速查找匹配串的算法，它的作用其实就是本题问题：如何快速在「原字符串」中找到「匹配字符串」。 上述的朴素解法，不考虑剪枝的话复杂度是 O(m∗n) 的，而 KMP 算法的复杂度为 O(m+n)。 KMP 之所以能够在 O(m+n) 复杂度内完成查找，是因为其能在「非完全匹配」的过程中提取到有效信息进行复用，以减少「重复匹配」的消耗。个人理解:通过构造一个next数组,使的前缀一样的部分能够快速跳转,前缀一样,但是后面的一个不一样,那就可以直接通过next数组跳转到上一个前缀一样的位置的下标,然后继续匹配.所以这个算法的关键就是构造next数组.看到的一个比较好的实现方式就是在主串和匹配串前面都加上一个空格,这样就可以保证next数组的下标从1开始,这样就可以避免很多边界问题. 先贴一个实现: class Solution // KMP 算法 // ss: 原串(string) pp: 匹配串(pattern) public int strStr(String ss, String pp) if (pp.isEmpty()) return 0; // 分别读取原串和匹配串的长度 int n = ss.length(), m = pp.length(); // 原串和匹配串前面都加空格，使其下标从 1 开始 ss = + ss; pp = + pp; char[] s = ss.toCharArray(); char[] p = pp.toCharArray(); // 构建 next 数组，数组长度为匹配串的长度（next 数组是和匹配串相关的） int[] next = new int[m + 1]; // 构造过程 i = 2，j = 0 开始，i 小于等于匹配串长度 【构造 i 从 2 开始】 for (int i = 2, j = 0; i = m; i++) // 匹配不成功的话，j = next(j) while (j 0 p[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++ if (p[i] == p[j + 1]) j++; // 更新 next[i]，结束本次循环，i++ next[i] = j; // 匹配过程，i = 1，j = 0 开始，i 小于等于原串长度 【匹配 i 从 1 开始】 for (int i = 1, j = 0; i = n; i++) // 匹配不成功 j = next(j) while (j 0 s[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++，结束本次循环后 i++ if (s[i] == p[j + 1]) j++; // 整一段匹配成功，直接返回下标 if (j == m) return i - m; return -1; next数组构造: 匹配过程: 题目344.反转字符串力扣题目链接 aa^b: 先把a和b中，不相同的位保存到a，现在a中置1的位，代表原始的a和b不相同的位，而0，就是a和b相同的位。 ba^b: 不相同的位是1和原始b异或，就得到原始a的那个位的值；相同的位是0和原始b异或就是原始a或者原始b的值（本来就相同）。现在得到的就是原始a的值，现在存在b中。 aa^b：和上面相同。 a，b已经交换。 1.反转字符串II力扣题目链接 二叉树二叉树基础 满二叉树:如果一棵二叉树只有度为0的结点和度为2的结点，并且度为0的结点在同一层上，则这棵二叉树为满二叉树。 完全二叉树:在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层（h从1开始），则该层包含 1~ 2^(h-1) 个节点。也就是只有最后一层右侧不满,前面都是满的. 二叉搜索树:二叉搜索树是一个有序树 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树 平衡二叉搜索树:平衡二叉搜索树：又被称为AVL（Adelson-Velsky and Landis）树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 二叉树的存储方式二叉树可以链式存储，也可以顺序存储。 二叉树遍历顺序前序遍历：中左右中序遍历：左中右后序遍历：左右中 public class TreeNode int val; TreeNode left; TreeNode right; TreeNode() TreeNode(int val) this.val = val; TreeNode(int val, TreeNode left, TreeNode right) this.val = val; this.left = left; this.right = right; 二叉树递归遍历递归的三要素:1.确定递归函数的参数和返回值： 确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。 2.确定终止条件： 写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。 3.确定单层递归的逻辑： 确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。 递归三部曲 确定递归函数的参数以及返回值 确定终止条件 确定单层递归的逻辑 参数：需要二叉树的根节点，还需要一个计数器，这个计数器用来计算二叉树的一条边之和是否正好是目标和，计数器为int型。再来看返回值，递归函数什么时候需要返回值？什么时候不需要返回值？这里总结如下三点： 如果需要搜索整棵二叉树且不用处理递归返回值，递归函数就不要返回值。（这种情况就是本文下半部分介绍的113.路径总和ii） 如果需要搜索整棵二叉树且需要处理递归返回值，递归函数就需要返回值。 （这种情况我们在236. 二叉树的最近公共祖先 中介绍） 如果要搜索其中一条符合条件的路径，那么递归一定需要返回值，因为遇到符合条件的路径了就要及时返回。（本题的情况） 终止条件终止条件如果是判断叶子节点，递归的过程中就不要让空节点进入递归了。 // 前序遍历·递归·LC144_二叉树的前序遍历class Solution public ListInteger preorderTraversal(TreeNode root) ListInteger result = new ArrayListInteger(); preorder(root, result); return result; public void preorder(TreeNode root, ListInteger result) if (root == null) return; result.add(root.val); preorder(root.left, result); preorder(root.right, result); // 中序遍历·递归·LC94_二叉树的中序遍历class Solution public ListInteger inorderTraversal(TreeNode root) ListInteger res = new ArrayList(); inorder(root, res); return res; void inorder(TreeNode root, ListInteger list) if (root == null) return; inorder(root.left, list); list.add(root.val); // 注意这一句 inorder(root.right, list); // 后序遍历·递归·LC145_二叉树的后序遍历class Solution public ListInteger postorderTraversal(TreeNode root) ListInteger res = new ArrayList(); postorder(root, res); return res; void postorder(TreeNode root, ListInteger list) if (root == null) return; postorder(root.left, list); postorder(root.right, list); list.add(root.val); // 注意这一句 二叉树迭代遍历144.二叉树的前序遍历https://leetcode.cn/problems/binary-tree-preorder-traversal/94.二叉树的中序遍历https://leetcode.cn/problems/binary-tree-inorder-traversal/145.二叉树的后序遍历https://leetcode.cn/problems/binary-tree-postorder-traversal/ 统一写法!!!!!!!!思路:将访问的节点放入栈中，把要处理的节点也放入栈中但是要做标记。实现方式: 方法一：就是要处理的节点放入栈之后，紧接着放入一个空指针作为标记。 这种方法可以叫做空指针标记法。 方法二：加一个 boolean 值跟随每个节点，false (默认值) 表示需要为该节点和它的左右儿子安排在栈中的位次，true 表示该节点的位次之前已经安排过了，可以收割节点了。 这种方法可以叫做boolean 标记法。 这种方法更容易理解，在面试中更容易写出来。前序遍历代码:public ListInteger preorderTraversal(TreeNode root) ListInteger result = new LinkedList(); DequeTreeNode st = new LinkedList(); if (root != null) st.push(root); while (!st.isEmpty()) TreeNode node = st.peek(); if (node != null) st.pop(); // 将该节点弹出，避免重复操作，下面再将右左中节点添加到栈中（前序遍历-中左右，入栈顺序右左中） if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 else // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 return result; 中序遍历代码:public ListInteger inorderTraversal(TreeNode root) ListInteger result = new LinkedList(); DequeTreeNode st = new LinkedList(); if (root != null) st.push(root); while (!st.isEmpty()) TreeNode node = st.peek(); if (node != null) st.pop(); // 将该节点弹出，避免重复操作，下面再将右中左节点添加到栈中（中序遍历-左中右，入栈顺序右中左） if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） else // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 return result; 后序遍历代码:public ListInteger postorderTraversal(TreeNode root) ListInteger result = new LinkedList(); DequeTreeNode st = new LinkedList(); if (root != null) st.push(root); while (!st.isEmpty()) TreeNode node = st.peek(); if (node != null) st.pop(); // 将该节点弹出，避免重复操作，下面再将中右左节点添加到栈中（后序遍历-左右中，入栈顺序中右左） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） else // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 return result; 直接用栈模拟的写法:前序 中序 后序的写法不一样. // 前序遍历顺序：中-左-右，入栈顺序：中-右-左class Solution public ListInteger preorderTraversal(TreeNode root) ListInteger result = new ArrayList(); if (root == null) return result; StackTreeNode stack = new Stack(); stack.push(root); while (!stack.isEmpty()) TreeNode node = stack.pop(); result.add(node.val); if (node.right != null) stack.push(node.right); if (node.left != null) stack.push(node.left); return result; // 中序遍历顺序: 左-中-右 入栈顺序： 左-右class Solution public ListInteger inorderTraversal(TreeNode root) ListInteger result = new ArrayList(); if (root == null) return result; StackTreeNode stack = new Stack(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) if (cur != null) stack.push(cur); cur = cur.left; else cur = stack.pop(); result.add(cur.val); cur = cur.right; return result; // 后序遍历顺序 左-右-中 入栈顺序：中-左-右 出栈顺序：中-右-左， 最后翻转结果class Solution public ListInteger postorderTraversal(TreeNode root) ListInteger result = new ArrayList(); if (root == null) return result; StackTreeNode stack = new Stack(); stack.push(root); while (!stack.isEmpty()) TreeNode node = stack.pop(); result.add(node.val); if (node.left != null) stack.push(node.left); if (node.right != null) stack.push(node.right); Collections.reverse(result); return result; 二叉树的层序遍历(BFS)// 102.二叉树的层序遍历class Solution public ListListInteger resList = new ArrayListListInteger(); public ListListInteger levelOrder(TreeNode root) //checkFun01(root,0); checkFun02(root); return resList; //BFS--递归方式 public void checkFun01(TreeNode node, Integer deep) if (node == null) return; deep++; if (resList.size() deep) //当层级增加时，list的Item也增加，利用list的索引值进行层级界定 ListInteger item = new ArrayListInteger(); resList.add(item); resList.get(deep - 1).add(node.val); checkFun01(node.left, deep); checkFun01(node.right, deep); //BFS--迭代方式--借助队列 public void checkFun02(TreeNode node) if (node == null) return; QueueTreeNode que = new LinkedListTreeNode(); que.offer(node); while (!que.isEmpty()) ListInteger itemList = new ArrayListInteger(); int len = que.size(); while (len 0) TreeNode tmpNode = que.poll(); itemList.add(tmpNode.val); if (tmpNode.left != null) que.offer(tmpNode.left); if (tmpNode.right != null) que.offer(tmpNode.right); len--; resList.add(itemList); Morris遍历学习了一个Morris遍历方式,这种方式可以在O(1)的空间复杂度下完成二叉树的遍历,并且不需要使用栈来存储节点.贴一个自己画的伪代码:前序和中序可以通过调整代码实现,后序可以通过向右的前序然后结果翻转来实现. //前序class Solution public ListInteger preorderTraversal(TreeNode root) ListInteger res = new LinkedList(); TreeNode cur = root; while(cur != null) if(cur.left == null) res.add(cur.val); cur =cur.right; //向右走 else TreeNode pre = cur.left; while(pre.right != null pre.right != cur) pre = pre.right; //开始建立索引 if(pre.right == null) //是一个第一次来的节点 pre.right = cur; res.add(cur.val); cur = cur.left; else //并不是第一次来的节点 pre.right = null; cur = cur.right; return res; //中序class Solution public ListInteger inorderTraversal(TreeNode root) ListInteger res =new LinkedList(); TreeNode cur = root; while(cur != null) if(cur.left == null) res.add(cur.val); cur = cur.right; else TreeNode pre = cur.left; while(pre.right != null pre.right != cur) pre = pre.right; if(pre.right == null) pre.right = cur; cur = cur.left; else pre.right = null; res.add(cur.val); cur = cur.right; return res; //后序import java.util.ArrayList;import java.util.Collections;import java.util.List;class TreeNode int val; TreeNode left; TreeNode right; TreeNode(int x) val = x; public class MorrisPostorderTraversal public ListInteger postorderTraversal(TreeNode root) ListInteger res = new ArrayList(); TreeNode curr = root; TreeNode prev = null; while (curr != null) if (curr.right == null) res.add(curr.val); // 右子树为空，直接访问 curr = curr.left; // 转向左子树 else // 找到右子树的最左节点（即后继节点） prev = curr.right; while (prev.left != null prev.left != curr) prev = prev.left; if (prev.left == null) prev.left = curr; // 建立线索 res.add(curr.val); // 访问当前节点（前序遍历位置） curr = curr.right; // 转向右子树 else prev.left = null; // 恢复树结构 curr = curr.left; // 转向左子树 Collections.reverse(res); // 反转结果，得到后序遍历 return res; 二叉树题目翻转二叉树dfs或者bfs都可做 //DFS递归class Solution /** * 前后序遍历都可以 * 中序不行，因为先左孩子交换孩子，再根交换孩子（做完后，右孩子已经变成了原来的左孩子），再右孩子交换孩子（此时其实是对原来的左孩子做交换） */ public TreeNode invertTree(TreeNode root) if (root == null) return null; invertTree(root.left); invertTree(root.right); swapChildren(root); return root; private void swapChildren(TreeNode root) TreeNode tmp = root.left; root.left = root.right; root.right = tmp; //BFSclass Solution public TreeNode invertTree(TreeNode root) if (root == null) return null; ArrayDequeTreeNode deque = new ArrayDeque(); deque.offer(root); while (!deque.isEmpty()) int size = deque.size(); while (size-- 0) TreeNode node = deque.poll(); swap(node); if (node.left != null) deque.offer(node.left); if (node.right != null) deque.offer(node.right); return root; public void swap(TreeNode root) TreeNode temp = root.left; root.left = root.right; root.right = temp; 对称二叉树这道题递归调用很简单,迭代调用用一个队列就可以,退出条件可以注意一下. /** * 递归法 */public boolean isSymmetric1(TreeNode root) return compare(root.left, root.right);private boolean compare(TreeNode left, TreeNode right) if (left == null right != null) return false; if (left != null right == null) return false; if (left == null right == null) return true; if (left.val != right.val) return false; // 比较外侧 boolean compareOutside = compare(left.left, right.right); // 比较内侧 boolean compareInside = compare(left.right, right.left); return compareOutside compareInside;/** * 迭代法 * 使用双端队列，相当于两个栈 */public boolean isSymmetric2(TreeNode root) DequeTreeNode deque = new LinkedList(); deque.offerFirst(root.left); deque.offerLast(root.right); while (!deque.isEmpty()) TreeNode leftNode = deque.pollFirst(); TreeNode rightNode = deque.pollLast(); if (leftNode == null rightNode == null) continue; // if (leftNode == null rightNode != null) // return false;// // if (leftNode != null rightNode == null) // return false;// // if (leftNode.val != rightNode.val) // return false;// // 以上三个判断条件合并 if (leftNode == null || rightNode == null || leftNode.val != rightNode.val) return false; deque.offerFirst(leftNode.left); deque.offerFirst(leftNode.right); deque.offerLast(rightNode.right); deque.offerLast(rightNode.left); return true;/** * 迭代法 * 使用普通队列 *///public boolean isSymmetric3(TreeNode root) QueueTreeNode deque = new LinkedList(); deque.offer(root.left); deque.offer(root.right); while (!deque.isEmpty()) TreeNode leftNode = deque.poll(); TreeNode rightNode = deque.poll(); if (leftNode == null rightNode == null) continue; // if (leftNode == null rightNode != null) // return false;// // if (leftNode != null rightNode == null) // return false;// // if (leftNode.val != rightNode.val) // return false;// // 以上三个判断条件合并 if (leftNode == null || rightNode == null || leftNode.val != rightNode.val) return false; // 这里顺序与使用Deque不同 deque.offer(leftNode.left); deque.offer(rightNode.right); deque.offer(leftNode.right); deque.offer(rightNode.left); return true; 二叉树最大深度104.二叉树的最大深度 class Solution /** * 递归法 */ public int maxDepth(TreeNode root) if (root == null) return 0; int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); return Math.max(leftDepth, rightDepth) + 1; class Solution /** * 递归法(求深度法) */ //定义最大深度 int maxnum = 0; public int maxDepth(TreeNode root) ans(root,0); return maxnum; //递归求解最大深度 void ans(TreeNode tr,int tmp) if(tr==null) return; tmp++; maxnum = maxnumtmp?tmp:maxnum; ans(tr.left,tmp); ans(tr.right,tmp); tmp--; class Solution /** * 迭代法，使用层序遍历 */ public int maxDepth(TreeNode root) if(root == null) return 0; DequeTreeNode deque = new LinkedList(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) int size = deque.size(); depth++; for (int i = 0; i size; i++) TreeNode node = deque.poll(); if (node.left != null) deque.offer(node.left); if (node.right != null) deque.offer(node.right); return depth; 二叉树的所有路径 https://leetcode.cn/problems/binary-tree-paths/递归和回溯.PS:递归和回溯永远要放在一起!!! //解法一//方式一class Solution /** * 递归法 */ public ListString binaryTreePaths(TreeNode root) ListString res = new ArrayList();// 存最终的结果 if (root == null) return res; ListInteger paths = new ArrayList();// 作为结果中的路径 traversal(root, paths, res); return res; private void traversal(TreeNode root, ListInteger paths, ListString res) paths.add(root.val);// 前序遍历，中 // 遇到叶子结点 if (root.left == null root.right == null) // 输出 StringBuilder sb = new StringBuilder();// StringBuilder用来拼接字符串，速度更快 for (int i = 0; i paths.size() - 1; i++) sb.append(paths.get(i)).append(-); sb.append(paths.get(paths.size() - 1));// 记录最后一个节点 res.add(sb.toString());// 收集一个路径 return; // 递归和回溯是同时进行，所以要放在同一个花括号里 if (root.left != null) // 左 traversal(root.left, paths, res); paths.remove(paths.size() - 1);// 回溯 if (root.right != null) // 右 traversal(root.right, paths, res); paths.remove(paths.size() - 1);// 回溯 //方式二class Solution ListString result = new ArrayList(); public ListString binaryTreePaths(TreeNode root) deal(root, ); return result; public void deal(TreeNode node, String s) if (node == null) return; if (node.left == null node.right == null) result.add(new StringBuilder(s).append(node.val).toString()); return; String tmp = new StringBuilder(s).append(node.val).append(-).toString(); deal(node.left, tmp); deal(node.right, tmp); 迭代法: // 解法二class Solution /** * 迭代法 */ public ListString binaryTreePaths(TreeNode root) ListString result = new ArrayList(); if (root == null) return result; StackObject stack = new Stack(); // 节点和路径同时入栈 stack.push(root); stack.push(root.val + ); while (!stack.isEmpty()) // 节点和路径同时出栈 String path = (String) stack.pop(); TreeNode node = (TreeNode) stack.pop(); // 若找到叶子节点 if (node.left == null node.right == null) result.add(path); //右子节点不为空 if (node.right != null) stack.push(node.right); stack.push(path + - + node.right.val); //左子节点不为空 if (node.left != null) stack.push(node.left); stack.push(path + - + node.left.val); return result; 404.左叶子之和https://leetcode.cn/problems/sum-of-left-leaves/ class Solution public int sumOfLeftLeaves(TreeNode root) if (root == null) return 0; int leftValue = sumOfLeftLeaves(root.left); // 左 int rightValue = sumOfLeftLeaves(root.right); // 右 int midValue = 0; if (root.left != null root.left.left == null root.left.right == null) midValue = root.left.val; int sum = midValue + leftValue + rightValue; // 中 return sum; class Solution public int sumOfLeftLeaves(TreeNode root) if (root == null) return 0; StackTreeNode stack = new Stack (); stack.add(root); int result = 0; while (!stack.isEmpty()) TreeNode node = stack.pop(); if (node.left != null node.left.left == null node.left.right == null) result += node.left.val; if (node.right != null) stack.add(node.right); if (node.left != null) stack.add(node.left); return result; 513.找树左下角的值https://leetcode.cn/problems/find-bottom-left-tree-value/ // 递归法class Solution private int Deep = -1; private int value = 0; public int findBottomLeftValue(TreeNode root) value = root.val; findLeftValue(root,0); return value; private void findLeftValue (TreeNode root,int deep) if (root == null) return; if (root.left == null root.right == null) if (deep Deep) value = root.val; Deep = deep; if (root.left != null) findLeftValue(root.left,deep + 1); if (root.right != null) findLeftValue(root.right,deep + 1); //迭代法class Solution public int findBottomLeftValue(TreeNode root) QueueTreeNode queue = new LinkedList(); queue.offer(root); int res = 0; while (!queue.isEmpty()) int size = queue.size(); for (int i = 0; i size; i++) TreeNode poll = queue.poll(); if (i == 0) res = poll.val; if (poll.left != null) queue.offer(poll.left); if (poll.right != null) queue.offer(poll.right); return res; 路径总和https://leetcode.cn/problems/path-sum/ 106.从中序与后序遍历序列构造二叉树 class Solution MapInteger, Integer map; // 方便根据数值查找位置 public TreeNode buildTree(int[] inorder, int[] postorder) map = new HashMap(); for (int i = 0; i inorder.length; i++) // 用map保存中序序列的数值对应位置 map.put(inorder[i], i); return findNode(inorder, 0, inorder.length, postorder,0, postorder.length); // 前闭后开 public TreeNode findNode(int[] inorder, int inBegin, int inEnd, int[] postorder, int postBegin, int postEnd) // 参数里的范围都是前闭后开 if (inBegin = inEnd || postBegin = postEnd) // 不满足左闭右开，说明没有元素，返回空树 return null; int rootIndex = map.get(postorder[postEnd - 1]); // 找到后序遍历的最后一个元素在中序遍历中的位置 TreeNode root = new TreeNode(inorder[rootIndex]); // 构造结点 int lenOfLeft = rootIndex - inBegin; // 保存中序左子树个数，用来确定后序数列的个数 root.left = findNode(inorder, inBegin, rootIndex, postorder, postBegin, postBegin + lenOfLeft); root.right = findNode(inorder, rootIndex + 1, inEnd, postorder, postBegin + lenOfLeft, postEnd - 1); return root; 106.从前序与后序遍历序列构造二叉树 class Solution MapInteger,Integer mp = new HashMap(); public TreeNode constructFromPrePost(int[] preorder, int[] postorder) int n = preorder.length; for(int i=0 ; in ; ++i) mp.put(postorder[i],i); return dfs(preorder,0,n-1,postorder,0,n-1); private TreeNode dfs(int[] preorder , int pre_beg,int pre_end ,int[] postorder, int pos_beg , int pos_end) //闭区间 先判断是否终止 if(pre_begpre_end || pos_begpos_end) return null; int mid = preorder[pre_beg]; if(pre_beg == pre_end || pos_beg == pos_end) return new TreeNode(mid); int nx_l = preorder[pre_beg+1]; int ind_nxl = mp.get(nx_l); int lenOfLeft = ind_nxl - pos_beg + 1; TreeNode root = new TreeNode(mid); root.left = dfs(preorder,pre_beg+1,pre_beg+lenOfLeft,postorder,pos_beg,pos_beg+lenOfLeft-1); root.right = dfs(preorder,pre_beg+lenOfLeft+1,pre_end,postorder,ind_nxl+1,pos_end-1); return root; 617.合并二叉树https://leetcode.cn/problems/merge-two-binary-trees/ 700.二叉搜索树中的搜索https://leetcode.cn/problems/search-in-a-binary-search-tree/ class Solution // 递归，普通二叉树 public TreeNode searchBST(TreeNode root, int val) if (root == null || root.val == val) return root; TreeNode left = searchBST(root.left, val); if (left != null) return left; return searchBST(root.right, val); class Solution // 递归，利用二叉搜索树特点，优化 public TreeNode searchBST(TreeNode root, int val) if (root == null || root.val == val) return root; if (val root.val) return searchBST(root.left, val); else return searchBST(root.right, val); class Solution // 迭代，普通二叉树 public TreeNode searchBST(TreeNode root, int val) if (root == null || root.val == val) return root; StackTreeNode stack = new Stack(); stack.push(root); while (!stack.isEmpty()) TreeNode pop = stack.pop(); if (pop.val == val) return pop; if (pop.right != null) stack.push(pop.right); if (pop.left != null) stack.push(pop.left); return null; class Solution // 迭代，利用二叉搜索树特点，优化，可以不需要栈 public TreeNode searchBST(TreeNode root, int val) while (root != null) if (val root.val) root = root.left; else if (val root.val) root = root.right; else return root; return null; 98.验证二叉搜索树记住最重要的就是:二叉搜索树的中序遍历是有序的. //使用統一迭代法class Solution public boolean isValidBST(TreeNode root) StackTreeNode stack = new Stack(); TreeNode pre = null; if(root != null) stack.add(root); while(!stack.isEmpty()) TreeNode curr = stack.peek(); if(curr != null) stack.pop(); if(curr.right != null) stack.add(curr.right); stack.add(curr); stack.add(null); if(curr.left != null) stack.add(curr.left); else stack.pop(); TreeNode temp = stack.pop(); if(pre != null pre.val = temp.val) return false; pre = temp; return true; class Solution // 递归 TreeNode max; public boolean isValidBST(TreeNode root) if (root == null) return true; // 左 boolean left = isValidBST(root.left); if (!left) return false; // 中 if (max != null root.val = max.val) return false; max = root; // 右 boolean right = isValidBST(root.right); return right; class Solution // 迭代 public boolean isValidBST(TreeNode root) if (root == null) return true; StackTreeNode stack = new Stack(); TreeNode pre = null; while (root != null || !stack.isEmpty()) while (root != null) stack.push(root); root = root.left;// 左 // 中，处理 TreeNode pop = stack.pop(); if (pre != null pop.val = pre.val) return false; pre = pop; root = pop.right;// 右 return true; // 简洁实现·递归解法class Solution public boolean isValidBST(TreeNode root) return validBST(Long.MIN_VALUE, Long.MAX_VALUE, root); boolean validBST(long lower, long upper, TreeNode root) if (root == null) return true; if (root.val = lower || root.val = upper) return false; return validBST(lower, root.val, root.left) validBST(root.val, upper, root.right); // 简洁实现·中序遍历class Solution private long prev = Long.MIN_VALUE; public boolean isValidBST(TreeNode root) if (root == null) return true; if (!isValidBST(root.left)) return false; if (root.val = prev) // 不满足二叉搜索树条件 return false; prev = root.val; return isValidBST(root.right); 二叉树的最近公共祖先https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/ class Solution public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) if (root == null || root == p || root == q) // 递归结束条件 return root; // 后序遍历 TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left == null right == null) // 若未找到节点 p 或 q return null; else if(left == null right != null) // 若找到一个节点 return right; else if(left != null right == null) // 若找到一个节点 return left; else // 若找到两个节点 return root; //迭代public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) int max = Integer.MAX_VALUE; StackTreeNode st = new Stack(); TreeNode cur = root, pre = null; while (cur != null || !st.isEmpty()) while (cur != null) st.push(cur); cur = cur.left; cur = st.pop(); if (cur.right == null || cur.right == pre) // p/q是 中/左 或者 中/右 , 返回中 if (cur == p || cur == q) if ((cur.left != null cur.left.val == max) || (cur.right != null cur.right.val == max)) return cur; cur.val = max; // p/q是 左/右 , 返回中 if (cur.left != null cur.left.val == max cur.right != null cur.right.val == max) return cur; // MAX_VALUE 往上传递 if ((cur.left != null cur.left.val == max) || (cur.right != null cur.right.val == max)) cur.val = max; pre = cur; cur = null; else st.push(cur); cur = cur.right; return null; 二叉搜索树的最近公共祖先https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-search-tree/ 450.删除二叉搜索树中的节点https://leetcode.cn/problems/delete-node-in-a-bst/ // 解法1(最好理解的版本)class Solution public TreeNode deleteNode(TreeNode root, int key) if (root == null) return root; if (root.val == key) if (root.left == null) return root.right; else if (root.right == null) return root.left; else TreeNode cur = root.right; while (cur.left != null) cur = cur.left; cur.left = root.left; root = root.right; return root; if (root.val key) root.left = deleteNode(root.left, key); if (root.val key) root.right = deleteNode(root.right, key); return root; 修剪二叉搜索树https://leetcode.cn/problems/trim-a-binary-search-tree/ class Solution public: TreeNode* trimBST(TreeNode* root, int low, int high) if (root == nullptr) return nullptr; if (root-val low) return trimBST(root-right, low, high); if (root-val high) return trimBST(root-left, low, high); root-left = trimBST(root-left, low, high); root-right = trimBST(root-right, low, high); return root; ; 回溯算法 回溯的理论基础什么是回溯法回溯法也可以叫做回溯搜索法，它是一种搜索的方式。回溯是递归的副产品，只要有递归就会有回溯。 因为回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，如果想让回溯法高效一些，可以加一些剪枝的操作，但也改不了回溯法就是穷举的本质。 回溯法解决的问题 回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 组合是不强调元素顺序的，排列是强调元素顺序。记住组合无序，排列有序，就可以了。 回溯法模板 回溯函数模板返回值以及参数 在回溯算法中，我的习惯是函数起名字为backtracking，这个起名大家随意。 回溯算法中函数返回值一般为void。再来看一下参数，因为回溯算法需要的参数可不像二叉树递归的时候那么容易一次性确定下来，所以一般是先写逻辑，然后需要什么参数，就填什么参数。回溯函数伪代码如下：void backtracking(参数) 回溯函数终止条件 既然是树形结构，那么我们在讲解二叉树的递归 (opens new window)的时候，就知道遍历树形结构一定要有终止条件。所以回溯也有要终止条件。什么时候达到了终止条件，树中就可以看出，一般来说搜到叶子节点了，也就找到了满足条件的一条答案，把这个答案存放起来，并结束本层递归。所以回溯函数终止条件伪代码如下： if (终止条件) 存放结果; return; 回溯搜索的遍历过程 在上面我们提到了，回溯法一般是在集合中递归搜索，集合的大小构成了树的宽度，递归的深度构成的树的深度。 void backtracking(参数) if (终止条件) 存放结果; return; for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 回溯题目第77题. 组合 贪心算法动态规划数位DP单调栈图论图论基础图的种类整体上一般分为 有向图 和 无向图。加权有向图，就是图中边是有权值的 度无向图中有几条边连接该节点，该节点就有几度。 在有向图中，每个节点有出度和入度。 出度：从该节点出发的边的个数。 入度：指向该节点边的个数。 连通图在无向图中，任何两个节点都是可以到达的，我们称之为连通图 强连通图在有向图中，任何两个节点是可以相互到达的 连通分量在无向图中的极大连通子图称之为该图的一个连通分量。 强连通分量在有向图中极大强连通子图称之为该图的强连通分量。 图的构造一般使用邻接表、邻接矩阵 或者用类来表示。 字典序字典序字典树:我觉得比较关键的点是: 字典树左子树字典序一定比右子树小. 字典树想要到右侧兄弟节点,直接num++就可以. 关键就是要找到以当前数字为根的十叉树的元素总个数.看着这个图就比较好理解了: 这张图也很好: class Solution /** * 以当前数字为根的十叉树的元素总个数 (包括当前数字) * * @param num 当前数字 (需要先 cast 成 long, 因为 num*10 可能导致 int 溢出) * @param n 数字的最大值 * @return */ private int count(long num, int n) int cnt = 0; // 元素总个数 int width = 1; // 当前层数的宽度, 第一层只有 num 一个元素, 所以第一层宽度为 1 while (true) if (num + width - 1 = n) // n 的值大于等于当前层的最大值, 说明当前层数的个数可以全部添加 cnt += width; num *= 10; width *= 10; else // n 的值小于当前层的最大值则只能添加部分个数或者不添加, 并跳出循环 if (n - num = 0) cnt += n - num + 1; break; return cnt; public int findKthNumber(int n, int k) int cnt = 0; // 已经经过的元素个数, 开始一个元素都没有经过, 所以个数为 0 int num = 1; // 第一个元素 (经过 i 个元素, 当前 num 是第 i + 1 元素) // 要找到第 k 个元素, 需要经过 k - 1 个元素 while (true) if (cnt == k - 1) // 经过了 k - 1 个元素找到了第 k 个元素 break; int temp = count((long) num, n); // 以 num 为根, 以 n 为最大值的十叉树的元素总个数 if (cnt + temp = k) // 以 num 为根的十叉树内有第 k 个元素 num *= 10; cnt++; else if (cnt + temp k) // 以 num 为根的十叉树内没有第 k 个元素 num++; cnt += temp; return num;","tags":["基础","leetcode","算法"],"categories":["算法笔记"]}]