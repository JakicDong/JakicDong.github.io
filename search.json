[{"title":"2025.10.13学习日记","path":"/2025/10/13/学习日记25年10月/2025.10.13学习笔记/","content":"今日学习内容3DGSLOD-Activated 3D Gaussian Splatting: Dynamic Chunking for Scalable Real-Time Rendering 力扣每日一题算法hot10016 - 23 100 sql502- 50 Java复习进度Java进阶之路集合篇框架写完了.Java SE5656Java集合框架3030Java并发编程7171JVM5454MySQL61 - 8383Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法MYDB 简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"SQL刷题笔记","path":"/2025/10/13/算法笔记/SQL刷题笔记/","content":"DISTINCT去重 SELECT TWEET_ID FROM TWEETS WHERE CHAR_LENGTH(CONTENT) 15;char长度 datediff(日期1, 日期2)得到的结果是日期1与日期2相差的天数。 timestampdiff(时间类型, 日期1, 日期2)这个函数和上面diffdate的正、负号规则刚好相反。日期1大于日期2，结果为负，日期1小于日期2，结果为正。","tags":["基础","算法","sql"],"categories":["算法笔记"]},{"title":"2025.10.12学习日记","path":"/2025/10/12/学习日记25年10月/2025.10.12学习笔记/","content":"今日学习内容3DGSLOD-Activated 3D Gaussian Splatting: Dynamic Chunking for Scalable Real-Time Rendering 力扣每日一题DP题目. 算法hot10016 100 Java复习进度Java进阶之路集合篇框架写完了.Java SE5656Java集合框架3030Java并发编程7171JVM5454MySQL60 - 6183Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"2025.10.11学习日记","path":"/2025/10/11/学习日记25年10月/2025.10.11学习笔记/","content":"今日学习内容3DGSLOD-Activated 3D Gaussian Splatting: Dynamic Chunking for Scalable Real-Time Rendering 力扣每日一题DP题目. 算法hot10016 100 Java复习进度Java进阶之路集合篇框架写完了.Java SE5656Java集合框架3030Java并发编程7171JVM22 - 5454MySQL55 - 6083Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"2025.10.10学习日记","path":"/2025/10/10/学习日记25年10月/2025.10.10学习笔记/","content":"今日学习内容3DGSLOD-Activated 3D Gaussian Splatting: Dynamic Chunking for Scalable Real-Time Rendering 力扣每日一题一道后缀和的题目,正难则反. 算法hot10010-16 100 Java复习进度Java进阶之路集合篇框架写完了.Java SE5656Java集合框架3030Java并发编程42 - 7171JVM0 - 2254MySQL5583Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"JVM学习笔记","path":"/2025/10/10/Java问答笔记/JVM学习笔记/","content":"一、引言1.什么是 JVM?JVM，也就是 Java 虚拟机，它是 Java 实现跨平台的基石。 程序运行之前，需要先通过编译器将 Java 源代码文件编译成 Java 字节码文件； 程序运行时，JVM 会对字节码文件进行逐行解释，翻译成机器码指令，并交给对应的操作系统去执行。 三分恶面渣逆袭：Java语言编译运行 这样就实现了 Java 一次编译，处处运行的特性。 说说 JVM 的其他特性？①、JVM 可以自动管理内存，通过垃圾回收器回收不再使用的对象并释放内存空间。 ②、JVM 包含一个即时编译器 JIT，它可以在运行时将热点代码缓存到 codeCache 中，下次执行的时候不用再一行一行的解释，而是直接执行缓存后的机器码，执行效率会大幅提高。 截图来自美团技术 ③、任何可以通过 Java 编译的语言，比如说 Groovy、Kotlin、Scala 等，都可以在 JVM 上运行。 三分恶面渣逆袭：JVM跨语言 为什么要学习 JVM？学习 JVM 可以帮助我们开发者更好地优化程序性能、避免内存问题。 比如说了解 JVM 的内存模型和垃圾回收机制，可以帮助我们更合理地配置内存、减少 GC 停顿。 比如说掌握 JVM 的类加载机制可以帮助我们排查类加载冲突或异常。 再比如说，JVM 还提供了很多调试和监控工具，可以帮助我们分析内存和线程的使用情况，从而解决内存溢出内存泄露等问题。 2.说说 JVM 的组织架构（补充）推荐阅读：大白话带你认识 JVM JVM 大致可以划分为三个部分：类加载器、运行时数据区和执行引擎。 截图来源于网络 ① 类加载器，负责从文件系统、网络或其他来源加载 Class 文件，将 Class 文件中的二进制数据读入到内存当中。 ② 运行时数据区，JVM 在执行 Java 程序时，需要在内存中分配空间来处理各种数据，这些内存区域按照 Java 虚拟机规范可以划分为方法区、堆、虚拟机栈、程序计数器和本地方法栈。 ③ 执行引擎，也是 JVM 的心脏，负责执行字节码。它包括一个虚拟处理器、即时编译器 JIT 和垃圾回收器。 二、内存管理3.🌟能说一下 JVM 的内存区域吗？推荐阅读：深入理解 JVM 的运行时数据区 按照 Java 虚拟机规范，JVM 的内存区域可以细分为程序计数器、虚拟机栈、本地方法栈、堆和方法区。 三分恶面渣逆袭：Java虚拟机运行时数据区 其中方法区和堆是线程共享的，虚拟机栈、本地方法栈和程序计数器是线程私有的。 介绍一下程序计数器？程序计数器也被称为 PC 寄存器，是一块较小的内存空间。它可以看作是当前线程所执行的字节码行号指示器。 介绍一下 Java 虚拟机栈？Java 虚拟机栈的生命周期与线程相同。 当线程执行一个方法时，会创建一个对应的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，然后栈帧会被压入虚拟机栈中。当方法执行完毕后，栈帧会从虚拟机栈中移除。 三分恶面渣逆袭：Java虚拟机栈 一个什么都没有的空方法，空的参数都没有，那局部变量表里有没有变量？对于静态方法，由于不需要访问实例对象 this，因此在局部变量表中不会有任何变量。 对于非静态方法，即使是一个完全空的方法，局部变量表中也会有一个用于存储 this 引用的变量。this 引用指向当前实例对象，在方法调用时被隐式传入。 详细解释一下： 比如说有这样一段代码： public class VarDemo1 public void emptyMethod() // 什么都没有 public static void staticEmptyMethod() // 什么都没有 用 javap -v VarDemo1 命令查看编译后的字节码，就可以在 emptyMethod 中看到这样的内容： 二哥的 Java 进阶之路：javap emptyMethod 这里的 locals=1 表示局部变量表有一个变量，即 this，Slot 0 位置存储了 this 引用。 而在静态方法 staticEmptyMethod 中，你会看到这样的内容： 二哥的 Java 进阶之路：javap staticEmptyMethod 这里的 locals0 表示局部变量表为空，因为静态方法属于类级别方法，不需要 this 引用，也就没有局部变量。 介绍一下本地方法栈？本地方法栈与虚拟机栈相似，区别在于虚拟机栈是为 JVM 执行 Java 编写的方法服务的，而本地方法栈是为 Java 调用本地 native 方法服务的，通常由 CC++ 编写。 在本地方法栈中，主要存放了 native 方法的局部变量、动态链接和方法出口等信息。当一个 Java 程序调用一个 native 方法时，JVM 会切换到本地方法栈来执行这个方法。 介绍一下本地方法栈的运行场景？当 Java 应用需要与操作系统底层或硬件交互时，通常会用到本地方法栈。 比如调用操作系统的特定功能，如内存管理、文件操作、系统时间、系统调用等。 详细说明一下： 比如说获取系统时间的 System.currentTimeMillis() 方法就是调用本地方法，来获取操作系统当前时间的。 二哥的Java 进阶之路：currentTimeMillis方法源码 再比如 JVM 自身的一些底层功能也需要通过本地方法来实现。像 Object 类中的 hashCode() 方法、clone() 方法等。 二哥的Java 进阶之路：hashCode方法源码 native 方法解释一下？推荐阅读：手把手教你用 C语言实现 Java native 本地方法 native 方法是在 Java 中通过 native 关键字声明的，用于调用非 Java 语言，如 CC++ 编写的代码。Java 可以通过 JNI，也就是 Java Native Interface 与底层系统、硬件设备、或者本地库进行交互。 介绍一下 Java 堆？堆是 JVM 中最大的一块内存区域，被所有线程共享，在 JVM 启动时创建，主要用来存储 new 出来的对象。 二哥的 Java 进阶之路：堆 Java 中“几乎”所有的对象都会在堆中分配，堆也是垃圾收集器管理的目标区域。 从内存回收的角度来看，由于垃圾收集器大部分都是基于分代收集理论设计的，所以堆又被细分为新生代、老年代、Eden空间、From Survivor空间、To Survivor空间等。 三分恶面渣逆袭：Java 堆内存结构 随着 JIT 编译器的发展和逃逸技术的逐渐成熟，“所有的对象都会分配到堆上”就不再那么绝对了。 从 JDK 7 开始，JVM 默认开启了逃逸分析，意味着如果某些方法中的对象引用没有被返回或者没有在方法体外使用，也就是未逃逸出去，那么对象可以直接在栈上分配内存。 堆和栈的区别是什么？堆属于线程共享的内存区域，几乎所有 new 出来的对象都会堆上分配，生命周期不由单个方法调用所决定，可以在方法调用结束后继续存在，直到不再被任何变量引用，最后被垃圾收集器回收。 栈属于线程私有的内存区域，主要存储局部变量、方法参数、对象引用等，通常随着方法调用的结束而自动释放，不需要垃圾收集器处理。 介绍一下方法区？方法区并不真实存在，属于 Java 虚拟机规范中的一个逻辑概念，用于存储已被 JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。 在 HotSpot 虚拟机中，方法区的实现称为永久代 PermGen，但在 Java 8 及之后的版本中，已经被元空间 Metaspace 所替代。 变量存在堆栈的什么位置？对于局部变量，它存储在当前方法栈帧中的局部变量表中。当方法执行完毕，栈帧被回收，局部变量也会被释放。 public void method() int localVar = 100; // 局部变量，存储在栈帧中的局部变量表里 对于静态变量来说，它存储在 Java 虚拟机规范中的方法区中，在 Java 7 中是永久代，在 Java8 及以后 是元空间。 public class StaticVarDemo public static int staticVar = 100; // 静态变量，存储在方法区中 4.说一下 JDK 1.6、1.7、1.8 内存区域的变化？JDK 1.6 使用永久代来实现方法区： 三分恶面渣逆袭：JDK 1.6内存区域 JDK 1.7 时仍然是永久代，但发生了一些细微变化，比如将字符串常量池、静态变量存放到了堆上。 三分恶面渣逆袭：JDK 1.7内存区域 在 JDK 1.8 时，直接在内存中划出了一块区域，叫元空间，来取代之前放在 JVM 内存中的永久代，并将运行时常量池、类常量池都移动到了元空间。 三分恶面渣逆袭：JDK 1.8内存区域 5.为什么使用元空间替代永久代？客观上，永久代会导致 Java 应用程序更容易出现内存溢出的问题，因为它要受到 JVM 内存大小的限制。 HotSpot 虚拟机的永久代大小可以通过 -XX：MaxPermSize 参数来设置，32 位机器默认的大小为 64M，64 位的机器则为 85M。 而 J9 和 JRockit 虚拟机就不存在这种限制，只要没有触碰到进程可用的内存上限，例如 32 位系统中的 4GB 限制，就不会出问题。 主观上，当 Oracle 收购 BEA 获得了 JRockit 的所有权后，就准备把 JRockit 中的优秀功能移植到 HotSpot 中。 如 Java Mission Control 管理工具。 但因为两个虚拟机对方法区实现有差异，导致这项工作遇到了很多阻力。 考虑到 HotSpot 虚拟机未来的发展，JDK 6 的时候，开发团队就打算放弃永久代了。 JDK 7 的时候，前进了一小步，把原本放在永久代的字符串常量池、静态变量等移动到了堆中。 JDK 8 就终于完成了这项移出工作，这样的好处就是，元空间的大小不再受到 JVM 内存的限制，而是可以像 J9 和 JRockit 那样，只要系统内存足够，就可以一直用。 6.🌟对象创建的过程了解吗？当我们使用 new 关键字创建一个对象时，JVM 首先会检查 new 指令的参数是否能在常量池中定位到类的符号引用，然后检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有，就先执行类加载。 二哥的 Java 进阶之路：对象的创建过程 如果已经加载，JVM 会为对象分配内存完成初始化，比如数值类型的成员变量初始值是 0，布尔类型是 false，对象类型是 null。 接下来会设置对象头，里面包含了对象是哪个类的实例、对象的哈希码、对象的 GC 分代年龄等信息。 最后，JVM 会执行构造方法 init 完成赋值操作，将成员变量赋值为预期的值，比如 int age = 18，这样一个对象就创建完成了。 对象的销毁过程了解吗？当对象不再被任何引用指向时，就会变成垃圾。垃圾收集器会通过可达性分析算法判断对象是否存活，如果对象不可达，就会被回收。 垃圾收集器通过标记清除、标记复制、标记整理等算法来回收内存，将对象占用的内存空间释放出来。 可以通过 java -XX:+PrintCommandLineFlags -version 和 java -XX:+PrintGCDetails -version 命令查看 JVM 的 GC 收集器。 二哥的 Java 进阶之路：JVM 使用的垃圾收集器 可以看到，我本机安装的 JDK 8 默认使用的是 Parallel Scavenge + Parallel Old。 不同参数代表对应的垃圾收集器表单： 新生代 老年代 JVM参数 Serial Serial -XX:+UseSerialGC Parallel Scavenge Serial -XX:+UseParallelGC -XX:-UseParallelOldGC Parallel Scavenge Parallel Old -XX:+UseParallelGC -XX:+UseParallelOldGC Parallel New CMS -XX:+UseParNewGC -XX:+UseConcMarkSweepGC G1 -XX:+UseG1GC 7.堆内存是如何分配的？在堆中为对象分配内存时，主要使用两种策略：指针碰撞和空闲列表。 三分恶面渣逆袭：指针碰撞和空闲列表 指针碰撞适用于管理简单、碎片化较少的内存区域，如年轻代；而空闲列表适用于内存碎片化较严重或对象大小差异较大的场景如老年代。 什么是指针碰撞？假设堆内存是一个连续的空间，分为两个部分，一部分是已经被使用的内存，另一部分是未被使用的内存。 在分配内存时，Java 虚拟机会维护一个指针，指向下一个可用的内存地址，每次分配内存时，只需要将指针向后移动一段距离，如果没有发生碰撞，就将这段内存分配给对象实例。 什么是空闲列表？JVM 维护一个列表，记录堆中所有未占用的内存块，每个内存块都记录有大小和地址信息。 当有新的对象请求内存时，JVM 会遍历空闲列表，寻找足够大的空间来存放新对象。 分配后，如果选中的内存块未被完全利用，剩余的部分会作为一个新的内存块加入到空闲列表中。 8.new 对象时，堆会发生抢占吗？会。 Baeldung：堆抢占 new 对象时，指针会向右移动一个对象大小的距离，假如一个线程 A 正在给字符串对象 s 分配内存，另外一个线程 B 同时为 ArrayList 对象 l 分配内存，两个线程就发生了抢占。 JVM 怎么解决堆内存分配的竞争问题？为了解决堆内存分配的竞争问题，JVM 为每个线程保留了一小块内存空间，被称为 TLAB，也就是线程本地分配缓冲区，用于存放该线程分配的对象。 Baeldung：TLAB 当线程需要分配对象时，直接从 TLAB 中分配。只有当 TLAB 用尽或对象太大需要直接在堆中分配时，才会使用全局分配指针。 这里简单测试一下 TLAB。 可以通过 java -XX:+PrintFlagsFinal -version | grep TLAB 命令查看当前 JVM 是否开启了 TLAB。 二哥的 Java 进阶之路：查看 TLAB 如果开启了 TLAB，会看到类似以下的输出，其中 bool UseTLAB 的值为 true。 我们编写一个简单的测试类，创建大量对象并强制触发垃圾回收，查看 TLAB 的使用情况。 class TLABDemo public static void main(String[] args) for (int i = 0; i 10_000_000; i++) allocate(); // 创建大量对象 System.gc(); // 强制触发垃圾回收 private static void allocate() // 小对象分配，通常会使用 TLAB byte[] bytes = new byte[64]; 在 VM 参数中添加 -XX:+UseTLAB -XX:+PrintTLAB -XX:+PrintGCDetails -XX:+PrintGCDateStamps，运行后可以看到这样的内容： 二哥的 Java 进阶之路：测试 TLAB waste：未使用的 TLAB 空间。 alloc：分配到 TLAB 的空间。 refills：TLAB 被重新填充的次数。 可以看到，当前线程的 TLAB 目标大小为 10,496 KB（desired_size: 10496KB）；未发生慢分配（slow allocs: 0）；分配效率直接拉满（alloc: 1.00000 52494KB）。 当使用 -XX:-UseTLAB -XX:+PrintGCDetails 关闭 TLAB 时，会看到类似以下的输出： 二哥的 Java 进阶之路：关闭 TLAB 直接出现了两次 GC，因为没有 TLAB，Eden 区更快被填满，导致年轻代 GC。年轻代 GC 频繁触发，一部分长生命周期对象被晋升到老年代，间接导致老年代 GC 触发。 9.能说一下对象的内存布局吗？好的。 对象的内存布局是由 Java 虚拟机规范定义的，但具体的实现细节各有不同，如 HotSpot 和 OpenJ9 就不一样。 就拿我们常用的 HotSpot 来说吧。 对象在内存中包括三部分：对象头、实例数据和对齐填充。 三分恶面渣逆袭：对象的存储布局 说说对象头的作用？对象头是对象存储在内存中的元信息，包含了Mark Word、类型指针等信息。 Mark Word 存储了对象的运行时状态信息，包括锁、哈希值、GC 标记等。在 64 位操作系统下占 8 个字节，32 位操作系统下占 4 个字节。 类型指针指向对象所属类的元数据，也就是 Class 对象，用来支持多态、方法调用等功能。 除此之外，如果对象是数组类型，还会有一个额外的数组长度字段。占 4 个字节。 类型指针会被压缩吗？类型指针可能会被压缩，以节省内存空间。比如说在开启压缩指针的情况下占 4 个字节，否则占 8 个字节。在 JDK 8 中，压缩指针默认是开启的。 可以通过 java -XX:+PrintFlagsFinal -version | grep UseCompressedOops 命令来查看 JVM 是否开启了压缩指针。 二哥的 Java 进阶之路：查看 JVM 是否开启压缩指针 如果压缩指针开启，输出结果中的 bool UseCompressedOops 值为 true。 实例数据了解吗？了解一些。 实例数据是对象实际的字段值，也就是成员变量的值，按照字段在类中声明的顺序存储。 class ObjectDemo int age; String name; JVM 会对这些数据进行对齐重排，以提高内存访问速度。 对齐填充了解吗？由于 JVM 的内存模型要求对象的起始地址是 8 字节对齐（64 位 JVM 中），因此对象的总大小必须是 8 字节的倍数。 如果对象头和实例数据的总长度不是 8 的倍数，JVM 会通过填充额外的字节来对齐。 比如说，如果对象头 + 实例数据 14 字节，则需要填充 2 个字节，使总长度变为 16 字节。 为什么非要进行 8 字节对齐呢？因为 CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 缓存行的大小。如果不进行内存对齐，则可能出现跨缓存行访问，导致额外的缓存行加载，CPU 的访问效率就会降低。 rickiyang：缓存行污染 比如说上图中 obj1 占 6 个字节，由于没有对齐，导致这一行缓存中多了 2 个字节 obj2 的数据，当 CPU 访问 obj2 的时候，就会导致缓存行刷新。 也就说，8 字节对齐，是为了效率的提高，以空间换时间的一种方案。 rickiyang：000 结尾 new Object() 对象的内存大小是多少？推荐阅读：高端面试必备：一个 Java 对象占用多大内存 一般来说，目前的操作系统都是 64 位的，并且 JDK 8 中的压缩指针是默认开启的，因此在 64 位的 JVM 上，new Object()的大小是 16 字节（12 字节的对象头 + 4 字节的对齐填充）。 rickiyang：Java 对象模型 对象头的大小是固定的，在 32 位 JVM 上是 8 字节，在 64 位 JVM 上是 16 字节；如果开启了压缩指针，就是 12 字节。 实例数据的大小取决于对象的成员变量和它们的类型。对于new Object()来说，由于默认没有成员变量，因此我们可以认为此时的实例数据大小是 0。 假如 MyObject 对象有三个成员变量，分别是 int、long 和 byte 类型，那么它们占用的内存大小分别是 4 字节、8 字节和 1 字节。 class MyObject int a; // 4 字节 long b; // 8 字节 byte c; // 1 字节 考虑到对齐填充，MyObject 对象的总大小为 12（对象头） + 4（a） + 8（b） + 1（c） + 7（填充） 32 字节。 用过 JOL 查看对象的内存布局吗？用过。 JOL 是一款分析 JVM 对象布局的工具。 第一步，在 pom.xml 中引入 JOL 依赖： dependency groupIdorg.openjdk.jol/groupId artifactIdjol-core/artifactId version0.9/version/dependency 第二步，使用 JOL 编写代码示例： public class JOLSample public static void main(String[] args) // 打印JVM详细信息（可选） System.out.println(VM.current().details()); // 创建Object实例 Object obj = new Object(); // 打印Object实例的内存布局 String layout = ClassLayout.parseInstance(obj).toPrintable(); System.out.println(layout); 第三步，运行代码，查看输出结果： 二哥的 Java 进阶之路：JOL 运行结果 可以看到有 OFFSET、SIZE、TYPE DESCRIPTION、VALUE 这几个信息。 OFFSET：偏移地址，单位字节； SIZE：占用的内存大小，单位字节； TYPE DESCRIPTION：类型描述，其中 object header 为对象头； VALUE：对应内存中当前存储的值，二进制 32 位； 从上面的结果能看到，对象头是 12 个字节，还有 4 个字节的 padding，new Object() 一共 16 个字节。 对象的引用大小了解吗？推荐阅读：Object o new Object()占多少个字节？ 在 64 位 JVM 上，未开启压缩指针时，对象引用占用 8 字节；开启压缩指针时，对象引用会被压缩到 4 字节。HotSpot 虚拟机默认是开启压缩指针的。 dijia478：对象头 我们来验证一下： class ReferenceSizeExample private static class ReferenceHolder Object reference; public static void main(String[] args) System.out.println(VM.current().details()); System.out.println(ClassLayout.parseClass(ReferenceHolder.class).toPrintable()); 运行代码，查看输出结果： 二哥的 Java 进阶之路：对象的引用有多大？ ReferenceHolder.reference 的大小为 4 字节。 10.JVM 怎么访问对象的？主流的方式有两种：句柄和直接指针。 两种方式的区别在于，句柄是通过一个中间的句柄表来定位对象的，而直接指针则是通过引用直接指向对象的内存地址。 优点是，对象被移动时只需要修改句柄表中的指针，而不需要修改对象引用本身。 三分恶面渣逆袭：通过句柄访问对象 在直接指针访问中，引用直接存储对象的内存地址；对象的实例数据和类型信息都存储在堆中固定的内存区域。 优点是访问速度更快，因为少了一次句柄的寻址操作。缺点是如果对象在内存中移动，引用需要更新为新的地址。 三分恶面渣逆袭：通过直接指针访问对象 HotSpot 虚拟机主要使用直接指针来进行对象访问。 11.说一下对象有哪几种引用？四种，分别是强引用、软引用、弱引用和虚引用。 三分恶面渣逆袭：四种引用总结 强引用是 Java 中最常见的引用类型。使用 new 关键字赋值的引用就是强引用，只要强引用关联着对象，垃圾收集器就不会回收这部分对象，即使内存不足。 // str 就是一个强引用String str = new String(沉默王二); 软引用于描述一些非必须对象，通过 SoftReference 类实现。软引用的对象在内存不足时会被回收。 // softRef 就是一个软引用SoftReferenceString softRef = new SoftReference(new String(沉默王二)); 弱引用用于描述一些短生命周期的非必须对象，如 ThreadLocal 中的 Entry，就是通过 WeakReference 类实现的。弱引用的对象会在下一次垃圾回收时会被回收，不论内存是否充足。 static class Entry extends WeakReferenceThreadLocal? /** The value associated with this ThreadLocal. */ Object value; //节点类 Entry(ThreadLocal? k, Object v) //key赋值 super(k); //value赋值 value = v; 虚引用主要用来跟踪对象被垃圾回收的过程，通过 PhantomReference 类实现。虚引用的对象在任何时候都可能被回收。 // phantomRef 就是一个虚引用PhantomReferenceString phantomRef = new PhantomReference(new String(沉默王二), new ReferenceQueue()); 12.Java 堆的内存分区了解吗？了解。Java 堆被划分为新生代和老年代两个区域。 三分恶面渣逆袭：Java堆内存划分 新生代又被划分为 Eden 空间和两个 Survivor 空间（From 和 To）。 新创建的对象会被分配到 Eden 空间。当 Eden 区填满时，会触发一次 Minor GC，清除不再使用的对象。存活下来的对象会从 Eden 区移动到 Survivor 区。 对象在新生代中经历多次 GC 后，如果仍然存活，会被移动到老年代。当老年代内存不足时，会触发 Major GC，对整个堆进行垃圾回收。 13.说一下新生代的区域划分？新生代的垃圾收集主要采用标记-复制算法，因为新生代的存活对象比较少，每次复制少量的存活对象效率比较高。 基于这种算法，虚拟机将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次分配内存只使用 Eden 和其中一块 Survivor。发生垃圾收集时，将 Eden 和 Survivor 中仍然存活的对象一次性复制到另外一块 Survivor 空间上，然后直接清理掉 Eden 和已用过的那块 Survivor 空间。默认 Eden 和 Survivor 的大小比例是 8∶1。 三分恶面渣逆袭：新生代内存划分 14.🌟对象什么时候会进入老年代？对象通常会在年轻代中分配，随着时间的推移和垃圾收集的进程，某些满足条件的对象会进入到老年代中，如长期存活的对象。 二哥的 Java 进阶之路：对象进入老年代 长期存活的对象如何判断？JVM 会为对象维护一个“年龄”计数器，记录对象在新生代中经历 Minor GC 的次数。每次 GC 未被回收的对象，其年龄会加 1。 当超过一个特定阈值，默认值是 15，就会被认为老对象了，需要重点关照。这个年龄阈值可以通过 JVM 参数-XX:MaxTenuringThreshold来设置。 可以通过 jinfo -flag MaxTenuringThreshold $(jps | grep -i nacos | awk print $1) 来查看当前 JVM 的年龄阈值。 二哥的 Java 进阶之路：年龄阈值 如果应用中的对象存活时间较短，可以适当调大这个值，让对象在新生代多待一会儿 如果对象存活时间较长，可以适当调小这个值，让对象更快进入老年代，减少在新生代的复制次数 大对象如何判断？大对象是指占用内存较大的对象，如大数组、长字符串等。 int[] array = new int[1000000];String str = new String(new char[1000000]); 其大小由 JVM 参数 -XX:PretenureSizeThreshold 控制，但在 JDK 8 中，默认值为 0，也就是说默认情况下，对象仅根据 GC 存活的次数来判断是否进入老年代。 二哥的 Java 进阶之路：PretenureSizeThreshold G1 垃圾收集器中，大对象会直接分配到 HUMONGOUS 区域。当对象大小超过一个 Region 容量的 50% 时，会被认为是大对象。 有梦想的肥宅：G1 Region 的大小可以通过 JVM 参数 -XX:G1HeapRegionSize 来设置，默认情况下从 1MB 到 32MB 不等，会根据堆内存大小动态调整。 可以通过 java -XX:+UseG1GC -XX:+PrintGCDetails -version 查看 G1 垃圾收集器的相关信息。 二哥的 Java 进阶之路：UseG1GC 从结果上来看，我本机上 G1 的堆大小为 2GB，Region 的大小为 4MB。 动态年龄判定了解吗？如果 Survivor 区中所有对象的总大小超过了一定比例，通常是 Survivor 区的一半，那么年龄较小的对象也可能会被提前晋升到老年代。 这是因为如果年龄较小的对象在 Survivor 区中占用了较大的空间，会导致 Survivor 区中的对象复制次数增多，影响垃圾回收的效率。 15.STW 了解吗？了解。 JVM 进行垃圾回收的过程中，会涉及到对象的移动，为了保证对象引用在移动过程中不被修改，必须暂停所有的用户线程，像这样的停顿，我们称之为Stop The World。简称 STW。 如何暂停线程呢？JVM 会使用一个名为安全点（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤： JVM 发出暂停信号； 线程执行到安全点后，挂起自身并等待垃圾收集完成； 垃圾回收器完成 GC 操作； 线程恢复执行。 什么是安全点？安全点是 JVM 的一种机制，常用于垃圾回收的 STW 操作，用于让线程在执行到某些特定位置时，可以被安全地暂停。 通常位于方法调用、循环跳转、异常处理等位置，以保证线程暂停时数据的一致性。 用个通俗的比喻，老王去拉车，车上的东西很重，老王累的汗流浃背，但是老王不能在上坡或者下坡时休息，只能在平地上停下来擦擦汗，喝口水。 三分恶面渣逆袭：老王拉车只能在平路休息 推荐大家看看这个HotSpot JVM Deep Dive - Safepoint，对 safe point 有一个比较深入地解释。 16.对象一定分配在堆中吗？不一定。 默认情况下，Java 对象是在堆中分配的，但 JVM 会进行逃逸分析，来判断对象的生命周期是否只在方法内部，如果是的话，这个对象可以在栈上分配。 举例来说，下面的代码中，对象 new Person() 的生命周期只在 testStackAllocation 方法内部，因此 JVM 会将这个对象分配在栈上。 public void testStackAllocation() Person p = new Person(); // 对象可能分配在栈上 p.name = 沉默王二是只狗; p.age = 18; System.out.println(p.name); 什么是逃逸分析？逃逸分析是一种 JVM 优化技术，用来分析对象的作用域和生命周期，判断对象是否逃逸出方法或线程。 可以通过分析对象的引用流向，判断对象是否被方法返回、赋值到全局变量、传递到其他线程等，来确定对象是否逃逸。 如果对象没有逃逸，就可以进行栈上分配、同步消除、标量替换等优化，以提高程序的性能。 可以通过 java -XX:+PrintFlagsFinal -version | grep DoEscapeAnalysis 来确认 JVM 是否开启了逃逸分析。 二哥的 Java 进阶之路：JVM 开启了逃逸分析 逃逸具体是指什么？根据对象逃逸的范围，可以分为方法逃逸和线程逃逸。 当对象被方法外部的代码引用，生命周期超出了方法的范围，那么对象就必须分配在堆中，由垃圾收集器管理。 public Person createPerson() return new Person(); // 对象逃逸出方法 比如说 new Person() 创建的对象被返回，那么这个对象就逃逸出当前方法了。 三分恶面渣逆袭：方法逃逸 再比如说，对象被另外一个线程引用，生命周期超出了当前线程，那么对象就必须分配在堆中，并且线程之间需要同步。 public void threadEscapeExample() Person p = new Person(); // 对象逃逸到另一个线程 new Thread(() - System.out.println(p); ).start(); 对象 new Person() 被另外一个线程引用了，发生了线程逃逸。 逃逸分析会带来什么好处？主要有三个。 第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。 第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。 第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为标量变量，避免对象分配。 public void scalarReplacementExample() Point p = new Point(1, 2); System.out.println(p.getX() + p.getY()); 如果 Point 对象未逃逸，JVM 可以优化为： int x = 1;int y = 2;System.out.println(x + y); 17.内存溢出和内存泄漏了解吗？内存溢出，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError。 ListString list = new ArrayList();while (true) list.add(OutOfMemory.repeat(1000)); // 无限增加内存 可能是因为堆、元空间、栈或直接内存不足导致的。可以通过优化内存配置、减少对象分配来解决。 内存泄漏是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。 内存泄漏通常是因为长期存活的对象持有短期存活对象的引用，又没有及时释放，从而导致短期存活对象无法被回收而导致的。 class MemoryLeakExample private static ListObject staticList = new ArrayList(); public void addObject() staticList.add(new Object()); // 对象不会被回收 用一个比较有味道的比喻来形容就是，内存溢出是排队去蹲坑，发现没坑了；内存泄漏，就是有人占着茅坑不拉屎，导致坑位不够用。 三分恶面渣逆袭：内存泄漏、内存溢出 18.能手写内存溢出的例子吗？可以。 我就拿最常见的堆内存溢出来完成吧，堆内存溢出通常是因为创建了大量的对象，且长时间无法被垃圾收集器回收，导致的。 class HeapSpaceErrorGenerator public static void main(String[] args) // 第一步，创建一个大的容器 Listbyte[] bigObjects = new ArrayList(); try // 第二步，循环写入数据 while (true) // 第三步，创建一个大对象，一个大约 10M 的数组 byte[] bigObject = new byte[10 * 1024 * 1024]; // 第四步，将大对象添加到容器中 bigObjects.add(bigObject); catch (OutOfMemoryError e) System.out.println(OutOfMemoryError 发生在 + bigObjects.size() + 对象后); throw e; 很快就会发生内存溢出。 这就相当于一个房子里，不断堆积不能被回收的杂物，那么房子很快就会被堆满了。 也可以通过 VM 参数设置堆内存大小为 -Xmx128M，然后运行程序，出现的内存溢出的时间会更快。 二哥的 Java 进阶之路：添加 -Xmx128M VM 参数 可以看到，堆内存溢出发生在 11 个对象后。 二哥的 Java 进阶之路：堆内存溢出 19.内存泄漏可能由哪些原因导致呢？比如说： ①、静态的集合中添加的对象越来越多，但却没有及时清理；静态变量的生命周期与应用程序相同，如果静态变量持有对象的引用，这些对象将无法被 GC 回收。 class OOM static List list = new ArrayList(); public void oomTests() Object obj = new Object(); list.add(obj); ②、单例模式下对象持有的外部引用无法及时释放；单例对象在整个应用程序的生命周期中存活，如果单例对象持有其他对象的引用，这些对象将无法被回收。 class Singleton private static final Singleton INSTANCE = new Singleton(); private ListObject objects = new ArrayList(); public static Singleton getInstance() return INSTANCE; ③、数据库、IO、Socket 等连接资源没有及时关闭； try Connection conn = null; Class.forName(com.mysql.jdbc.Driver); conn = DriverManager.getConnection(url, , ); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(....); catch (Exception e) finally //不关闭连接 ④、 ThreadLocal 的引用未被清理，线程退出后仍然持有对象引用；在线程执行完后，要调用 ThreadLocal 的 remove 方法进行清理。 ThreadLocalObject threadLocal = new ThreadLocal();threadLocal.set(new Object()); // 未清理 20.有没有处理过内存泄漏问题？推荐阅读： 一次内存溢出的排查优化实战 JVM 性能监控工具之命令行篇 JVM 性能监控工具之可视化篇 有。 当时在做技术派项目的时候，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题。 我用可视化的监控工具 VisualVM，配合 JDK 自带的 jstack 等命令行工具进行了排查。 大致的过程我回想了一下，主要有 7 个步骤： 第一步，使用 jps -l 查看运行的 Java 进程 ID。 二哥的 Java 进阶之路：jps 查看技术派的进程 ID 第二步，使用top -p [pid] 查看进程使用 CPU 和内存占用情况。 二哥的 Java 进阶之路：top -p 第三步，使用 top -Hp [pid] 查看进程下的所有线程占用 CPU 和内存情况。 二哥的 Java 进阶之路：top -Hp 第四步，抓取线程栈：jstack -F 29452 29452.txt，可以多抓几次做个对比。 29452 为 pid，顺带作为文件名。 二哥的 Java 进阶之路：jstack 看看有没有线程死锁、死循环或长时间等待这些问题。 二哥的 Java 进阶之路：另外一组线程 id 的堆栈 第五步，可以使用jstat -gcutil [pid] 5000 10 每隔 5 秒输出 GC 信息，输出 10 次，查看 YGC 和 Full GC 次数。 二哥的 Java 进阶之路：jstat 通常会出现 YGC 不增加或增加缓慢，而 Full GC 增加很快。 或使用 jstat -gccause [pid] 5000 输出 GC 摘要信息。 二哥的 Java 进阶之路：jstat 或使用 jmap -heap [pid] 查看堆的摘要信息，关注老年代内存使用是否达到阀值，若达到阀值就会执行 Full GC。 二哥的 Java 进阶之路：jmap 如果发现 Full GC 次数太多，就很大概率存在内存泄漏了。 第六步，生成 dump 文件，然后借助可视化工具分析哪个对象非常多，基本就能定位到问题根源了。 执行命令 jmap -dump:format=b,file=heap.hprof 10025 会输出进程 10025 的堆快照信息，保存到文件 heap.hprof 中。 二哥的 Java 进阶之路：jmap 第七步，使用图形化工具分析，如 JDK 自带的 VisualVM，从菜单 文件 装入 dump 文件。 VisualVM 然后在结果观察内存占用最多的对象，找到内存泄漏的源头。 21.有没有处理过内存溢出问题？有。 当时在做技术派的时候，由于上传的文件过大，没有正确处理，导致一下子撑爆了内存，程序直接崩溃了。 我记得是通过导出堆转储文件进行分析发现的。 第一步，使用 jmap 命令手动生成 Heap Dump 文件： jmap -dump:format=b,file=heap.hprof pid 然后使用 MAT、JProfiler 等工具进行分析，查看内存中的对象占用情况。 一般来说： 如果生产环境的内存还有很多空余，可以适当增大堆内存大小来解决，例如 -Xmx4g 参数。 或者检查代码中是否存在内存泄漏，如未关闭的资源、长生命周期的对象等。 之后，在本地进行压力测试，模拟高负载情况下的内存表现，确保修改有效，且没有引入新的问题。 22.什么情况下会发生栈溢出？（补充）栈溢出发生在程序调用栈的深度超过 JVM 允许的最大深度时。 栈溢出的本质是因为线程的栈空间不足，导致无法再为新的栈帧分配内存。 二哥的Java进阶之路：栈帧 当一个方法被调用时，JVM 会在栈中分配一个栈帧，用于存储该方法的执行信息。如果方法调用嵌套太深，栈帧不断压入栈中，最终会导致栈空间耗尽，抛出 StackOverflowError。 最常见的栈溢出场景就是递归调用，尤其是没有正确的终止条件下，会导致递归无限进行。 class StackOverflowExample public static void recursiveMethod() // 没有终止条件的递归调用 recursiveMethod(); public static void main(String[] args) recursiveMethod(); // 导致栈溢出 另外，如果方法中定义了特别大的局部变量，栈帧会变得很大，导致栈空间更容易耗尽。 public class LargeLocalVariables public static void method() int[] largeArray = new int[1000000]; // 大量局部变量 method(); // 递归调用 public static void main(String[] args) method(); // 导致栈溢出 三、垃圾收集23.🌟讲讲 JVM 的垃圾回收机制（补充） 本题是增补的内容 参照：深入理解 JVM 的垃圾回收机制 垃圾回收就是对内存堆中已经死亡的或者长时间没有使用的对象进行清除或回收。 JVM 在做 GC 之前，会先搞清楚什么是垃圾，什么不是垃圾，通常会通过可达性分析算法来判断对象是否存活。 二哥的 Java 进阶之路：可达性分析 在确定了哪些垃圾可以被回收后，垃圾收集器（如 CMS、G1、ZGC）要做的事情就是进行垃圾回收，可以采用标记清除算法、复制算法、标记整理算法、分代收集算法等。 技术派项目使用的 JDK 8，采用的是 CMS 垃圾收集器。 java -XX:+UseConcMarkSweepGC \\ -XX:+UseParNewGC \\ -XX:CMSInitiatingOccupancyFraction=75 \\ -XX:+UseCMSInitiatingOccupancyOnly \\ -jar your-application.jar 垃圾回收的过程是什么？Java 的垃圾回收过程主要分为标记存活对象、清除无用对象、以及内存压缩整理三个阶段。不同的垃圾回收器在执行这些步骤时会采用不同的策略和算法。 24.🌟如何判断对象仍然存活？Java 通过可达性分析算法来判断一个对象是否还存活。 通过一组名为 “GC Roots” 的根对象，进行递归扫描，无法从根对象到达的对象就是“垃圾”，可以被回收。 三分恶面渣逆袭：GC Root 这也是 G1、CMS 等主流垃圾收集器使用的主要算法。 什么是引用计数法？每个对象有一个引用计数器，记录引用它的次数。当计数器为零时，对象可以被回收。 三分恶面渣逆袭：引用计数法 引用计数法无法解决循环引用的问题。例如，两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收。 做可达性分析的时候，应该有哪些前置性的操作？在进行垃圾回收之前，JVM 会暂停所有正在执行的应用线程。 这是因为可达性分析过程必须确保在执行分析时，内存中的对象关系不会被应用线程修改。如果不暂停应用线程，可能会出现对象引用的改变，导致垃圾回收过程中判断对象是否可达的结果不一致，从而引发严重的内存错误或数据丢失。 25.Java 中可作为 GC Roots 的引用有哪几种？ 推荐阅读：深入理解垃圾回收机制 推荐阅读：R 大的所谓“GC roots” 所谓的 GC Roots，就是一组必须活跃的引用，它们是程序运行时的起点，是一切引用链的源头。在 Java 中，GC Roots 包括以下几种： 虚拟机栈中的引用（方法的参数、局部变量等） 本地方法栈中 JNI 的引用 类静态变量 运行时常量池中的常量（String 或 Class 类型） 二哥的 java 进阶之路：GC Roots 说说虚拟机栈中的引用？来看下面这段代码： public class StackReference public void greet() Object localVar = new Object(); // 这里的 localVar 是一个局部变量，存在于虚拟机栈中 System.out.println(localVar.toString()); public static void main(String[] args) new StackReference().greet(); 在 greet 方法中，localVar 是一个局部变量，存在于虚拟机栈中，可以被认为是 GC Roots。 在 greet 方法执行期间，localVar 引用的对象是活跃的，因为它是从 GC Roots 可达的。 当 greet 方法执行完毕后，localVar 的作用域结束，localVar 引用的 Object 对象不再由任何 GC Roots 引用（假设没有其他引用指向这个对象），因此它将有资格作为垃圾被回收掉 😁。 说说本地方法栈中 JNI 的引用？Java 通过 JNI 提供了一种机制，允许 Java 代码调用本地代码（通常是 C 或 C++ 编写的代码）。 当调用 Java 方法时，虚拟机会创建一个栈帧并压入虚拟机栈，而当它调用本地方法时，虚拟机会通过动态链接直接调用指定的本地方法。 pecuyu：动态链接 JNI 引用是在 Java 本地接口代码中创建的引用，这些引用可以指向 Java 堆中的对象。 // 假设的JNI方法public native void nativeMethod();// 假设在C/C++中实现的本地方法/* * Class: NativeExample * Method: nativeMethod * Signature: ()V */JNIEXPORT void JNICALL Java_NativeExample_nativeMethod(JNIEnv *env, jobject thisObj) jobject localRef = (*env)-NewObject(env, ...); // 在本地方法栈中创建JNI引用 // localRef 引用的Java对象在本地方法执行期间是活跃的 在本地代码中，localRef 是对 Java 对象的一个 JNI 引用，它在本地方法执行期间保持 Java 对象活跃，可以被认为是 GC Roots。 一旦 JNI 方法执行完毕，除非这个引用是全局的，否则它指向的对象将会被作为垃圾回收掉（假设没有其他地方再引用这个对象）。 说说类静态变量？来看下面这段代码： public class StaticFieldReference private static Object staticVar = new Object(); // 类静态变量 public static void main(String[] args) System.out.println(staticVar.toString()); StaticFieldReference 类中的 staticVar 引用了一个 Object 对象，这个引用存储在元空间，可以被认为是 GC Roots。 只要 StaticFieldReference 类未被卸载，staticVar 引用的对象都不会被垃圾回收。如果 StaticFieldReference 类被卸载（这通常发生在其类加载器被垃圾回收时），那么 staticVar 引用的对象也将有资格被垃圾回收（如果没有其他引用指向这个对象）。 说说运行时常量池中的常量？来看这段代码： class ConstantPoolReference public static final String CONSTANT_STRING = Hello, World; // 常量，存在于运行时常量池中 public static final Class? CONSTANT_CLASS = Object.class; // 类类型常量 public static void main(String[] args) System.out.println(CONSTANT_STRING); System.out.println(CONSTANT_CLASS.getName()); 在 ConstantPoolReference 中，CONSTANT_STRING 和 CONSTANT_CLASS 作为常量存储在运行时常量池。它们可以用来作为 GC Roots。 这些常量引用的对象（字符串”Hello, World”和 Object.class 类对象）在常量池中，只要包含这些常量的 ConstantPoolReference 类未被卸载，这些对象就不会被垃圾回收。 26.finalize()方法了解吗？垃圾回收就是古代的秋后问斩，finalize() 就是刀下留人，在人犯被处决之前，还要做最后一次审计，青天大老爷会看看有没有什么冤情，需不需要刀下留人。 三分恶面渣逆袭：刀下留人 如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记，随后进行一次筛选。 筛选的条件是对象是否有必要执行 finalize()方法。 如果对象在 finalize() 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。 譬如把自己 （this 关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它就”逃过一劫“；但是如果没有抓住这个机会，那么对象就真的要被回收了。 27.🌟垃圾收集算法了解吗？垃圾收集算法主要有三种，分别是标记-清除算法、标记-复制算法和标记-整理算法。 说说标记-清除算法？标记-清除算法分为两个阶段： 标记：标记所有需要回收的对象 清除：回收所有被标记的对象 三分恶面渣逆袭：标记-清除算法 优点是实现简单，缺点是回收过程中会产生内存碎片。 说说标记-复制算法？标记-复制算法可以解决标记-清除算法的内存碎片问题，因为它将内存空间划分为两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后清理掉这一块。 三分恶面渣逆袭：标记-复制算法 缺点是浪费了一半的内存空间。 说说标记-整理算法？标记-整理算法是标记-清除复制算法的升级版，它不再划分内存空间，而是将存活的对象向内存的一端移动，然后清理边界以外的内存。 标记-整理算法 缺点是移动对象的成本比较高。 说说分代收集算法？分代收集算法是目前主流的垃圾收集算法，它根据对象存活周期的不同将内存划分为几块，一般分为新生代和老年代。 二哥的 Java 进阶之路：Java 堆划分 新生代用复制算法，因为大部分对象生命周期短。老年代用标记-整理算法，因为对象存活率较高。 为什么要用分代收集呢？分代收集算法的核心思想是根据对象的生命周期优化垃圾回收。 新生代的对象生命周期短，使用复制算法可以快速回收。老年代的对象生命周期长，使用标记-整理算法可以减少移动对象的成本。 标记复制的标记过程和复制过程会不会停顿？在标记-复制算法 中，标记阶段和复制阶段都会触发STW。 标记阶段停顿是为了保证对象的引用关系不被修改。 复制阶段停顿是防止对象在复制过程中被修改。 28.Minor GC、Major GC、Mixed GC、Full GC 都是什么意思？Minor GC 也称为 Young GC，是指发生在年轻代的垃圾收集。年轻代包含 Eden 区以及两个 Survivor 区。 二哥的 Java 进阶之路：Java 堆划分 Major GC 也称为 Old GC，主要指的是发生在老年代的垃圾收集。是 CMS 的特有行为。 Mixed GC 是 G1 垃圾收集器特有的一种 GC 类型，它在一次 GC 中同时清理年轻代和部分老年代。 Full GC 是最彻底的垃圾收集，涉及整个 Java 堆和方法区。它是最耗时的 GC，通常在 JVM 压力很大时发生。 FULL gc怎么去清理的？Full GC 会从 GC Root 出发，标记所有可达对象。新生代使用复制算法，清空 Eden 区。老年代使用标记-整理算法，回收对象并消除碎片。 停顿时间较长，会影响系统响应性能。 29.Young GC 什么时候触发？如果 Eden 区没有足够的空间时，就会触发 Young GC 来清理新生代。 30.什么时候会触发 Full GC？在进行 Young GC 的时候，如果发现老年代可用的连续内存空间 新生代历次 Young GC 后升入老年代的对象总和的平均大小，说明本次 Young GC 后升入老年代的对象大小，可能超过了老年代当前可用的内存空间，就会触发 Full GC。 执行 Young GC 后老年代没有足够的内存空间存放转入的对象，会立即触发一次 Full GC。 System.gc()、jmap -dump 等命令会触发 full gc。 空间分配担保是什么？空间分配担保是指在进行 Minor GC 前，JVM 会确保老年代有足够的空间存放从新生代晋升的对象。如果老年代空间不足，可能会触发 Full GC。 31.🌟知道哪些垃圾收集器？推荐阅读：深入理解 JVM 的垃圾收集器：CMS、G1、ZGC JVM 的垃圾收集器主要分为两大类：分代收集器和分区收集器，分代收集器的代表是 CMS，分区收集器的代表是 G1 和 ZGC。 三分恶面渣逆袭：HotSpot虚拟机垃圾收集器 CMS 是第一个关注 GC 停顿时间的垃圾收集器，JDK 1.5 时引入，JDK9 被标记弃用，JDK14 被移除。 G1 在 JDK 1.7 时引入，在 JDK 9 时取代 CMS 成为了默认的垃圾收集器。 ZGC 是 JDK11 推出的一款低延迟垃圾收集器，适用于大内存低延迟服务的内存管理和回收，在 128G 的大堆下，最大停顿时间才 1.68 ms，性能远胜于 G1 和 CMS。 说说 Serial 收集器？Serial 收集器是最基础、历史最悠久的收集器。 如同它的名字（串行），它是一个单线程工作的收集器，使用一个处理器或一条收集线程去完成垃圾收集工作。并且进行垃圾收集时，必须暂停其他所有工作线程，直到垃圾收集结束——这就是所谓的“Stop The World”。 SerialSerial Old 收集器的运行过程如图： 三分恶面渣逆袭：SerialSerial Old收集器运行示意图 说说 ParNew 收集器？ParNew 收集器实质上是 Serial 收集器的多线程并行版本，使用多条线程进行垃圾收集。 ParNewSerial Old 收集器运行示意图如下： 三分恶面渣逆袭：ParNewSerial Old收集器运行示意图 说说 Parallel Scavenge 收集器？Parallel Scavenge 收集器是一款新生代收集器，基于标记-复制算法实现，也能够并行收集。和 ParNew 有些类似，但 Parallel Scavenge 主要关注的是垃圾收集的吞吐量——所谓吞吐量，就是 CPU 用于运行用户代码的时间和总消耗时间的比值，比值越大，说明垃圾收集的占比越小。 三分恶面渣逆袭：吞吐量 根据对象存活周期的不同会将内存划分为几块，一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 说说 Serial Old 收集器？Serial Old 是 Serial 收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。 说说 Parallel Old 收集器？Parallel Old 是 Parallel Scavenge 收集器的老年代版本，基于标记-整理算法实现，使用多条 GC 线程在 STW 期间同时进行垃圾回收。 三分恶面渣逆袭：Parallel Old收集器 说说 CMS 收集器？CMS 在 JDK 1.5 时引入，JDK 9 时被标记弃用，JDK 14 时被移除。 CMS 是一种低延迟的垃圾收集器，采用标记-清除算法，分为初始标记、并发标记、重新标记和并发清除四个阶段，优点是垃圾回收线程和应用线程同时运行，停顿时间短，适合延迟敏感的应用，但容易产生内存碎片，可能触发 Full GC。 小潘：CMS 说说 G1 收集器？G1 在 JDK 1.7 时引入，在 JDK 9 时取代 CMS 成为默认的垃圾收集器。 G1 是一种面向大内存、高吞吐场景的垃圾收集器，它将堆划分为多个小的 Region，通过标记-整理算法，避免了内存碎片问题。优点是停顿时间可控，适合大堆场景，但调优较复杂。 有梦想的肥宅：G1 说说 ZGC 收集器？ZGC 是 JDK 11 时引入的一款低延迟的垃圾收集器，最大特点是将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间。 它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖指针染色来管理对象状态。 得物技术：指针染色 标记对象的可达性：通过在指针上增加标记位，不需要额外的标记位即可判断对象的存活状态。 重定位状态：在对象被移动时，可以通过指针染色来更新对象的引用，而不需要等待全局同步。 适用于需要超低延迟的场景，比如金融交易系统、电商平台。 垃圾回收器的作用是什么？垃圾回收器的核心作用是自动管理 Java 应用程序的运行时内存。它负责识别哪些内存是不再被应用程序使用的，并释放这些内存以便重新使用。 这一过程减少了程序员手动管理内存的负担，降低了内存泄漏和溢出错误的风险。 32.🌟能详细说一下 CMS 的垃圾收集过程吗？三分恶面渣逆袭：Concurrent Mark Sweep收集器运行示意图 CMS 使用标记-清除算法进行垃圾收集，分 4 大步： 初始标记：标记所有从 GC Roots 直接可达的对象，这个阶段需要 STW，但速度很快。 并发标记：从初始标记的对象出发，遍历所有对象，标记所有可达的对象。这个阶段是并发进行的。 重新标记：完成剩余的标记工作，包括处理并发阶段遗留下来的少量变动，这个阶段通常需要短暂的 STW 停顿。 并发清除：清除未被标记的对象，回收它们占用的内存空间。 你提到了remark，那它remark具体是怎么执行的？三色标记法？是的，remark 阶段通常会结合三色标记法来执行，确保在并发标记期间所有存活对象都被正确标记。目的是修正并发标记阶段中可能遗漏的对象引用变化。 在 remark 阶段，垃圾收集器会停止应用线程，以确保在这个阶段不会有引用关系的进一步变化。这种暂停通常很短暂。remark 阶段主要包括以下操作： 处理写屏障记录的引用变化：在并发标记阶段，应用程序可能会更新对象的引用（比如一个黑色对象新增了对一个白色对象的引用），这些变化通过写屏障记录下来。在 remark 阶段，GC 会处理这些记录，确保所有可达对象都正确地标记为灰色或黑色。 扫描灰色对象：再次遍历灰色对象，处理它们的所有引用，确保引用的对象正确标记为灰色或黑色。 清理：确保所有引用关系正确处理后，灰色对象标记为黑色，白色对象保持不变。这一步完成后，所有存活对象都应当是黑色的。 什么是三色标记法？Java全栈架构师：三色标记法 三色标记法用于标记对象的存活状态，它将对象分为三类： 白色（White）：尚未访问的对象。垃圾回收结束后，仍然为白色的对象会被认为是不可达的对象，可以回收。 灰色（Gray）：已经访问到但未标记完其引用的对象。灰色对象是需要进一步处理的。 黑色（Black）：已经访问到并且其所有引用对象都已经标记过。黑色对象是完全处理过的，不需要再处理。 三色标记法的工作流程： ①、初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为灰色。 ②、并发标记（Concurrent Marking）：在此阶段，标记所有灰色对象引用的对象为灰色，然后将灰色对象自身标记为黑色。这个过程是并发的，和应用线程同时进行。 此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。 ③、重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。 ④、使用写屏障（Write Barrier）来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收。 推荐阅读：小道哥的三色标记 33.🌟G1 垃圾收集器了解吗？G1 在 JDK 1.7 时引入，在 JDK 9 时取代 CMS 成为默认的垃圾收集器。 有梦想的肥宅：G1 收集器 G1 把 Java 堆划分为多个大小相等的独立区域Region，每个区域都可以扮演新生代或老年代的角色。 同时，G1 还有一个专门为大对象设计的 Region，叫 Humongous 区。 大对象的判定规则是，如果一个大对象超过了一个 Region 大小的 50%，比如每个 Region 是 2M，只要一个对象超过了 1M，就会被放入 Humongous 中。 这种区域化管理使得 G1 可以更灵活地进行垃圾收集，只回收部分区域而不是整个新生代或老年代。 G1 收集器的运行过程大致可划分为这几个步骤： ①、并发标记，G1 通过并发标记的方式找出堆中的垃圾对象。并发标记阶段与应用线程同时执行，不会导致应用线程暂停。 ②、混合收集，在并发标记完成后，G1 会计算出哪些区域的回收价值最高（也就是包含最多垃圾的区域），然后优先回收这些区域。这种回收方式包括了部分新生代区域和老年代区域。 选择回收成本低而收益高的区域进行回收，可以提高回收效率和减少停顿时间。 ③、可预测的停顿，G1 在垃圾回收期间仍然需要「Stop the World」。不过，G1 在停顿时间上添加了预测机制，用户可以 JVM 启动时指定期望停顿时间，G1 会尽可能地在这个时间内完成垃圾回收。 三分恶面渣逆袭：G1收集器运行示意图 34.有了 CMS，为什么还要引入 G1？ 特性 CMS G1 设计目标 低停顿时间 可预测的停顿时间 并发性 是 是 内存碎片 是，容易产生碎片 否，通过区域划分和压缩减少碎片 收集代数 年轻代和老年代 整个堆，但区分年轻代和老年代 并发阶段 并发标记、并发清理 并发标记、并发清理、并发回收 停顿时间预测 较难预测 可配置停顿时间目标 容易出现的问题 内存碎片、Concurrent Mode Failure 较少出现长时间停顿 CMS 适用于对延迟敏感的应用场景，主要目标是减少停顿时间，但容易产生内存碎片。 G1 则提供了更好的停顿时间预测和内存压缩能力，适用于大内存和多核处理器环境。 35.你们线上用的什么垃圾收集器？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，因为它不仅能满足低停顿的要求，而且解决了 CMS 的浮动垃圾问题、内存碎片问题。 G1 非常适合大内存、多核处理器的环境。 可以通过以下命令查看当前 JVM 的垃圾收集器： java -XX:+PrintCommandLineFlags -version 二哥的 Java 进阶之路：JDK 默认垃圾收集器 UseParallelGC Parallel Scavenge + Parallel Old，表示新生代用Parallel Scavenge收集器，老年代使用Parallel Old 收集器。 因此你也可以这样回答： 我们系统的业务相对复杂，但并发量并不是特别高，所以我们选择了适用于多核处理器、能够并行处理垃圾回收任务，且能提供高吞吐量的Parallel GC。 但这个说法不讨喜，你也可以回答： 我们系统采用的是 CMS 收集器，能够最大限度减少应用暂停时间。 工作中项目使用的什么垃圾回收算法？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，G1 采用的是分区式标记-整理算法，将堆划分为多个区域，按需回收，适用于大内存和多核环境，能够同时考虑吞吐量和暂停时间。 或者： 我们系统采用的是 CMS 收集器，CMS 采用的是标记-清除算法，能够并发标记和清除垃圾，减少暂停时间，适用于对延迟敏感的应用。 再或者： 我们系统采用的是 Parallel 收集器，Parallel 采用的是年轻代使用复制算法，老年代使用标记-整理算法，适用于高吞吐量要求的应用。 36.垃圾收集器应该如何选择？如果应用程序只需要一个很小的内存空间（大约 100 MB），或者对停顿时间没有特殊的要求，可以选择 Serial 收集器。 如果优先考虑应用程序的峰值性能，并且没有时间要求，或者可以接受 1 秒或更长的停顿时间，可以选择 Parallel 收集器。 如果响应时间比吞吐量优先级高，或者垃圾收集暂停必须保持在大约 1 秒以内，可以选择 CMS G1 收集器。 如果响应时间是高优先级的，或者堆空间比较大，可以选择 ZGC 收集器。 四、JVM 调优37.用过哪些性能监控的命令行工具？操作系统层面，我用过 top、vmstat、iostat、netstat 等命令，可以监控系统整体的资源使用情况，比如说内存、CPU、IO 使用情况、网络使用情况。 JDK 自带的命令行工具层面，我用过 jps、jstat、jinfo、jmap、jhat、jstack、jcmd 等，可以查看 JVM 运行时信息、内存使用情况、堆栈信息等。 你一般都怎么用jmap？①、我一般会使用 jmap -heap pid 查看堆内存摘要，包括新生代、老年代、元空间等。 二哥的Java 进阶之路：jmap -heap ②、或者使用 jmap -histo pid 查看对象分布。 二哥的Java 进阶之路：jmap -histo ③、还有生成堆转储文件：jmap -dump:format=b,file=path pid。 二哥的Java 进阶之路：jmap -dump 38.了解哪些可视化的性能监控工具？我自己用过的可视化工具主要有： ①、JConsole：JDK 自带的监控工具，可以用来监视 Java 应用程序的运行状态，包括内存使用、线程状态、类加载、GC 等。 三分恶面渣逆袭：JConsole概览 ②、VisualVM：一个基于 NetBeans 的可视化工具，在很长一段时间内，VisualVM 都是 Oracle 官方主推的故障处理工具。集成了多个 JDK 命令行工具的功能，非常友好。 三分恶面渣逆袭：VisualVM安装插件 ③、Java Mission Control：JMC 最初是 JRockit VM 中的诊断工具，但在 Oracle JDK7 Update 40 以后，就绑定到了 HotSpot VM 中。不过后来又被 Oracle 开源出来作为了一个单独的产品。 三分恶面渣逆袭：JMC主要界面 用过哪些第三方的工具？①、MAT：一个 Java 堆内存分析工具，主要用于分析和查找 Java 堆中的内存泄漏和内存消耗问题；可以从 Java 堆转储文件中分析内存使用情况，并提供丰富的报告，如内存泄漏疑点、最大对象和 GC 根信息；支持通过图形界面查询对象，以及检查对象间的引用关系。 ②、GChisto：GC 日志分析工具，可以帮助我们优化垃圾收集行为和调整 GC 性能。 ③、JProfiler：一个全功能的商业化 Java 性能分析工具，提供 CPU、 内存和线程的实时分析。 ④、arthas：阿里巴巴开源的 Java 诊断工具，主要用于线上的应用诊断；支持在不停机的情况下进行诊断；可以提供包括 JVM 信息查看、监控、Trace 命令、反编译等功能。 ⑤、async-profiler：一个低开销的性能分析工具，支持生成火焰图，适用于复杂性能问题的分析。 39.JVM 的常见参数配置知道哪些？配置堆内存大小的参数有哪些？ -Xms：初始堆大小 -Xmx：最大堆大小 -XX:NewSize=n：设置年轻代大小 -XX:NewRatio=n：设置年轻代和年老代的比值。如：n 为 3 表示年轻代和年老代比值为 1：3，年轻代占总和的 14 -XX:SurvivorRatio=n：年轻代中 Eden 区与两个 Survivor 区的比值。如 n3 表示 Eden 占 3 Survivor 占 2，一个 Survivor 区占整个年轻代的 15 配置 GC 收集器的参数有哪些？ -XX:+UseSerialGC：设置串行收集器 -XX:+UseParallelGC：设置并行收集器 -XX:+UseParalledlOldGC：设置并行老年代收集器 -XX:+UseConcMarkSweepGC：设置并发收集器 配置并行收集的参数有哪些？ -XX:MaxGCPauseMillis=n：设置最大垃圾回收停顿时间 -XX:GCTimeRatio=n：设置垃圾回收时间占程序运行时间的比例 -XX:+CMSIncrementalMode：设置增量模式，适合单 CPU 环境 -XX:ParallelGCThreads=n：设置并行收集器的线程数 打印 GC 回收的过程日志信息的参数有哪些？ -XX:+PrintGC：输出 GC 日志 -XX:+PrintGCDetails：输出 GC 详细日志 -XX:+PrintGCTimeStamps：输出 GC 的时间戳（以基准时间的形式） -Xloggc:filename：日志文件的输出路径 40.做过 JVM 调优吗？做过。 JVM 调优是一个复杂的过程，调优的对象包括堆内存、垃圾收集器和 JVM 运行时参数等。 二哥的 Java 进阶之路：JVM 调优 如果堆内存设置过小，可能会导致频繁的垃圾回收。所以在技术派实战项目中，启动 JVM 的时候配置了 -Xms 和 -Xmx 参数，让堆内存最大可用内存为 2G（我用的丐版服务器）。 在项目运行期间，我会使用 JVisualVM 定期观察和分析 GC 日志，如果发现频繁的 Full GC，我会特意关注一下老年代的使用情况。 接着，通过分析 Heap dump 寻找内存泄漏的源头，看看是否有未关闭的资源，长生命周期的大对象等。 之后进行代码优化，比如说减少大对象的创建、优化数据结构的使用方式、减少不必要的对象持有等。 41.CPU 占用过高怎么排查？三分恶面渣逆袭：CPU飙高 首先，使用 top 命令查看 CPU 占用情况，找到占用 CPU 较高的进程 ID。 top haikuotiankongdong：top 命令结果 接着，使用 jstack 命令查看对应进程的线程堆栈信息。 jstack -l pid thread-dump.txt 上面 👆🏻 这个命令会将所有线程的堆栈信息输出到 thread-dump.txt 文件中。 然后再使用 top 命令查看进程中线程的占用情况，找到占用 CPU 较高的线程 ID。 top -H -p pid haikuotiankongdong：Java 进程中的线程情况 注意，top 命令显示的线程 ID 是十进制的，而 jstack 输出的是十六进制的，所以需要将线程 ID 转换为十六进制。 printf %x PID 接着在 jstack 的输出中搜索这个十六进制的线程 ID，找到对应的堆栈信息。 Thread-5 #21 prio=5 os_prio=0 tid=0x00007f812c018800 nid=0x1a85 runnable [0x00007f811c000000] java.lang.Thread.State: RUNNABLE at com.example.MyClass.myMethod(MyClass.java:123) at ... 最后，根据堆栈信息定位到具体的业务方法，查看是否有死循环、频繁的垃圾回收、资源竞争导致的上下文频繁切换等问题。 42.内存飙高问题怎么排查？内存飚高一般是因为创建了大量的 Java 对象导致的，如果持续飙高则说明垃圾回收跟不上对象创建的速度，或者内存泄漏导致对象无法回收。 排查的方法主要分为以下几步： 第一，先观察垃圾回收的情况，可以通过 jstat -gc PID 1000 查看 GC 次数和时间。 或者使用 jmap -histo PID | head -20 查看堆内存占用空间最大的前 20 个对象类型。 第二步，通过 jmap 命令 dump 出堆内存信息。 二哥的 Java 进阶之路：dump 第三步，使用可视化工具分析 dump 文件，比如说 VisualVM，找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。 二哥的 Java 进阶之路：分析 43.频繁 minor gc 怎么办？频繁的 Minor GC 通常意味着新生代中的对象频繁地被垃圾回收，可能是因为新生代空间设置的过小，或者是因为程序中存在大量的短生命周期对象（如临时变量）。 可以使用 GC 日志进行分析，查看 GC 的频率和耗时，找到频繁 GC 的原因。 -XX:+PrintGCDetails -Xloggc:gc.log 或者使用监控工具查看堆内存的使用情况，特别是新生代（Eden 和 Survivor 区）的使用情况。 如果是因为新生代空间不足，可以通过 -Xmn 增加新生代的大小，减缓新生代的填满速度。 java -Xmn256m your-app.jar 如果对象需要长期存活，但频繁从 Survivor 区晋升到老年代，可以通过 -XX:SurvivorRatio 参数调整 Eden 和 Survivor 的比例。默认比例是 8:1，表示 8 个空间用于 Eden，1 个空间用于 Survivor 区。 -XX:SurvivorRatio=6 调整为 6 的话，会减少 Eden 区的大小，增加 Survivor 区的大小，以确保对象在 Survivor 区中存活的时间足够长，避免过早晋升到老年代。 44.频繁 Full GC 怎么办？频繁的 Full GC 通常意味着老年代中的对象频繁地被垃圾回收，可能是因为老年代空间设置的过小，或者是因为程序中存在大量的长生命周期对象。 该怎么排查 Full GC 频繁问题？我厂会通过专门的性能监控系统，查看 GC 的频率和堆内存的使用情况，然后根据监控数据分析 GC 的原因。 如果是小厂，可以这么回复。 我一般会使用 JDK 的自带工具，包括 jmap、jstat 等。 # 查看堆内存各区域的使用率以及GC情况jstat -gcutil -h20 pid 1000# 查看堆内存中的存活对象，并按空间排序jmap -histo pid | head -n20# dump堆内存文件jmap -dump:format=b,file=heap pid 或者使用一些可视化的工具，比如 VisualVM、JConsole 等，查看堆内存的使用情况。 假如是因为大对象直接分配到老年代导致的 Full GC 频繁，可以通过 -XX:PretenureSizeThreshold 参数设置大对象直接进入老年代的阈值。 或者将大对象拆分成小对象，减少大对象的创建。比如说分页。 假如是因为内存泄漏导致的频繁 Full GC，可以通过分析堆内存 dump 文件找到内存泄漏的对象，再找到内存泄漏的代码位置。 假如是因为长生命周期的对象进入到了老年代，要及时释放资源，比如说 ThreadLocal、数据库连接、IO 资源等。 假如是因为 GC 参数配置不合理导致的频繁 Full GC，可以通过调整 GC 参数来优化 GC 行为。或者直接更换更适合的 GC 收集器，如 G1、ZGC 等。 五、类加载机制45.🌟了解类的加载机制吗？（补充）了解。 JVM 的操作对象是 Class 文件，JVM 把 Class 文件中描述类的数据结构加载到内存中，并对数据进行校验、解析和初始化，最终转化成可以被 JVM 直接使用的类型，这个过程被称为类加载机制。 其中最重要的三个概念就是：类加载器、类加载过程和双亲委派模型。 类加载器：负责加载类文件，将类文件加载到内存中，生成 Class 对象。 类加载过程：包括加载、验证、准备、解析和初始化等步骤。 双亲委派模型：当一个类加载器接收到类加载请求时，它会把请求委派给父——类加载器去完成，依次递归，直到最顶层的类加载器，如果父——类加载器无法完成加载请求，子类加载器才会尝试自己去加载。 46.类加载器有哪些？主要有四种： ①、启动类加载器，负责加载 JVM 的核心类库，如 rt.jar 和其他核心库位于JAVA_HOME/jre/lib目录下的类。 ②、扩展类加载器，负责加载JAVA_HOME/jre/lib/ext目录下，或者由系统属性java.ext.dirs指定位置的类库，由sun.misc.Launcher$ExtClassLoader 实现。 ③、应用程序类加载器，负责加载 classpath 的类库，由sun.misc.Launcher$AppClassLoader实现。 我们编写的任何类都是由应用程序类加载器加载的，除非显式使用自定义类加载器。 ④、用户自定义类加载器，通常用于加载网络上的类、执行热部署（动态加载和替换应用程序的组件），或者为了安全考虑，从不同的源加载类。 通过继承java.lang.ClassLoader类来实现。 47.能说一下类的生命周期吗？一个类从被加载到虚拟机内存中开始，到从内存中卸载，整个生命周期需要经过七个阶段：加载 、验证、准备、解析、初始化、使用和卸载。 三分恶面渣逆袭：类的生命周期 48.🌟类装载的过程知道吗？ 推荐阅读：一文彻底搞懂 Java 类加载机制 知道。 类装载过程包括三个阶段：载入、链接和初始化。 ①、载入：将类的二进制字节码加载到内存中。 ②、链接可以细分为三个小的阶段： 验证：检查类文件格式是否符合 JVM 规范 准备：为类的静态变量分配内存并设置默认值。 解析：将符号引用替换为直接引用。 ③、初始化：执行静态代码块和静态变量初始化。 在准备阶段，静态变量已经被赋过默认初始值了，在初始化阶段，静态变量将被赋值为代码期望赋的值。比如说 static int a = 1;，在准备阶段，a 的值为 0，在初始化阶段，a 的值为 1。 换句话说，初始化阶段是在执行类的构造方法，也就是 javap 中看到的 clinit()。 载入过程 JVM 会做什么？三分恶面渣逆袭：载入 1）通过一个类的全限定名来获取定义此类的二进制字节流。 2）将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 3）在内存中生成一个代表这个类的 java.lang.Class 对象，作为这个类的访问入口。 49.🌟什么是双亲委派模型？双亲委派模型要求类加载器在加载类时，先委托父加载器尝试加载，只有父加载器无法加载时，子加载器才会加载。 三分恶面渣逆袭：双亲委派模型 这个过程会一直向上递归，也就是说，从子加载器到父加载器，再到更上层的加载器，一直到最顶层的启动类加载器。 启动类加载器会尝试加载这个类。如果它能够加载这个类，就直接返回；如果它不能加载这个类，就会将加载任务返回给委托它的子加载器。 子加载器尝试加载这个类。如果子加载器也无法加载这个类，它就会继续向下传递这个加载任务，依此类推。 直到某个加载器能够加载这个类，或者所有加载器都无法加载这个类，最终抛出 ClassNotFoundException。 49.为什么要用双亲委派模型？①、避免类的重复加载：父加载器加载的类，子加载器无需重复加载。 ②、保证核心类库的安全性：如 java.lang.* 只能由 Bootstrap ClassLoader 加载，防止被篡改。 50.如何破坏双亲委派机制？重写 ClassLoader 的 loadClass() 方法。 如果不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法，那些无法被父类加载器加载的类最终会通过这个方法被加载。 51.有哪些破坏双亲委派模型的典型例子？我了解的有两种： 第一种：SPI 机制加载 JDBC 驱动。 第二种：热部署框架。 三分恶面渣逆袭：双亲委派模型的三次破坏 说说SPI 机制？SPI 是 Java 的一种扩展机制，用于加载和注册第三方类库，常见于 JDBC、JNDI 等框架。 双亲委派模型会优先让父类加载器加载类，而 SPI 需要动态加载子类加载器中的实现。 根据双亲委派模型，java.sql.Driver 类应该由父加载器加载，但父类加载器无法加载由子类加载器定义的驱动类，如 MySQL 的 com.mysql.cj.jdbc.Driver。 那么只能使用 SPI 机制通过 META-INF/services 文件指定服务提供者的实现类。 ClassLoader cl = Thread.currentThread().getContextClassLoader();EnumerationDriver drivers = ServiceLoader.load(Driver.class, cl).iterator(); DriverManager 使用了线程上下文类加载器来加载 SPI 的实现类，从而允许子类加载器加载具体的 JDBC 驱动。 说说热部署？热部署是指在不重启服务器的情况下更新应用程序代码，需要替换旧版本的类，但旧版本的类可能由父加载器加载。 如 Spring Boot 的 DevTools 通常会自定义类加载器，优先加载新的类版本。 52.Tomcat 的类加载机制了解吗？了解。 Tomcat 基于双亲委派模型进行了一些扩展，主要的类加载器有： Bootstrap ClassLoader：加载 Java 的核心类库； Catalina ClassLoader：加载 Tomcat 的核心类库； Shared ClassLoader：加载共享类库，允许多个 Web 应用共享某些类库； WebApp ClassLoader：加载 Web 应用程序的类库，支持多应用隔离和优先加载应用自定义的类库（破坏了双亲委派模型）。 Tomcat类加载器 53.你觉得应该怎么实现一个热部署功能？热部署是指在不重启服务器的情况下，动态加载、更新或卸载应用程序的组件，比如类、配置文件等。 需要在类加载器的基础上，实现类的重新加载。 我的思路是： 第一步，使用文件监控机制，如 Java NIO 的 WatchService 来监控类文件或配置文件的变化。当监控到文件变更时，触发热部署流程。 class FileWatcher public static void watchDirectoryPath(Path path) // 检查路径是否是有效目录 if (!isDirectory(path)) System.err.println(Provided path is not a directory: + path); return; System.out.println(Starting to watch path: + path); // 获取文件系统的 WatchService try (WatchService watchService = path.getFileSystem().newWatchService()) // 注册目录监听服务，监听创建、修改和删除事件 path.register(watchService, ENTRY_CREATE, ENTRY_MODIFY, ENTRY_DELETE); while (true) WatchKey key; try // 阻塞直到有事件发生 key = watchService.take(); catch (InterruptedException e) System.out.println(WatchService interrupted, stopping directory watch.); Thread.currentThread().interrupt(); break; // 处理事件 for (WatchEvent? event : key.pollEvents()) processEvent(event); // 重置 key，如果失败则退出 if (!key.reset()) System.out.println(WatchKey no longer valid. Exiting watch loop.); break; catch (IOException e) System.err.println(An error occurred while setting up the WatchService: + e.getMessage()); e.printStackTrace(); private static boolean isDirectory(Path path) return Files.isDirectory(path, LinkOption.NOFOLLOW_LINKS); private static void processEvent(WatchEvent? event) WatchEvent.Kind? kind = event.kind(); // 处理事件类型 if (kind == OVERFLOW) System.out.println(Event overflow occurred. Some events might have been lost.); return; @SuppressWarnings(unchecked) Path fileName = ((WatchEventPath) event).context(); System.out.println(Event: + kind.name() + , File affected: + fileName); public static void main(String[] args) // 设置监控路径为当前目录 Path pathToWatch = Paths.get(.); watchDirectoryPath(pathToWatch); 第二步，创建一个自定义类加载器，继承java.lang.ClassLoader，并重写findClass()方法，用来加载新的类文件。 class HotSwapClassLoader extends ClassLoader public HotSwapClassLoader() super(ClassLoader.getSystemClassLoader()); @Override protected Class? findClass(String name) throws ClassNotFoundException // 加载指定路径下的类文件字节码 byte[] classBytes = loadClassData(name); if (classBytes == null) throw new ClassNotFoundException(name); // 调用defineClass将字节码转换为Class对象 return defineClass(name, classBytes, 0, classBytes.length); private byte[] loadClassData(String name) // 实现从文件系统或其他来源加载类文件的字节码 // ... return null; 友情提示：Intellij IDEA 提供了热部署功能，当我们修改了代码后，IDEA 会自动保存并编译，如果是 Web 项目，还可以在 Chrome 浏览器中装一个 LiveReload 插件，一旦编译完成，页面就会自动刷新看到最新的效果。对于测试或者调试来说，非常方便。 54.说说解释执行和编译执行的区别（补充）先说解释和编译的区别： 解释：将源代码逐行转换为机器码。 编译：将源代码一次性转换为机器码。 一个是逐行，一个是一次性，再来说说解释执行和编译执行的区别： 解释执行：程序运行时，将源代码逐行转换为机器码，然后执行。 编译执行：程序运行前，将源代码一次性转换为机器码，然后执行。 Java 一般被称为“解释型语言”，因为 Java 代码在执行前，需要先将源代码编译成字节码，然后在运行时，再由 JVM 的解释器“逐行”将字节码转换为机器码，然后执行。 这也是 Java 被诟病“慢”的主要原因。 但 JIT 的出现打破了这种刻板印象，JVM 会将热点代码（即运行频率高的代码）编译后放入 CodeCache，当下次执行再遇到这段代码时，会从 CodeCache 中直接读取机器码，然后执行。 因此，Java 的执行效率得到了大幅提升。 图片来源于美团技术博客 Java 面试指南（付费）收录的腾讯 Java 后端实习一面原题：说说 Java 解释执行的流程。","tags":["基础","Java"],"categories":["Java问答笔记"]},{"title":"2025.10.9学习日记","path":"/2025/10/09/学习日记25年10月/2025.10.9学习笔记/","content":"今日学习内容3DGS今天将没实现的树结构索引实现了,基本我的创新点的编码阶段结束了,接下来就是测试和优化.重新梳理一下目前的整体架构:1.首先按照任意方式渲染一个高质量的基础模型.2.然后通过3D平滑滤波或者重要性滤波获取不同细节度的模型(高 中 低).3.然后对3D空间进行分块,作为逻辑分块,然后对每个分块进行一个合并分裂处理,对点数量少于阈值的分块进行合并,对于点数量过多的分块进行分裂,然后优化后的分块作为物理分块.4.然后对每一个逻辑分块,计算其分块激活表,也就是当相机位于这个分块范围内的时候,哪些分块使用高精度模型,哪些中低精度(按照空间距离来分).5.渲染时,三种模型对应树结构全部存储在CUDA中,根据相机位置快速索引到对应逻辑分区,根据该逻辑分区对应的激活表快速索引到对应物理分区,然后根据物理分区对应的模型进行渲染. 目前测试下来,帧率可以稳定达到60FPS,并且是无论场景大小,都能够保持该帧率.原版当视角拉远时,帧率会下降严重,并且模型细节会丢失. 力扣每日一题一道类似DP的题目. 算法hot100十道题目. Java复习进度Java进阶之路集合篇框架写完了.Java SE5656Java集合框架3030Java并发编程22 - 4271JVMMySQL5583Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"2025.10.4学习日记","path":"/2025/10/04/学习日记25年10月/2025.10.4学习笔记/","content":"今日学习内容3DGS力扣每日一题Java复习进度Java进阶之路集合篇框架写完了.Java SE5656Java集合框架3030Java并发编程16 - 2271JVMMySQL5583Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"2025.10.3学习日记","path":"/2025/10/03/学习日记25年10月/2025.10.3学习笔记/","content":"今日学习内容3DGS力扣每日一题3D接雨水 class Solution private static final int[][] DIRS = 0, -1, 0, 1, -1, 0, 1, 0; public int trapRainWater(int[][] heightMap) int m = heightMap.length, n = heightMap[0].length; PriorityQueueint[] pq = new PriorityQueue((a, b) - (a[0] - b[0])); for (int i = 0; i m; i++) for (int j = 0; j n; j++) if (i == 0 || i == m - 1 || j == 0 || j == n - 1) pq.add(new int[]heightMap[i][j], i, j); //相当于存放木桶边界 heightMap[i][j] = -1; // 标记 (i,j) 访问过 int ans = 0; while (!pq.isEmpty()) int[] t = pq.poll(); // 去掉短板 int minHeight = t[0], i = t[1], j = t[2]; // minHeight 是木桶的短板 for (int[] d : DIRS) int x = i + d[0], y = j + d[1]; // (i,j) 的邻居 if (0 = x x m 0 = y y n heightMap[x][y] = 0) // (x,y) 没有访问过 // 如果 (x,y) 的高度小于 minHeight，那么接水量为 minHeight - heightMap[x][y] ans += Math.max(minHeight - heightMap[x][y], 0); // 给木桶新增一块高为 max(minHeight, heightMap[x][y]) 的木板 pq.add(new int[]Math.max(minHeight, heightMap[x][y]), x, y); heightMap[x][y] = -1; // 标记 (x,y) 访问过 return ans; Java复习进度Java进阶之路集合篇框架写完了. Java SE5656Java集合框架3030Java并发编程14 - 16 71JVMMySQL5583Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法三道题目. 简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-10"]},{"title":"2025.9.30学习日记","path":"/2025/09/30/学习日记25年9月/2025.9.30学习笔记/","content":"今日学习内容3DGS完善了LOD渲染的代码,优化了渲染时的逻辑,不去将所有分块的模型进行拼接成混合模型后去渲染,而是直接通过激活高斯索引表,来直接索引渲染,减少大量的拼接和io时间.总体流程如下: 先训练一个高精度模型,方法可以随意替换. 然后原本模型作为高模,然后执行3D平滑滤波,获取中模,再执行3D平滑滤波获取低模. 对空间进行分块,每个分块对应三种清晰度模型,然后每一个分块都对应一个激活高斯表,激活高斯表就代表当相机位于该分块时,哪些分块使用高模,那些使用中模,哪些使用低模. 渲染时,根据相机坐标位置直接计算对应哪个分块,根据对应分块激活高斯表获取索引,直接进行索引渲染. 力扣每日一题一道简单的模拟题. Java复习进度Java进阶之路集合篇框架写完了. Java SE5656Java集合框架3030Java并发编程1471JVMMySQL5583Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"Linux学习笔记","path":"/2025/09/29/Java问答笔记/Linux学习笔记/","content":"1. Linux 常用命令推荐阅读：常用高频 Linux 速查备忘手册 我自己常用的 Linux 命令有： top 用来查看系统资源 ps -ef | grep java 查看 Java 进程 netstat 查看网络连接 ping 测试网络连通性 find 查找文件 chmod 修改文件权限 kill 终止进程 df 查看磁盘空间 mkdir 创建目录、rm 删除文件、cp 复制文件、mv 移动文件 zip 压缩文件、unzip 解压文件等等这些。 Java 面试指南（付费）收录的作业帮面经同学 1 Java 后端一面面试原题：常用linux命令 Java 面试指南（付费）收录的虾皮面经同学 13 一面面试原题：常见的linux命令 Java 面试指南（付费）收录的阿里云面经同学 22 面经：linux常用命令 文件操作的命令有哪些？ ls：列出目录内容。ls -l显示详细信息，ls -a显示隐藏文件。 cd：更改当前目录。cd ..回到上级目录，cd ~回到用户的主目录。 pwd：显示当前工作目录的完整路径。 cp：复制文件或目录。cp source_file target_file复制文件，cp -r source_directory target_directory复制目录。 mv：移动或重命名文件或目录。 rm：删除文件或目录。rm -r递归删除目录及其内容。 mkdir：创建新目录。 cat：查看文件内容。cat file1 file2合并文件内容显示。 Windows下如何创建空文件Windows 下我还是比较习惯使用右键菜单新建一个文件，然后重命名。 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：如果是Windows，如何去创建一个空文件的？ 如何查看系统的日志文件？在 Linux 中，可以通过 cat、more、less、tail、head 等命令查看系统日志文件。 也可以直接通过 vim 打开日志文件，然后按照关键字去搜查对应的日志信息。 常见的系统日志文件包括： /var/log/syslog：包含系统范围内的消息和错误日志，包括启动日志、内核日志等，是排查系统问题的首选日志文件之一。 /var/log/messages：类似于 syslog，但通常更多关注系统级别的消息和错误。 二哥编程星球球友枕云眠美团 AI 面试原题：如何查看系统日志文件，常见的系统日志文件有哪些 系统管理的命令有哪些？ ps：显示当前运行的进程。ps aux显示所有进程。 top：实时显示进程动态。 kill：终止进程。kill -9 PID强制终止。 df：显示磁盘空间使用情况。df -h以易读格式显示。 du：显示目录或文件的磁盘使用情况。 free：显示内存和交换空间的使用情况。 chmod：更改文件或目录的权限。 chown：更改文件或目录的所有者和所属组。 如何查看Linux进程或CPU使用情况？top 命令可以实时查看所有进程的 CPU 和内存使用情况。 二哥的 Java 进阶之路：top面板 ps aux --sort=-%cpu | head -5可以查看 CPU 使用率最高的 5 个进程。 二哥的 Java 进阶之路：ps 命令 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：怎么查看一个进程的Cpu 使用率呢？ 如何查看Linux内存使用情况？可以使用 watch 配合 free 命令实时监控内存使用情况。如 watch -n 1 free -m每秒刷新一次内存使用情况。 二哥的 Java 进阶之路：free Java 面试指南（付费）收录的腾讯面经同学 29 Java 后端一面原题：如何看Linux进程或CPU使用情况？Linux查看内存情况？ 如何查看系统负载？top 命令是实时查看系统性能的常用工具，系统负载信息通常显示在 top 命令输出的顶部。它还显示了系统运行的进程、内存使用情况等。 二哥的 Java 进阶之路：TOP 命令 二哥编程星球球友枕云眠美团 AI 面试原题：如何查看系统负载，系统中的 load average 含义是什么 Load Average 是什么？load average 是一个反映系统负载的指标，表示在一段时间内系统正在处理的平均进程数量。通常，它包含三个数值，分别对应过去 1 分钟、5 分钟和 15 分钟的平均负载。 比如说上图中出现的 load average: 1.80, 1.74, 1.83 表示： 1.80：表示过去 1 分钟内，系统平均有 1.80 个进程在等待处理（包括 CPU 正在处理和等待被调度的进程）。 1.74：表示过去 5 分钟内的平均负载。 1.83：表示过去 15 分钟内的平均负载。 load average 的数值可以看作是系统的工作队列长度（等待处理的任务数量）。如果这个数值接近或等于 CPU 核心数，说明系统的负载是合理的。 如果 load average 大于 CPU 核心数，表示系统的进程比 CPU 能处理的多，系统可能处于过载状态。 在单核系统中，load average 数值超过 1 通常意味着系统繁忙（有任务在等待 CPU）。 在多核系统中，假设有 N 个 CPU 核心，load average 接近 N 时表示系统正处于高负载状态，但还在可接受范围内。如果 load average 超过 N，则意味着系统可能过载。 macOS 上可以通过 sysctl -a | grep machdep.cpu.core_count 查看 CPU 核心数，我本机目前是 16 核。 二哥的 Java 进阶之路：macOS 的 CPU 核心数 二哥编程星球球友枕云眠美团 AI 面试原题：如何查看系统负载，系统中的 load average 含义是什么 chmod 的参数讲一下？chmod 命令在 Linux 中用来改变文件或目录的访问权限。这个命令的使用可以基于符号表示法（也称为文本方法）或者八进制数表示法。 像 chmod 777 file 赋予文件所有权限，就属于八进制数表示法。7=4+2+1，分别代表读、写、执行权限。 Linux 中的权限可以应用于三种类别的用户： 文件所有者（u） 与文件所有者同组的用户（g） 其他用户（o） 图片来源于网络 ①、符号模式 符号模式使用字母来表示权限，如下： 读（r） 写（w） 执行（x） 所有（a） 例如： chmod u+w file：给文件所有者添加写权限。 chmod g-r file：移除组用户的读权限。 chmod o+x file：给其他用户添加执行权限。 chmod u=rwx,g=rx,o=r file：设置文件所有者具有读写执行权限，组用户具有读执行权限，其他用户具有读权限。 ②、数字模式 数字模式使用三位八进制数来表示权限，每位数字代表不同的用户类别（所有者、组、其他用户），数字是其各自权限值的总和： 读（r） 4 写（w） 2 执行（x） 1 图片来源于网络 因此，权限模式可以是从 0（无权限）到 7（读写执行权限）的任何值。 chmod 755 file：使得文件所有者有读写执行（7）权限，组用户和其他用户有读和执行（5）权限。 chmod 644 file：使得文件所有者有读写（6）权限，而组用户和其他用户只有读（4）权限。 kill -9 中的 9 是什么意思？kill -9 PID 是一种强制终止进程的方式，其中的 9 表示信号编号，代表 SIGKILL 信号。 Java 面试指南（付费）收录的阿里云面经同学 22 面经：kill -9 9的意义是什么 网络管理的命令有哪些？ ping：检查与远程服务器的连接。 wget：从网络上下载文件。 ifconfig：显示网络接口的配置信息。 netstat：显示网络连接、路由表和网络接口信息。 如何查看8080端口的连接数？可以通过 netstat 命令查看，如netstat -an | grep :8080 | grep tcp | wc -l。 二哥的 Java 进阶之路：netstat 命令查看 8080 端口 -a：显示所有网络连接和监听端口。 -n：以数字形式显示地址和端口。 grep :8080：过滤出 8080 端口的连接。 grep tcp：仅显示 TCP 连接。 wc -l：统计匹配到的行数，即连接数。 也可以使用 ss 命令，它是 netstat 的替代工具；还可以使用 lsof 命令，它可以列出当前系统打开的文件和套接字。 Java 面试指南（付费）收录的腾讯面经同学 29 Java 后端一面原题：Linux系统的8080端口有多少个TCP连接，怎么看？ 压缩和解压的命令有哪些？ tar：打包或解包.tar文件。tar cvf archive.tar files打包，tar xvf archive.tar解包。 gzip gunzip：压缩或解压.gz文件。 zip unzip：压缩或解压.zip文件。 查找文件的命令有哪些？ find：在目录树中查找文件。find /directory/ -name filename。 Liunx 下查找一个文件怎么做？在 Linux 环境下查找文件，有多种命令和方法可以使用。find 命令是最常用的文件查找工具之一，它可以在指定目录下递归查找符合条件的文件和目录。 例如：在当前目录及其子目录中查找名为 “example.txt” 的文件 find . -name example.txt 例如：查找 /home 目录中所有 .txt 结尾的文件： find /home -name *.txt 例如：查找 /var/log 目录中修改时间在 7 天以前的 .log 文件 find /var/log -name *.log -mtime +7 Java 面试指南（付费）收录的用友金融一面原题：Linux 的常用命令 Java 面试指南（付费）收录的华为 OD 面经同学 1 一面面试原题：Linux 使用过哪些命令 Java 面试指南（付费）收录的小公司面经同学 5 Java 后端面试原题：Liunx 下查找一个文件怎么做 Java 面试指南（付费）收录的华为 OD 面经同学 1 一面面试原题：chmod 的参数讲一下? 2. Linux 系统管理用户和用户组有什么区别？在 Linux 中，用户和用户组是系统权限管理的核心概念。 每个用户在 Linux 中都有一个独立的账户，用于标识该用户并控制其对系统资源的访问。用户包括普通用户和超级用户（root）。普通用户的权限有限，只能访问和修改自己拥有的文件和目录，而超级用户拥有系统的最高权限，能够执行任何操作。 每个用户在系统中都有一个唯一的用户 ID（UID），以及一个关联的用户名（login name）。 用户组是用户的集合，用于简化权限管理。每个用户可以属于一个或多个用户组，而每个用户组都有一个唯一的组 ID（GID）。通过将用户分配到不同的组，系统可以更方便地管理对文件和目录的访问权限。 一个文件或目录可以由一个用户和一个用户组拥有，系统根据文件或目录的所有者和所属组来确定其他用户对它的访问权限。 可以使用 groupadd 命令来创建新的用户组。例如： sudo groupadd developers 可以使用 useradd 命令来创建新的用户。创建用户时可以指定该用户的默认用户组、主目录等。例如，创建一个名为 johndoe 的用户，并将其添加到 developers 组： sudo useradd -m -g developers johndoe -m：表示创建用户的同时创建用户的主目录（通常在/home/username）。 -g：指定用户的初始用户组。 二哥编程星球球友枕云眠美团 AI 面试原题：解释linux中的用户和用户组概念，如何创建新用户和用户组 如何用linux命令去查找某个qps?如果服务通过网络提供访问，可以使用 netstat 或 ss 命令统计特定端口的连接数，并结合 watch 命令来监控实时的连接速率。 例如，统计 HTTPS 服务（通常运行在端口 443）每秒的请求数： watch -n 1 netstat -an | grep :443 | grep ESTABLISHED | wc -l 解释一下： netstat -an：显示所有连接和监听端口。 grep :443 ：过滤出端口 443 的连接。 grep ESTABLISHED：过滤出已经建立的连接。 wc -l：统计连接数。 watch -n 1：每秒刷新一次命令的输出。 观察连接数的变化，可以大致估算出每秒的请求数。 二哥的 Java 进阶之路：技术派的 443 请求数 Java 面试指南（付费）收录的作业帮面经同学 1 Java 后端一面面试原题：用linux命令去查找某个qps 3. Git 常用命令Git 常用命令有哪些？ git clone repository-url：克隆远程仓库。 git status：查看工作区和暂存区的状态。 git add file：将文件添加到暂存区。 git commit -m message：提交暂存区的文件到本地仓库。 git log：查看提交历史。 git merge branch-name：合并指定分支到当前分支。 git checkout branch-name：切换分支。 git pull：拉取远程仓库的更新。 图文详解 2 道 Linux 面试高频题，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二。 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇 👍 面渣逆袭 Java 集合框架篇 👍 面渣逆袭 Java 并发编程篇 👍 面渣逆袭 JVM 篇 👍 面渣逆袭 Spring 篇 👍 面渣逆袭 Redis 篇 👍 面渣逆袭 MyBatis 篇 👍 面渣逆袭 MySQL 篇 👍 面渣逆袭操作系统篇 👍 面渣逆袭计算机网络篇 👍 面渣逆袭 RocketMQ 篇 👍 面渣逆袭分布式篇 👍 面渣逆袭微服务篇 👍 面渣逆袭设计模式篇 👍 面渣逆袭 Linux 篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。","tags":["基础","Linux"],"categories":["Java问答笔记"]},{"title":"MyBatis学习笔记","path":"/2025/09/29/Java问答笔记/MyBatis学习笔记/","content":"6400 字 30 张手绘图，详解 23 道 MyBatis 面试高频题（让天下没有难背的八股），面渣背会这些 MyBatis 八股文，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 大家好，我是二哥呀，面渣逆袭系列继续，这节我们的主角是 MyBatis，作为当前国内最流行的 ORM 框架，是我们这些 crud 选手最趁手的工具，赶紧来看看面试都会问哪些问题吧。 基础1. 说说什么是 MyBatis? MyBatis logo 先吹一下： Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费精力去处理加载驱动、创建连接、创建 statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。 MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 再说一下缺点 SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL 语句的功底有一定要求 SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库 ORM 是什么? ORM简单示意图 ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术。简单来说，ORM 是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中。 为什么说 Mybatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？ Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。 而 Mybatis 在查询关联对象或关联集合对象时，需要手动编写 SQL 来完成，所以，被称之为半自动 ORM 映射工具。 JDBC 编程有哪些不足之处，MyBatis 是如何解决的？ JDBC编程的不足 1、数据连接创建、释放频繁造成系统资源浪费从而影响系统性能，在 mybatis-config.xml 中配置数据链接池，使用连接池统一管理数据库连接。 2、sql 语句写在代码中造成代码不易维护，将 sql 语句配置在 XXXXmapper.xml 文件中与 java 代码分离。 3、向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数一一对应。Mybatis 自动将 java 对象映射至 sql 语句。 4、对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。Mybatis 自动将 sql 执行结果映射至 java 对象。 2. Hibernate 和 MyBatis 有什么区别？相同点 都是对 jdbc 的封装，都是应用于持久层的框架。 这还用说？ 不同点 1）映射关系 MyBatis 是一个半自动映射的框架，配置 Java 对象与 sql 语句执行结果的对应关系，多表关联关系配置简单 Hibernate 是一个全表映射的框架，配置 Java 对象与数据库表的对应关系，多表关联关系配置复杂 2）SQL 优化和移植性 Hibernate 对 SQL 语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但 SQL 语句优化困难。 MyBatis 需要手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用 SQL 语句操作数据库，不支持数据库无关性，但 sql 语句优化容易。 3）MyBatis 和 Hibernate 的适用场景不同 Mybatis vs Hibernate Hibernate 是标准的 ORM 框架，SQL 编写量较少，但不够灵活，适合于需求相对稳定，中小型的软件项目，比如：办公自动化系统 MyBatis 是半 ORM 框架，需要编写较多 SQL，但是比较灵活，适合于需求变化频繁，快速迭代的项目，比如：电商网站 3. MyBatis 使用过程？生命周期？MyBatis 基本使用的过程大概可以分为这么几步： Mybatis基本使用步骤 1）创建 SqlSessionFactory 可以从配置或者直接编码来创建 SqlSessionFactory String resource = org/mybatis/example/mybatis-config.xml;InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 2）通过 SqlSessionFactory 创建 SqlSession SqlSession（会话）可以理解为程序和数据库之间的桥梁 SqlSession session = sqlSessionFactory.openSession(); 3）通过 sqlsession 执行数据库操作 可以通过 SqlSession 实例来直接执行已映射的 SQL 语句： Blog blog = (Blog)session.selectOne(org.mybatis.example.BlogMapper.selectBlog, 101); 更常用的方式是先获取 Mapper(映射)，然后再执行 SQL 语句： BlogMapper mapper = session.getMapper(BlogMapper.class);Blog blog = mapper.selectBlog(101); 4）调用 session.commit()提交事务 如果是更新、删除语句，我们还需要提交一下事务。 5）调用 session.close()关闭会话 最后一定要记得关闭会话。 说说 MyBatis 生命周期？上面提到了几个 MyBatis 的组件，一般说的 MyBatis 生命周期就是这些组件的生命周期。 SqlSessionFactoryBuilder 一旦创建了 SqlSessionFactory，就不再需要它了。 因此 SqlSessionFactoryBuilder 实例的生命周期只存在于方法的内部。 SqlSessionFactory SqlSessionFactory 是用来创建 SqlSession 的，相当于一个数据库连接池，每次创建 SqlSessionFactory 都会使用数据库资源，多次创建和销毁是对资源的浪费。所以 SqlSessionFactory 是应用级的生命周期，而且应该是单例的。 SqlSession SqlSession 相当于 JDBC 中的 Connection，SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的生命周期是一次请求或一个方法。 Mapper 映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的，它的生命周期在 sqlsession 事务方法之内，一般会控制在方法级。 MyBatis主要组件生命周期 当然，万物皆可集成 Spring，MyBatis 通常也是和 Spring 集成使用，Spring 可以帮助我们创建线程安全的、基于事务的 SqlSession 和映射器，并将它们直接注入到我们的 bean 中，我们不需要关心它们的创建过程和生命周期，那就是另外的故事了。 这个应该会 4. 在 mapper 中如何传递多个参数？ mapper传递多个参数方法 方法 1：顺序传参法 public User selectUser(String name, int deptId);select id=selectUser resultMap=UserResultMap select * from user where user_name = #0 and dept_id = #1/select \\#里面的数字代表传入参数的顺序。 这种方法不建议使用，sql 层表达不直观，且一旦顺序调整容易出错。 方法 2：@Param 注解传参法 public User selectUser(@Param(userName) String name, int @Param(deptId) deptId);select id=selectUser resultMap=UserResultMap select * from user where user_name = #userName and dept_id = #deptId/select \\#里面的名称对应的是注解@Param 括号里面修饰的名称。 这种方法在参数不多的情况还是比较直观的，（推荐使用）。 方法 3：Map 传参法 public User selectUser(MapString, Object params);select id=selectUser parameterType=java.util.Map resultMap=UserResultMap select * from user where user_name = #userName and dept_id = #deptId/select \\#里面的名称对应的是 Map 里面的 key 名称。 这种方法适合传递多个参数，且参数易变能灵活传递的情况。 方法 4：Java Bean 传参法 public User selectUser(User user);select id=selectUser parameterType=com.jourwon.pojo.User resultMap=UserResultMap select * from user where user_name = #userName and dept_id = #deptId/select \\#里面的名称对应的是 User 类里面的成员属性。 这种方法直观，需要建一个实体类，扩展不容易，需要加属性，但代码可读性强，业务逻辑处理方便，推荐使用。（推荐使用）。 5. 实体类属性名和表中字段名不一样 ，怎么办? 第 1 种： 通过在查询的 SQL 语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。 select id=getOrder parameterType=int resultType=com.jourwon.pojo.Order select order_id id, order_no orderno ,order_price price form orders where order_id=#id;/select 第 2 种： 通过 resultMap 中的result来映射字段名和实体类属性名的一一对应的关系。 select id=getOrder parameterType=int resultMap=orderResultMap select * from orders where order_id=#id/selectresultMap type=com.jourwon.pojo.Order id=orderResultMap !–用id属性来映射主键字段– id property=id column=order_id !–用result属性来映射非主键字段，property为实体类属性名，column为数据库表中的属性– result property =orderno column =order_no/ result property=price column=order_price //resultMap 6. Mybatis 是否可以映射 Enum 枚举类？ Mybatis 当然可以映射枚举类，不单可以映射枚举类，Mybatis 可以映射任何对象到表的一列上。映射方式为自定义一个 TypeHandler，实现 TypeHandler 的 setParameter()和 getResult()接口方法。 TypeHandler 有两个作用，一是完成从 javaType 至 jdbcType 的转换，二是完成 jdbcType 至 javaType 的转换，体现为 setParameter()和 getResult()两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。 7. #{}和${}的区别?# 是预编译处理，$ 是字符串替换。 ①、当使用 # 时，MyBatis 会在 SQL 执行之前，将占位符替换为问号 ?，并使用参数值来替代这些问号。 由于 # 使用了预处理，所以能有效防止 SQL 注入，确保参数值在到达数据库之前被正确地处理和转义。 select id=selectUser resultType=User SELECT * FROM users WHERE id = #id/select ②、当使用 $ 时，参数的值会直接替换到 SQL 语句中去，而不会经过预处理。 这就存在 SQL 注入的风险，因为参数值会直接拼接到 SQL 语句中，假如参数值是 1 or 1=1，那么 SQL 语句就会变成 SELECT * FROM users WHERE id = 1 or 1=1，这样就会导致查询出所有用户的结果。 $ 通常用于那些不能使用预处理的场合，比如说动态表名、列名、排序等，要提前对参数进行安全性校验。 select id=selectUsersByOrder resultType=User SELECT * FROM users ORDER BY $columnName ASC/select Java 面试指南（付费）收录的小公司面经合集同学 1 Java 后端面试原题：Mybatis#()和$()有什么区别? Java 面试指南（付费）收录的京东面经同学 5 Java 后端技术一面面试原题：#{}和${}的区别 8. 模糊查询 like 语句该怎么写? concat拼接like 1 ’%$question%’ 可能引起 SQL 注入，不推荐 2 %#question% 注意：因为#…解析成 sql 语句时候，会在变量外侧自动加单引号’ ‘，所以这里 % 需要使用双引号” “，不能使用单引号 ’ ‘，不然会查不到任何结果。 3 CONCAT(%,#question,%) 使用 CONCAT()函数，（推荐 ✨） 4 使用 bind 标签（不推荐） select id=listUserLikeUsername resultType=com.jourwon.pojo.Useremsp;emsp;bind name=pattern value=% + username + % /emsp;emsp;select id,sex,age,username,password from person where username LIKE #pattern/select 9. Mybatis 能执行一对一、一对多的关联查询吗？当然可以，不止支持一对一、一对多的关联查询，还支持多对多、多对一的关联查询。 MyBatis级联 一对一association 比如订单和支付是一对一的关系，这种关联的实现： 实体类: public class Order private Integer orderId; private String orderDesc; /** * 支付对象 */ private Pay pay; //…… 结果映射 !-- 订单resultMap --resultMap id=peopleResultMap type=cn.fighter3.entity.Order id property=orderId column=order_id / result property=orderDesc column=order_desc/ !--一对一结果映射-- association property=pay javaType=cn.fighter3.entity.Pay id column=payId property=pay_id/ result column=account property=account/ /association/resultMap 查询就是普通的关联查 select id=getTeacher resultMap=getTeacherMap parameterType=int select * from order o left join pay p on o.order_id=p.order_id where o.order_id=#orderId/select 一对多collection 比如商品分类和商品，是一对多的关系。 实体类 public class Category private int categoryId; private String categoryName; /** * 商品列表 **/ ListProduct products; //…… 结果映射 resultMap type=Category id=categoryBean id column=categoryId property=category_id / result column=categoryName property=category_name / !-- 一对多的关系 -- !-- property: 指的是集合属性的值, ofType：指的是集合中元素的类型 -- collection property=products ofType=Product id column=product_id property=productId / result column=productName property=productName / result column=price property=price / /collection/resultMap 查询 查询就是一个普通的关联查询 !-- 关联查询分类和产品表 --select id=listCategory resultMap=categoryBean select c.*, p.* from category_ c left join product_ p on c.id = p.cid/select 那么多对一、多对多怎么实现呢？还是利用association和collection，篇幅所限，这里就不展开了。 10. Mybatis 是否支持延迟加载？原理？ Mybatis 支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabledtrue|false。 它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原理。 当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。 11. 如何获取生成的主键? 新增标签中添加：keyProperty” ID “ 即可 insert id=insert useGeneratedKeys=true keyProperty=userId insert into user( user_name, user_password, create_time) values(#userName, #userPassword , #createTime, jdbcType= TIMESTAMP)/insert 这时候就可以完成回填主键 mapper.insert(user);user.getId; 12. MyBatis 支持动态 SQL 吗？MyBatis 中有一些支持动态 SQL 的标签，它们的原理是使用 OGNL 从 SQL 参数对象中计算表达式的值，根据表达式的值动态拼接 SQL，以此来完成动态 SQL 的功能。 MyBatis if 根据条件来组成 where 子句 select id=findActiveBlogWithTitleLike resultType=BlogSELECT * FROM BLOGWHERE state = ‘ACTIVE’if test=title != null AND title like #title/if/select choose (when, otherwise) 这个和 Java 中的 switch 语句有点像 select id=findActiveBlogLike resultType=BlogSELECT * FROM BLOG WHERE state = ‘ACTIVE’choose when test=title != null AND title like #title /when when test=author != null and author.name != null AND author_name like #author.name /when otherwise AND featured = 1 /otherwise/choose/select trim (where, set) where可以用在所有的查询条件都是动态的情况 select id=findActiveBlogLike resultType=BlogSELECT * FROM BLOGwhere if test=state != null state = #state /if if test=title != null AND title like #title /if if test=author != null and author.name != null AND author_name like #author.name /if/where/select set 可以用在动态更新的时候 update id=updateAuthorIfNecessary update Author set if test=username != nullusername=#username,/if if test=password != nullpassword=#password,/if if test=email != nullemail=#email,/if if test=bio != nullbio=#bio/if /set where id=#id/update foreach 看到名字就知道了，这个是用来循环的，可以对集合进行遍历 select id=selectPostIn resultType=domain.blog.PostSELECT *FROM POST Pwhere foreach item=item index=index collection=list open=ID in ( separator=, close=) nullable=true #item /foreach/where/select 13. MyBatis 如何执行批量操作？ MyBatis批量操作 第一种方法：使用 foreach 标签 foreach 的主要用在构建 in 条件中，它可以在 SQL 语句中进行迭代一个集合。foreach 标签的属性主要有 item，index，collection，open，separator，close。 item 表示集合中每一个元素进行迭代时的别名，随便起的变量名； index 指定一个名字，用于表示在迭代过程中，每次迭代到的位置，不常用； open 表示该语句以什么开始，常用“(”； separator 表示在每次进行迭代之间以什么符号作为分隔符，常用“,”； close 表示以什么结束，常用“)”。 在使用 foreach 的时候最关键的也是最容易出错的就是 collection 属性，该属性是必须指定的，但是在不同情况下，该属性的值是不一样的，主要有以下 3 种情况： 如果传入的是单参数且参数类型是一个 List 的时候，collection 属性值为 list 如果传入的是单参数且参数类型是一个 array 数组的时候，collection 的属性值为 array 如果传入的参数是多个的时候，我们就需要把它们封装成一个 Map 了，当然单参数也可以封装成 map，实际上如果你在传入参数的时候，在 MyBatis 里面也是会把它封装成一个 Map 的，map 的 key 就是参数名，所以这个时候 collection 属性值就是传入的 List 或 array 对象在自己封装的 map 里面的 key 看看批量保存的两种用法： !-- MySQL下批量保存，可以foreach遍历 mysql支持values(),(),()语法 -- //推荐使用insert id=addEmpsBatch INSERT INTO emp(ename,gender,email,did) VALUES foreach collection=emps item=emp separator=, (#emp.eName,#emp.gender,#emp.email,#emp.dept.id) /foreach/insert !-- 这种方式需要数据库连接属性allowMutiQueries=true的支持 如jdbc.url=jdbc:mysql://localhost:3306/mybatis?allowMultiQueries=true --insert id=addEmpsBatch foreach collection=emps item=emp separator=; INSERT INTO emp(ename,gender,email,did) VALUES(#emp.eName,#emp.gender,#emp.email,#emp.dept.id) /foreach/insert 第二种方法：使用 ExecutorType.BATCH Mybatis 内置的 ExecutorType 有 3 种，默认为 simple，该模式下它为每个语句的执行创建一个新的预处理语句，单条提交 sql；而 batch 模式重复使用已经预处理的语句，并且批量执行所有更新语句，显然 batch 性能将更优； 但 batch 模式也有自己的问题，比如在 Insert 操作时，在事务没有提交之前，是没有办法获取到自增的 id，在某些情况下不符合业务的需求。 具体用法如下： //批量保存方法测试@Testpublic void testBatch() throws IOException SqlSessionFactory sqlSessionFactory = getSqlSessionFactory(); //可以执行批量操作的sqlSession SqlSession openSession = sqlSessionFactory.openSession(ExecutorType.BATCH); //批量保存执行前时间 long start = System.currentTimeMillis(); try EmployeeMapper mapper = openSession.getMapper(EmployeeMapper.class); for (int i = 0; i 1000; i++) mapper.addEmp(new Employee(UUID.randomUUID().toString().substring(0, 5), b, 1)); openSession.commit(); long end = System.currentTimeMillis(); //批量保存执行后的时间 System.out.println(执行时长 + (end - start)); //批量 预编译sql一次==》设置参数==》10000次==》执行1次 677 //非批量 （预编译=设置参数=执行 ）==》10000次 1121 finally openSession.close(); mapper 和 mapper.xml 如下 public interface EmployeeMapper //批量保存员工 Long addEmp(Employee employee); mapper namespace=com.jourwon.mapper.EmployeeMapper !--批量保存员工 -- insert id=addEmp insert into employee(lastName,email,gender) values(#lastName,#email,#gender) /insert/mapper 14. 说说 Mybatis 的一级、二级缓存？ 一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 SqlSession，各个 SqlSession 之间的缓存相互隔离，当 Session flush 或 close 之后，该 SqlSession 中的所有 Cache 就将清空，MyBatis 默认打开一级缓存。 Mybatis一级缓存 二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper(Namespace)，可以在多个 SqlSession 之间共享，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置。 Mybatis二级缓存示意图 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 原理15. 能说说 MyBatis 的工作原理吗？我们已经大概知道了 MyBatis 的工作流程，按工作原理，可以分为两大步：生成会话工厂、会话运行。 MyBatis的工作流程 MyBatis 是一个成熟的框架，篇幅限制，这里抓大放小，来看看它的主要工作流程。 构建会话工厂 构造会话工厂也可以分为两步： 构建会话工厂 获取配置 获取配置这一步经过了几步转化，最终由生成了一个配置类 Configuration 实例，这个配置类实例非常重要，主要作用包括： 读取配置文件，包括基础配置文件和映射文件 初始化基础配置，比如 MyBatis 的别名，还有其它的一些重要的类对象，像插件、映射器、ObjectFactory 等等 提供一个单例，作为会话工厂构建的重要参数 它的构建过程也会初始化一些环境变量，比如数据源 public SqlSessionFactory build(Reader reader, String environment, Properties properties) SqlSessionFactory var5; //省略异常处理 //xml配置构建器 XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); //通过转化的Configuration构建SqlSessionFactory var5 = this.build(parser.parse()); 构建 SqlSessionFactory SqlSessionFactory 只是一个接口，构建出来的实际上是它的实现类的实例，一般我们用的都是它的实现类 DefaultSqlSessionFactory， public SqlSessionFactory build(Configuration config) return new DefaultSqlSessionFactory(config); 会话运行 会话运行是 MyBatis 最复杂的部分，它的运行离不开四大组件的配合： MyBatis会话运行四大关键组件 Executor（执行器） Executor 起到了至关重要的作用，SqlSession 只是一个门面，相当于客服，真正干活的是是 Executor，就像是默默无闻的工程师。它提供了相应的查询和更新方法，以及事务方法。 Environment environment = this.configuration.getEnvironment();TransactionFactory transactionFactory = this.getTransactionFactoryFromEnvironment(environment);tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit);//通过Configuration创建executorExecutor executor = this.configuration.newExecutor(tx, execType);var8 = new DefaultSqlSession(this.configuration, executor, autoCommit); StatementHandler（数据库会话器） StatementHandler，顾名思义，处理数据库会话的。我们以 SimpleExecutor 为例，看一下它的查询方法，先生成了一个 StatementHandler 实例，再拿这个 handler 去执行 query。 public E ListE doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException Statement stmt = null; List var9; try Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(this.wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt = this.prepareStatement(handler, ms.getStatementLog()); var9 = handler.query(stmt, resultHandler); finally this.closeStatement(stmt); return var9; 再以最常用的 PreparedStatementHandler 看一下它的 query 方法，其实在上面的prepareStatement已经对参数进行了预编译处理，到了这里，就直接执行 sql，使用 ResultHandler 处理返回结果。 public E ListE query(Statement statement, ResultHandler resultHandler) throws SQLException PreparedStatement ps = (PreparedStatement)statement; ps.execute(); return this.resultSetHandler.handleResultSets(ps); ParameterHandler （参数处理器） PreparedStatementHandler 里对 sql 进行了预编译处理 public void parameterize(Statement statement) throws SQLException this.parameterHandler.setParameters((PreparedStatement)statement); 这里用的就是 ParameterHandler，setParameters 的作用就是设置预编译 SQL 语句的参数。 里面还会用到 typeHandler 类型处理器，对类型进行处理。 public interface ParameterHandler Object getParameterObject(); void setParameters(PreparedStatement var1) throws SQLException; ResultSetHandler（结果处理器） 我们前面也看到了，最后的结果要通过 ResultSetHandler 来进行处理，handleResultSets 这个方法就是用来包装结果集的。Mybatis 为我们提供了一个 DefaultResultSetHandler，通常都是用这个实现类去进行结果的处理的。 public interface ResultSetHandler E ListE handleResultSets(Statement var1) throws SQLException; E CursorE handleCursorResultSets(Statement var1) throws SQLException; void handleOutputParameters(CallableStatement var1) throws SQLException; 它会使用 typeHandle 处理类型，然后用 ObjectFactory 提供的规则组装对象，返回给调用者。 整体上总结一下会话运行： 会话运行的简单示意图 我们最后把整个的工作流程串联起来，简单总结一下： MyBatis整体工作原理图 读取 MyBatis 配置文件——mybatis-config.xml 、加载映射文件——映射文件即 SQL 映射文件，文件中配置了操作数据库的 SQL 语句。最后生成一个配置对象。 构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。 创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。 Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。 StatementHandler：数据库会话器，串联起参数映射的处理和运行结果映射的处理。 参数处理：对输入参数的类型进行处理，并预编译。 结果处理：对返回结果的类型进行处理，根据对象映射规则，返回相应的对象。 16. MyBatis 的功能架构是什么样的？ MyBatis功能架构 我们一般把 Mybatis 的功能架构分为三层： API 接口层：提供给外部使用的接口 API，开发人员通过这些本地 API 来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的 SQL 查找、SQL 解析、SQL 执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 17. 为什么 Mapper 接口不需要实现类？四个字回答：动态代理，我们来看一下获取 Mapper 的过程： Mapper代理 获取 Mapper 我们都知道定义的 Mapper 接口是没有实现类的，Mapper 映射其实是通过动态代理实现的。 BlogMapper mapper = session.getMapper(BlogMapper.class); 七拐八绕地进去看一下，发现获取 Mapper 的过程，需要先获取 MapperProxyFactory——Mapper 代理工厂。 public T T getMapper(ClassT type, SqlSession sqlSession) MapperProxyFactoryT mapperProxyFactory = (MapperProxyFactory)this.knownMappers.get(type); if (mapperProxyFactory == null) throw new BindingException(Type + type + is not known to the MapperRegistry.); else try return mapperProxyFactory.newInstance(sqlSession); catch (Exception var5) throw new BindingException(Error getting mapper instance. Cause: + var5, var5); MapperProxyFactory MapperProxyFactory 的作用是生成 MapperProxy（Mapper 代理对象）。 public class MapperProxyFactoryT private final ClassT mapperInterface; …… protected T newInstance(MapperProxyT mapperProxy) return Proxy.newProxyInstance(this.mapperInterface.getClassLoader(), new Class[]this.mapperInterface, mapperProxy); public T newInstance(SqlSession sqlSession) MapperProxyT mapperProxy = new MapperProxy(sqlSession, this.mapperInterface, this.methodCache); return this.newInstance(mapperProxy); 这里可以看到动态代理对接口的绑定，它的作用就是生成动态代理对象（占位），而代理的方法被放到了 MapperProxy 中。 MapperProxy MapperProxy 里，通常会生成一个 MapperMethod 对象，它是通过 cachedMapperMethod 方法对其进行初始化的，然后执行 excute 方法。 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable try return Object.class.equals(method.getDeclaringClass()) ? method.invoke(this, args) : this.cachedInvoker(method).invoke(proxy, method, args, this.sqlSession); catch (Throwable var5) throw ExceptionUtil.unwrapThrowable(var5); MapperMethod MapperMethod 里的 excute 方法，会真正去执行 sql。这里用到了命令模式，其实绕一圈，最终它还是通过 SqlSession 的实例去运行对象的 sql。 public Object execute(SqlSession sqlSession, Object[] args) Object result; Object param; …… case SELECT: if (this.method.returnsVoid() this.method.hasResultHandler()) this.executeWithResultHandler(sqlSession, args); result = null; else if (this.method.returnsMany()) result = this.executeForMany(sqlSession, args); else if (this.method.returnsMap()) result = this.executeForMap(sqlSession, args); else if (this.method.returnsCursor()) result = this.executeForCursor(sqlSession, args); else param = this.method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(this.command.getName(), param); if (this.method.returnsOptional() (result == null || !this.method.getReturnType().equals(result.getClass()))) result = Optional.ofNullable(result); break; …… 18.Mybatis 都有哪些 Executor 执行器？ Mybatis Executor类型 Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。 ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 MapString, Statement内，供下一次使用。简言之，就是重复使用 Statement 对象。 BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。 作用范围：Executor 的这些特点，都严格限制在 SqlSession 生命周期范围内。 Mybatis 中如何指定使用哪一种 Executor 执行器？ 在 Mybatis 配置文件中，在设置（settings）可以指定默认的 ExecutorType 执行器类型，也可以手动给 DefaultSqlSessionFactory 的创建 SqlSession 的方法传递 ExecutorType 类型参数，如SqlSession openSession(ExecutorType execType)。 配置默认的执行器。SIMPLE 就是普通的执行器；REUSE 执行器会重用预处理语句（prepared statements）； BATCH 执行器将重用语句并执行批量更新。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 插件19. 说说 Mybatis 的插件运行原理，如何编写一个插件？ 插件的运行原理？ Mybatis 会话的运行需要 ParameterHandler、ResultSetHandler、StatementHandler、Executor 这四大对象的配合，插件的原理就是在这四大对象调度的时候，插入一些我我们自己的代码。 MyBatis插件原理简图 Mybatis 使用 JDK 的动态代理，为目标对象生成代理对象。它提供了一个工具类Plugin，实现了InvocationHandler接口。 Plugin中调用插件方法 使用Plugin生成代理对象，代理对象在调用方法的时候，就会进入 invoke 方法，在 invoke 方法中，如果存在签名的拦截方法，插件的 intercept 方法就会在这里被我们调用，然后就返回结果。如果不存在签名方法，那么将直接反射调用我们要执行的方法。 如何编写一个插件？ 我们自己编写 MyBatis 插件，只需要实现拦截器接口 Interceptor (org.apache.ibatis. plugin Interceptor ），在实现类中对拦截对象和方法进行处理。 实现 Mybatis 的 Interceptor 接口并重写 intercept()方法 这里我们只是在目标对象执行目标方法的前后进行了打印； public class MyInterceptor implements Interceptor Properties props=null; @Override public Object intercept(Invocation invocation) throws Throwable System.out.println(before……); //如果当前代理的是一个非代理对象，那么就会调用真实拦截对象的方法 // 如果不是它就会调用下个插件代理对象的invoke方法 Object obj=invocation.proceed(); System.out.println(after……); return obj; 然后再给插件编写注解，确定要拦截的对象，要拦截的方法 @Intercepts(@Signature( type = Executor.class, //确定要拦截的对象 method = update, //确定要拦截的方法 args = MappedStatement.class,Object.class //拦截方法的参数))public class MyInterceptor implements Interceptor Properties props=null; @Override public Object intercept(Invocation invocation) throws Throwable System.out.println(before……); //如果当前代理的是一个非代理对象，那么就会调用真实拦截对象的方法 // 如果不是它就会调用下个插件代理对象的invoke方法 Object obj=invocation.proceed(); System.out.println(after……); return obj; 最后，再 MyBatis 配置文件里面配置插件 plugins plugin interceptor=xxx.MyPlugin property name=dbType,value=mysql/ /plugin/plugins 20. MyBatis 是如何进行分页的？分页插件的原理是什么？ MyBatis 是如何分页的？ MyBatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页。可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的原理是什么？ 分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，拦截 Executor 的 query 方法 在执行查询的时候，拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。 举例：select * from student，拦截 sql 后重写为：select t.* from (select * from student) t limit 0, 10 可以看一下一个大概的 MyBatis 通用分页拦截器： Mybatis-通用分页拦截器 补充21.说说 JDBC 的执行步骤？ 2024 年 03 月 19 日增补 Java 数据库连接（JDBC）是一个用于执行 SQL 语句的 Java API，它为多种关系数据库提供了统一访问的机制。使用 JDBC 操作数据库通常涉及以下步骤： 第一步，加载数据库驱动 在与数据库建立连接之前，首先需要通过Class.forName()方法加载对应的数据库驱动。这一步确保 JDBC 驱动注册到了DriverManager类中。 Class.forName(com.mysql.cj.jdbc.Driver); 第二步，建立数据库连接 使用DriverManager.getConnection()方法建立到数据库的连接。这一步需要提供数据库 URL、用户名和密码作为参数。 Connection conn = DriverManager.getConnection( jdbc:mysql://localhost:3306/databaseName, username, password); 第三步，创建Statement对象 通过建立的数据库连接对象Connection创建Statement、PreparedStatement或CallableStatement对象，用于执行 SQL 语句。 Statement stmt = conn.createStatement(); 或者创建PreparedStatement对象（预编译 SQL 语句，适用于带参数的 SQL）： PreparedStatement pstmt = conn.prepareStatement(SELECT * FROM tableName WHERE column = ?);pstmt.setString(1, value); 第四步，执行 SQL 语句 使用Statement或PreparedStatement对象执行 SQL 语句。 执行查询（SELECT）语句时，使用executeQuery()方法，它返回ResultSet对象； 执行更新（INSERT、UPDATE、DELETE）语句时，使用executeUpdate()方法，它返回一个整数表示受影响的行数。 ResultSet rs = stmt.executeQuery(SELECT * FROM tableName); 或 int affectedRows = stmt.executeUpdate(UPDATE tableName SET column = value WHERE condition); 第五步，处理结果集 如果执行的是查询操作，需要处理ResultSet对象来获取数据。 while (rs.next()) String data = rs.getString(columnName); // 处理每一行数据 第六步，关闭资源 最后，需要依次关闭ResultSet、Statement和Connection等资源，释放数据库连接等资源。 if (rs != null) rs.close();if (stmt != null) stmt.close();if (conn != null) conn.close(); 在 Java 开发中，通常会使用 JDBC 模板库（如 Spring 的 JdbcTemplate）或 ORM 框架（如 Hibernate、MyBatis、MyBatis-Plus）来简化数据库操作和资源管理。 Java 面试指南（付费）收录的京东同学 10 后端实习一面的原题：JDBC 的执行步骤 22.创建连接拿到的是什么对象？在 JDBC 的执行步骤中，创建连接后拿到的对象是java.sql.Connection对象。这个对象是 JDBC API 中用于表示数据库连接的接口，它提供了执行 SQL 语句、管理事务等一系列操作的方法。 Connection对象代表了应用程序和数据库的一个连接会话。 通过调用DriverManager.getConnection()方法并传入数据库的 URL、用户名和密码等信息来获得这个对象。 一旦获得Connection对象，就可以使用它来创建执行 SQL 语句的Statement、PreparedStatement和CallableStatement对象，以及管理事务等。 Java 面试指南（付费）收录的京东同学 10 后端实习一面的原题：创建连接拿到的是什么对象 23.Statement 与 PreparedStatement 的区别 2024 年 03 月 19 日增补 Statement和PreparedStatement都是用于执行 SQL 语句的接口，但它们之间存在几个关键的区别： ①、每次执行Statement对象的executeQuery或executeUpdate方法时，SQL 语句在数据库端都需要重新编译和执行。这适用于一次性执行的 SQL 语句。 Statement 不支持参数化查询。如果需要在 SQL 语句中插入变量，通常需要通过字符串拼接的方式来实现，这会增加 SQL 注入攻击的风险。 ②、PreparedStatement 代表预编译的 SQL 语句的对象。这意味着 SQL 语句在PreparedStatement对象创建时就被发送到数据库进行预编译。 之后，可以通过设置参数值来多次高效地执行这个 SQL 语句。这不仅减少了数据库编译 SQL 语句的开销，也提高了性能，尤其是对于重复执行的 SQL 操作。 PreparedStatement 支持参数化查询，即可以在 SQL 语句中使用问号（?）作为参数占位符。通过setXxx方法（如setString、setInt）设置参数，可以有效防止 SQL 注入。 总的来说，PreparedStatement相比Statement有着更好的性能和更高的安全性，是执行 SQL 语句的首选方式，尤其是在处理含有用户输入的动态查询时。 Java 面试指南（付费）收录的京东同学 10 后端实习一面的原题：statement 和 preparedstatement 的区别 24. 什么是 SQL 注入？如何防止 SQL 注入？SQL 注入是一种代码注入技术，通过在输入字段中插入专用的 SQL 语句，从而欺骗数据库执行恶意 SQL，以获取敏感数据、修改数据，或者删除数据等。 比如说有这样一段代码： studentId = getRequestString(studentId);lookupStudent = SELECT * FROM students WHERE studentId = + studentId 用户在输入框中输入 117 进行查询： cloudflare：SQL 查询 实际的 SQL 语句类似于： SELECT * FROM students WHERE studentId = 117 这是我们期望用户输入的正确方式。但是，如果用户输入了117 OR 1=1，那么 SQL 语句就变成了： SELECT * FROM students WHERE studentId = 117 OR 1=1 由于1=1为真，所以这个查询将返回所有学生的信息，而不仅仅是 ID 为 117 的学生。 cloudflare：SQL 注入 为了防止 SQL 注入，可以采取以下措施： ①、使用参数化查询 使用参数化查询，即使用PreparedStatement对象，通过setXxx方法设置参数值，而不是通过字符串拼接 SQL 语句。这样可以有效防止 SQL 注入。 String query = SELECT * FROM users WHERE username = ?;PreparedStatement pstmt = connection.prepareStatement(query);pstmt.setString(1, userName); // userName 是用户输入ResultSet rs = pstmt.executeQuery(); ? 是一个参数占位符，userName 是外部输入。这样即便用户输入了恶意的 SQL 语句，也只会被视为参数的一部分，不会改变查询的结构。 ②、限制用户输入 对用户输入进行验证和过滤，只允许输入预期的数据，不允许输入特殊字符或 SQL 关键字。 ③、使用 ORM 框架 比如，在 MyBatis 中，使用#占位符来代替直接拼接 SQL 语句，MyBatis 会自动进行参数化处理。 select id=selectUser resultType=User SELECT * FROM users WHERE username = #userName/select 假如 userName 传入的值是 9;DROP TABLE SYS_USER;，传入的删除表 SQL 也不会执行，因为它会被当作参数值。 SELECT * FROM users WHERE username = 9;DROP TABLE SYS_USER; Java 面试指南（付费）收录的字节跳动面经同学 13 Java 后端二面面试原题：什么是 SQL 注入，怎么避免，什么是参数化 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：如何防范sql的注入攻击呢？ 图文详解 23 道 MyBatis 面试高频题，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇 👍 面渣逆袭 Java 集合框架篇 👍 面渣逆袭 Java 并发编程篇 👍 面渣逆袭 JVM 篇 👍 面渣逆袭 Spring 篇 👍 面渣逆袭 Redis 篇 👍 面渣逆袭 MyBatis 篇 👍 面渣逆袭 MySQL 篇 👍 面渣逆袭操作系统篇 👍 面渣逆袭计算机网络篇 👍 面渣逆袭 RocketMQ 篇 👍 面渣逆袭分布式篇 👍 面渣逆袭微服务篇 👍 面渣逆袭设计模式篇 👍 面渣逆袭 Linux 篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。","tags":["基础","MyBatis"],"categories":["Java问答笔记"]},{"title":"RocketMQ学习笔记","path":"/2025/09/29/Java问答笔记/RocketMQ学习笔记/","content":"1.1 万字 45 张手绘图，详解 23 道 RocketMQ 面试高频题（让天下没有难背的八股），面渣背会这些 RocketMQ 八股文，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 基础1.为什么要使用消息队列呢？消息队列（Message Queue, MQ）是一种非常重要的中间件技术，广泛应用于分布式系统中，以提高系统的可用性、解耦能力和异步通信效率。 ①、解耦 生产者将消息放入队列，消费者从队列中取出消息，这样一来，生产者和消费者之间就不需要直接通信，生产者只管生产消息，消费者只管消费消息，这样就实现了解耦。 三分恶面渣逆袭：消息队列解耦 像 PmHub 中的任务审批，就用了 RocketMQ 来做解耦。 PmHub 的面试系列教程 ②、异步： 系统可以将那些耗时的任务放在消息队列中异步处理，从而快速响应用户的请求。比如说，用户下单后，系统可以先返回一个下单成功的消息，然后将订单信息放入消息队列中，后台系统再去处理订单信息。 三分恶面渣逆袭：消息队列异步 ③、削峰： 削峰填谷是一种常见的技术手段，用于应对系统高并发请求的瞬时流量高峰，通过消息队列，可以将瞬时的高峰流量转化为持续的低流量，从而保护系统不会因为瞬时的高流量而崩溃。 三分恶面渣逆袭：消息队列削峰 如何用RocketMQ做削峰填谷的？用户请求到达系统后，由生产者接收请求并将其转化为消息，发送到 RocketMQ 队列中。队列用来充当缓冲区，将大量请求按照顺序排队，这样就可以削减请求高峰时对后端服务的直接压力。 不仅如此，生产者通过异步方式发送消息，还可以快速响应用户请求。 消费者从 RocketMQ 队列中按照一定速率读取消息并进行处理。可以根据后端处理能力和当前负载情况动态调整消费者的消费速率，达到填谷的效果。 Java 面试指南（付费）收录的字节跳动同学 7 Java 后端实习一面的原题：有了解过 MQ 吗？ Java 面试指南（付费）收录的腾讯面经同学 24 面试原题：如何用消息队列做削峰填谷的？ Java 面试指南（付费）收录的美团面经同学 4 一面面试原题：项目里用 RocketMQ 做削峰，还有什么场景适合消息队列 Java 面试指南（付费）收录的字节跳动同学 20 测开一面的原题：RocketMQ有什么用，你一般拿来做什么 2.为什么要选择 RocketMQ? 四大消息队列对比 我们系统主要面向 C 端用户，有一定的并发量，对性能也有比较高的要求，所以选择了低延迟、吞吐量比较高，可用性比较好的 RocketMQ。 3.RocketMQ 有什么优缺点？RocketMQ 优点： 单机吞吐量：十万级 可用性：非常高，分布式架构 消息可靠性：经过参数优化配置，消息可以做到 0 丢失 功能支持：MQ 功能较为完善，还是分布式的，扩展性好 支持 10 亿级别的消息堆积，不会因为堆积导致性能下降 源码是 Java，方便结合公司自己的业务二次开发 天生为金融互联网领域而生，对于可靠性要求很高的场景，尤其是电商里面的订单扣款，以及业务削峰，在大量交易涌入时，后端可能无法及时处理的情况 RoketMQ在稳定性上可能更值得信赖，这些业务场景在阿里双 11 已经经历了多次考验，如果你的业务有上述并发场景，建议可以选择RocketMQ RocketMQ 缺点： 支持的客户端语言不多，目前是 Java 及 c++，其中 c++不成熟 没有在 MQ 核心中去实现JMS等接口，有些系统要迁移需要修改大量代码 说说你对 RocketMQ 的理解？ 牧小农：RocketMQ 的作用 RocketMQ 是阿里巴巴开源的一款分布式消息中间件，具有高吞吐量、低延迟和高可用性。其主要组件包括生产者、消费者、Broker、Topic 和队列。消息由生产者发送到 Broker，再根据路由规则存储到队列中，消费者从队列中拉取消息进行处理。适用于异步解耦和流量削峰等场景。 Java 面试指南（付费）收录的京东同学 4 云实习面试原题：说说你对RocketMQ的理解 4.消息队列有哪些消息模型？消息队列有两种模型：队列模型和发布订阅模型。 队列模型 这是最初的一种消息队列模型，对应着消息队列“发-存-收”的模型。生产者往某个队列里面发送消息，一个队列可以存储多个生产者的消息，一个队列也可以有多个消费者，但是消费者之间是竞争关系，也就是说每条消息只能被一个消费者消费。 队列模型 发布订阅模型 如果需要将一份消息数据分发给多个消费者，并且每个消费者都要求收到全量的消息。很显然，队列模型无法满足这个需求。解决的方式就是发布订阅模型。 在发布 - 订阅模型中，消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息。 发布-订阅模型 它和 “队列模式” 的异同：生产者就是发布者，队列就是主题，消费者就是订阅者，无本质区别。唯一的不同点在于：一份消息数据是否可以被多次消费。 5.那 RocketMQ 的消息模型呢？RocketMQ 使用的消息模型是标准的发布-订阅模型，在 RocketMQ 的术语表中，生产者、消费者和主题，与发布-订阅模型中的概念是完全一样的。 RocketMQ 本身的消息是由下面几部分组成： RocketMQ消息的组成 Message Message（消息）就是要传输的信息。 一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。 一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题。 Topic Topic（主题）可以看做消息的归类，它是消息的第一级类型。比如一个电商系统可以分为：交易消息、物流消息等，一条消息必须有一个 Topic 。 Topic 与生产者和消费者的关系非常松散，一个 Topic 可以有 0 个、1 个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。 一个 Topic 也可以被 0 个、1 个、多个消费者订阅。 Tag Tag（标签）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。 标签有助于保持你的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。 Group RocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响，也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。 消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。默认情况，如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息。 Message Queue Message Queue（消息队列），一个 Topic 下可以设置多个消息队列，Topic 包括多个 Message Queue ，如果一个 Consumer 需要获取 Topic 下所有的消息，就要遍历所有的 Message Queue。 RocketMQ 还有一些其它的 Queue——例如 ConsumerQueue。 Offset 在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。 也可以这么说，Queue 是一个长度无限的数组，Offset 就是下标。 RocketMQ 的消息模型中，这些就是比较关键的概念了。画张图总结一下： 6.消息的消费模式了解吗？消息消费模式有两种：Clustering（集群消费）和Broadcasting（广播消费）。 两种消费模式 默认情况下就是集群消费，这种模式下一个消费者组共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。 而广播消费消息会发给消费者组中的每一个消费者进行消费。 7.RoctetMQ 基本架构了解吗？先看图，RocketMQ 的基本架构： RocketMQ架构 RocketMQ 一共有四个部分组成：NameServer，Broker，Producer 生产者，Consumer 消费者，它们对应了：发现、发、存、收，为了保证高可用，一般每一部分都是集群部署的。 8.那能介绍一下这四部分吗？类比一下我们生活的邮政系统—— 邮政系统要正常运行，离不开下面这四个角色， 一是发信者，二 是收信者， 三是负责暂存传输的邮局， 四是负责协调各个地方邮局的管理机构。对应到 RocketMQ 中，这四个角色就是 Producer、 Consumer、 Broker 、NameServer。 RocketMQ类比邮政体系 NameServerNameServer 是一个无状态的服务器，角色类似于 Kafka 使用的 Zookeeper，但比 Zookeeper 更轻量。 特点： 每个 NameServer 结点之间是相互独立，彼此没有任何信息交互。 Nameserver 被设计成几乎是无状态的，通过部署多个结点来标识自己是一个伪集群，Producer 在发送消息前从 NameServer 中获取 Topic 的路由信息也就是发往哪个 Broker，Consumer 也会定时从 NameServer 获取 Topic 的路由信息，Broker 在启动时会向 NameServer 注册，并定时进行心跳连接，且定时同步维护的 Topic 到 NameServer。 功能主要有两个： 1、和 Broker 结点保持长连接。 2、维护 Topic 的路由信息。 Broker消息存储和中转角色，负责存储和转发消息。 Broker 内部维护着一个个 Consumer Queue，用来存储消息的索引，真正存储消息的地方是 CommitLog（日志文件）。 RocketMQ存储-图片来源官网 单个 Broker 与所有的 Nameserver 保持着长连接和心跳，并会定时将 Topic 信息同步到 NameServer，和 NameServer 的通信底层是通过 Netty 实现的。 Producer消息生产者，业务端负责发送消息，由用户自行实现和分布式部署。 Producer由用户进行分布式部署，消息由Producer通过多种负载均衡模式发送到Broker集群，发送低延时，支持快速失败。 RocketMQ 提供了三种方式发送消息：同步、异步和单向 同步发送：同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。 异步发送：异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。 单向发送：单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。 Consumer消息消费者，负责消费消息，一般是后台系统负责异步消费。 Consumer也由用户部署，支持 PUSH 和 PULL 两种消费模式，支持集群消费和广播消费，提供实时的消息订阅机制。 Pull：拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。 Push：推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但其实从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 进阶9.如何保证消息的可用性可靠性不丢失呢？消息可能在哪些阶段丢失呢？可能会在这三个阶段发生丢失：生产阶段、存储阶段、消费阶段。 所以要从这三个阶段考虑： 消息传递三阶段 生产在生产阶段，主要通过请求确认机制，来保证消息的可靠传递。 1、同步发送的时候，要注意处理响应结果和异常。如果返回响应 OK，表示消息成功发送到了 Broker，如果响应失败，或者发生其它异常，都应该重试。 2、异步发送的时候，应该在回调方法里检查，如果发送失败或者异常，都应该进行重试。 3、如果发生超时的情况，也可以通过查询日志的 API，来检查是否在 Broker 存储成功。 存储存储阶段，可以通过配置可靠性优先的 Broker 参数来避免因为宕机丢消息，简单说就是可靠性优先的场景都应该使用同步。 1、消息只要持久化到 CommitLog（日志文件）中，即使 Broker 宕机，未消费的消息也能重新恢复再消费。 2、Broker 的刷盘机制：同步刷盘和异步刷盘，不管哪种刷盘都可以保证消息一定存储在 pagecache 中（内存中），但是同步刷盘更可靠，它是 Producer 发送消息后等数据持久化到磁盘之后再返回响应给 Producer。 同步刷盘和异步刷盘-图片来源官网 3、Broker 通过主从模式来保证高可用，Broker 支持 Master 和 Slave 同步复制、Master 和 Slave 异步复制模式，生产者的消息都是发送给 Master，但是消费既可以从 Master 消费，也可以从 Slave 消费。同步复制模式可以保证即使 Master 宕机，消息肯定在 Slave 中有备份，保证了消息不会丢失。 消费消费从 Consumer 角度分析，如何保证消息被成功消费？ Consumer 保证消息成功消费的关键在于确认的时机，不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认。因为消息队列维护了消费的位置，逻辑执行失败了，没有确认，再去队列拉取消息，就还是之前的一条。 10.如何处理消息重复的问题呢？RocketMQ 可以保证消息一定投递，且不丢失，但无法保证消息不重复消费。 因此，需要在业务端做好消息的幂等性处理，或者做消息去重。 三分恶面渣逆袭：幂等和去重 幂等性是指一个操作可以执行多次而不会产生副作用，即无论执行多少次，结果都是相同的。可以在业务逻辑中加入检查逻辑，确保同一消息多次消费不会产生副作用。 例如，在支付场景下，消费者消费扣款的消息，对一笔订单执行扣款操作，金额为100元。 如果因网络不稳定等原因导致扣款消息重复投递，消费者重复消费了该扣款消息，但最终的业务结果要保证只扣款一次，金额为100元。如果扣款操作是符合要求的，那么就可以认为整个消费过程实现了消息幂等。 消息去重，是指在消费者消费消息之前，先检查一下是否已经消费过这条消息，如果消费过了，就不再消费。 业务端可以通过一个专门的表来记录已经消费过的消息 ID，每次消费消息之前，先查询一下这个表，如果已经存在，就不再消费。 public void processMessage(String messageId, String message) if (!isMessageProcessed(messageId)) // 处理消息 markMessageAsProcessed(messageId); private boolean isMessageProcessed(String messageId) // 查询去重表，检查消息ID是否存在private void markMessageAsProcessed(String messageId) // 将消息ID插入去重表 如何保证消息的幂等性？ 勇哥：消费幂等 首先，消息必须携带业务唯一标识，可以通过雪花算法生成全局唯一 ID。 Message msg = new Message(TOPIC /* Topic */, TAG /* Tag */, (Hello RocketMQ + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ );message.setKey(ORDERID_100); // 订单编号SendResult sendResult = producer.send(message); 其次，在消费者接收到消息后，判断 Redis 中是否存在该业务主键的标志位，若存在标志位，则认为消费成功，否则执行业务逻辑，执行完成后，在缓存中添加标志位。 public ConsumeConcurrentlyStatus consumeMessage(ListMessageExt msgs, ConsumeConcurrentlyContext context) try for (MessageExt messageExt : msgs) String bizKey = messageExt.getKeys(); // 唯一业务主键 //1. 判断是否存在标志 if(redisTemplate.hasKey(RedisKeyConstants.WAITING_SEND_LOCK + bizKey)) continue; //2. 执行业务逻辑 //TODO do business //3. 设置标志位 redisTemplate.opsForValue().set(RedisKeyConstants.WAITING_SEND_LOCK + bizKey, 1, 72, TimeUnit.HOURS); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; catch (Exception e) logger.error(consumeMessage error: , e); return ConsumeConcurrentlyStatus.RECONSUME_LATER; 然后，利用数据库的唯一索引来防止业务的重复插入。 CREATE TABLE `t_order` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `order_id` varchar(64) NOT NULL COMMENT 订单编号, `order_name` varchar(64) NOT NULL COMMENT 订单名称, PRIMARY KEY (`id`), UNIQUE KEY `order_id` (`order_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=订单表; 最后，在数据库表中使用版本号，通过乐观锁机制来保证幂等性。每次更新操作时检查版本号是否一致，只有一致时才执行更新并递增版本号。如果版本号不一致，则说明操作已被执行过，拒绝重复操作。 public void updateRecordWithOptimisticLock(int id, String newValue, int expectedVersion) int updatedRows = jdbcTemplate.update( UPDATE records SET value = ?, version = version + 1 WHERE id = ? AND version = ?, newValue, id, expectedVersion ); if (updatedRows == 0) throw new OptimisticLockingFailureException(Record has been modified by another transaction); 或者悲观锁机制，通过数据库的锁机制来保证幂等性。 public void updateRecordWithPessimisticLock(int id) jdbcTemplate.queryForObject(SELECT * FROM records WHERE id = ? FOR UPDATE, id); jdbcTemplate.update(UPDATE records SET value = ? WHERE id = ?, newValue, id); 雪花算法了解吗？雪花算法是由 Twitter 开发的一种分布式唯一 ID 生成算法。 技术派教程：雪花算法 雪花算法以 64 bit 来存储组成 ID 的4 个部分： 最高位占1 bit，始终为 0，表示正数。 中位占 41 bit，值为毫秒级时间戳； 中下位占 10 bit，机器 ID（包括数据中心 ID 和机器 ID），可以支持 1024 个节点。 末位占 12 bit，值为当前毫秒内生成的不同的自增序列，值的上限为 4096； 目前雪花算法的实现比较多，可以直接使用 Hutool 工具类库中的 IdUtil.getSnowflake() 方法来获取雪花 ID。 long id = IdUtil.getSnowflakeNextId(); Java 面试指南（付费）收录的京东同学 4 云实习面试原题：如何处理消息重复消费的问题？如何保证幂等性？雪花算法了解吗？ 11.怎么处理消息积压？发生了消息积压，这时候就得想办法赶紧把积压的消息消费完，就得考虑提高消费能力，一般有两种办法： 消息积压处理 消费者扩容：如果当前 Topic 的 Message Queue 的数量大于消费者数量，就可以对消费者进行扩容，增加消费者，来提高消费能力，尽快把积压的消息消费玩。 消息迁移 Queue 扩容：如果当前 Topic 的 Message Queue 的数量小于或者等于消费者数量，这种情况，再扩容消费者就没什么用，就得考虑扩容 Message Queue。可以新建一个临时的 Topic，临时的 Topic 多设置一些 Message Queue，然后先用一些消费者把消费的数据丢到临时的 Topic，因为不用业务处理，只是转发一下消息，还是很快的。接下来用扩容的消费者去消费新的 Topic 里的数据，消费完了之后，恢复原状。 消息迁移扩容消费 12.顺序消息如何实现？RocketMQ 实现顺序消息的关键在于保证消息生产和消费过程中严格的顺序控制，即确保同一业务的消息按顺序发送到同一个队列中，并由同一个消费者线程按顺序消费。 三分恶面渣逆袭：顺序消息 局部顺序消息如何实现？局部顺序消息保证在某个逻辑分区或业务逻辑下的消息顺序，例如同一个订单或用户的消息按顺序消费，而不同订单或用户之间的顺序不做保证。 三分恶面渣逆袭：部分顺序消息 全局顺序消息如何实现？全局顺序消息保证消息在整个系统范围内的严格顺序，即消息按照生产的顺序被消费。 可以将所有消息发送到一个单独的队列中，确保所有消息按生产顺序发送和消费。 三分恶面渣逆袭：全局顺序消息 Java 面试指南（付费）收录的京东面经同学 2 后端面试原题：说说mq原理，怎么保证消息接受顺序？ Java 面试指南（付费）收录的收钱吧面经同学 1 Java 后端一面面试原题：RocketMQ的顺序消息？ 13.如何实现消息过滤？有两种方案： 一种是在 Broker 端按照 Consumer 的去重逻辑进行过滤，这样做的好处是避免了无用的消息传输到 Consumer 端，缺点是加重了 Broker 的负担，实现起来相对复杂。 另一种是在 Consumer 端过滤，比如按照消息设置的 tag 去重，这样的好处是实现起来简单，缺点是有大量无用的消息到达了 Consumer 端只能丢弃不处理。 一般采用 Cosumer 端过滤，如果希望提高吞吐量，可以采用 Broker 过滤。 对消息的过滤有三种方式： 消息过滤 根据 Tag 过滤：这是最常见的一种，用起来高效简单 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(CID_EXAMPLE);consumer.subscribe(TOPIC, TAGA || TAGB || TAGC); SQL 表达式过滤：SQL 表达式过滤更加灵活 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(please_rename_unique_group_name_4);// 只有订阅的消息有这个属性a, a =0 and a = 3consumer.subscribe(TopicTest, MessageSelector.bySql(a between 0 and 3);consumer.registerMessageListener(new MessageListenerConcurrently() @Override public ConsumeConcurrentlyStatus consumeMessage(ListMessageExt msgs, ConsumeConcurrentlyContext context) return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; );consumer.start(); Filter Server 方式：最灵活，也是最复杂的一种方式，允许用户自定义函数进行过滤 14.延时消息了解吗？电商的订单超时自动取消，就是一个典型的利用延时消息的例子，用户提交了一个订单，就可以发送一个延时消息，1h 后去检查这个订单的状态，如果还是未付款就取消订单释放库存。 RocketMQ 是支持延时消息的，只需要在生产消息的时候设置消息的延时级别： // 实例化一个生产者来产生延时消息DefaultMQProducer producer = new DefaultMQProducer(ExampleProducerGroup);// 启动生产者producer.start();int totalMessagesToSend = 100;for (int i = 0; i totalMessagesToSend; i++) Message message = new Message(TestTopic, (Hello scheduled message + i).getBytes()); // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel) message.setDelayTimeLevel(3); // 发送消息 producer.send(message); 但是目前 RocketMQ 支持的延时级别是有限的： private String messageDelayLevel = 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h; RocketMQ 怎么实现延时消息的？简单，八个字：临时存储+定时任务。 Broker 收到延时消息了，会先发送到主题（SCHEDULE_TOPIC_XXXX）的相应时间段的 Message Queue 中，然后通过一个定时任务轮询这些队列，到期后，把消息投递到目标 Topic 的队列中，然后消费者就可以正常消费这些消息。 延迟消息处理流程-图片来源见水印 15.怎么实现分布式消息事务的？半消息？半消息：是指暂时还不能被 Consumer 消费的消息，Producer 成功发送到 Broker 端的消息，但是此消息被标记为 “暂不可投递” 状态，只有等 Producer 端执行完本地事务后经过二次确认了之后，Consumer 才能消费此条消息。 依赖半消息，可以实现分布式消息事务，其中的关键在于二次确认以及消息回查： RocketMQ实现消息事务 1、Producer 向 broker 发送半消息 2、Producer 端收到响应，消息发送成功，此时消息是半消息，标记为 “不可投递” 状态，Consumer 消费不了。 3、Producer 端执行本地事务。 4、正常情况本地事务执行完成，Producer 向 Broker 发送 CommitRollback，如果是 Commit，Broker 端将半消息标记为正常消息，Consumer 可以消费，如果是 Rollback，Broker 丢弃此消息。 5、异常情况，Broker 端迟迟等不到二次确认。在一定时间后，会查询所有的半消息，然后到 Producer 端查询半消息的执行情况。 6、Producer 端查询本地事务的状态 7、根据事务的状态提交 commitrollback 到 broker 端。（5，6，7 是消息回查） 8、消费者段消费到消息之后，执行本地事务。 16.死信队列知道吗？死信队列用于存储那些无法被正常处理的消息，这些消息被称为死信（Dead Letter）。 阿里云官方文档：死信队列 产生死信的原因是，消费者在处理消息时发生异常，且达到了最大重试次数。当消费失败的原因排查并解决后，可以重发这些死信消息，让消费者重新消费；如果暂时无法处理，为避免到期后死信消息被删除，可以先将死信消息导出并进行保存。 Java 面试指南（付费）收录的京东同学 4 云实习面试原题：说说 RocketMQ 的死信队列 17.如何保证 RocketMQ 的高可用？NameServer 因为是无状态，且不相互通信的，所以只要集群部署就可以保证高可用。 NameServer集群 RocketMQ 的高可用主要是在体现在 Broker 的读和写的高可用，Broker 的高可用是通过集群和主从实现的。 Broker集群、主从示意图 Broker 可以配置两种角色：Master 和 Slave，Master 角色的 Broker 支持读和写，Slave 角色的 Broker 只支持读，Master 会向 Slave 同步消息。 也就是说 Producer 只能向 Master 角色的 Broker 写入消息，Cosumer 可以从 Master 和 Slave 角色的 Broker 读取消息。 Consumer 的配置文件中，并不需要设置是从 Master 读还是从 Slave 读，当 Master 不可用或者繁忙的时候， Consumer 的读请求会被自动切换到从 Slave。有了自动切换 Consumer 这种机制，当一个 Master 角色的机器出现故障后，Consumer 仍然可以从 Slave 读取消息，不影响 Consumer 读取消息，这就实现了读的高可用。 如何达到发送端写的高可用性呢？在创建 Topic 的时候，把 Topic 的多个 Message Queue 创建在多个 Broker 组上（相同 Broker 名称，不同 brokerId 机器组成 Broker 组），这样当 Broker 组的 Master 不可用后，其他组 Master 仍然可用， Producer 仍然可以发送消息 RocketMQ 目前还不支持把 Slave 自动转成 Master ，如果机器资源不足，需要把 Slave 转成 Master ，则要手动停止 Slave 色的 Broker ，更改配置文件，用新的配置文件启动 Broker。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 原理18.说一下 RocketMQ 的整体工作流程？简单来说，RocketMQ 是一个分布式消息队列，也就是消息队列+分布式系统。 作为消息队列，它是发-存-收的一个模型，对应的就是 Producer、Broker、Cosumer；作为分布式系统，它要有服务端、客户端、注册中心，对应的就是 Broker、ProducerConsumer、NameServer 所以我们看一下它主要的工作流程：RocketMQ 由 NameServer 注册中心集群、Producer 生产者集群、Consumer 消费者集群和若干 Broker（RocketMQ 进程）组成： Broker 在启动的时候去向所有的 NameServer 注册，并保持长连接，每 30s 发送一次心跳 Producer 在发送消息的时候从 NameServer 获取 Broker 服务器地址，根据负载均衡算法选择一台服务器来发送消息 Conusmer 消费消息的时候同样从 NameServer 获取 Broker 地址，然后主动拉取消息来消费 RocketMQ整体工作流程 19.为什么 RocketMQ 不使用 Zookeeper 作为注册中心呢？Kafka 我们都知道采用 Zookeeper 作为注册中心——当然也开始逐渐去 Zookeeper，RocketMQ 不使用 Zookeeper 其实主要可能从这几方面来考虑： 基于可用性的考虑，根据 CAP 理论，同时最多只能满足两个点，而 Zookeeper 满足的是 CP，也就是说 Zookeeper 并不能保证服务的可用性，Zookeeper 在进行选举的时候，整个选举的时间太长，期间整个集群都处于不可用的状态，而这对于一个注册中心来说肯定是不能接受的，作为服务发现来说就应该是为可用性而设计。 基于性能的考虑，NameServer 本身的实现非常轻量，而且可以通过增加机器的方式水平扩展，增加集群的抗压能力，而 Zookeeper 的写是不可扩展的，Zookeeper 要解决这个问题只能通过划分领域，划分多个 Zookeeper 集群来解决，首先操作起来太复杂，其次这样还是又违反了 CAP 中的 A 的设计，导致服务之间是不连通的。 持久化的机制来带的问题，ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，而对于一个简单的服务发现的场景来说，这其实没有太大的必要，这个实现方案太重了。而且本身存储的数据应该是高度定制化的。 消息发送应该弱依赖注册中心，而 RocketMQ 的设计理念也正是基于此，生产者在第一次发送消息的时候从 NameServer 获取到 Broker 地址后缓存到本地，如果 NameServer 整个集群不可用，短时间内对于生产者和消费者并不会产生太大影响。 20.Broker 是怎么保存数据的呢？RocketMQ 主要的存储文件包括 CommitLog 文件、ConsumeQueue 文件、Indexfile 文件。 消息存储的整体的设计： 消息存储整体设计-来源官网 CommitLog：消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认 1G, 文件名长度为 20 位，左边补零，剩余为起始偏移量，比如 00000000000000000000 代表了第一个文件，起始偏移量为 0，文件大小为 1G1073741824；当第一个文件写满了，第二个文件为 00000000001073741824，起始偏移量为 1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。 CommitLog 文件保存于${Rocket_Home}storecommitlog 目录中，从图中我们可以明显看出来文件名的偏移量，每个文件默认 1G，写满后自动生成一个新的文件。 CommitLog ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能，由于 RocketMQ 是基于主题 topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 topic 检索消息是非常低效的。 Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset，消息大小 size 和消息 Tag 的 HashCode 值。 ConsumeQueue 文件可以看成是基于 Topic 的 CommitLog 索引文件，故 ConsumeQueue 文件夹的组织方式如下：topicqueuefile 三层组织结构，具体存储路径为：$HOMEstoreconsumequeue{topic}{queueId}{fileName}。同样 ConsumeQueue 文件采取定长设计，每一个条目共 20 个字节，分别为 8 字节的 CommitLog 物理偏移量、4 字节的消息长度、8 字节 tag hashcode，单个文件由 30W 个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue 文件大小约 5.72M； Comsumer Queue IndexFile：IndexFile（索引文件）提供了一种可以通过 key 或时间区间来查询消息的方法。Index 文件的存储位置是： {fileName}，文件名 fileName 是以创建时的时间戳命名的，固定的单个 IndexFile 文件大小约为 400M，一个 IndexFile 可以保存 2000W 个索引，IndexFile 的底层存储设计为在文件系统中实现 HashMap 结构，故 RocketMQ 的索引文件其底层实现为 hash 索引。 IndexFile文件示意图 总结一下：RocketMQ 采用的是混合型的存储结构，即为 Broker 单个实例下所有的队列共用一个日志数据文件（即为 CommitLog）来存储。 RocketMQ 的混合型存储结构(多个 Topic 的消息实体内容都存储于一个 CommitLog 中)针对 Producer 和 Consumer 分别采用了数据和索引部分相分离的存储结构，Producer 发送消息至 Broker 端，然后 Broker 端使用同步或者异步的方式对消息刷盘持久化，保存至 CommitLog 中。 只要消息被刷盘持久化至磁盘文件 CommitLog 中，那么 Producer 发送的消息就不会丢失。正因为如此，Consumer 也就肯定有机会去消费这条消息。当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，如果一个消息拉取请求未拉取到消息，Broker 允许等待 30s 的时间，只要这段时间内有新消息到达，将直接返回给消费端。 这里，RocketMQ 的具体做法是，使用 Broker 端的后台服务线程—ReputMessageService 不停地分发请求并异步构建 ConsumeQueue（逻辑消费队列）和 IndexFile（索引文件）数据。 21.说说 RocketMQ 怎么对文件进行读写的？RocketMQ 对文件的读写巧妙地利用了操作系统的一些高效文件读写方式——PageCache、顺序读写、零拷贝。 PageCache、顺序读取 在 RocketMQ 中，ConsumeQueue 逻辑消费队列存储的数据较少，并且是顺序读取，在 page cache 机制的预读取作用下，Consume Queue 文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。而对于 CommitLog 消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取，严重影响性能。如果选择合适的系统 IO 调度算法，比如设置调度算法为“Deadline”（此时块存储采用 SSD 的话），随机读的性能也会有所提升。 页缓存（PageCache)是 OS 对文件的缓存，用于加速对文件的读写。一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于 OS 使用 PageCache 机制对读写访问操作进行了性能优化，将一部分的内存用作 PageCache。对于数据的写入，OS 会先写入至 Cache 内，随后通过异步的方式由 pdflush 内核线程将 Cache 内的数据刷盘至物理磁盘上。对于数据的读取，如果一次读取文件时出现未命中 PageCache 的情况，OS 从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取。 零拷贝 另外，RocketMQ 主要通过 MappedByteBuffer 对文件进行读写操作。其中，利用了 NIO 中的 FileChannel 模型将磁盘上的物理文件直接映射到用户态的内存地址中（这种 Mmap 的方式减少了传统 IO，将磁盘文件数据在操作系统内核地址空间的缓冲区，和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（正因为需要使用内存映射机制，故 RocketMQ 的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存）。 说说什么是零拷贝?在操作系统中，使用传统的方式，数据需要经历几次拷贝，还要经历用户态内核态切换。 传统文件传输示意图-来源《图解操作系统》 从磁盘复制数据到内核态内存； 从内核态内存复制到用户态内存； 然后从用户态内存复制到网络驱动的内核态内存； 最后是从网络驱动的内核态内存复制到网卡中进行传输。 所以，可以通过零拷贝的方式，减少用户态与内核态的上下文切换和内存拷贝的次数，用来提升 IO 的性能。零拷贝比较常见的实现方式是mmap，这种机制在 Java 中是通过 MappedByteBuffer 实现的。 mmap示意图-来源《图解操作系统》 22.消息刷盘怎么实现的呢？RocketMQ 提供了两种刷盘策略：同步刷盘和异步刷盘 同步刷盘：在消息达到 Broker 的内存之后，必须刷到 commitLog 日志文件中才算成功，然后返回 Producer 数据已经发送成功。 异步刷盘：异步刷盘是指消息达到 Broker 内存后就返回 Producer 数据已经发送成功，会唤醒一个线程去将数据持久化到 CommitLog 日志文件中。 Broker 在消息的存取时直接操作的是内存（内存映射文件），这可以提供系统的吞吐量，但是无法避免机器掉电时数据丢失，所以需要持久化到磁盘中。 刷盘的最终实现都是使用NIO中的 MappedByteBuffer.force() 将映射区的数据写入到磁盘，如果是同步刷盘的话，在Broker把消息写到CommitLog映射区后，就会等待写入完成。 异步而言，只是唤醒对应的线程，不保证执行的时机，流程如图所示。 异步刷盘 23.能说下 RocketMQ 的负载均衡是如何实现的？RocketMQ 中的负载均衡都在 Client 端完成，具体来说的话，主要可以分为 Producer 端发送消息时候的负载均衡和 Consumer 端订阅消息的负载均衡。 Producer 的负载均衡Producer 端在发送消息的时候，会先根据 Topic 找到指定的 TopicPublishInfo，在获取了 TopicPublishInfo 路由信息后，RocketMQ 的客户端在默认方式下 selectOneMessageQueue()方法会从 TopicPublishInfo 中的 messageQueueList 中选择一个队列（MessageQueue）进行发送消息。具这里有一个 sendLatencyFaultEnable 开关变量，如果开启，在随机递增取模的基础上，再过滤掉 not available 的 Broker 代理。 所谓的”latencyFaultTolerance”，是指对之前失败的，按一定的时间做退避。例如，如果上次请求的 latency 超过 550Lms，就退避 3000Lms；超过 1000L，就退避 60000L；如果关闭，采用随机递增取模的方式选择一个队列（MessageQueue）来发送消息，latencyFaultTolerance 机制是实现消息发送高可用的核心关键所在。 Consumer 的负载均衡在 RocketMQ 中，Consumer 端的两种消费模式（PushPull）都是基于拉模式来获取消息的，而在 Push 模式只是对 pull 模式的一种封装，其本质实现为消息拉取线程在从服务器拉取到一批消息后，然后提交到消息消费线程池后，又“马不停蹄”的继续向服务器再次尝试拉取消息。如果未拉取到消息，则延迟一下又继续拉取。在两种基于拉模式的消费方式（PushPull）中，均需要 Consumer 端知道从 Broker 端的哪一个消息队列中去获取消息。因此，有必要在 Consumer 端来做负载均衡，即 Broker 端中多个 MessageQueue 分配给同一个 ConsumerGroup 中的哪些 Consumer 消费。 Consumer 端的心跳包发送 在 Consumer 启动后，它就会通过定时任务不断地向 RocketMQ 集群中的所有 Broker 实例发送心跳包（其中包含了，消息消费分组名称、订阅关系集合、消息通信模式和客户端 id 的值等信息）。Broker 端在收到 Consumer 的心跳消息后，会将它维护在 ConsumerManager 的本地缓存变量—consumerTable，同时并将封装后的客户端网络通道信息保存在本地缓存变量—channelInfoTable 中，为之后做 Consumer 端的负载均衡提供可以依据的元数据信息。 Consumer 端实现负载均衡的核心类—RebalanceImpl 在 Consumer 实例的启动流程中的启动 MQClientInstance 实例部分，会完成负载均衡服务线程—RebalanceService 的启动（每隔 20s 执行一次）。 通过查看源码可以发现，RebalanceService 线程的 run()方法最终调用的是 RebalanceImpl 类的 rebalanceByTopic()方法，这个方法是实现 Consumer 端负载均衡的核心。 rebalanceByTopic()方法会根据消费者通信类型为“广播模式”还是“集群模式”做不同的逻辑处理。这里主要来看下集群模式下的主要处理流程： (1) 从 rebalanceImpl 实例的本地缓存变量—topicSubscribeInfoTable 中，获取该 Topic 主题下的消息消费队列集合（mqSet）； (2) 根据 topic 和 consumerGroup 为参数调用 mQClientFactory.findConsumerIdList()方法向 Broker 端发送通信请求，获取该消费组下消费者 Id 列表； (3) 先对 Topic 下的消息消费队列、消费者 Id 排序，然后用消息队列分配策略算法（默认为：消息队列的平均分配算法），计算出待拉取的消息队列。这里的平均分配算法，类似于分页的算法，将所有 MessageQueue 排好序类似于记录，将所有消费端 Consumer 排好序类似页数，并求出每一页需要包含的平均 size 和每个页面记录的范围 range，最后遍历整个 range 而计算出当前 Consumer 端应该分配到的的 MessageQueue。 Cosumer分配 (4) 然后，调用 updateProcessQueueTableInRebalance()方法，具体的做法是，先将分配到的消息队列集合（mqSet）与 processQueueTable 做一个过滤比对。 上图中 processQueueTable 标注的红色部分，表示与分配到的消息队列集合 mqSet 互不包含。将这些队列设置 Dropped 属性为 true，然后查看这些队列是否可以移除出 processQueueTable 缓存变量，这里具体执行 removeUnnecessaryMessageQueue()方法，即每隔 1s 查看是否可以获取当前消费处理队列的锁，拿到的话返回 true。如果等待 1s 后，仍然拿不到当前消费处理队列的锁则返回 false。如果返回 true，则从 processQueueTable 缓存变量中移除对应的 Entry； 上图中 processQueueTable 的绿色部分，表示与分配到的消息队列集合 mqSet 的交集。判断该 ProcessQueue 是否已经过期了，在 Pull 模式的不用管，如果是 Push 模式的，设置 Dropped 属性为 true，并且调用 removeUnnecessaryMessageQueue()方法，像上面一样尝试移除 Entry； 最后，为过滤后的消息队列集合（mqSet）中的每个 MessageQueue 创建一个 ProcessQueue 对象并存入 RebalanceImpl 的 processQueueTable 队列中（其中调用 RebalanceImpl 实例的 computePullFromWhere(MessageQueue mq)方法获取该 MessageQueue 对象的下一个进度消费值 offset，随后填充至接下来要创建的 pullRequest 对象属性中），并创建拉取请求对象—pullRequest 添加到拉取列表—pullRequestList 中，最后执行 dispatchPullRequest()方法，将 Pull 消息的请求对象 PullRequest 依次放入 PullMessageService 服务线程的阻塞队列 pullRequestQueue 中，待该服务线程取出后向 Broker 端发起 Pull 消息的请求。其中，可以重点对比下，RebalancePushImpl 和 RebalancePullImpl 两个实现类的 dispatchPullRequest()方法不同，RebalancePullImpl 类里面的该方法为空。 消息消费队列在同一消费组不同消费者之间的负载均衡，其核心设计理念是在一个消息消费队列在同一时间只允许被同一消费组内的一个消费者消费，一个消息消费者能同时消费多个消息队列。 24.RocketMQ 消息长轮询了解吗？所谓的长轮询，就是 Consumer 拉取消息，如果对应的 Queue 如果没有数据，Broker 不会立即返回，而是把 PullReuqest hold 起来，等待 queue 有了消息后，或者长轮询阻塞时间到了，再重新处理该 queue 上的所有 PullRequest。 长轮询简单示意图 PullMessageProcessor#processRequest //如果没有拉到数据case ResponseCode.PULL_NOT_FOUND:// broker 和 consumer 都允许 suspend，默认开启if (brokerAllowSuspend hasSuspendFlag) long pollingTimeMills = suspendTimeoutMillisLong; if (!this.brokerController.getBrokerConfig().isLongPollingEnable()) pollingTimeMills = this.brokerController.getBrokerConfig().getShortPollingTimeMills(); String topic = requestHeader.getTopic(); long offset = requestHeader.getQueueOffset(); int queueId = requestHeader.getQueueId(); //封装一个PullRequest PullRequest pullRequest = new PullRequest(request, channel, pollingTimeMills, this.brokerController.getMessageStore().now(), offset, subscriptionData, messageFilter); //把PullRequest挂起来 this.brokerController.getPullRequestHoldService().suspendPullRequest(topic, queueId, pullRequest); response = null; break; 挂起的请求，有一个服务线程会不停地检查，看 queue 中是否有数据，或者超时。 PullRequestHoldService#run() @Overridepublic void run() log.info( service started, this.getServiceName()); while (!this.isStopped()) try if (this.brokerController.getBrokerConfig().isLongPollingEnable()) this.waitForRunning(5 * 1000); else this.waitForRunning(this.brokerController.getBrokerConfig().getShortPollingTimeMills()); long beginLockTimestamp = this.systemClock.now(); //检查hold住的请求 this.checkHoldRequest(); long costTime = this.systemClock.now() - beginLockTimestamp; if (costTime 5 * 1000) log.info([NOTIFYME] check hold request cost ms., costTime); catch (Throwable e) log.warn(this.getServiceName() + service has exception. , e); log.info( service end, this.getServiceName()); 图文详解 RocketMQ 面试高频题，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇 👍 面渣逆袭 Java 集合框架篇 👍 面渣逆袭 Java 并发编程篇 👍 面渣逆袭 JVM 篇 👍 面渣逆袭 Spring 篇 👍 面渣逆袭 Redis 篇 👍 面渣逆袭 MyBatis 篇 👍 面渣逆袭 MySQL 篇 👍 面渣逆袭操作系统篇 👍 面渣逆袭计算机网络篇 👍 面渣逆袭 RocketMQ 篇 👍 面渣逆袭分布式篇 👍 面渣逆袭微服务篇 👍 面渣逆袭设计模式篇 👍 面渣逆袭 Linux 篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。","tags":["基础","RocketMQ"],"categories":["Java问答笔记"]},{"title":"分布式学习笔记","path":"/2025/09/29/Java问答笔记/分布式学习笔记/","content":"8800 字 25 张手绘图，详解 12 道分布式面试高频题（让天下没有难背的八股），面渣背会这些八股文，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 分布式理论1. 说说 CAP 原则？CAP 原则又称 CAP 定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这 3 个基本需求，最多只能同时满足其中的 2 个。 选项 描述 Consistency（一致性） 指数据在多个副本之间能够保持一致的特性（严格的一致性） Availability（可用性） 指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据） Partition tolerance（分区容错性） 分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障 2. 为什么 CAP 不可兼得呢？首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。 那么分区容错性（P）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷。 那么满足分区容错的基础上，能不能同时满足一致性和可用性？ 假如现在有两个分区N1和N2，N1 和 N2 分别有不同的分区存储 D1 和 D2，以及不同的服务 S1 和 S2。 在满足一致性 的时候，N1 和 N2 的数据要求值一样的，D1D2。 在满足可用性的时候，无论访问 N1 还是 N2，都能获取及时的响应。 假如现在有这样的场景： 用户访问了 N1，修改了 D1 的数据。 用户再次访问，请求落在了 N2。此时 D1 和 D2 的数据不一致。 接下来： 保证一致性：此时 D1 和 D2 数据不一致，要保证一致性就不能返回不一致的数据，可用性无法保证。 保证可用性：立即响应，可用性得到了保证，但是此时响应的数据和 D1 不一致，一致性无法保证。 所以，可以看出，分区容错的前提下，一致性和可用性是矛盾的。 3. CAP 对应的模型和应用？CA without P 理论上放弃 P（分区容错性），则 C（强一致性）和 A（可用性）是可以保证的。实际上分区是不可避免的，严格上 CA 指的是允许分区后各子系统依然保持 CA。 CA 模型的常见应用： 集群数据库 xFS 文件系统 CP without A 放弃 A（可用），相当于每个请求都需要在 Server 之间强一致，而 P（分区）会导致同步时间无限延长，如此 CP 也是可以保证的。很多传统的数据库分布式事务都属于这种模式。 CP 模型的常见应用： 分布式数据库 分布式锁 AP wihtout C 要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的 NoSQL 都属于此类。 AP 模型常见应用： Web 缓存 DNS 举个大家更熟悉的例子，像我们熟悉的注册中心ZooKeeper、Eureka、Nacos中： ZooKeeper 保证的是 CP Eureka 保证的则是 AP Nacos 不仅支持 CP 也支持 AP 4. BASE 理论了解吗？BASE（Basically Available、Soft state、Eventual consistency）是基于 CAP 理论逐步演化而来的，核心思想是即便不能达到强一致性（Strong consistency），也可以根据应用特点采用适当的方式来达到最终一致性（Eventual consistency）的效果。 BASE 的主要含义： Basically Available（基本可用） 什么是基本可用呢？假设系统出现了不可预知的故障，但还是能用，只是相比较正常的系统而言，可能会有响应时间上的损失，或者功能上的降级。 Soft State（软状态） 什么是硬状态呢？要求多个节点的数据副本都是一致的，这是一种“硬状态”。 软状态也称为弱状态，相比较硬状态而言，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。 Eventually Consistent（最终一致性） 上面说了软状态，但是不应该一直都是软状态。在一定时间后，应该到达一个最终的状态，保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间取决于网络延时、系统负载、数据复制方案设计等等因素。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 分布式锁单体时代，可以直接用本地锁来实现对竞争资源的加锁，分布式环境下就要用到分布式锁了。 5. 有哪些分布式锁的实现方案呢？常见的分布式锁实现方案有三种：MySQL分布式锁、ZooKepper分布式锁、Redis分布式锁。 5.1 MySQL 分布式锁如何实现呢？用数据库实现分布式锁比较简单，就是创建一张锁表，数据库对字段作唯一性约束。 加锁的时候，在锁表中增加一条记录即可；释放锁的时候删除记录就行。 如果有并发请求同时提交到数据库，数据库会保证只有一个请求能够得到锁。 这种属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、高性能的场景中用的不多。 5.2 ZooKeeper 如何实现分布式锁？ZooKeeper 也是常见分布式锁实现方法。 ZooKeeper 的数据节点和文件目录类似，例如有一个 lock 节点，在此节点下建立子节点是可以保证先后顺序的，即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。 所以我们可以用此特性实现分布式锁。以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，每个服务在目录下创建节点，如果它的节点，序号在目录下最小，那么就获取到锁，否则等待。释放锁，就是删除服务创建的节点。 ZK 实际上是一个比较重的分布式组件，实际上应用没那么多了，所以用 ZK 实现分布式锁，其实相对也比较少。 5.3 Redis 怎么实现分布式锁？Redis 实现分布式锁，是当前应用最广泛的分布式锁实现方式。 Redis 执行命令是单线程的，Redis 实现分布式锁就是利用这个特性。 实现分布式锁最简单的一个命令：setNx(set if not exist)，如果不存在则更新： setNx resourceName value 加锁了之后如果机器宕机，那我这个锁就无法释放，所以需要加入过期时间，而且过期时间需要和 setNx 同一个原子操作，在 Redis2.8 之前需要用 lua 脚本，但是 redis2.8 之后 redis 支持 nx 和 ex 操作是同一原子操作。 set resourceName value ex 5 nx Redission 当然，一般生产中都是使用 Redission 客户端，非常良好地封装了分布式锁的 api，而且支持 RedLock。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 分布式事务6.什么是分布式事务?在分布式环境下，会涉及到多个数据库，比如说支付库、商品库、订单库。因此要保证跨服务的事务一致性就变得非常复杂。 三分恶面渣逆袭：多个数据库 分布式事务其实就是将单一库的事务概念扩大到了多库，目的是为了保证跨服的数据一致性。 7.分布式事务有哪些常见的实现方案？分布式事务的实现方式主要包括： 二阶段提交（2PC）：通过准备和提交阶段保证一致性，但性能较差。 三阶段提交（3PC）：在 2PC 的基础上增加了一个超时机制，降低了阻塞，但依旧存在数据不一致的风险。 TCC：根据业务逻辑拆分为 Try、Confirm 和 Cancel 三个阶段，适合锁定资源的业务场景。 本地消息表：在数据库中存储事务事件，通过定时任务处理消息。 基于 MQ 的分布式事务：通过消息队列来实现异步确保，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。 7.1 说说 2PC 两阶段提交？说到 2PC，就不得先说分布式事务中的 XA 协议。 在这个协议里，有三个角色： AP（Application）：应用系统（服务） TM（Transaction Manager）：事务管理器（全局事务管理） RM（Resource Manager）：资源管理器（数据库） XA 协议采用两阶段提交方式来管理分布式事务。XA 接口提供资源管理器与事务管理器之间进行通信的标准接口。 两阶段提交的思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是回滚操作。 准备阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交 提交阶段：事务协调器要求每个数据库提交数据，或者回滚数据。 优点：尽量保证了数据的强一致，实现成本较低，在各大主流数据库都有自己实现，对于 MySQL 是从 5.5 开始支持。 缺点: 单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。 同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。 数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。 7.2 3PC（三阶段提交）了解吗？三阶段提交（3PC）是二阶段提交（2PC）的一种改进版本 ，为解决两阶段提交协议的单点故障和同步阻塞问题。 三阶段提交有这么三个阶段：CanCommit，PreCommit，DoCommit三个阶段 CanCommit：准备阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。 PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务，参与者执行完操作之后返回 ACK 响应，同时开始等待最终指令。 DoCommit：提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务： 如果所有参与者都返回正确的ACK响应，则提交事务 如果参与者有一个或多个参与者收到错误的ACK响应或者超时，则中断事务 如果参与者无法及时接收到来自协调者的提交或者中断事务请求时，在等待超时之后，会继续进行事务提交 可以看出，三阶段提交解决的只是两阶段提交中单体故障和同步阻塞的问题，因为加入了超时机制，这里的超时的机制作用于 预提交阶段 和 提交阶段。如果等待 预提交请求 超时，参与者直接回到准备阶段之前。如果等到提交请求超时，那参与者就会提交事务了。 无论是 2PC 还是 3PC 都不能保证分布式系统中的数据 100%一致。 7.3 TCC 了解吗？TCC（Try Confirm Cancel） ，是两阶段提交的一个变种，针对每个操作，都需要有一个其对应的确认和取消操作，当操作成功时调用确认操作，当操作失败时调用取消操作，类似于二阶段提交，只不过是这里的提交和回滚是针对业务上的，所以基于 TCC 实现的分布式事务也可以看做是对业务的一种补偿机制。 Try：尝试待执行的业务。订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于 1，然后将可用库存数量设置为库存剩余数量-1，。 Confirm：确认执行业务，如果 Try 阶段执行成功，接着执行 Confirm 阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量。 Cancel：取消待执行的业务，如果 Try 阶段执行失败，执行 Cancel 阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量。 TCC 是业务层面的分布式事务，保证最终一致性，不会一直持有资源的锁。 优点： 把数据库层的二阶段提交交给应用层来实现，规避了数据库的 2PC 性能低下问题 缺点：TCC 的 Try、Confirm 和 Cancel 操作功能需业务提供，开发成本高。TCC 对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作 7.4 本地消息表了解吗？本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。 例如，可以在订单库新增一个消息表，将新增订单和新增消息放到一个事务里完成，然后通过轮询的方式去查询消息表，将消息推送到 MQ，库存服务去消费 MQ。 执行流程： 订单服务，添加一条订单和一条消息，在一个事务里提交 订单服务，使用定时任务轮询查询状态为未同步的消息表，发送到 MQ，如果发送失败，就重试发送 库存服务，接收 MQ 消息，修改库存表，需要保证幂等操作 如果修改成功，调用 rpc 接口修改订单系统消息表的状态为已完成或者直接删除这条消息 如果修改失败，可以不做处理，等待重试 订单服务中的消息有可能由于业务问题会一直重复发送，所以为了避免这种情况可以记录一下发送次数，当达到次数限制之后报警，人工接入处理；库存服务需要保证幂等，避免同一条消息被多次消费造成数据不一致。 本地消息表这种方案实现了最终一致性，需要在业务系统里增加消息表，业务逻辑中多一次插入的 DB 操作，所以性能会有损耗，而且最终一致性的间隔主要有定时任务的间隔时间决定 7.5 MQ 消息事务了解吗？基于 MQ 的分布式事务是指将两个事务通过消息队列进行异步解耦，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。 订单服务执行自己的本地事务，并发送消息到 MQ，库存服务接收到消息后，执行自己的本地事务，如果消费失败，可以利用重试机制确保最终一致性。 三分恶面渣逆袭：基于 MQ 的分布式事务 延迟队列在分布式事务中通常用于异步补偿、定时校验和故障重试等场景，确保数据最终一致性。 当主事务执行完成后，延迟队列会在一定时间后检查各子事务的状态，如果有失败的子事务，可以触发补偿操作，重试或回滚事务。 当分布式锁因为某些原因未被正常释放时，可以通过延迟队列在超时后自动释放锁，防止死锁。 7.6 最大努力通知了解吗？最大努力通知相比实现会简单一些，适用于一些对最终一致性实时性要求没那么高的业务，比如支付通知，短信通知。 以支付通知为例，业务系统调用支付平台进行支付，支付平台进行支付，进行操作支付之后支付平台会去同步通知业务系统支付操作是否成功，如果不成功，会一直异步重试，但是会有一个最大通知次数，如果超过这个次数后还是通知失败，就不再通知，业务系统自行调用支付平台提供一个查询接口，供业务系统进行查询支付操作是否成功。 执行流程： 业务系统调用支付平台支付接口， 并在本地进行记录，支付状态为支付中 支付平台进行支付操作之后，无论成功还是失败，同步给业务系统一个结果通知 如果通知一直失败则根据重试规则异步进行重试，达到最大通知次数后，不再通知 支付平台提供查询订单支付操作结果接口 业务系统根据一定业务规则去支付平台查询支付结果 8.你们用什么？能说一下 Seata 吗？我们用比较常用的是 Seata——自己去实现分布式事务调度还是比较麻烦的。 Seata 的设计目标是对业务无侵入，因此它是从业务无侵入的两阶段提交（全局事务）着手，在传统的两阶段上进行改进，他把一个分布式事务理解成一个包含了若干分支事务的全局事务。而全局事务的职责是协调它管理的分支事务达成一致性，要么一起成功提交，要么一起失败回滚。也就是一荣俱荣一损俱损~ Seata 中存在这么几种重要角色： TC（Transaction Coordinator）：事务协调者。管理全局的分支事务的状态，用于全局性事务的提交和回滚。 TM（Transaction Manager）：事务管理者。用于开启、提交或回滚事务。 RM（Resource Manager）：资源管理器。用于分支事务上的资源管理，向 TC 注册分支事务，上报分支事务的状态，接收 TC 的命令来提交或者回滚分支事务。 Seata 整体执行流程： 服务 A 中的 TM 向 TC 申请开启一个全局事务，TC 就会创建一个全局事务并返回一个唯一的 XID 服务 A 中的 RM 向 TC 注册分支事务，然后将这个分支事务纳入 XID 对应的全局事务管辖中 服务 A 开始执行分支事务 服务 A 开始远程调用 B 服务，此时 XID 会根据调用链传播 服务 B 中的 RM 也向 TC 注册分支事务，然后将这个分支事务纳入 XID 对应的全局事务管辖中 服务 B 开始执行分支事务 全局事务调用处理结束后，TM 会根据有误异常情况，向 TC 发起全局事务的提交或回滚 TC 协调其管辖之下的所有分支事务，决定是提交还是回滚 Java 面试指南（付费）收录的字节跳动同学 17 后端技术面试原题：分布式事务怎么实现 为什么要用延迟队列 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 分布式一致性算法9.分布式算法 paxos 了解么 ？Paxos 有点类似前面说的 2PC，3PC，但比这两种算法更加完善。在很多多大厂都得到了工程实践，比如阿里的 OceanBase 的 分布式数据库， Google 的 chubby 分布式锁 。 Paxos 算法是什么？Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一。 Paxos 算法的工作流程？角色在 Paxos 中有这么几个角色： Proposer（提议者） : 提议者提出提案，用于投票表决。 Accecptor（接受者） : 对提案进行投票，并接受达成共识的提案。 Learner（学习者） : 被告知投票的结果，接受达成共识的提案。 在实际中，一个节点可以同时充当不同角色。 提议者提出提案，提案编号+value，可以表示为[M,V]，每个提案都有唯一编号，而且编号的大小是趋势递增的。 算法流程Paxos 算法包含两个阶段，第一阶段 Prepare(准备) 、第二阶段 Accept(接受) 。 Prepare(准备)阶段 提议者提议一个新的提案 P[Mn,?]，然后向接受者的某个超过半数的子集成员发送编号为 Mn 的准备请求 如果一个接受者收到一个编号为 Mn 的准备请求，并且编号 Mn 大于它已经响应的所有准备请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给提议者，同时该接受者会承诺不会再批准任何编号小于 Mn 的提案。 总结一下，接受者在收到提案后，会给与提议者两个承诺与一个应答： 两个承诺： 承诺不会再接受提案号小于或等于 Mn 的 Prepare 请求 承诺不会再接受提案号小于 Mn 的 Accept 请求 一个应答： 不违背以前作出的承诺的前提下，回复已经通过的提案中提案号最大的那个提案所设定的值和提案号 Mmax，如果这个值从来没有被任何提案设定过，则返回空值。如果不满足已经做出的承诺，即收到的提案号并不是决策节点收到过的最大的，那允许直接对此 Prepare 请求不予理会。 Accept(接受)阶段 如果提议者收到来自半数以上的接受者对于它发出的编号为 Mn 的准备请求的响应，那么它就会发送一个针对[Mn,Vn]的接受请求给接受者，注意 Vn 的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它可以随意选定一个值。 如果接受者收到这个针对[Mn,Vn]提案的接受请求，只要该接受者尚未对编号大于 Mn 的准备请求做出响应，它就可以通过这个提案。 当提议者收到了多数接受者的接受应答后，协商结束，共识决议形成，将形成的决议发送给所有学习节点进行学习。 所以 Paxos 算法的整体详细流程如下： Paxos 算法有什么缺点吗？怎么优化？前面描述的可以称之为 Basic Paxos 算法，在单提议者的前提下是没有问题的，但是假如有多个提议者互不相让，那么就可能导致整个提议的过程进入了死循环。 Lamport 提出了 Multi Paxos 的算法思想。 Multi Paxos 算法思想，简单说就是在多个提议者的情况下，选出一个 Leader（领导者），由领导者作为唯一的提议者，这样就可以解决提议者冲突的问题。 10.说说 Raft 算法？Raft 算法是什么？Raft 也是一个 一致性算法，和 Paxos 目标相同。但它还有另一个名字 - 易于理解的一致性算法。Paxos 和 Raft 都是为了实现 一致性 产生的。这个过程如同选举一样，参选者 需要说服 大多数选民 (Server) 投票给他，一旦选定后就跟随其操作。Paxos 和 Raft 的区别在于选举的 具体过程 不同。 Raft 算法的工作流程？Raft 算法的角色Raft 协议将 Server 进程分为三种角色： Leader（领导者） Follower（跟随者） Candidate（候选人） 就像一个民主社会，领导者由跟随者投票选出。刚开始没有 领导者，所有集群中的 参与者 都是 跟随者。 那么首先开启一轮大选。在大选期间 所有跟随者 都能参与竞选，这时所有跟随者的角色就变成了 候选人，民主投票选出领袖后就开始了这届领袖的任期，然后选举结束，所有除 领导者 的 候选人 又变回 跟随者 服从领导者领导。 这里提到一个概念 「任期」，用术语 Term 表达。 三类角色的变迁图如下： Leader 选举过程Raft 使用心跳（heartbeat）触发 Leader 选举。当 Server 启动时，初始化为 Follower。Leader 向所有 Followers 周期性发送 heartbeat。如果 Follower 在选举超时时间内没有收到 Leader 的 heartbeat，就会等待一段随机的时间后发起一次 Leader 选举。 Follower 将其当前 term 加一然后转换为 Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC 。结果有以下三种情况： 赢得了多数（超过 12）的选票，成功选举为 Leader； 收到了 Leader 的消息，表示有其它服务器已经抢先当选了 Leader； 没有 Server 赢得多数的选票，Leader 选举失败，等待选举时间超时（Election Timeout）后发起下一次选举。 选出 Leader 后，Leader 通过 定期 向所有 Follower 发送 心跳信息 维持其统治。若 Follower 一段时间未收到 Leader 的 心跳，则认为 Leader 可能已经挂了，然后再次发起 选举 过程。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 分布式设计11.说说什么是幂等性？ 什么是幂等性？ 幂等性是一个数学概念，用在接口上：用在接口上就可以理解为：同一个接口，多次发出同一个请求，请求的结果是一致的。 简单说，就是多次调用如一次。 什么是幂等性问题？ 在系统的运行中，可能会出现这样的问题： 用户在填写某些form表单时，保存按钮不小心快速点了两次，表中竟然产生了两条重复的数据，只是 id 不一样。 开发人员在项目中为了解决接口超时问题，通常会引入了重试机制。第一次请求接口超时了，请求方没能及时获取返回结果（此时有可能已经成功了），于是会对该请求重试几次，这样也会产生重复的数据。 mq 消费者在读取消息时，有时候会读取到重复消息，也会产生重复的数据。 这些都是常见的幂等性问题。 在分布式系统里，只要下游服务有写（保存、更新）的操作，都有可能会产生幂等性问题。 PS:幂等和防重有些不同，防重强调的防止数据重复，幂等强调的是多次调用如一次，防重包含幂等。 怎么保证接口幂等性？ insert 前先 select 在保存数据的接口中，在insert前，先根据requestId等字段先select一下数据。如果该数据已存在，则直接返回，如果不存在，才执行 insert操作。 加唯一索引 加唯一索引是个非常简单但很有效的办法，如果重复插入数据的话，就会抛出异常，为了保证幂等性，一般需要捕获这个异常。 如果是java程序需要捕获：DuplicateKeyException异常，如果使用了spring框架还需要捕获：MySQLIntegrityConstraintViolationException异常。 加悲观锁 更新逻辑，比如更新用户账户余额，可以加悲观锁，把对应用户的哪一行数据锁住。同一时刻只允许一个请求获得锁，其他请求则等待。 select * from user id=123 for update; 这种方式有一个缺点，获取不到锁的请求一般只能报失败，比较难保证接口返回相同值。 加乐观锁 更新逻辑，也可以用乐观锁，性能更好。可以在表中增加一个timestamp或者version字段，例如version: 在更新前，先查询一下数据，将 version 也作为更新的条件，同时也更新 version： update user set amount=amount+100,version=version+1 where id=123 and version=1; 更新成功后，version 增加，重复更新请求进来就无法更新了。 建防重表 有时候表中并非所有的场景都不允许产生重复的数据，只有某些特定场景才不允许。这时候，就可以使用防重表的方式。 例如消息消费中，创建防重表，存储消息的唯一 ID，消费时先去查询是否已经消费，已经消费直接返回成功。 状态机 有些业务表是有状态的，比如订单表中有：1-下单、2-已支付、3-完成、4-撤销等状态，可以通过限制状态的流动来完成幂等。 分布式锁 直接在数据库上加锁的做法性能不够友好，可以使用分布式锁的方式，目前最流行的分布式锁实现是通过 Redis，具体实现一般都是使用 Redission 框架。 token 机制 请求接口之前，需要先获取一个唯一的 token，再带着这个 token 去完成业务操作，服务端根据这个 token 是否存在，来判断是否是重复的请求。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 分布式限流12.你了解哪些限流算法？ 计数器 计数器比较简单粗暴，比如我们要限制 1s 能够通过的请求数，实现的思路就是从第一个请求进来开始计时，在接下来的 1s 内，每个请求进来请求数就+1，超过最大请求数的请求会被拒绝，等到 1s 结束后计数清零，重新开始计数。 这种方式有个很大的弊端：比如前 10ms 已经通过了最大的请求数，那么后面的 990ms 的请求只能拒绝，这种现象叫做“突刺现象”。 漏桶算法 就是桶底出水的速度恒定，进水的速度可能快慢不一，但是当进水量大于出水量的时候，水会被装在桶里，不会直接被丢弃；但是桶也是有容量限制的，当桶装满水后溢出的部分还是会被丢弃的。 算法实现：可以准备一个队列来保存暂时处理不了的请求，然后通过一个线程池定期从队列中获取请求来执行。 令牌桶算法 令牌桶就是生产访问令牌的一个地方，生产的速度恒定，用户访问的时候当桶中有令牌时就可以访问，否则将触发限流。 实现方案：Guava RateLimiter 限流 Guava RateLimiter 是一个谷歌提供的限流，其基于令牌桶算法，比较适用于单实例的系统。 这一期的分布式面试题就整理到这里了，主要是偏理论的一些问题，分布式其实是个很大的类型，比如分布式调用、分布式治理…… 所以，这篇文章只是个开始，后面还会有分布式调用（RPC）、微服务相关的主题文章，敬请期待。 图文详解 12 道分布式面试高频题，这次面试，一定吊打面试官，整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇 👍 面渣逆袭 Java 集合框架篇 👍 面渣逆袭 Java 并发编程篇 👍 面渣逆袭 JVM 篇 👍 面渣逆袭 Spring 篇 👍 面渣逆袭 Redis 篇 👍 面渣逆袭 MyBatis 篇 👍 面渣逆袭 MySQL 篇 👍 面渣逆袭操作系统篇 👍 面渣逆袭计算机网络篇 👍 面渣逆袭 RocketMQ 篇 👍 面渣逆袭分布式篇 👍 面渣逆袭微服务篇 👍 面渣逆袭设计模式篇 👍 面渣逆袭 Linux 篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。","tags":["基础","分布式"],"categories":["Java问答笔记"]},{"title":"操作系统学习笔记","path":"/2025/09/29/Java问答笔记/操作系统学习笔记/","content":"1 万字 51 张手绘图，详解 34 道操作系统面试高频题（让天下没有难背的八股），面渣背会这些 OS 八股文，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 引论01、什么是操作系统？操作系统（Operating System, OS）是计算机系统中管理硬件和软件资源的中间层系统，屏蔽了硬件的复杂性，并且为用户提供了便捷的交互方式，比如说 Windows、Linux、MacOS 等。 三分恶面渣逆袭：操作系统是什么 02、操作系统主要有哪些功能？ 三分恶面渣逆袭：操作系统主要功能 ①、负责创建和终止进程。进程是正在运行的程序实例，每个进程都有自己的地址空间和资源。 ②、负责为进程分配资源，比如说内存，并在进程终止时回收内存。 ③、提供创建、删除、读写文件的功能，并组织文件的存储结构，比如说目录。 ④、通过设备驱动程序控制和管理计算机的硬件设备，如键盘、鼠标、打印机等。 Java 面试指南（付费）收录的比亚迪面经同学 1 面试原题：操作系统的主要功能，以及上下文切换 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 操作系统结构03、什么是内核？可以这么说，内核是一个计算机程序，它是操作系统的核心，提供了操作系统最核心的能力，可以控制操作系统中所有的内容。 04、什么是用户态和内核态？在计算机系统中，内存可以分为两大区域：内核空间（Kernel Space）和用户空间（User Space）。这种划分主要用于保护系统稳定性和安全性。 内核空间，是操作系统内核代码及其运行时数据结构所在的内存区域，拥有对系统所有资源的完全访问权限，如进程管理、内存管理、文件系统、网络堆栈等。 ⽤户空间，是操作系统为应用程序（如用户运行的进程）分配的内存区域，用户空间中的进程不能直接访问硬件或内核数据结构，只能通过系统调用与内核通信。 二哥的 Java 进阶之路：用户空间和内核空间 当程序使⽤⽤户空间时，我们常说该程序在 ⽤户态 执⾏，⽽当程序使内核空间时，程序则在 内核态 执⾏。 Java 面试指南（付费）收录的得物面经同学 8 一面面试原题：内核空间和用户空间是什么，什么时候会进入内核空间 05、用户态和内核态是如何切换的？当应用程序执行系统调用时，CPU 将从用户态切换到内核态，进入内核空间执行相应的内核代码，然后再切换回用户态。 三分恶面渣逆袭：用户态内核态切换 系统调用是应用程序请求操作系统内核提供服务的接口，如文件操作（如 open、read、write）、进程控制（如 fork、exec）、内存管理（如 mmap）等。 Java 面试指南（付费）收录的得物面经同学 8 一面面试原题：内核空间和用户空间是什么，什么时候会进入内核空间 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 进程和线程06、并行和并发有什么区别？并发就是在一段时间内，多个任务都会被处理；但在某一时刻，只有一个任务在执行。单核处理器做到的并发，其实是利用时间片的轮转，例如有两个进程 A 和 B，A 运行一个时间片之后，切换到 B，B 运行一个时间片之后又切换到 A。因为切换速度足够快，所以宏观上表现为在一段时间内能同时运行多个程序。 并行就是在同一时刻，有多个任务在执行。这个需要多核处理器才能完成，在微观上就能同时执行多条指令，不同的程序被放到不同的处理器上运行，这个是物理上的多个进程同时进行。 并发和并行 07、什么是进程上下文切换？ 三分恶面渣逆袭：进程上下文切换 上下文切换是操作系统在多任务处理环境中，将 CPU 从一个进程切换到另一个进程的过程。通过让多个进程共享 CPU 资源，使系统能够并发执行多个任务。 进程上下文切换通畅包含以下几个步骤： 保存当前进程的上下文：操作系统保存当前进程的 CPU 寄存器，程序状态等关键信息。 选择下一个进程：调度程序选择下一个要执行的进程。 恢复上一个进程的上下文。 切换到下一个进程。 08、进程有哪些状态？当一个进程开始运行时，它可能会经历下面这几种状态： 上图中各个状态的意义： 运⾏状态（Runing）：该时刻进程占⽤ CPU； 就绪状态（Ready）：可运⾏，由于其他进程处于运⾏状态⽽暂时停⽌运⾏； 阻塞状态（Blocked）：该进程正在等待某⼀事件发⽣（如等待输⼊输出操作的完成）⽽暂时停⽌运⾏，这时，即使给它 CPU 控制权，它也⽆法运⾏； 进程3种状态 当然，进程还有另外两个基本状态： 创建状态（new）：进程正在被创建时的状态； 结束状态（Exit）：进程正在从系统中消失时的状态； 进程5种状态 09、什么是僵尸进程？僵尸进程是已完成且处于终止状态，但在进程表中却仍然存在的进程。 僵尸进程一般发生有父子关系的进程中，一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中。 10、什么是孤儿进程？一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程。孤儿进程将被 init 进程 (进程 ID 为 1 的进程) 所收养，并由 init 进程对它们完成状态收集工作。因为孤儿进程会被 init 进程收养，所以孤儿进程不会对系统造成危害。 11、进程有哪些调度算法？进程调度是操作系统中的核心功能之一，它负责决定哪些进程在何时使用 CPU。这一决定基于系统中的进程调度算法。 DIDA-lJ-进程调度算法 ①、先来先服务 这是最简单的调度算法，也称为先进先出（FIFO）。进程按照请求 CPU 的顺序进行调度。这种方式易于实现，但可能会导致较短的进程等待较长进程执行完成，从而产生“饥饿”现象。 三分恶面渣逆袭：先来先服务 ②、短作业优先 选择预计运行时间最短的进程优先执行。这种方式可以减少平均等待时间和响应时间，但缺点是很难准确预知进程的执行时间，并且可能因为短作业一直在执行，导致长作业持续被推迟执行。 三分恶面渣逆袭：短作业优先 ③、优先级调度 在这种调度方式中，每个进程都被分配一个优先级。CPU 首先分配给优先级最高的进程。优先级调度可以是非抢占式的或抢占式的。在非抢占式优先级调度中，进程一旦开始执行将一直运行直到完成；在抢占式优先级调度中，更高优先级的进程可以中断正在执行的低优先级进程。 三分恶面渣逆袭：优先级调度 ④、时间片轮转 时间片轮转调度为每个进程分配一个固定的时间段，称为时间片，进程可以在这个时间片内运行。如果进程在时间片结束时还没有完成，它将被放回队列的末尾。时间片轮转是公平的调度方式，可以保证所有进程得到公平的 CPU 时间，适用于共享系统。 三分恶面渣逆袭：时间片轮转 ⑤、最短剩余时间优先 这是短作业优先的一种改进形式，它是抢占式的。即如果一个新进程的预计执行时间比当前运行进程的剩余时间短，调度器将暂停当前的进程，并切换到新进程。这种方法也可以最小化平均等待时间，但同样面临预测执行时间的困难。 ⑥ 多级反馈队列 一个进程需要执行100 哥时间片，如果采用时间片轮转调度算法，那么需要交互 100 次。 多级队列就是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列的时间片大小不同，比如 2,4,6,8······。进程在第一个队列没执行完，就会被移到下一个队列。 这种方式下，之前的进程只需要交换 7 次就可以了。每个队列优先权不一样，最上面的队列优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。 可以将这种调度算法看成是时间片轮转调度算法与优先级调度算法的结合。 DIDA-lJ-多级反馈队列 Java 面试指南（付费）收录的华为面经同学 9 Java 通用软件开发一面面试原题：进程的调度方式 12、进程间通信有哪些方式？推荐阅读：编程十万问：进程间通信的方式有哪些？ 进程间通信的方式有 6 种，管道、信号、消息队列、共享内存、信号量和套接字。 编程十万问：进程间通信 简单说说管道：管道可以理解成不同进程之间的传话筒，一方发声，一方接收，声音的介质可以是空气或者电缆。 进程间的管道就是内核中的一串缓存，从管道的一端写入数据，另一端读取。数据只能单向流动，遵循先进先出（FIFO）的原则。 编程十万问：管道 ①、匿名管道：允许具有亲缘关系的进程（如父子进程）进行通信。 三分恶面渣逆袭：“奉先我儿” 使用 C 语言在 UnixLinux 环境下通过匿名管道实现两个进程（通常是父子进程）之间通信的示例： #include stdio.h#include stdlib.h#include unistd.h#include string.hint main() int pipefd[2]; pid_t cpid; char buf; // 创建管道 if (pipe(pipefd) == -1) perror(pipe); exit(EXIT_FAILURE); // 创建子进程 cpid = fork(); if (cpid == -1) perror(fork); exit(EXIT_FAILURE); if (cpid == 0) /* 子进程 */ close(pipefd[1]); // 关闭写端 // 从管道读取数据 while (read(pipefd[0], buf, 1) 0) write(STDOUT_FILENO, buf, 1); write(STDOUT_FILENO, , 1); close(pipefd[0]); exit(EXIT_SUCCESS); else /* 父进程 */ close(pipefd[0]); // 关闭读端 // 向管道写入数据 write(pipefd[1], Hello, Child!, 13); close(pipefd[1]); // 关闭写端，触发EOF wait(NULL); // 等待子进程退出 exit(EXIT_SUCCESS); ②、命名管道：允许无亲缘关系的进程通信，通过在文件系统中创建一个特殊类型的文件来实现。 缺点：管道的效率低，不适合进程间频繁地交换数据。 简单说说信号：信号可以理解成以前的 BB 机，用于通知接收进程某件事情发生了，是一种较为简单的通信方式，主要用于处理异步事件。 比如kill -9 1050就表示给 PID 为 1050 的进程发送SIGKIL信号。 这里顺带普及一下 Linux 中常用的信号： SIGHUP：当我们退出终端（Terminal）时，由该终端启动的所有进程都会接收到这个信号，默认动作为终止进程。 SIGINT：程序终止（interrupt）信号。按 Ctrl+C 时发出，大家应该在操作终端时有过这种操作。 SIGQUIT：和 SIGINT 类似，按 Ctrl+\\ 键将发出该信号。它会产生核心转储文件，将内存映像和程序运行时的状态记录下来。 SIGKILL：强制杀死进程，本信号不能被阻塞和忽略。 SIGTERM：与 SIGKILL 不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出。 简单说说消息队列：消息队列是保存在内核中的消息链表，按照消息的类型进行消息传递，具有较高的可靠性和稳定性。 编程十万问：消息队列 缺点：消息体有一个最大长度的限制，不适合比较大的数据传输；存在用户态与内核态之间的数据拷贝开销。 编程十万问：消息队列 简单说说共享内存：允许两个或多个进程共享一个给定的内存区，一个进程写⼊的东西，其他进程⻢上就能看到。 共享内存是最快的进程间通信方式，它是针对其他进程间通信方式运行效率低而专门设计的。 三分恶面渣逆袭：共享内存 缺点：当多进程竞争同一个共享资源时，会造成数据错乱的问题。 简单说说信号量：信号量可以理解成红绿灯，红灯停（信号量为零），绿灯行（信号量非零）。它本质上是一个计数器，用来控制对共享资源的访问数量。 三分恶面渣逆袭：信号量 它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。Java 中的 java.util.concurrent.Semaphore 类就实现了类似的功能。 控制信号量的⽅式有两种原⼦操作： ⼀个是 P 操作（wait，减操作），当进程希望获取资源时，它会执行 P 操作。如果信号量的值大于 0，表示有资源可用，信号量的值减 1，进程继续执行。如果信号量的值为 0，表示没有可用资源，进程进入等待状态，直到信号量的值变为大于 0。 另⼀个是 V 操作（signal，加操作），当进程释放资源时，它会执行 V 操作，信号量的值加 1。如果有其他进程因为等待该资源而被阻塞，这时会唤醒其中一个进程。 编程十万问：信号量 简单说说套接字 Socket：这个和 Java 中的 Socket 很相似，提供网络通信的端点，可以让不同机器上运行的进程之间进行双向通信。 Java 面试指南（付费）收录的华为一面原题：说一下进程的通信机制 Java 面试指南（付费）收录的字节跳动商业化一面的原题：进程和线程区别，线程共享内存和进程共享内存的区别 Java 面试指南（付费）收录的字节跳动商业化一面的原题：进程间如何通信 Java 面试指南（付费）收录的华为面经同学 6 Java 通用软件开发一面面试原题：说说你对 JVM 调优的了解 Java 面试指南（付费）收录的美团同学 2 优选物流调度技术 2 面面试原题：进程间的通信方式，代码使用匿名管道使两个进程通信 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：linux下进程的通信方式有哪几种？ 13、进程和线程的联系和区别？进程是一个正在执行的程序实例。每个进程都有自己独立的地址空间、全局变量、堆栈、和文件描述符等资源。 线程是进程中的一个执行单元。一个进程可以包含多个线程，它们共享进程的地址空间和资源。 多线程-图片来源于网络 每个进程在独立的地址空间中运行，不会直接影响其他进程。线程共享同一个进程的内存空间、全局变量和文件描述符。 进程切换需要保存和恢复大量的上下文信息，代价较高。线程切换相对较轻量，因为线程共享进程的地址空间，只需要保存和恢复线程私有的数据。 线程的生命周期由进程控制，进程终止时，其所有线程也会终止。 特性 进程 线程 地址空间 独立 共享 内存开销 高 低 上下文切换 慢，开销大 快，开销小 通信 需要 IPC 机制，开销较大 共享内存，直接通信 创建销毁 开销大，较慢 开销小，较快 并发性 低 高 崩溃影响 一个进程崩溃不会影响其他进程 一个线程崩溃可能导致整个进程崩溃 Java 面试指南（付费）收录的得物面经同学 8 一面面试原题：Linux中进程和线程的区别 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：进程和线程的主要区别是什么样子？ 14、线程上下文切换了解吗？这还得看线程是不是属于同⼀个进程： 当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样； 当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据； 所以，线程的上下⽂切换相⽐进程，开销要⼩很多。 15、线程有哪些实现方式？主要有三种线程的实现⽅式： 内核态线程实现：在内核空间实现的线程，由内核直接管理直接管理线程。 内核态线程实现 ⽤户态线程实现：在⽤户空间实现线程，不需要内核的参与，内核对线程无感知。 用户态线程 混合线程实现：现代操作系统基本都是将两种方式结合起来使用。用户态的执行系统负责进程内部线程在非阻塞时的切换；内核态的操作系统负责阻塞线程的切换。即我们同时实现内核态和用户态线程管理。其中内核态线程数量较少，而用户态线程数量较多。每个内核态线程可以服务一个或多个用户态线程。 混合线程实现 16、线程间如何同步？同步解决的是多线程操作共享资源的问题，不管线程之间是如何穿插执行的，最后的结果都是正确的。 在操作系统层面，保证线程同步的方式有很多，比如锁、信号量等。那在此之前，需要先了解什么是临界区。 cxuan：使用临界区的互斥 临界区：对共享资源访问的程序片段，我们希望这段代码是互斥的，可以保证在某个时刻只能被一个线程执行，也就是说一个线程在临界区执行时，其它线程应该被阻止进入临界区。 临界区不仅针对线程，同样针对进程。同步的实现方式有： ①、互斥锁 使⽤加锁操作和解锁操作可以解决并发线程进程的互斥问题。 任何想进⼊临界区的线程，必须先执⾏加锁操作。若加锁操作顺利通过，则线程可进⼊临界区；在完成对临界资源的访问后再执⾏解锁操作，以释放该临界资源。 加锁和解锁锁住的是什么呢？可以是临界区对象，也可以只是一个简单的互斥量，例如互斥量是0无锁，1表示加锁。 根据锁的实现不同，可以分为忙等待锁和⽆忙等待锁。 忙等待锁（也称为自旋锁，Spinlock）是指当一个线程试图获取锁时，如果该锁已经被其他线程持有，当前线程不会立即进入休眠或阻塞，而是不断地检查锁的状态，直到该锁可用为止。这个过程被称为忙等待（busy waiting），因为线程在等待锁时仍然占用 CPU 资源，处于活跃状态。优点是避免了线程的上下文切换。 无忙等待锁是指当一个线程尝试获取锁时，如果锁已经被其他线程持有，当前线程不会忙等待，而是主动让出 CPU，进入阻塞状态或休眠状态，等待锁释放。当锁被释放时，线程被唤醒并重新尝试获取锁。这类锁的主要目的是避免忙等待带来的 CPU 资源浪费。 ②、信号量 信号量是操作系统提供的⼀种协调共享资源访问的⽅法。通常表示资源的数量，对应的变量是⼀个整型（sem）变量。 另外，还有两个原⼦操作的系统调⽤函数来控制信号量，分别是： P 操作：当线程想要进入临界区时，会尝试执行 P 操作。如果信号量的值大于 0，信号量值减 1，线程可以进入临界区；否则，线程会被阻塞，直到信号量大于 0。 V 操作：当线程退出临界区时，执行 V 操作，信号量的值加 1，释放一个被阻塞的线程。 Java 面试指南（付费）收录的拼多多面经同学 4 技术一面面试原题：操作系统内核对象实现同步与互斥 17、什么是死锁？在两个或者多个并发线程中，如果每个线程持有某种资源，而又等待其它线程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组线程产生了死锁。通俗的讲就是两个或多个线程无限期的阻塞、相互等待的一种状态。 死锁 18、死锁产生有哪些条件？产生死锁需要同时满足四个必要条件： 互斥条件（Mutual Exclusion）：资源不能被多个进程共享，即资源一次只能被一个进程使用。如果一个资源已经被分配给了一个进程，其他进程必须等待，直到该资源被释放。 持有并等待条件（Hold and Wait）：一个进程已经持有了至少一个资源，同时还在等待获取其他被占用的资源。在此期间，该进程不会释放已经持有的资源。 不可剥夺条件（No Preemption）：已分配给进程的资源不能被强制剥夺，只有持有该资源的进程可以主动释放资源。 循环等待条件（Circular Wait）：存在一个进程集合 ，其中 等待 持有的资源， 等待 持有的资源，依此类推，直到 等待 持有的资源，形成一个进程等待环。 假设有两个进程 和 ，以及两个资源 和 ，一个简单的死锁场景是这样的： 持有资源 ，并请求资源 。 持有资源 ，并请求资源 。 在这种情况下，发生死锁的步骤如下： 互斥条件： 和 都只能被一个进程占用。 持有并等待条件： 持有 并等待 ，同时 持有 并等待 。 不可剥夺条件： 和 都不能被强制从 和 中剥夺。 循环等待条件： 等待 持有的 ，而 等待 持有的 ，形成一个循环。 Java 面试指南（付费）收录的字节跳动面经同学 8 Java 后端实习一面面试原题：死锁条件 Java 面试指南（付费）收录的拼多多面经同学 4 技术一面面试原题：死锁的条件 19、如何避免死锁呢？产⽣死锁的有四个必要条件：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。 避免死锁，破坏其中的一个就可以。 消除互斥条件 这个是没法实现，因为很多资源就是只能被一个线程占用，例如锁。 消除请求并持有条件 消除这个条件的办法很简单，就是一个线程一次请求其所需要的所有资源。 消除不可剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可剥夺这个条件就破坏掉了。 消除环路等待条件 可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后就不存在环路了。 20、活锁和饥饿锁了解吗？饥饿锁： 饥饿锁，这个饥饿指的是资源饥饿，某个线程一直等不到它所需要的资源，从而无法向前推进，就像一个人因为饥饿无法成长。 活锁： 在活锁状态下，处于活锁线程组里的线程状态可以改变，但是整个活锁组的线程无法推进。 活锁可以用两个人过一条很窄的小桥来比喻：为了让对方先过，两个人都往旁边让，但两个人总是让到同一边。这样，虽然两个人的状态一直在变化，但却都无法往前推进。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 内存管理21、物理内存和虚拟内存有什么区别？物理内存指的是计算机中实际存在的硬件内存。物理内存是计算机用于存储运行中程序和数据的实际内存资源，操作系统和应用程序最终都必须使用物理内存来执行。 也就是我们常说的那个 8G、16G、64G 的内存条。 虚拟内存是操作系统提供的一种内存管理技术，它使得应用程序认为自己有连续的、独立的内存空间，而实际上，这个虚拟内存可能部分存储在物理内存上，部分存储在 磁盘（如硬盘的交换分区或页面文件） 中。 三分恶面渣逆袭：虚拟内存 虚拟内存的核心思想是通过硬件和操作系统的配合，为每个进程提供一个独立的、完整的虚拟地址空间，解决物理内存不足的问题。 ①、每个进程都有自己的虚拟地址空间，虚拟内存使用的是逻辑地址，它与实际的物理内存地址不同，必须经过地址转换才能映射到物理内存。 ②、操作系统通过 页表（Page Table） 将虚拟地址映射到物理地址。当程序访问某个虚拟地址时，CPU 会通过页表找到对应的物理地址。 ③、操作系统将虚拟内存划分为若干个页（Pages），每个页可以被映射到物理内存中的一个页面。如果物理内存不够，操作系统会将不常用的页暂时存储到磁盘的交换区（Swap）中，这个过程叫做页交换（Paging）。 Java 面试指南（付费）收录的深信服面经同学 3 Java 后端线下一面面试原题：指针是存在虚拟内存中，问物理内存和虚拟内存的区别 22、什么是内存分段？程序是由若⼲个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就⽤分段（Segmentation）的形式把这些段分离出来。 分段机制下的虚拟地址由两部分组成，段号和段内偏移量。 虚拟地址和物理地址通过段表映射，段表主要包括段号、段的界限。 虚拟地址、段表、物理地址 我们来看一个映射，虚拟地址：段 3、段偏移量 500 —- 段基地址 7000+段偏移量 500 —- 物理地址：8700+。 段虚拟地址映射 23、什么是内存分页？分⻚是把整个虚拟和物理内存空间切成⼀段段固定尺⼨的⼤⼩。这样⼀个连续并且尺⼨固定的内存空间，我们叫**⻚**（Page）。在 Linux 下，每⼀⻚的⼤⼩为 4KB 。 访问分页系统中内存数据需要两次的内存访问 ：一次是从内存中访问页表，从中找到指定的物理页号，加上页内偏移得到实际物理地址，第二次就是根据第一次得到的物理地址访问内存取出数据。 内存分页 24、多级页表知道吗？推荐阅读：操作系统导论：多级页表 多级页表（Multilevel Page Table）是一种内存管理技术，用于在虚拟内存系统中高效地管理和转换虚拟地址到物理地址。它通过分层结构减少页表所需的内存开销，以解决单级页表在大地址空间中的效率问题。 三分恶面渣逆袭：多级页表示意图 在虚拟内存系统中，虚拟地址需要转换为物理地址。页表是实现这种转换的关键数据结构。对于 32 位系统，一个进程的地址空间可以达到 4 GB，如果使用单级页表，每个页表条目（PTE）占用 4 字节，则需要 4 MB 的内存来存储页表。然而，许多进程只使用其中的一小部分地址空间，导致单级页表的内存浪费。 多级页表通过将单级页表拆分为多个层级，减少了内存浪费。以两级页表为例： 一级页表（页目录）：存储二级页表的地址。每个页目录条目（PDE）指向一个二级页表。 二级页表（页表）：存储实际的页框地址。每个页表条目（PTE）指向一个物理页框。 虚拟地址分为多个部分，每一部分用于索引相应层级的页表。例如，对于一个 32 位地址和 4 KB 页大小的两级页表： 高 10 位：一级页表索引（页目录索引）。 中 10 位：二级页表索引（页表索引）。 低 12 位：页内偏移。 Java 面试指南（付费）收录的得物面经同学 1 面试原题：多级页表 25、什么是快表？同样利用了局部性原理，即在⼀段时间内，整个程序的执⾏仅限于程序中的某⼀部分。相应地，执⾏所访问的存储空间也局限于某个内存区域。 利⽤这⼀特性，把最常访问的⼏个⻚表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯⽚中，加⼊了⼀个专⻔存放程序最常访问的⻚表项的 Cache，这个 Cache 就是 TLB（Translation Lookaside Buffer） ，通常称为⻚表缓存、转址旁路缓存、快表等。 TLB示意图-来源参考[3] 26、分页和分段有什么区别？ 段是信息的逻辑单位，它是根据用户的需要划分的，因此段对用户是可见的 ；页是信息的物理单位，是为了管理主存的方便而划分的，对用户是透明的。 段的大小不固定，有它所完成的功能决定；页的大小固定，由系统决定 段向用户提供二维地址空间；页向用户提供的是一维地址空间 段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制。 27、什么是交换空间？操作系统把物理内存(Physical RAM)分成一块一块的小内存，每一块内存被称为页(page)。当内存资源不足时，Linux 把某些页的内容转移至磁盘上的一块空间上，以释放内存空间。磁盘上的那块空间叫做交换空间(swap space)，而这一过程被称为交换(swapping)。物理内存和交换空间的总容量就是虚拟内存的可用容量。 用途： 物理内存不足时一些不常用的页可以被交换出去，腾给系统。 程序启动时很多内存页被用来初始化，之后便不再需要，可以交换出去。 33、什么是缺页中断？（补充） 2024 年 03 月 29 日增补 缺页中断（Page Fault）是虚拟内存管理的一个重要概念。当一个程序访问的页（页面）不在物理内存中时，就会发生缺页中断。操作系统需要从磁盘上的交换区（或页面文件）中将缺失的页调入内存。 举个例子，你正在一间图书馆（内存）里查找一本特定的书（数据程序页），图书馆的书架（内存空间）能放的书是有限的。现在，如果你找的那本书正好在书架上，那太好了，直接拿来阅读（内存命中）。 但如果书架上没有（缺页），你需要先去找图书管理员。 图书管理员（操作系统）注意到书架上缺了这本书，然后去仓库里帮你找（缺页中断）。找到书之后，管理员发现书架已经满了，需要先从书架上拿掉一本书（页面置换算法决定哪本书被拿掉），然后把新找到的书放上去，最后把书递给你。 这个过程中，“去仓库找书并换回来”的这一过程就像是发生了缺页中断，而决定哪本书被移出书架以腾出位置放新书的规则，就是页面置换算法在做的事情。 这么做的目的是尽量确保你常读的书都能在书架（内存）上直接找到，避免每次都要去仓库（硬盘）搜寻，因为去仓库找书的过程比较耗时。 Java 面试指南（付费）收录的字节跳动面经同学 9 飞书后端技术一面面试原题：操作系统缺页中断，页面置换算法 28、页面置换算法有哪些？推荐阅读：页面置换算法详解 页面置换算法的目标是最小化缺页中断的次数，常见的页面置换算法有最佳⻚⾯置换算法（OPT）、先进先出置换算法（FIFO）、最近最久未使⽤的置换算法（LRU）和时钟页面置换算法等。 三分恶面渣逆袭：常见页面置换算法 ①、最佳⻚⾯置换算法 基本思路是，淘汰以后不会使用的页面。这是理论上的最佳算法，因为它可以保证最低的缺页率。但在实际应用中，由于无法预知未来的访问模式，OPT 通常无法实现。 Leophen：OPT ②、先进先出置换算法 基本思路是，优先淘汰最早进入内存的页面。FIFO 算法维护一个队列，新来的页面加入队尾，当发生页面置换时，队头的页面（即最早进入内存的页面）被移出。 三分恶面渣逆袭：按照进入内存早晚构建的页面链表 ③、最近最久未使⽤的置换算法 基本思路是，淘汰最近没有使用的页面。LRU 算法根据页面的访问历史来进行置换，最长时间未被访问的页面将被置换出去。 相对更接近最优算法的效果，因为最近未使用的页面可能在将来也不会被使用。但 LRU 算法的实现需要跟踪页面的访问历史，可能会增加系统的开销。 三分恶面渣逆袭：LRU实现 ④、时钟页面置换算法 时钟算法是 LRU 的一种近似和实现简单的形式。它通过一个循环列表（类似时钟的指针）遍历页面，每个页面有一个使用位，当页面被访问时，使用位设置为 1。 当需要页面置换时，时钟指针会顺时针移动，直到找到使用位为 0 的页面进行置换。这个过程类似于给每个页面一个二次机会。算法执行时，会先将使用位从 1 清零，如果该页面再次被访问，它的使用位再次被设置为 1。 三分恶面渣逆袭：时钟页面置换算法 ⑤、最不常⽤置换算法 根据页面被访问的频率进行置换，访问次数最少的页面最先被置换。实现较为复杂，需要记录每个页面的访问频率。 Java 面试指南（付费）收录的字节跳动面经同学 9 飞书后端技术一面面试原题：操作系统缺页中断，页面置换算法 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 文件29、硬链接和软链接有什么区别？ 硬链接就是在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是源文件的 inode。删除任意一个条目，文件还是存在，只要引用数量不为 0。但是硬链接有限制，它不能跨越文件系统，也不能对目录进行链接。 硬链接-来源参考[3] 软链接相当于重新创建⼀个⽂件，这个⽂件有独⽴的 inode，但是这个**⽂件的内容是另外⼀个⽂件的路径**，所以访问软链接的时候，实际上相当于访问到了另外⼀个⽂件，所以软链接是可以跨⽂件系统的，甚⾄⽬标⽂件被删除了，链接⽂件还是在的，只不过打不开指向的文件了而已。 软链接-来源参考[3] IO30、零拷贝了解吗？假如需要文件传输，使用传统 IO，数据读取和写入是用户空间到内核空间来回赋值，而内核空间的数据是通过操作系统的 IO 接口从磁盘读取或者写入，这期间发生了多次用户态和内核态的上下文切换，以及多次数据拷贝。 传统文件传输示意图-来源参考[3] 为了提升 IO 性能，就需要减少用户态与内核态的上下文切换和内存拷贝的次数。 这就用到了我们零拷贝的技术，零拷贝技术实现主要有两种： mmap + write mmap() 系统调⽤函数会直接把内核缓冲区⾥的数据「映射」到⽤户空间，这样，操作系统内核与⽤户空间就不需要再进⾏任何的数据拷⻉操作。 mmap示意图-来源参考[3] sendfile 在 Linux 内核版本 2.1 中，提供了⼀个专⻔发送⽂件的系统调⽤函数 sendfile() 。 ⾸先，它可以替代前⾯的 read() 和 write() 这两个系统调⽤，这样就可以减少⼀次系统调⽤，也就减少了 2 次上下⽂切换的开销。 其次，该系统调⽤，可以直接把内核缓冲区⾥的数据拷⻉到 socket 缓冲区⾥，不再拷⻉到⽤户态，这样就只有 2 次上下⽂切换，和 3 次数据拷⻉。 sendfile示意图-来源参考[3] 很多开源项目如 Kafka、RocketMQ 都采用了零拷贝技术来提升 IO 效率。 31、聊聊阻塞与⾮阻塞 IO、 同步与异步 IO？ 阻塞 IO 先来看看阻塞 IO，当⽤户程序执⾏ read ，线程会被阻塞，⼀直等到内核数据准备好，并把数据从内核缓冲区拷⻉到应⽤程序的缓冲区中，当拷⻉过程完成， read 才会返回。 注意，阻塞等待的是内核数据准备好和数据从内核态拷⻉到⽤户态这两个过程。 阻塞IO 非阻塞 IO ⾮阻塞的 read 请求在数据未准备好的情况下⽴即返回，可以继续往下执⾏，此时应⽤程序不断轮询内核，直到数据准备好，内核将数据拷⻉到应⽤程序缓冲区， read 调⽤才可以获取到结果。 非阻塞IO 基于非阻塞的 IO 多路复用 我们上面的非阻塞 IO 有一个问题，什么问题呢？应用程序要一直轮询，这个过程没法干其它事情，所以引入了IO **多路复⽤**技术。 当内核数据准备好时，以事件通知应⽤程序进⾏操作。 基于非阻塞的IO多路复用 注意：⽆论是阻塞 IO、还是⾮阻塞 IO、非阻塞 IO 多路复用，都是同步调⽤。因为它们在 read 调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷⻉效率不⾼，read 调⽤就会在这个同步过程中等待⽐较⻓的时间。 异步 IO 真正的异步 IO 是内核数据准备好和数据从内核态拷⻉到⽤户态这两个过程都不⽤等待。 发起 aio_read 之后，就⽴即返回，内核⾃动将数据从内核空间拷⻉到应⽤程序空间，这个拷⻉过程同样是异步的，内核⾃动完成的，和前⾯的同步操作不⼀样，应⽤程序并不需要主动发起拷⻉动作。 异步IO 拿例子理解几种 IO 模型 老三关注了很多 UP 主，有些 UP 主是老鸽子，到了更新的时间： 阻塞 IO 就是，老三不干别的，就干等着，盯着 UP 的更新。 非阻塞 IO 就是，老三发现 UP 没更，就去喝个茶什么的，过一会儿来盯一次，一直等到 UP 更新。 基于⾮阻塞的 IO 多路复⽤好⽐，老三发现 UP 没更，就去干别的，过了一会儿 B 站推送消息了，老三一看，有很多条，就去翻动态，看看等的 UP 是不是更新了。 异步 IO 就是，老三说 UP 你该更了，UP 赶紧爆肝把视频做出来，然后把视频亲自呈到老三面前，这个过程不用等待。 鸽宗 32、详细讲一讲 IO 多路复用？ 我们先了解什么是 IO 多路复用？ 我们在传统的 IO 模型中，如果服务端需要支持多个客户端，我们可能要为每个客户端分配一个进程线程。 不管是基于重一点的进程模型，还是轻一点的线程模型，假如连接多了，操作系统是扛不住的。 所以就引入了IO 多路复用 技术。 简单说，就是一个进程线程维护多个 Socket，这个多路复用就是多个连接复用一个进程线程。 IO多路复用 我们来看看 IO 多路复用三种实现机制： select select 实现多路复⽤的⽅式是： 将已连接的 Socket 都放到⼀个**⽂件描述符集合**fd_set，然后调⽤ select 函数将 fd_set 集合拷⻉到内核⾥，让内核来检查是否有⽹络事件产⽣，检查的⽅式很粗暴，就是通过遍历 fd_set 的⽅式，当检查到有事件产⽣后，将此 Socket 标记为可读或可写， 接着再把整个 fd_set 拷⻉回⽤户态⾥，然后⽤户态还需要再通过遍历的⽅法找到可读或可写的 Socket，再对其处理。 select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符。 select 机制的缺点： （1）每次调用 select，都需要把 fd_set 集合从用户态拷贝到内核态，如果 fd_set 集合很大时，那这个开销也很大，比如百万连接却只有少数活跃连接时这样做就太没有效率。 （2）每次调用 select 都需要在内核遍历传递进来的所有 fd_set，如果 fd_set 集合很大时，那这个开销也很大。 （3）为了减少数据拷贝带来的性能损坏，内核对被监控的 fd_set 集合大小做了限制，一般为 1024，如果想要修改会比较麻烦，可能还需要编译内核。 （4）每次调用 select 之前都需要遍历设置监听集合，重复工作。 poll poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了 select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。 但是 poll 和 select 并没有太⼤的本质区别，都是使⽤线性结构存储进程关注的 Socket 集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socke，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。 epoll epoll 通过两个⽅⾯，很好解决了 selectpoll 的问题。 第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的 socket 通过 epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是 O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 selectpoll 每次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。 第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个 socket 有事件发⽣时，通过回调函数，内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤ epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 selectpoll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。 epoll接口作用-来源参考[3] epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的 Socket 的数⽬也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，epoll 被称为解决 C10K 问题的利器。 34.普通内存比一般的机械硬盘快多少？（补充） 2024 年 04 月 10 日增补 机械硬盘，也叫 HDD（Hard Disk Drive），是一种通过磁盘旋转和磁头移动来存储数据的设备，读写速度比较慢，通常比内存的速度慢 10 万倍左右。 HDD 的访问时间大约在 5-10ms，数据传输速率约为 100 到 200 MBs。 内存，也就是 RAM（Random Access Memory），访问时间大约在 10-100ns，数据传输速率约为数十 GBs。 固态硬盘（Solid State Drive，SSD），SSD 的读写速度比 HDD 快 200 倍左右，价格也在逐渐下降，已经逐渐取代了 HDD。 图片来源于网络 Java 面试指南（付费）收录的奇安信面经同学 1 Java 技术一面面试原题：普通内存比一般的机械硬盘快多少？ 图文详解 34 道操作系统面试高频题，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇 👍 面渣逆袭 Java 集合框架篇 👍 面渣逆袭 Java 并发编程篇 👍 面渣逆袭 JVM 篇 👍 面渣逆袭 Spring 篇 👍 面渣逆袭 Redis 篇 👍 面渣逆袭 MyBatis 篇 👍 面渣逆袭 MySQL 篇 👍 面渣逆袭操作系统篇 👍 面渣逆袭计算机网络篇 👍 面渣逆袭 RocketMQ 篇 👍 面渣逆袭分布式篇 👍 面渣逆袭微服务篇 👍 面渣逆袭设计模式篇 👍 面渣逆袭 Linux 篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。","tags":["基础","操作系统"],"categories":["Java问答笔记"]},{"title":"设计模式学习笔记","path":"/2025/09/29/Java问答笔记/设计模式学习笔记/","content":"设计模式是软件工程中常用的解决特定问题的模版或者蓝图，可以帮助我们开发者以一种更加清晰、高效和可重用的方式来编写代码。通常分为三类： 创建型模式：涉及对象实例化，用于创建对象的模式，可以增加程序的灵活性和可重用性。常见的创建型模式有工厂方法、抽象工厂、单例、建造者、原型等。 结构型模式：涉及类和对象的组合，用于设计类和对象的结构，以便更好地实现程序的功能。常见的结构型模式有适配器、桥接、组合、装饰、外观、享元、代理等。 行为型模式：关注对象之间的通信，包括责任链、命令、解释器、迭代器、中介者、备忘录、观察者、状态、策略、模板方法、访问者等。 01、什么是责任链模式？ 推荐阅读：refactoringguru.cn：责任链模式 责任链模式（Chain of Responsibility Pattern）是一种行为设计模式，它使多个对象都有机会处理请求，从而避免了请求的发送者和接收者之间的耦合关系。 请求会沿着一条链传递，直到有一个对象处理它为止。这种模式常用于处理不同类型的请求以及在不确定具体接收者的情况下将请求传递给多个对象中的一个。 天未：图解 23 种设计模式 基本概念责任链模式主要包括以下几个角色： Handler（抽象处理者）：定义了一个处理请求的接口或抽象类，其中通常会包含一个指向链中下一个处理者的引用。 ConcreteHandler（具体处理者）：实现抽象处理者的处理方法，如果它能处理请求，则处理；否则将请求转发给链中的下一个处理者。 Client（客户端）：创建处理链，并向链的第一个处理者对象提交请求。 工作流程 客户端将请求发送给链上的第一个处理者对象。 处理者接收到请求后，决定自己是否有能力进行处理。 如果可以处理，就处理请求。 如果不能处理，就将请求转发给链上的下一个处理者。 过程重复，直到链上的某个处理者能处理该请求或者链上没有更多的处理者。 应用场景责任链模式适用于以下场景： 有多个对象可以处理同一请求，但具体由哪个对象处理则在运行时动态决定。 在不明确指定接收者的情况下，向多个对象中的一个提交请求。 需要动态组织和管理处理者时。 优缺点优点： 降低耦合度：它将请求的发送者和接收者解耦。 增加了给对象指派职责的灵活性：可以在运行时动态改变链中的成员或调整它们的次序。 可以方便地增加新的处理类，在不影响现有代码的情况下扩展功能。 缺点： 请求可能不会被处理：如果没有任何处理者处理请求，它可能会达到链的末端并被丢弃。 性能问题：一个请求可能会在链上进行较长的遍历，影响性能。 调试困难：特别是在链较长时，调试可能会比较麻烦。 实现示例假设有一个日志系统，根据日志的严重性级别（错误、警告、信息）将日志消息发送给不同的处理器处理。 abstract class Logger public static int INFO = 1; public static int DEBUG = 2; public static int ERROR = 3; protected int level; // 责任链中的下一个元素 protected Logger nextLogger; public void setNextLogger(Logger nextLogger) this.nextLogger = nextLogger; public void logMessage(int level, String message) if (this.level = level) write(message); if (nextLogger != null) nextLogger.logMessage(level, message); abstract protected void write(String message);class ConsoleLogger extends Logger public ConsoleLogger(int level) this.level = level; @Override protected void write(String message) System.out.println(Standard Console::Logger: + message); class ErrorLogger extends Logger public ErrorLogger(int level) this.level = level; @Override protected void write(String message) System.out.println(Error Console::Logger: + message); class FileLogger extends Logger public FileLogger(int level) this.level = level; @Override protected void write(String message) System.out.println(File::Logger: + message); public class ChainPatternDemo private static Logger getChainOfLoggers() Logger errorLogger = new ErrorLogger(Logger.ERROR); Logger fileLogger = new FileLogger(Logger.DEBUG); Logger consoleLogger = new ConsoleLogger(Logger.INFO); errorLogger.setNextLogger(fileLogger); fileLogger.setNextLogger(consoleLogger); return errorLogger; public static void main(String[] args) Logger loggerChain = getChainOfLoggers(); loggerChain.logMessage(Logger.INFO, INFO 级别); loggerChain.logMessage(Logger.DEBUG, Debug 级别); loggerChain.logMessage(Logger.ERROR, Error 级别); 在这个示例中，创建了一个日志处理链。不同级别的日志将被相应级别的处理器处理。责任链模式让日志系统的扩展和维护变得更加灵活。 输出结果： Standard Console::Logger: INFO 级别File::Logger: Debug 级别Standard Console::Logger: Debug 级别Error Console::Logger: Error 级别File::Logger: Error 级别Standard Console::Logger: Error 级别 Java 面试指南（付费）收录的华为 OD 原题：请说说责任链模式。 02、什么是工厂模式？ 推荐阅读：refactoringguru.cn：工厂模式 工厂模式（Factory Pattern）属于创建型设计模式，主要用于创建对象，而不暴露创建对象的逻辑给客户端。 其在父类中提供一个创建对象的方法， 允许子类决定实例化对象的类型。 举例来说，卡车 Truck 和轮船 Ship 都必须实现运输工具 Transport 接口，该接口声明了一个名为 deliver 的方法。 卡车都实现了 deliver 方法，但是卡车的 deliver 是在陆地上运输，而轮船的 deliver 是在海上运输。 refactoringguru.cn：工厂模式 调用工厂方法的代码（客户端代码）无需了解不同子类之间的差别，只管调用接口的 deliver 方法即可。 工厂模式的主要类型①、简单工厂模式（Simple Factory）：它引入了创建者的概念，将实例化的代码从应用程序的业务逻辑中分离出来。简单工厂模式包括一个工厂类，它提供一个方法用于创建对象。 class SimpleFactory public static Transport createTransport(String type) if (truck.equalsIgnoreCase(type)) return new Truck(); else if (ship.equalsIgnoreCase(type)) return new Ship(); return null; public static void main(String[] args) Transport truck = SimpleFactory.createTransport(truck); truck.deliver(); Transport ship = SimpleFactory.createTransport(ship); ship.deliver(); ②、工厂方法模式（Factory Method）：定义一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类的实例化推迟到子类进行。 interface Transport void deliver();class Truck implements Transport @Override public void deliver() System.out.println(在陆地上运输); class Ship implements Transport @Override public void deliver() System.out.println(在海上运输); interface TransportFactory Transport createTransport();class TruckFactory implements TransportFactory @Override public Transport createTransport() return new Truck(); class ShipFactory implements TransportFactory @Override public Transport createTransport() return new Ship(); public class FactoryMethodPatternDemo public static void main(String[] args) TransportFactory truckFactory = new TruckFactory(); Transport truck = truckFactory.createTransport(); truck.deliver(); TransportFactory shipFactory = new ShipFactory(); Transport ship = shipFactory.createTransport(); ship.deliver(); 应用场景 数据库访问层（DAL）组件：工厂方法模式适用于数据库访问层，其中需要根据不同的数据库（如MySQL、PostgreSQL、Oracle）创建不同的数据库连接。工厂方法可以隐藏这些实例化逻辑，只提供一个统一的接口来获取数据库连接。 日志记录：当应用程序需要实现多种日志记录方式（如向文件记录、数据库记录或远程服务记录）时，可以使用工厂模式来设计一个灵活的日志系统，根据配置或环境动态决定具体使用哪种日志记录方式。 Java 面试指南（付费）收录的华为一面原题：说下工厂模式，场景 03、什么是单例模式？ 推荐阅读：refactoringguru.cn：单例模式 单例模式（Singleton Pattern）是一种创建型设计模式，它确保一个类只有一个实例，并提供一个全局访问点来获取该实例。单例模式主要用于控制对某些共享资源的访问，例如配置管理器、连接池、线程池、日志对象等。 refactoringguru.cn：单例模式 实现单例模式的关键点？ 私有构造方法：确保外部代码不能通过构造器创建类的实例。 私有静态实例变量：持有类的唯一实例。 公有静态方法：提供全局访问点以获取实例，如果实例不存在，则在内部创建。 常见的单例模式实现？①、饿汉式如何实现单例？饿汉式单例（Eager Initialization）在类加载时就急切地创建实例，不管你后续用不用得到，这也是饿汉式的来源，简单但不支持延迟加载实例。 public class Singleton private static final Singleton instance = new Singleton(); private Singleton() public static Singleton getInstance() return instance; ②、懒汉式如何实现单例？懒汉式单例（Lazy Initialization）在实际使用时才创建实例，“确实懒”（😂）。这种实现方式需要考虑线程安全问题，因此一般会带上 synchronized 关键字。 public class Singleton private static Singleton instance; private Singleton() public static synchronized Singleton getInstance() if (instance == null) instance = new Singleton(); return instance; 在技术派实战项目中，我就使用了懒汉式单例模式，实现了一个基于微信 native 支付的 Service。 技术派：基于双重判空的懒汉式单例 ③、双重检查锁如何实现单例？双重检查锁用 synchronized 同步代码块替代了 synchronized 同步方法。并且在 instance 前加上 volatile 关键字，防止指令重排，因为 instance = new Singleton() 并不是一个原子操作，可能会被重排序，导致其他线程获取到未初始化完成的实例。 class Singleton private static volatile Singleton instance; private Singleton() public static Singleton getInstance() if (instance == null) synchronized (Singleton.class) if (instance == null) instance = new Singleton(); return instance; 当 instance 创建后，再次调用 getInstance 方法时，不会进入同步代码块，从而提高了性能。 ④、静态内部类如何实现单例？利用 Java 的静态内部类（Static Nested Class）和类加载机制来实现线程安全的延迟初始化。 public class Singleton private Singleton() private static class SingletonHolder private static final Singleton INSTANCE = new Singleton(); public static Singleton getInstance() return SingletonHolder.INSTANCE; 当第一次加载 Singleton 类时并不会初始化 SingletonHolder，只有在第一次调用 getInstance 方法时才会导致 SingletonHolder 被加载，从而实例化 instance。 ⑤、枚举如何实现单例？使用枚举（Enum）实现单例是最简单的方式，不仅不需要考虑线程同步问题，还能防止反射攻击和序列化问题。 public enum Singleton INSTANCE; // 可以添加实例方法 单例模式的好处有哪些？单例模式能确保一个类仅有一个实例，并提供一个全局访问点来访问这个实例。 这对于需要控制资源使用或需要共享资源的情况非常有用，比如数据库连接池，通过单例模式，可以避免对资源的重复创建和销毁，从而提高资源利用率和系统性能。 单例模式有几种实现方式？单例模式有 5 种实现方式，常见的有饿汉式、懒汉式、双重检查锁定、静态内部类和枚举。 Java 面试指南（付费）收录的华为一面原题：说下单例模式，有几种 Java 面试指南（付费）收录的腾讯面经同学 22 暑期实习一面面试原题：单例模式的好处 Java 面试指南（付费）收录的美团面经同学 16 暑期实习一面面试原题：讲讲设计模式，讲讲单例模式有哪些情况（饿汉和懒汉），具体该如何使用 Java 面试指南（付费）收录的字节跳动面经同学 1 Java 后端技术一面面试原题：单例模式有几种实现方式？单例模式最常用的实现方式是哪种？为什么？ Java 面试指南（付费）收录的腾讯云智面经同学 16 一面面试原题：手写单例模式，各种情况，怎么保证线程安全？ Java 面试指南（付费）收录的携程面经同学 10 Java 暑期实习一面面试原题：单例模式，如何线程安全 Java 面试指南（付费）收录的同学 D 小米一面原题：单例模式有几种 04、了解哪些设计模式？单例模式、策略模式和工厂模式。 在需要控制资源访问，如配置管理、连接池管理时经常使用单例模式。它确保了全局只有一个实例，并提供了一个全局访问点。 在有多种算法或策略可以切换使用的情况下，我会使用策略模式。像技术派实战项目中，我就使用策略模式对接了讯飞星火、OpenAI、智谱 AI 等多家大模型，实现了一个可以自由切换大模型基座的智能助手服务。 技术派派聪明 AI 助手 策略模式的好处是，不用在代码中写 ifelse 判断，而是将不同的 AI 服务封装成不同的策略类，通过工厂模式创建不同的 AI 服务实例，从而实现 AI 服务的动态切换。 后面想添加新的 AI 服务，只需要增加一个新的策略类，不需要修改原有代码，这样就提高了代码的可扩展性。 Java 面试指南（付费）收录的字节跳动面经同学 1 Java 后端技术一面面试原题：了解哪些设计模式？ Java 面试指南（付费）收录的奇安信面经同学 1 Java 技术一面面试原题：你真正使用过哪些设计模式？ Java 面试指南（付费）收录的农业银行面经同学 7 Java 后端面试原题：介绍你熟悉的设计模式 Java 面试指南（付费）收录的华为 OD 面经同学 1 一面面试原题：你了解的设计模式 Java 面试指南（付费）收录的百度面经同学 1 文心一言 25 实习 Java 后端面试原题：你有哪些熟悉的设计模式？ Java 面试指南（付费）收录的招银网络科技面经同学 9 Java 后端技术一面面试原题：说一说常用的设计模式 Java 面试指南（付费）收录的vivo 面经同学 10 技术一面面试原题：了解哪些设计模式，开闭原则 Java 面试指南（付费）收录的京东面经同学 5 Java 后端技术一面面试原题：设计模式 05、什么是策略模式？策略模式是一种行为型设计模式，它定义了一系列的算法，将每个算法封装起来，使得它们可以相互替换。这种模式通常用于实现不同的业务规则，其中每种策略封装了特定的行为或算法。 图片来源于天未（闵大为） 特别适合优化程序中的复杂条件分支语句（if-else）。 在策略模式中，有三个角色：上下文、策略接口和具体策略。 策略接口：定义所有支持算法的公共接口。 具体策略：实现策略接口的类，提供具体的算法实现。 上下文：使用策略的类。通常包含一个引用指向策略接口，可以在运行时改变其具体策略。 技术派教程 比如说在技术派中，用户可以自由切换 AI 服务，服务端可以通过 ifesle 进行判断，但如果后续需要增加新的 AI 服务，就需要修改代码，这样不够灵活。 因此，我们使用了策略模式，将不同的 AI 服务封装成不同的策略类，通过工厂模式创建不同的 AI 服务实例，从而实现 AI 服务的动态切换。 @Servicepublic class PaiAiDemoServiceImpl extends AbsChatService @Override public AISourceEnum source() return AISourceEnum.PAI_AI; @Slf4j@Servicepublic class ChatGptAiServiceImpl extends AbsChatService @Override public AISourceEnum source() return AISourceEnum.CHAT_GPT_3_5; @Slf4j@Servicepublic class XunFeiAiServiceImpl extends AbsChatService @Override public AISourceEnum source() return AISourceEnum.XUN_FEI_AI; Java 面试指南（付费）收录的京东面经同学 1 Java 技术一面面试原题：谈谈对gpt的了解，大语言模型的原理，基于大模型如何去和一些业务做结合，有什么场景可以做，项目中用了哪些设计模式 Java 面试指南（付费）收录的微众银行同学 1 Java 后端一面的原题：if else过多怎么解决？ Java 面试指南（付费）收录的美团同学 2 优选物流调度技术 2 面面试原题：设计模式，策略模式 Java 面试指南（付费）收录的美团面经同学 4 一面面试原题：策略模式，自己的代码用过什么设计模式 Java 面试指南（付费）收录的字节跳动同学 17 后端技术面试原题：用过哪些策略模式 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇👍 面渣逆袭 Java 集合框架篇👍 面渣逆袭 Java 并发编程篇👍 面渣逆袭 JVM 篇👍 面渣逆袭 Spring 篇👍 面渣逆袭 Redis 篇👍 面渣逆袭 MyBatis 篇👍 面渣逆袭 MySQL 篇👍 面渣逆袭操作系统篇👍 面渣逆袭计算机网络篇👍 面渣逆袭RocketMQ篇👍 面渣逆袭分布式篇👍 面渣逆袭微服务篇👍 面渣逆袭设计模式篇 👍 面渣逆袭Linux篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括Java基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 图文详解 5 道设计模式面试高频题，这次吊打面试官，我觉得稳了（手动 dog）。","tags":["基础","设计模式"],"categories":["Java问答笔记"]},{"title":"微服务学习笔记","path":"/2025/09/29/Java问答笔记/微服务学习笔记/","content":"1.4 万字 33 张手绘图，详解 33 道微服务（Dubbo、Spring Cloud）面试高频题（让天下没有难背的八股），面渣背会这些八股文，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 概览1.什么是微服务？微服务（Microservices）是一种软件架构风格，将一个大型应用程序划分为一组小型、自治且松耦合的服务。每个微服务负责执行特定的业务功能，并通过轻量级通信机制（如 HTTP）相互协作。每个微服务可以独立开发、部署和扩展，使得应用程序更加灵活、可伸缩和可维护。 在微服务的架构演进中，一般可能会存在这样的演进方向：单体式–服务化–微服务。 单体服务一般是所有项目最开始的样子： 单体服务（Monolithic Service）是一种传统的软件架构方式，将整个应用程序作为一个单一的、紧耦合的单元进行开发和部署。单体服务通常由多个模块组成，这些模块共享同一个数据库和代码库。然而，随着应用程序规模的增长，单体服务可能变得庞大且难以维护，且部署和扩展困难。 后来，单体服务过大，维护困难，渐渐演变到了分布式的 SOA： SOA（Service-Oriented Architecture，面向服务的架构）是一种软件架构设计原则，强调将应用程序拆分为相互独立的服务，通过标准化的接口进行通信。SOA 关注于服务的重用性和组合性，但并没有具体规定服务的大小。 微服务是在 SOA 的基础上进一步发展而来，是一种特定规模下的服务拆分和部署方式。微服务架构强调将应用程序拆分为小型、自治且松耦合的服务，每个服务都专注于特定的业务功能。这种架构使得应用程序更加灵活、可伸缩和可维护。 需要注意的是，微服务是一种特定的架构风格，而 SOA 是一种设计原则。微服务可以看作是对 SOA 思想的一种具体实践方式，但并不等同于 SOA。 架构演进简图 微服务与单体服务的区别在于规模和部署方式。微服务将应用程序拆分为更小的、自治的服务单元，每个服务都有自己的数据库和代码库，可以独立开发、测试、部署和扩展，带来了更大的灵活性、可维护性、可扩展性和容错性。 2.微服务带来了哪些挑战？微服务架构不是万金油，尽它有很多优点，但是对于是否采用微服务架构，是否将原来的单体服务进行拆分，还是要考虑到服务拆分后可能带来的一些挑战和问题： 微服务带来的挑战 系统复杂性增加：一个服务拆成了多个服务，整体系统的复杂性增加，需要处理服务之间的通信、部署、监控和维护等方面的复杂性。 服务间通信开销：微服务之间通过网络进行通信，传递数据需要额外的网络开销和序列化开销，可能导致性能瓶颈和增加系统延迟。 数据一致性和事务管理：每个微服务都有自己的数据存储，数据一致性和跨服务的事务管理变得更加复杂，需要额外解决分布式事务和数据同步的问题。 部署和运维复杂性：微服务架构涉及多个独立部署的服务，对于部署、监控和容错机制的要求更高，需要建立适当的部署管道和自动化工具，以简化部署和运维过程。 团队沟通和协作成本：每个微服务都由专门的团队负责，可能增加团队之间的沟通和协作成本。需要有效的沟通渠道和协作机制，确保服务之间的协调和一致性。 服务治理和版本管理：随着微服务数量的增加，服务的治理和版本管理变得更加复杂。需要考虑服务的注册发现、负载均衡、监控和故障处理等方面，以确保整个系统的可靠性和稳定性。 分布式系统的复杂性：微服务架构涉及构建和管理分布式系统，而分布式系统本身具有一些固有的挑战，如网络延迟、分布式一致性和容错性。 简单说，采用微服务需要权衡这些问题和挑战，根据实际的需求来选择对应的技术方案，很多时候单体能搞定的也可以用单体，不能为了微服务而微服务。 3.现在有哪些流行的微服务解决方案？目前最主流的微服务开源解决方案有三种： Dubbo： Dubbo工作原理图-来源官网 Dubbo 是一个高性能、轻量级的 Java 微服务框架，最初由阿里巴巴（Alibaba）开发并于 2011 年开源。它提供了服务注册与发现、负载均衡、容错、分布式调用等功能，后来一度停止维护，在近两年，又重新开始迭代，并推出了 Dubbo3。 Dubbo 使用基于 RPC（Remote Procedure Call）的通信模型，具有较高的性能和可扩展性。它支持多种传输协议（如 TCP、HTTP、Redis）和序列化方式（如 JSON、Hessian、Protobuf），可根据需求进行配置。 Dubbo 更多地被认为是一个高性能的 RPC（远程过程调用）框架，一些服务治理功能依赖于第三方组件实现，比如使用 ZooKeeper、Apollo 等等。 Spring Cloud Netflix： Spring Cloud Netflix 是 Spring Cloud 的一个子项目，结合了 Netflix 开源的多个组件，但是 Netflix 自 2018 年停止维护和更新 Netflix OSS 项目，包括 Eureka、Hystrix 等组件，所以 Spring Cloud Netflix 也逐渐进入了维护模式。 该项目包含了许多流行的 Netflix 组件，如 Eureka（服务注册与发现）、Ribbon（客户端负载均衡）、Hystrix（断路器）、Zuul（API 网关）等。它们都是高度可扩展的、经过大规模实践验证的微服务组件。 Spring Cloud Alibaba： 这三种方案有什么区别吗？三种方案的区别： 特点 Dubbo Spring Cloud Netflix Spring Cloud Alibaba 开发语言 Java Java Java 服务治理 提供完整的服务治理功能 提供部分服务治理功能 提供完整的服务治理功能 服务注册与发现 ZooKeeperNacos EurekaConsul Nacos 负载均衡 自带负载均衡策略 Ribbon Ribbon\\Dubbo 负载均衡策略 服务调用 RPC 方式 RestTemplateFeign FeignRestTemplateDubbo 熔断器 Sentinel Hystrix SentinelResilience4j 配置中心 Apollo Spring Cloud Config Nacos Config API 网关 HigressAPISIX ZuulGateway Spring Cloud Gateway 分布式事务 Seata 不支持分布式事务 Seata 限流和降级 Sentinel Hystrix Sentinel 分布式追踪和监控 Skywalking Spring Cloud Sleuth + Zipkin SkyWalking 或 Sentinel Dashboard 微服务网格 Dubbo Mesh 不支持微服务网格 Service Mesh（Nacos+Dubbo Mesh） 社区活跃度 相对较高 目前较低 相对较高 孵化和成熟度 孵化较早，成熟度较高 成熟度较高 孵化较新，但迅速发展 Spring Cloud Alibaba 是 Spring Cloud 的另一个子项目，与阿里巴巴的分布式应用开发框架相关。它提供了一整套与 Alibaba 生态系统集成的解决方案。 该项目包括 Nacos（服务注册与发现、配置管理）、Sentinel（流量控制、熔断降级）、RocketMQ（消息队列）等组件，以及与 Alibaba Cloud（阿里云）的集成。它为构建基于 Spring Cloud 的微服务架构提供了丰富的选项。 据说 SpringCloud Alibaba 项目的发起人已经跑路去了腾讯，并发起了 SpringCloud Tecent 项目，社区发展存在隐忧。 在面试中，微服务一般主要讨论的是 Spring Cloud Netflix，其次是 Spring Cloud Alibaba，Dubbo 更多的是作为一个 RPC 框架来问。 4.说下微服务有哪些组件？微服务给系统开发带来了一些问题和挑战，如服务调用的复杂性、分布式事务的处理、服务的动态管理等。为了更好地解决这些问题和挑战，各种微服务治理的组件应运而生，充当微服务架构的基石和支撑。 微服务组件示意图 微服务的各个组件和常见实现： 注册中心：用于服务的注册与发现，管理微服务的地址信息。常见的实现包括： Spring Cloud Netflix：Eureka、Consul Spring Cloud Alibaba：Nacos 配置中心：用于集中管理微服务的配置信息，可以动态修改配置而不需要重启服务。常见的实现包括： Spring Cloud Netflix：Spring Cloud Config Spring Cloud Alibaba：Nacos Config 远程调用：用于在不同的微服务之间进行通信和协作。常见的实现保包括： RESTful API：如 RestTemplate、Feign RPC（远程过程调用）：如 Dubbo、gRPC API 网关：作为微服务架构的入口，统一暴露服务，并提供路由、负载均衡、安全认证等功能。常见的实现包括： Spring Cloud Netflix：Zuul、Gateway Spring Cloud Alibaba：Gateway、Apisix 等 分布式事务：保证跨多个微服务的一致性和原子性操作。常见的实现包括： Spring Cloud Alibaba：Seata 熔断器：用于防止微服务之间的故障扩散，提高系统的容错能力。常见的实现包括： Spring Cloud Netflix：Hystrix Spring Cloud Alibaba：Sentinel、Resilience4j 限流和降级：用于防止微服务过载，对请求进行限制和降级处理。常见的实现包括： Spring Cloud Netflix：Hystrix Spring Cloud Alibaba：Sentinel 分布式追踪和监控：用于跟踪和监控微服务的请求流程和性能指标。常见的实现包括： Spring Cloud Netflix：Spring Cloud Sleuth + Zipkin Spring Cloud Alibaba：SkyWalking、Sentinel Dashboard GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 注册中心5.注册中心是用来干什么的？注册中心是用来管理和维护分布式系统中各个服务的地址和元数据的组件。它主要用于实现服务发现和服务注册功能。 注册中心示意图 总结一下注册中心的作用： 服务注册：各个服务在启动时向注册中心注册自己的网络地址、服务实例信息和其他相关元数据。这样，其他服务就可以通过注册中心获取到当前可用的服务列表。 服务发现：客户端通过向注册中心查询特定服务的注册信息，获得可用的服务实例列表。这样客户端就可以根据需要选择合适的服务进行调用，实现了服务间的解耦。 负载均衡：注册中心可以对同一服务的多个实例进行负载均衡，将请求分发到不同的实例上，提高整体的系统性能和可用性。 故障恢复：注册中心能够监测和检测服务的状态，当服务实例发生故障或下线时，可以及时更新注册信息，从而保证服务能够正常工作。 服务治理：通过注册中心可以进行服务的配置管理、动态扩缩容、服务路由、灰度发布等操作，实现对服务的动态管理和控制。 6.SpringCloud 可以选择哪些注册中心？SpringCloud 可以与多种注册中心进行集成，常见的注册中心包括： Eureka：Eureka 是 Netflix 开源的服务发现框架，具有高可用、弹性、可扩展等特点，并与 Spring Cloud 集成良好。 Consul：Consul 是一种分布式服务发现和配置管理系统，由 HashiCorp 开发。它提供了服务注册、服务发现、健康检查、键值存储等功能，并支持多数据中心部署。 ZooKeeper：ZooKeeper 是 Apache 基金会开源的分布式协调服务，可以用作服务注册中心。它具有高可用、一致性、可靠性等特点。 Nacos：Nacos 是阿里巴巴开源的一个动态服务发现、配置管理和服务管理平台。它提供了服务注册和发现、配置管理、动态 DNS 服务等功能。 etcd：etcd 是 CoreOS 开源的一种分布式键值存储系统，可以被用作服务注册中心。它具有高可用、强一致性、分布式复制等特性。 7.说下 Eureka、ZooKeeper、Nacos 的区别？ 特性 Eureka ZooKeeper Nacos 开发公司 Netflix Apache 基金会 阿里巴巴 CAP AP（可用性和分区容忍性） CP（一致性和分区容忍性） 既支持 AP，也支持 CP 功能 服务注册与发现 分布式协调、配置管理、分布式锁 服务注册与发现、配置管理、服务管理 定位 适用于构建基于 HTTP 的微服务架构 通用的分布式协调服务框架 适用于微服务和云原生应用 访问协议 HTTP TCP HTTPDNS 自我保护 支持 - 支持 数据存储 内嵌数据库、多个实例形成集群 ACID 特性的分布式文件系统 ZAB 协议 内嵌数据库、MySQL 等 健康检查 Client Beat Keep Alive TCPHTTPMYSQLClient Beat 特点 简单易用、自我保护机制 高性能、强一致性 动态配置管理、流量管理、灰度发布等 可以看到 Eureka 和 ZooKeeper 的最大区别是一个支持AP，一个支持CP，Nacos 既支持既支持AP，也支持CP。 8.Eureka 实现原理了解吗？ Eureka原理示意图 Eureka 的实现原理，大概可以从这几个方面来看： 服务注册与发现: 当一个服务实例启动时，它会向 Eureka Server 发送注册请求，将自己的信息注册到注册中心。Eureka Server 会将这些信息保存在内存中，并提供 REST 接口供其他服务查询。服务消费者可以通过查询服务实例列表来获取可用的服务提供者实例，从而实现服务的发现。 服务健康检查: Eureka 通过心跳机制来检测服务实例的健康状态。服务实例会定期向 Eureka Server 发送心跳，也就是续约，以表明自己的存活状态。如果 Eureka Server 在一定时间内没有收到某个服务实例的心跳，则会将其标记为不可用，并从服务列表中移除，下线实例。 服务负载均衡: Eureka 客户端在调用其他服务时，会从本地缓存中获取服务的注册信息。如果缓存中没有对应的信息，则会向 Eureka Server 发送查询请求。Eureka Server 会返回一个可用的服务实例列表给客户端，客户端可以使用负载均衡算法选择其中一个进行调用。 其它的注册中心，如 Nacos、Consul 等等，在服务注册和发现上，实现原理都是大同小异。 9.Eureka Server 怎么保证高可用？Eureka Server 保证高可用，主要通过这三个方面来实现： Eureka Server 多实例部署: 通过将多个 Eureka Server 实例部署在不同的节点上，可以实现高可用性。当其中一个实例发生故障时，其他实例仍然可以提供服务，并保持注册信息的一致性。 服务注册信息的复制: 当一个服务实例向 Eureka Server 注册时，每个 Eureka Server 实例都会复制其他实例的注册信息，以保持数据的一致性。当某个 Eureka Server 实例发生故障时，其他实例可以接管其工作，保证整个系统的正常运行。 自我保护机制: Eureka 还具有自我保护机制。当 Eureka Server 节点在一定时间内没有接收到心跳时，它会进入自我保护模式。在自我保护模式下，Eureka Server 不再剔除注册表中的服务实例，以保护现有的注册信息。这样可以防止由于网络抖动或其他原因导致的误剔除，进一步提高系统的稳定性。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 配置中心10.为什么微服务需要配置中心？微服务架构中的每个服务通常都需要一些配置信息，例如数据库连接地址、服务端口、日志级别等。这些配置可能因为不同环境、不同部署实例或者动态运行时需要进行调整和管理。 微服务的实例一般非常多，如果每个实例都需要一个个地去做这些配置，那么运维成本将会非常大，这时候就需要一个集中化的配置中心，去管理这些配置。 11.SpringCloud 可以选择哪些配置中心？和注册中心一样，SpringCloud 也支持对多种配置中心的集成。常见的配置中心选型包括： Spring Cloud Config：官方推荐的配置中心，支持将配置文件存储在 Git、SVN 等版本控制系统中，并提供 RESTful API 进行访问和管理。 ZooKeeper：一个开源的分布式协调服务，可以用作配置中心。它具有高可用性、一致性和通知机制等特性。 Consul：另一个开源的分布式服务发现和配置管理工具，也可用作配置中心。支持多种配置文件格式，提供健康检查、故障转移和动态变更等功能。 Etcd：一个分布式键值存储系统，可用作配置中心。它使用基于 Raft 算法的一致性机制，提供分布式数据一致性保证。 Apollo：携程开源的配置中心，支持多种语言和框架。提供细粒度的配置权限管理、配置变更通知和灰度发布等高级特性，还有可视化的配置管理界面。 Nacos：阿里巴巴开源的服务发现、配置管理和服务管理平台，也可以作为配置中心使用。支持服务注册与发现、动态配置管理、服务健康监测和动态 DNS 服务等功能。 12.Nacos 配置中心的原理了解吗？配置中心，说白了就是一句话：配置信息的 CRUD。 配置中心 具体的实现大概可以分成这么几个部分： 配置信息存储：Nacos 默认使用内嵌数据库 Derby 来存储配置信息，还可以采用 MySQL 等关系型数据库。 注册配置信息：服务启动时，Nacos Client 会向 Nacos Server 注册自己的配置信息，这个注册过程就是把配置信息写入存储，并生成版本号。 获取配置信息：服务运行期间，Nacos Client 通过 API 从 Nacos Server 获取配置信息。Server 根据键查找对应的配置信息，并返回给 Client。 监听配置变化：Nacos Client 可以通过注册监听器的方式，实现对配置信息的监听。当配置信息发生变化时，Nacos Server 会通知已注册的监听器，并触发相应的回调方法。 13.Nacos 配置中心长轮询机制？一般来说客户端和服务端的交互分为两种：推（Push）和拉（Pull），Nacos 在Pull的基础上，采用了长轮询来进行配置的动态刷新。 在长轮询模式下，客户端定时向服务端发起请求，检查配置信息是否发生变更。如果没有变更，服务端会”hold”住这个请求，即暂时不返回结果，直到配置发生变化或达到一定的超时时间。 具体的实现过程如下： Nacos长轮询 客户端发起 Pull 请求，服务端检查配置是否有变更。如果没有变更，则设置一个定时任务，在一段时间后执行，并将当前的客户端连接加入到等待队列中。 在等待期间，如果配置发生变更，服务端会立即返回结果给客户端，完成一次”推送”操作。 如果在等待期间没有配置变更，等待时间达到预设的超时时间后，服务端会自动返回结果给客户端，即使配置没有变更。 如果在等待期间，通过 Nacos Dashboard 或 API 对配置进行了修改，会触发一个事件机制，服务端会遍历等待队列，找到发生变更的配置项对应的客户端连接，并将变更的数据通过连接返回，完成一次”推送”操作。 通过长轮询的方式，Nacos 客户端能够实时感知配置的变化，并及时获取最新的配置信息。同时，这种方式也降低了服务端的压力，避免了大量的长连接占用内存资源。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 远程调用14.能说下 HTTP 和 RPC 的区别吗？HTTP 和 RPC 不算是一个层面上的东西： 三分恶面渣逆袭：HTTP和RPC HTTP 是应用层协议，用于传输超文本数据，基于请求-响应模型，常用于 Web 开发、API 调用等场景。 RPC 是远程过程调用协议，用于实现分布式系统中不同节点之间的通信，基于方法调用模型，常用于构建面向服务的微服务架构。 在微服务架构中，Feign 和 Dubbo 都是用于实现远程调用的框架，Feign 基于 HTTP 协议，Dubbo 基于 RPC 协议。 如果硬要说区别的话，如下： - HTTP RPC 定义 HTTP（超文本传输协议）是一种用于传输超文本的协议。 RPC（远程过程调用）是一种用于实现分布式系统中不同节点之间通信的协议。 通信方式 基于请求-响应模型，客户端发送请求，服务器返回响应。 基于方法调用模型，客户端调用远程方法并等待结果。 传输协议 基于 TCP 协议，可使用其他传输层协议如 TLSSSL 进行安全加密。 可以使用多种传输协议，如 TCP、UDP 等。 数据格式 基于文本，常用的数据格式有 JSON、XML 等。 可以使用各种数据格式，如二进制、JSON、Protocol Buffers 等。 接口定义 使用 RESTful 风格的接口进行定义，常用的方法有 GET、POST、PUT、DELETE 等。 使用 IDL（接口定义语言）进行接口定义，如 Protocol Buffers、Thrift 等。 跨语言性 支持跨语言通信，可以使用 HTTP 作为通信协议实现不同语言之间的通信。 支持跨语言通信，可以使用 IDL 生成不同语言的客户端和服务端代码。 灵活性 更加灵活，适用于不同类型的应用场景，如 Web 开发、API 调用等。 更加高效，适用于需要高性能和低延迟的分布式系统。 RPC了解吗？RPC（Remote Procedure Call）是一种远程过程调用协议，用于实现分布式系统中不同节点之间的通信。它基于方法调用模型，允许客户端调用远程服务的方法，并等待结果返回。 像 gRPC、Dubbo、Thrift 等都是 RPC 框架，它们提供了 IDL（接口定义语言）来定义服务接口，以及序列化协议来进行数据传输。 Java 面试指南（付费）收录的腾讯面经同学 24 面试原题：RPC 了解吗？ 15.那 Feign 和 Dubbo 的区别呢？这两个才是适合拿来比较的东西： - Feign Dubbo 定义 Feign 是一个声明式的 Web 服务客户端，用于简化 HTTP API 的调用。 Dubbo 是一个分布式服务框架，用于构建面向服务的微服务架构。 通信方式 基于 HTTP 协议，使用 RESTful 风格的接口进行定义和调用。 基于 RPC 协议，支持多种序列化协议如 gRPC、Hessian 等。 服务发现 通常结合服务注册中心（如 Eureka、Consul）进行服务发现和负载均衡。 通过 ZooKeeper、Nacos 等进行服务注册和发现，并提供负载均衡功能。 服务治理 不直接提供服务治理功能，需要结合其他组件或框架进行服务治理。 提供服务注册与发现、负载均衡、容错机制、服务降级等服务治理功能。 跨语言性 支持跨语言通信，可以使用 HTTP 作为通信协议实现不同语言之间的通信。 支持跨语言通信，通过 Dubbo 的 IDL 生成不同语言的客户端和服务端代码。 生态系统 集成了 Spring Cloud 生态系统，与 Spring Boot 无缝集成。 拥有完整的生态系统，包括注册中心、配置中心、监控中心等组件。 适用场景 适用于构建 RESTful 风格的微服务架构，特别适合基于 HTTP 的微服务调用。 适用于构建面向服务的微服务架构，提供更全面的服务治理和容错机制。 需要注意的是，Feign 和 Dubbo 并不是互斥的关系。实际上，Dubbo 可以使用 HTTP 协议作为通信方式，而 Feign 也可以集成 RPC 协议进行远程调用。选择使用哪种远程调用方式取决于具体的业务需求和技术栈的选择。 16.说一下 Fegin?Feign 是一个声明式的 Web 服务客户端，它简化了使用基于 HTTP 的远程服务的开发。 Feign 是在 RestTemplate 和 Ribbon 的基础上进一步封装，使用 RestTemplate 实现 Http 调用，使用 Ribbon 实现负载均衡。 Feign封装 Feign 的主要特点和功能包括： 声明式 API：Feign 允许开发者使用简单的注解来定义和描述对远程服务的访问。通过使用注解，开发者可以轻松地指定 URL、HTTP 方法、请求参数、请求头等信息，使得远程调用变得非常直观和易于理解。 @FeignClient(name = example, url = https://api.example.com) public interface ExampleService @GetMapping(/endpoint) String getEndpointData(); 集成负载均衡：Feign 集成了 Ribbon 负载均衡器，可以自动实现客户端的负载均衡。它可以根据服务名和可用实例进行动态路由，并分发请求到不同的服务实例上，提高系统的可用性和可伸缩性。 容错机制：Feign 支持集成 Hystrix 容错框架，可以在调用远程服务时提供容错和断路器功能。当远程服务不可用或响应时间过长时，Feign 可以快速失败并返回预设的响应结果，避免对整个系统造成级联故障。 17.为什么 Feign 第一次调用耗时很长？主要原因是由于 Ribbon 的懒加载机制，当第一次调用发生时，Feign 会触发 Ribbon 的加载过程，包括从服务注册中心获取服务列表、建立连接池等操作，这个加载过程会增加首次调用的耗时。 ribbon: eager-load: enabled: true clients: service-1 那怎么解决这个问题呢？ 可以在应用启动时预热 Feign 客户端，自动触发一次无关紧要的调用，来提前加载 Ribbon 和其他相关组件。这样，就相当于提前进行了第一次调用。 18.Feign 怎么实现认证传递？比较常见的一个做法是，使用拦截器传递认证信息。可以通过实现RequestInterceptor接口来定义拦截器，在拦截器里，把认证信息添加到请求头中，然后将其注册到 Feign 的配置中。 @Configuration public class FeignClientConfig @Bean public RequestInterceptor requestInterceptor() return new RequestInterceptor() @Override public void apply(RequestTemplate template) // 添加认证信息到请求头中 template.header(Authorization, Bearer + getToken()); ; private String getToken() // 获取认证信息的逻辑，可以从SecurityContext或其他地方获取 // 返回认证信息的字符串形式 return your_token; 19.Fegin 怎么做负载均衡？Ribbon?在 Feign 中，负载均衡是通过集成 Ribbon 来实现的。 Ribbon 是 Netflix 开源的一个客户端负载均衡器，可以与 Feign 无缝集成，为 Feign 提供负载均衡的能力。 Ribbon 通过从服务注册中心获取可用服务列表，并通过负载均衡算法选择合适的服务实例进行请求转发，实现客户端的负载均衡。 客户端负载均衡 20.说说有哪些负载均衡算法？常见的负载均衡算法包含以下几种： 常见负载均衡算法 轮询算法（Round Robin）：轮询算法是最简单的负载均衡算法之一。它按照顺序将请求依次分配给每个后端服务器，循环往复。当请求到达时，负载均衡器按照事先定义的顺序选择下一个服务器。轮询算法适用于后端服务器具有相同的处理能力和性能的场景。 加权轮询算法（Weighted Round Robin）：加权轮询算法在轮询算法的基础上增加了权重的概念。每个后端服务器都被赋予一个权重值，权重值越高，被选中的概率就越大。这样可以根据服务器的处理能力和性能调整请求的分配比例，使得性能较高的服务器能够处理更多的请求。 随机算法（Random）：随机算法将请求随机分配给后端服务器。每个后端服务器有相等的被选中概率，没有考虑服务器的实际负载情况。这种算法简单快速，适用于后端服务器性能相近且无需考虑请求处理能力的场景。 加权随机算法（Weighted Random）：加权随机算法在随机算法的基础上引入了权重的概念。每个后端服务器被赋予一个权重值，权重值越高，被选中的概率就越大。这样可以根据服务器的处理能力和性能调整请求的分配比例。 最少连接算法（Least Connection）：最少连接算法会根据后端服务器当前的连接数来决定请求的分配。负载均衡器会选择当前连接数最少的服务器进行请求分配，以保证后端服务器的负载均衡。这种算法适用于后端服务器的处理能力不同或者请求的处理时间不同的场景。 哈希算法（Hash）：哈希算法会根据请求的某个特定属性（如客户端 IP 地址、请求 URL 等）计算哈希值，然后根据哈希值选择相应的后端服务器。 常见的负载均衡器，比如 Ribbion、Gateway 等等，基本都支持这些负载均衡算法。 关于 Dubbo，后面会单独出一期。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 服务容灾21.什么是服务雪崩？在微服务中，假如一个或者多个服务出现故障，如果这时候，依赖的服务还在不断发起请求，或者重试，那么这些请求的压力会不断在下游堆积，导致下游服务的负载急剧增加。不断累计之下，可能会导致故障的进一步加剧，可能会导致级联式的失败，甚至导致整个系统崩溃，这就叫服务雪崩。 服务雪崩 一般，为了防止服务雪崩，可以采用这些措施： 服务高可用部署：确保各个服务都具备高可用性，通过冗余部署、故障转移等方式来减少单点故障的影响。 限流和熔断：对服务之间的请求进行限流和熔断，以防止过多的请求涌入导致后端服务不可用。 缓存和降级：合理使用缓存来减轻后端服务的负载压力，并在必要时进行服务降级，保证核心功能的可用性。 22.什么是服务熔断？什么是服务降级？什么是服务熔断？服务熔断是微服务架构中的容错机制，用于保护系统免受服务故障或异常的影响。当某个服务出现故障或异常时，服务熔断可以快速隔离该服务，确保系统稳定可用。 它通过监控服务的调用情况，当错误率或响应时间超过阈值时，触发熔断机制，后续请求将返回默认值或错误信息，避免资源浪费和系统崩溃。 服务熔断还支持自动恢复，重新尝试对故障服务的请求，确保服务恢复正常后继续使用。 什么是服务降级？服务降级是也是一种微服务架构中的容错机制，用于在系统资源紧张或服务故障时保证核心功能的可用性。 当系统出现异常情况时，服务降级会主动屏蔽一些非核心或可选的功能，而只提供最基本的功能，以确保系统的稳定运行。通过减少对资源的依赖，服务降级可以保证系统的可用性和性能。 它可以根据业务需求和系统状况来制定策略，例如替换耗时操作、返回默认响应、返回静态错误页面等。 有哪些熔断降级方案实现？目前常见的服务熔断降级实现方案有这么几种： 框架 实现方案 特点 Spring Cloud Netflix Hystrix - 提供线程隔离、服务降级、请求缓存、请求合并等功能 - 可与 Spring Cloud 其他组件无缝集成 - 官方已宣布停止维护，推荐使用 Resilience4j 代替| Spring Cloud|Resilience4j|- 轻量级服务熔断库 - 提供类似于 Hystrix 的功能 - 具有更好的性能和更简洁的 API - 可与 Spring Cloud 其他组件无缝集成| Spring Cloud Alibaba|Sentinel|- 阿里巴巴开源的流量控制和熔断降级组件 - 提供实时监控、流量控制、熔断降级等功能 - 与 Spring Cloud Alibaba 生态系统紧密集成| Dubbo|Dubbo 自带熔断降级机制|- Dubbo 框架本身提供的熔断降级机制 - 可通过配置实现服务熔断和降级 - 与 Dubbo 的 RPC 框架紧密集成| 23.Hystrix 怎么实现服务容错？尽管已经不再更新，但是 Hystrix 是非常经典的服务容错开源库，它提供了多种机制来保护系统： Hystrix服务容错六大机制 服务熔断（Circuit Breaker）：Hystrix 通过设置阈值来监控服务的错误率或响应时间。当错误率或响应时间超过预设的阈值时，熔断器将会打开，后续的请求将不再发送到实际的服务提供方，而是返回预设的默认值或错误信息。这样可以快速隔离故障服务，防止故障扩散，提高系统的稳定性和可用性。 服务降级（Fallback）：当服务熔断打开时，Hystrix 可以提供一个备用的降级方法或返回默认值，以保证系统继续正常运行。开发者可以定义降级逻辑，例如返回缓存数据、执行简化的逻辑或调用其他可靠的服务，以提供有限但可用的功能。 import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand; /** * 服务降级示例 **/ @Service public class MyService @HystrixCommand(fallbackMethod = fallbackMethod) public String myServiceMethod() // 实际的服务调用逻辑 // ... public String fallbackMethod() // 降级方法的逻辑，当服务调用失败时会执行此方法 // 可以返回默认值或执行其他备用逻辑 // ... 请求缓存（Request Caching）：Hystrix 可以缓存对同一请求的响应结果，当下次请求相同的数据时，直接从缓存中获取，避免重复的网络请求，提高系统的性能和响应速度。 请求合并（Request Collapsing）：Hystrix 可以将多个并发的请求合并为一个批量请求，减少网络开销和资源占用。这对于一些高并发的场景可以有效地减少请求次数，提高系统的性能。 实时监控和度量（Real-time Monitoring and Metrics）：Hystrix 提供了实时监控和度量功能，可以对服务的执行情况进行监控和统计，包括错误率、响应时间、并发量等指标。通过监控数据，可以及时发现和解决服务故障或性能问题。 线程池隔离（Thread Pool Isolation）：Hystrix 将每个依赖服务的请求都放在独立的线程池中执行，避免因某个服务的故障导致整个系统的线程资源耗尽。通过线程池隔离，可以提高系统的稳定性和可用性。 24.Sentinel 怎么实现限流的？Sentinel 通过动态管理限流规则，根据定义的规则对请求进行限流控制。具体实现步骤如下： 定义资源：在 Sentinel 中，资源可以是 URL、方法等，用于标识需要进行限流的请求。 // 原本的业务方法. @SentinelResource(blockHandler = blockHandlerForGetUser) public User getUserById(String id) throw new RuntimeException(getUserById command failed); // blockHandler 函数，原方法调用被限流/降级/系统保护的时候调用 public User blockHandlerForGetUser(String id, BlockException ex) return new User(admin); 配置限流规则：在 Sentinel 的配置文件中定义资源的限流规则。规则可以包括资源名称、限流阈值、限流模式（令牌桶或漏桶）等。 private static void initFlowQpsRule() ListFlowRule rules = new ArrayList(); FlowRule rule1 = new FlowRule(); rule1.setResource(resource); // Set max qps to 20 rule1.setCount(20); rule1.setGrade(RuleConstant.FLOW_GRADE_QPS); rule1.setLimitApp(default); rules.add(rule1); FlowRuleManager.loadRules(rules); 监控流量：Sentinel 会监控每个资源的流量情况，包括请求的 QPS（每秒请求数）、线程数、响应时间等。 Sentinel控制台 限流控制：当请求到达时，Sentinel 会根据资源的限流规则判断是否需要进行限流控制。如果请求超过了限流阈值，则可以进行限制、拒绝或进行其他降级处理。 Sentinel总体框架-来源官网 Sentinel 采用的什么限流算法？Sentinel 使用滑动窗口限流算法来实现限流。 滑动窗口限流算法是一种基于时间窗口的限流算法。它将一段时间划分为多个时间窗口，并在每个时间窗口内统计请求的数量。通过动态地调整时间窗口的大小和滑动步长，可以更精确地控制请求的通过速率。 滑动窗口限流可以查看前面的分布式篇。 Sentinel 怎么实现集群限流？Sentinel 利用了 Token Server 和 Token Client 的机制来实现集群限流。 开启集群限流后，Client 向 Token Server 发送请求，Token Server 根据配置的规则决定是否限流。T Token Server和Client GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 服务网关25.什么是 API 网关？API 网关（API Gateway）是一种中间层服务器，用于集中管理、保护和路由对后端服务的访问。它充当了客户端与后端服务之间的入口点，提供了一组统一的接口来管理和控制 API 的访问。 网关示意图 API 网关的主要功能包括： 路由转发：API 网关根据请求的 URL 路径或其他标识，将请求路由到相应的后端服务。通过配置路由规则，可以灵活地将请求分发给不同的后端服务。 负载均衡：API 网关可以在后端服务之间实现负载均衡，将请求平均分发到多个实例上，提高系统的吞吐量和可扩展性。 安全认证与授权：API 网关可以集中处理身份验证和授权，确保只有经过身份验证的客户端才能访问后端服务。它可以与身份提供者（如 OAuth、OpenID Connect）集成，进行用户认证和授权操作。 缓存：API 网关可以缓存后端服务的响应，减少对后端服务的请求次数，提高系统性能和响应速度。 监控与日志：API 网关可以收集和记录请求的指标和日志，提供实时监控和分析，帮助开发人员和运维人员进行故障排查和性能优化。 数据转换与协议转换：API 网关可以在客户端和后端服务之间进行数据格式转换和协议转换，如将请求从 HTTP 转换为 WebSocket，或将请求的参数进行格式转换，以满足后端服务的需求。 API 版本管理：API 网关可以管理不同版本的 API，允许同时存在多个 API 版本，并通过路由规则将请求正确地路由到相应的 API 版本上。 …… 通过使用 API 网关，可以简化前端与后端服务的交互，提供统一的接口和安全性保障，同时也方便了服务治理和监控。它是构建微服务架构和实现 API 管理的重要组件之一。 26.SpringCloud 可以选择哪些 API 网关？使用 SpringCloud 开发，可以采用以下的 API 网关选型： Netflix Zuul（已停止更新）：Netflix Zuul 是 Spring Cloud 早期版本中提供的默认 API 网关。它基于 Servlet 技术栈，可以进行路由、过滤、负载均衡等功能。然而，自 2020 年 12 月起，Netflix 宣布停止对 Zuul 1 的维护，转而支持新的 API 网关项目。 Spring Cloud Gateway：Spring Cloud Gateway 是 Spring Cloud 官方推荐的 API 网关，取代了 Netflix Zuul。它基于非阻塞的 WebFlux 框架，充分利用了响应式编程的优势，并提供了路由、过滤、断路器、限流等特性。Spring Cloud Gateway 还支持与 Spring Cloud 的其他组件集成，如服务发现、负载均衡等。 Kong：Kong 是一个独立的、云原生的 API 网关和服务管理平台，可以与 Spring Cloud 集成。Kong 基于 Nginx，提供了强大的路由、认证、授权、监控和扩展能力。它支持多种插件和扩展，可满足不同的 API 管理需求。 APISIX：APISIX 基于 Nginx 和 Lua 开发，它具有强大的路由、流量控制、插件扩展等功能。APISIX 支持灵活的配置方式，可以根据需求进行动态路由、负载均衡和限流等操作。 …… 27.Spring Cloud Gateway 核心概念？ Gateway原理 在 Spring Cloud Gateway 里，有三个关键组件： Route（路由）：路由是 Spring Cloud Gateway 的基本构建块，它定义了请求的匹配规则和转发目标。通过配置路由，可以将请求映射到后端的服务实例或 URL 上。路由规则可以根据请求的路径、方法、请求头等条件进行匹配，并指定转发的目标 URI。 Predicate（断言）：断言用于匹配请求的条件，如果请求满足断言的条件，则会应用所配置的过滤器。Spring Cloud Gateway 提供了多种内置的断言，如 Path（路径匹配）、Method（请求方法匹配）、Header（请求头匹配）等，同时也支持自定义断言。 Filter（过滤器）：过滤器用于对请求进行处理和转换，可以修改请求、响应以及执行其他自定义逻辑。Spring Cloud Gateway 提供了多个内置的过滤器，如请求转发、请求重试、请求限流等。同时也支持自定义过滤器，可以根据需求编写自己的过滤器逻辑。 我们再来看下 Spring Cloud Gateway 的具体工作流程： SpringCloud工作流程图-来源官方文档 又有两个比较重要的概念： Gateway Handler（网关处理器）：网关处理器是 Spring Cloud Gateway 的核心组件，负责将请求转发到匹配的路由上。它根据路由配置和断言条件进行路由匹配，选择合适的路由进行请求转发。网关处理器还会依次应用配置的过滤器链，对请求进行处理和转换。 Gateway Filter Chain（网关过滤器链）：网关过滤器链由一系列过滤器组成，按照配置的顺序依次执行。每个过滤器可以在请求前、请求后或请求发生错误时进行处理。过滤器链的执行过程可以修改请求、响应以及执行其他自定义逻辑。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 链路追踪28.为什么要用微服务链路追踪？在微服务中，有的山下游可能有十几个服务，如果某一环出了问题，排查起来非常困难，所以，就需要进行链路追踪，来帮助排查问题。 SkyWalking界面 通过链路追踪，可以可视化地追踪请求从一个微服务到另一个微服务的调用情况。除了排查问题，链路追踪黑还可以帮助优化性能，可视化依赖关系、服务监控和告警。 29.SpringCloud 可以选择哪些微服务链路追踪方案？Spring Cloud 提供了多种选择的微服务链路追踪方案。以下是一些常用的方案： Zipkin：Zipkin 是一个开源的分布式实时追踪系统，由 Twitter 开发并贡献给开源社区。Spring Cloud Sleuth 提供了与 Zipkin 的集成，可以通过在微服务中添加相应的依赖和配置，将追踪信息发送到 Zipkin 服务器，并通过 Zipkin UI 进行可视化展示和查询。 Zipkin界面 Jaeger：Jaeger 是 Uber 开源的分布式追踪系统，也被纳入了 CNCF（云原生计算基金会）的维护。通过使用 Spring Cloud Sleuth 和 Jaeger 客户端库，可以将追踪信息发送到 Jaeger 并进行可视化展示和查询。 SkyWalking：Apache SkyWalking 是一款开源的应用性能监控与分析系统，提供了对 Java、.NET 和 Node.js 等语言的支持。它可以与 Spring Cloud Sleuth 集成，将追踪数据发送到 SkyWalking 服务器进行可视化展示和分析。 SkyWalking示例界面 Pinpoint：Pinpoint 是 Naver 开源的分布式应用性能监控系统，支持 Java 和 .NET。它提供了与 Spring Cloud Sleuth 的集成，可以将追踪数据发送到 Pinpoint 服务器，并通过其 UI 进行分析和监控。 Pinpoint示意图 这些方案都可以与 Spring Cloud Sleuth 进行集成，Spring Cloud Sleuth 是 Spring Cloud 中的一个组件，提供了在微服务调用时生成追踪信息的能力。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 分布式事务分布式事务可以查看前面的分布式基础篇。 30.Seata 支持哪些模式的分布式事务？Seata 以下几种模式的分布式事务： AT（Atomikos）模式：AT 模式是 Seata 默认支持的模式，也是最常用的模式之一。在 AT 模式下，Seata 通过在业务代码中嵌入事务上下文，实现对分布式事务的管理。Seata 会拦截并解析业务代码中的 SQL 语句，通过对数据库连接进行拦截和代理，实现事务的管理和协调。 AT模式示意图 TCC（Try-Confirm-Cancel）模式：TCC 模式是一种基于补偿机制的分布式事务模式。在 TCC 模式中，业务逻辑需要实现 Try、Confirm 和 Cancel 三个阶段的操作。Seata 通过调用业务代码中的 Try、Confirm 和 Cancel 方法，并在每个阶段记录相关的操作日志，来实现分布式事务的一致性。 Seata TCC模式 SAGA 模式：SAGA 模式是一种基于事件驱动的分布式事务模式。在 SAGA 模式中，每个服务都可以发布和订阅事件，通过事件的传递和处理来实现分布式事务的一致性。Seata 提供了与 SAGA 模式兼容的 Saga 框架，用于管理和协调分布式事务的各个阶段。 SAGA模式状态机引擎 XA 模式：XA 模式是一种基于两阶段提交（Two-Phase Commit）协议的分布式事务模式。在 XA 模式中，Seata 通过与数据库的 XA 事务协议进行交互，实现对分布式事务的管理和协调。XA 模式需要数据库本身支持 XA 事务，并且需要在应用程序中配置相应的 XA 数据源。 XA模式示意图 31.了解 Seata 的实现原理吗？Seata 的实现原理主要包括三个核心组件：事务协调器（Transaction Coordinator）、事务管理器（Transaction Manager）和资源管理器（Resource Manager）。 事务协调器（Transaction Coordinator）：事务协调器负责协调和管理分布式事务的整个过程。它接收事务的开始和结束请求，并根据事务的状态进行协调和处理。事务协调器还负责记录和管理事务的全局事务 ID（Global Transaction ID）和分支事务 ID（Branch Transaction ID）。 事务管理器（Transaction Manager）：事务管理器负责全局事务的管理和控制。它协调各个分支事务的提交或回滚，并保证分布式事务的一致性和隔离性。事务管理器还负责与事务协调器进行通信，并将事务的状态变更进行持久化。 资源管理器（Resource Manager）：资源管理器负责管理和控制各个参与者（Participant）的事务操作。它与事务管理器进行通信，并根据事务管理器的指令执行相应的事务操作，包括提交和回滚。 Seata领域模型 Seata 的实现原理基于两阶段提交（Two-Phase Commit）协议，具体的机制如下： 一阶段：在事务提交的过程中，首先进行预提交阶段。事务协调器向各个资源管理器发送预提交请求，资源管理器执行相应的事务操作并返回执行结果。在此阶段，业务数据和回滚日志记录在同一个本地事务中提交，并释放本地锁和连接资源。 二阶段：在预提交阶段成功后，进入真正的提交阶段。此阶段主要包括提交异步化和回滚反向补偿两个步骤： 提交异步化：事务协调器发出真正的提交请求，各个资源管理器执行最终的提交操作。这个阶段的操作是非常快速的，以确保事务的提交效率。 回滚反向补偿：如果在预提交阶段中有任何一个资源管理器返回失败结果，事务协调器发出回滚请求，各个资源管理器执行回滚操作，利用一阶段的回滚日志进行反向补偿。 Seata 的事务执行流程是什么样的？Seata 事务的执行流程可以简要概括为以下几个步骤： 事务发起方（Transaction Starter）发起全局事务：事务发起方是指发起分布式事务的应用程序或服务。它向 Seata 的事务协调器发送全局事务的开始请求，生成全局事务 ID（Global Transaction ID）。 事务协调器创建全局事务记录：事务协调器接收到全局事务的开始请求后，会为该事务创建相应的全局事务记录，并生成分支事务 ID（Branch Transaction ID）。 分支事务注册：事务发起方将全局事务 ID 和分支事务 ID 发送给各个参与者（Participant），即资源管理器。参与者将分支事务 ID 注册到本地事务管理器，并将事务的执行结果反馈给事务协调器。 执行业务逻辑：在分布式事务的上下文中，各个参与者执行各自的本地事务，即执行业务逻辑和数据库操作。 预提交阶段：事务发起方向事务协调器发送预提交请求，事务协调器将预提交请求发送给各个参与者。 执行本地事务确认：参与者接收到预提交请求后，执行本地事务的确认操作，并将本地事务的执行结果反馈给事务协调器。 全局事务提交或回滚：事务协调器根据参与者反馈的结果进行判断，如果所有参与者的本地事务都执行成功，事务协调器发送真正的提交请求给参与者，参与者执行最终的提交操作；如果有任何一个参与者的本地事务执行失败，事务协调器发送回滚请求给参与者，参与者执行回滚操作。 完成全局事务：事务协调器接收到参与者的提交或回滚结果后，根据结果更新全局事务的状态，并通知事务发起方全局事务的最终结果。 全局事务 ID 和分支事务 ID 是怎么传递的？全局事务 ID 和分支事务 ID 在分布式事务中通过上下文传递的方式进行传递。常见的传递方式包括参数传递、线程上下文传递和消息中间件传递。具体的传递方式可以根据业务场景和技术选型进行选择和调整。 Seata 的事务回滚是怎么实现的？ 事务日志记录 Seata 的事务回滚是通过回滚日志实现的。每个参与者在执行本地事务期间生成回滚日志，记录了对数据的修改操作。 当需要回滚事务时，事务协调器向参与者发送回滚请求，参与者根据回滚日志中的信息执行撤销操作，将数据恢复到事务开始前的状态。 回滚日志的管理和存储是 Seata 的核心机制，可以选择将日志存储在不同的介质中。通过回滚日志的持久化和恢复，Seata 确保了事务的一致性和恢复性。 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。 服务监控32.你们的服务怎么做监控和告警？我们使用 Prometheus 和 Grafana 来实现整个微服务集群的监控和告警： Prometheus：Prometheus 是一个开源的监控系统，具有灵活的数据模型和强大的查询语言，能够收集和存储时间序列数据。它可以通过 HTTP 协议定期拉取微服务的指标数据，并提供可扩展的存储和查询功能。 Grafana：Grafana 是一个开源的可视化仪表板工具，可以与 Prometheus 结合使用，创建实时和历史数据的仪表板。Grafana 提供了丰富的图表和可视化选项，可以帮助用户更好地理解和分析微服务的性能和状态。 Dashboard 33.你们的服务怎么做日志收集？日志收集有很多种方案，我们用的是ELK： Elasticsearch：Elasticsearch 是一个分布式搜索和分析引擎，用于存储和索引大量的日志数据。它提供了快速的搜索和聚合功能，可以高效地处理大规模的日志数据。 Logstash：Logstash 是一个用于收集、过滤和转发日志数据的工具。它可以从各种来源（如文件、网络、消息队列等）收集日志数据，并对数据进行处理和转换，然后将其发送到 Elasticsearch 进行存储和索引。 Kibana：Kibana 是一个用于日志数据可视化和分析的工具。它提供了丰富的图表、仪表盘和搜索功能，可以帮助用户实时监控和分析日志数据，发现潜在的问题和趋势。 简单说，这三者里Elasticsearch提供数据存储和检索能力，Logstash负责将日志收集到 ES，Kibana负责日志数据的可视化分析。 使用 ELK 进行微服务日志收集的一般流程如下： ELK流程 在每个微服务中配置日志输出：将微服务的日志输出到标准输出（stdout）或日志文件。 使用 Logstash 收集日志：配置 Logstash 收集器，通过配置输入插件（如文件输入、网络输入等）监听微服务的日志输出，并进行过滤和处理。 将日志数据发送到 Elasticsearch：配置 Logstash 的输出插件，将经过处理的日志数据发送到 Elasticsearch 进行存储和索引。 使用 Kibana 进行可视化和分析：通过 Kibana 连接到 Elasticsearch，创建仪表盘、图表和搜索查询，实时监控和分析微服务的日志数据。 除了应用最广泛的 ELK，还有一些其它的方案比如Fluentd、Graylog、Loki、Filebeat，一些云厂商也提供了付费方案，比如阿里云的sls。 1.3 万字 33 张手绘图，详解 33 道微服务（Dubbo、Spring Cloud）面试高频题（让天下没有难背的八股），面渣背会这些八股文，这次吊打面试官，我觉得稳了（手动 dog）。整理：沉默王二，戳转载链接，作者：三分恶，戳原文链接。 没有什么使我停留——除了目的，纵然岸旁有玫瑰、有绿荫、有宁静的港湾，我是不系之舟。 系列内容： 面渣逆袭 Java SE 篇 👍 面渣逆袭 Java 集合框架篇 👍 面渣逆袭 Java 并发编程篇 👍 面渣逆袭 JVM 篇 👍 面渣逆袭 Spring 篇 👍 面渣逆袭 Redis 篇 👍 面渣逆袭 MyBatis 篇 👍 面渣逆袭 MySQL 篇 👍 面渣逆袭操作系统篇 👍 面渣逆袭计算机网络篇 👍 面渣逆袭 RocketMQ 篇 👍 面渣逆袭分布式篇 👍 面渣逆袭微服务篇 👍 面渣逆袭设计模式篇 👍 面渣逆袭 Linux 篇 👍 GitHub 上标星 10000+ 的开源知识库《二哥的 Java 进阶之路》第一版 PDF 终于来了！包括 Java 基础语法、数组字符串、OOP、集合框架、Java IO、异常处理、Java 新特性、网络编程、NIO、并发编程、JVM 等等，共计 32 万余字，500+张手绘图，可以说是通俗易懂、风趣幽默……详情戳：太赞了，GitHub 上标星 10000+ 的 Java 教程 微信搜 沉默王二 或扫描下方二维码关注二哥的原创公众号沉默王二，回复 222 即可免费领取。","tags":["基础","微服务"],"categories":["Java问答笔记"]},{"title":"2025.9.29学习日记","path":"/2025/09/29/学习日记25年9月/2025.9.29学习笔记/","content":"今日学习内容3DGS力扣每日一题三角形最小得分今天的每日可以想到原问题可以转变成相同的子问题,想到了用记忆化搜索来做. public int minScoreTriangulation(int[] values) //原问题可以转换成更小的子问题 贪心的拆分为最小 int n = values.length; int[][] memo = new int[n][n]; for(int[] row:memo) Arrays.fill(row,-1); return dfs(0, n-1, values, memo);private int dfs(int i , int j , int[] v, int[][] memo) if(i + 1 == j) return 0; if(memo[i][j] != -1) return memo[i][j]; int res = Integer.MAX_VALUE; for(int k = i + 1 ; k j ; ++k) int subRes = dfs(i,k,v,memo) + dfs(k,j,v,memo) + v[i] * v[j] * v[k] ; res = Math.min(subRes,res); return memo[i][j] = res; 一比一翻译成为递推,值得注意的就是循环的方向.由于k比i大,为了保证i最后被传递,所以i要从大到小遍历.由于k比j小,为了保证j最后被传递,所以j要从小到大遍历. public int minScoreTriangulation(int[] v) //原问题可以转换成更小的子问题 贪心的拆分为最小 int n = v.length; int[][] f = new int[n][n]; for(int i = n - 3 ; i = 0 ; --i) for(int j = i + 2 ; j n ; ++j) f[i][j] = Integer.MAX_VALUE; for(int k = i + 1 ; k j ; ++k) f[i][j] = Math.min(f[i][j] , f[i][k] + f[k][j] + v[i] * v[j] * v[k]); return f[0][n-1]; Java复习进度Java SE0-5656Java集合框架0-3030Java并发编程1471JVMMySQL5583Redis1457Spring041操作系统计算机网络MyBatisRocketMQ分布式微服务设计模式Linux 算法上午做了两道单调栈 简历制作项目-TecHub项目-派聪明生活篇晚上回学校上课.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.28学习日记","path":"/2025/09/28/学习日记25年9月/2025.9.28学习笔记/","content":"今日学习内容3DGS力扣每日一题还是三角形的题目,枚举最长边,贪心的寻找最大的另外两边. Java基础看了6点. JAVA并发编程篇1471 学习中,正在完善笔记. Redis学习笔记1457 学习中,正在完善笔记. 算法做了一下周赛的模拟题,做出来两道半,第三题第一遍dp超时了,然后用前缀和进行优化之后就过了.三道单调栈的题目. 简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.27学习日记","path":"/2025/09/27/学习日记25年9月/2025.9.27学习笔记/","content":"今日学习内容3DGS可以运行了,但是帧率有点低,需要进一步优化一下. 力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记957 学习中,正在完善笔记. 算法简历制作项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.26学习日记","path":"/2025/09/26/学习日记25年9月/2025.9.26学习笔记/","content":"今日学习内容3DGS继续改代码的bug 力扣每日一题找能组成多少个三角形,思路就是枚举最长的那条边,然后对向双指针. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记957 学习中,正在完善笔记. 算法简历制作项目-TecHub项目-派聪明生活篇晚上回学校上课","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.25学习日记","path":"/2025/09/25/学习日记25年9月/2025.9.25学习笔记/","content":"今日学习内容3DGS继续完成分块渲染的代码.把渲染部分重构了.成功运行.但目前还是存在问题. 力扣每日一题一道DP题目,比较常规. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记957 学习中,正在完善笔记. 算法三道DP题目. 简历制作大概初版已经差不多了. 项目-TecHub看了五篇博客. 项目-派聪明生活篇晚上健身今天练的背,强度中等,最近身体都挺累的.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"简历对应的一些知识点","path":"/2025/09/24/Java问答笔记/简历对应的一些知识点/","content":"项目项目-TecHub重要项目介绍TecHub是一个基于 Spring Boot、MyBatis-Plus、MySQL、Redis、ElasticSearch、MongoDB、Docker、RabbitMQ 等技术栈实现的一个知识分享社区系统。 这个系统旨在为创作者提供一个可以发布文章和教程，并赚取佣金的社区平台，同时又兼顾一些社交属性，比如说用户可以通过阅读、点赞、收藏、评论的形式和作者互动。 与此同时，为了紧跟时代潮流，该系统还为用户提供了一套基于 OpenAI、讯飞星火等多家大模型的派聪明 AI 助手，帮助用户在工作和学习中大幅提效。 项目立意说一下，是否上线，上线考虑过用户数吗？由于平常热爱技术分享，于是就萌生一个大胆的想法，做一个比 CSDN、掘金、知乎更厉害的内容社区，起名叫TecHub。 TecHub里面用到的都是互联网当下最流行的技术架构，骨架是通过 Spring Boot+Mybatis-Plus 搭建的，并且是前后端分离的，admin 端用的是 React + TypeScript，其中还用到了数据库 MySQL、缓存中间件 Redis、搜索引擎 ElasticSearch、nosql 数据库 MongoDB、容器化技术 Docker、消息中间件 RabbitMQ、权限安全框架 SpringSecurity、日志框架 Logback、 接口文档 Knife4j、及时消息通信 WebSocket 等技术栈，并且对接了当前最火热的 AI 大模型 OpenAI 和讯飞星火 API，项目支持一键启动和部署，这个过程让我的技术得到了极大的提升。 最近还对接了微信支付和支付宝支付，以及文章付费阅读的功能，感觉有一个属于自己的实战项目可以不断地迭代，锤炼自己的技术，这个过程还是蛮有意思的。 具体的开发功能有哪些?1、整体前后端分离骨架的搭建，后端用的 Spring Boot+MyBatis-Plus+Redis+RabbitMQ，Admin 管理端用的 React，用户端用的 Thymeleaf。 2、使用 JWT + Session + Filter + AOP 完成用户登录和权限校验，支持微信扫码登录 3、作者使用 Markdown 发布教程、文章，图片会自动上传至 OSS 并使用 CDN 分发，用户可以点赞、收藏、评论，并且使用 RabbitMQ 进行异步消息处理 4、对接deepseek、讯飞星火、智谱 AI、字节豆包、阿里通义、OpenAI 等多家大模型，完成派聪明 AI 助手功能的开发，使用了策略模式和工厂模式，新增模型时非常简单，并通过 WebSocket 和 Stream 流实现及时通信和消息一点一点输出的效果 5、使用 Redis 实现作者白名单和用户活跃榜单，并且对热点数据进行缓存，为了提高缓存效率减轻 Redis 压力，还增加了本地缓存 Caffeine 作为二级缓存 6、借助 xxl-job 实现文章定时发布，ES 实现快速高效的文章查询等等。 微信公众号自动登录用户在前端点击登录时，会展示公众号二维码和待输入的验证码，并会和前端构建一个半长连接（相对于 WebSocket 的），同时保存验证码和半长连接之间的映射关系。 当用户向公众号发送验证码时，微信公众平台会将用户发送信息转发给技术派的服务器，然后通过验证码来识别请求登录的用户身份。 由于验证码保存了和半长链接的映射关系，所以可以找到该半长连接，实现用户的自动登录跳转。 微信扫码登录用户在 PC 网页端登录时，生成一个唯一的二维码，绑定请求端的身份信息，网页客户端和服务端保持链接，等待手机端扫描二维码获取授权 URL，微信端扫码登录成功后回调服务端接口（附带用户身份信息），并传回给应用服务器，完成登录。登录成功后，服务器生成一个 JWT 或 Session 并返回给 PC 端，用户完成登录。 通过 RabbitMQ 实现消息异步解耦#首先说一下，为什么要异步解耦？ 当用户订阅、点赞、评论时，就会触发消息通知，如果当用户操作时，消息同步发送，当消息出现异常时，会影响主流程，或者当消息过多时，也会有影响服务的性能，所以需要对消息进行异步解耦。 选择 RabbitMQ 的原因是什么呢？ 首先是社区活跃度高，然后 RabbitMQ 还提供了有一个易用的用户界面，可以让用户监控和管理消息，同时我们的系统对并发要求没有那么高，消息通知也可以无序，且 RabbitMQ 也支持消息路由，宕机后的消息也能自动恢复，所以就选择了 RabbitMQ。 RabbitMQ 连接池用的什么阻塞队列？为什么？队列大小？怎么确定这个大小是合适的？ 使用的是 LinkedBlockingQueue，它是一个基于链表的阻塞队列。 LinkedBlockingQueue 的优点是锁分离，很适合生产和消费频率差不多的场景，这样生产和消费互不干涉的执行，能达到不错的效率。 队列大小生产环境是 10 个，因为我们是小型系统，通常设置在 10-50 个足够用了。可以通过监控队列的使用率、请求等待时间，和 RabbitMQ 的负载来动态调整，比如说 pool.size() poolSize 如果经常接近 100%，说明连接池偏小；如果请求经常阻塞等待连接，说明连接池偏小；如果 RabbitMQ 负载过高，说明连接池偏大。 异常日志结合邮箱服务来实现异常报警当程序出现大量的异常，表明应用多半出现了问题，需要立马发送给项目owner 要实现这个方案，关键点就在于异常出现的感知与上报: 异常的捕获，并输出日志（这个感觉属于标配了吧，别告诉我现在还有应用不输出日志文件的…） 对于这个感知，借助logback的扩展机制，可以实现，后面介绍 异常上报：邮件发送 实现一个appender 然后在logback.xml中配置 ※ 技术派MysqlRedis缓存一致性 不好的方案: 先写 MySQL，再写 Redis: 橘黄色的线是请求 A，黑色的线是请求 B； 橘黄色的文字，是 MySQL 和 Redis 最终不一致的数据； 数据是从 10 更新为 11； 请求 A、B 都是先写 MySQL，然后再写 Redis，在高并发情况下，如果请求 A 在写 Redis 时卡了一会，请求 B 已经依次完成数据的更新，就会出现图中的问题。 不过这里有个前提，就是对于读请求，先去读 Redis，如果没有，再去读 DB，但是读请求不会再回写 Redis。 大白话说一下，就是读请求不会更新 Redis。 先写 Redis，再写 MySQL: 先删除 Redis，再写 MySQL: 这幅图和上面有些不一样，前面的请求 A 和 B 都是更新请求，这里的请求 A 是更新请求，但是请求 B 是读请求，且请求 B 的读请求会回写 Redis。 请求 A 先删除缓存，可能因为卡顿，数据一直没有更新到 MySQL，导致两者数据不一致。 这种情况出现的概率比较大，因为请求 A 更新 MySQL 可能耗时会比较长，而请求 B 的前两步都是查询，会非常快。 好的方案: 先删除 Redis，再写 MySQL，再删除 Redis 对于“先删除 Redis，再写 MySQL”，如果要解决最后的不一致问题，其实再对 Redis 重新删除即可，这个也是大家常说的“缓存双删”。 为了便于大家看图，对于蓝色的文字，“删除缓存 10”必须在“回写缓存10”后面，那如何才能保证一定是在后面呢？网上给出的第一个方案是，让请求 A 的最后一次删除，等待 500ms。 对于这种方案，看看就行，反正我是不会用，太 Low 了，风险也不可控。 建议异步串行化删除，即删除请求入队列 异步删除对线上业务无影响，串行化处理保障并发情况下正确删除。 如果双删失败怎么办，网上有给 Redis 加一个缓存过期时间的方案，这个不敢苟同。个人建议整个重试机制，可以借助消息队列的重试机制，也可以自己整个表，记录重试次数，方法很多。 简单小结一下： “缓存双删”不要用无脑的 sleep 500 ms； 通过消息队列的异步串行，实现最后一次缓存删除； 缓存删除失败，增加重试机制。 先写 MySQL，再删除 Redis 对于上面这种情况，对于第一次查询，请求 B 查询的数据是 10，但是 MySQL 的数据是 11，只存在这一次不一致的情况，对于不是强一致性要求的业务，可以容忍。（那什么情况下不能容忍呢，比如秒杀业务、库存服务等。） 当请求 B 进行第二次查询时，因为没有命中 Redis，会重新查一次 DB，然后再回写到 Redis。 这里需要满足 2 个条件： 缓存刚好自动失效； 请求 B 从数据库查出 10，回写缓存的耗时，比请求 A 写数据库，并且删除缓存的还长。 对于第二个条件，我们都知道更新 DB 肯定比查询耗时要长，所以出现这个情况的概率很小，同时满足上述条件的情况更小。 先写 MySQL，通过 Binlog，异步更新 Redis 这种方案，主要是监听 MySQL 的 Binlog，然后通过异步的方式，将数据更新到 Redis，这种方案有个前提，查询的请求，不会回写 Redis。 这个方案，会保证 MySQL 和 Redis 的最终一致性，但是如果中途请求 B 需要查询数据，如果缓存无数据，就直接查 DB；如果缓存有数据，查询的数据也会存在不一致的情况。 所以这个方案，是实现最终一致性的终极解决方案，但是不能保证实时性。 几种方案比较 我们对比上面讨论的 6 种方案： 先写 Redis，再写 MySQL:这种方案，我肯定不会用，万一 DB 挂了，你把数据写到缓存，DB 无数据，这个是灾难性的；我之前也见同学这么用过，如果写 DB 失败，对 Redis 进行逆操作，那如果逆操作失败呢，是不是还要搞个重试？ 先写 MySQL，再写 Redis: 对于并发量、一致性要求不高的项目，很多就是这么用的，我之前也经常这么搞，但是不建议这么做；当 Redis 瞬间不可用的情况，需要报警出来，然后线下处理。 先删除 Redis，再写 MySQL: 这种方式，我还真没用过，直接忽略吧。 先删除 Redis，再写 MySQL，再删除 Redis:这种方式虽然可行，但是感觉好复杂，还要搞个消息队列去异步删除 Redis。 先写 MySQL，再删除 Redis:比较推荐这种方式，删除 Redis 如果失败，可以再多重试几次，否则报警出来；这个方案，是实时性中最好的方案，在一些高并发场景中，推荐这种。 先写 MySQL，通过 Binlog，异步更新 Redis: 对于异地容灾、数据汇总等，建议会用这种方式，比如 binlog + kafka，数据的一致性也可以达到秒级；纯粹的高并发场景，不建议用这种方案，比如抢购、秒杀等。 个人结论： 实时一致性方案：采用“先写 MySQL，再删除 Redis”的策略，这种情况虽然也会存在两者不一致，但是需要满足的条件有点苛刻，所以是满足实时性条件下，能尽量满足一致性的最优解。 最终一致性方案：采用“先写 MySQL，通过 Binlog，异步更新 Redis”，可以通过 Binlog，结合消息队列异步更新 Redis，是最终一致性的最优解。 技术派canal实现MySQL和ES同步异步实现MySQL和ES同步. Canal 是一款常用的数据同步工具，其原理是基于 Binlog 订阅的方式实现，模拟一个 MySQL Slave 订阅 Binlog 日志，从而实现 CDC（Change Data Capture），将已提交的更改发送到下游。 主要流程如下： Canal 服务端向 MySQL 的 master 节点传输 dump 协议； MySQL 的 master 节点接收到 dump 请求后推送 Binlog 日志给 Canal 服务端，解析 Binlog 对象（原始为 byte 流）转成 Json 格式； Canal 客户端通过 TCP 协议或 MQ 形式监听 Canal 服务端，同步数据到 ES。 下面是 Canal 执行的核心流程，其中 Binlog Parser 主要负责 Binlog 的提取、解析和推送，EventSink 负责数据的过滤 、路由和加工，仅作了解即可。 Canal Adapter 的 application.yml 配置文件可以配置 ES 地址，然后进行数据同步 Redis分布式锁本地锁和分布式锁区别:synchronized和Lock锁；这两者都是本地锁。何为本地锁呢？本地锁就是该锁只针对当前节点有效，也就是当node A获取锁时，那么node B同样还可以获取锁，这种情况就是本地锁。如果服务只部署了一个节点的话，用这种本地锁是没有问题的。现现在很多系统为了抗高并发、高可用和高性能，会部署多节点(集群部署)，那么此时如果还用本地锁的话就会出现问题，因此分布式锁就诞生了。分布式锁就是当有一个节点获取到锁后，其它节点是不可以获取锁的。 Redis分布式锁和Zookeeper分布式锁区别: 谈起分布式集群，就绕不开CAP理论，也就是强一致性、可用性和分区容错性。三者只能选其二，不可兼容。这里我就不具体分析其原因之类了，直接步入两把分布式锁区别。 Redis分布式锁：它追求的高可用性和分区容错性。Redis在追求高可用性时会在 Redis 写入主节点数据后，立即返回成功，不关心异步主节点同步从节点数据是否成功。Redis是基于内存的，性能极高，官方给的指标是每秒可达到10W的吞吐量。 Zookeeper分布式锁：它追求的是强一致性和分区容错性。Zookeeper在写入主节点数据后会等到从节点同步完数据成后才会返回成功。为了数据的强一致性牺牲了一部分可用性。 两者综合对比下来，技术派为了追求用户体验度，就采用了Redis分布式锁来实现。 使用Redis分布式锁背景: 技术派使用Redis分布式锁的背景是，用户根据articleId查询文章详情，查询出结果后返回。 查询文章详情流程图如下所示： 如果并发量不是特别高的情况下没有问题，但就怕并发量高；会出现什么问题呢？什么时候出现呢？ 当缓存中没有数据，需要到MySQL中查询这一步。 问题出现点如下所示： 因为当高并发时，如果查询缓存中没有数据，大量的用户会同时去访问DB层MySQL，MySQL的资源是非常珍贵的，并且性能没有Redis好，很容易将我们的MySQL打宕机，进而影响整个服务。 针对这种问题，该怎么解决呢？ 当大量用户同时访问同一篇文章时，只允许一个用户去MySQL中获取数据。由于服务是集群化部署，就需要用到Redis分布式锁。 逻辑如下所示： 采用加锁的方式就能很好地保护DB层数据库，进而保证系统的高可用性。 Redis分布式锁的实现方式 这样的是实现方式,可以在执行完当前业务后直接释放锁,并且如果当执行的过程中节点挂了,锁也会自动释放. 但是这样也存在一个问题,就是当执行完业务后,锁会自动释放,为了防止执行的过程中节点挂了.所以需要通过设置过期时间,锁也会自动释放.并且通过设置value,可以防止锁被其他节点误删. 但是还存在一个问题就是,过期时间该如何设置呢? 时间设置过短：可能业务还没有执行完毕，过期时间已经到了，锁被释放，其他线程可以拿到锁去访问DB，违背了我们的初心。 时间设置过长：过长的话，可能在我们加锁成功后，还没有执行到释放锁，在这一段过程中节点宕机了，那么在锁未过期的这段时间，其他线程是不能获取锁的，这样也不好。 针对这个问题，可以写一个守护线程，然后每隔固定时间去查看业务是否执行完毕，如果没有的话就延长其过期时间，也就是为锁续期； 上面俗称看门狗机制，且已经有技术实现——Redission。下面我就详细讲解下Redission 实现分布式锁的逻辑。 Redission实现分布式锁redission实现分布式锁流程图： 杂项设计模式 抽象设计模式抽象设计模式包括了多种设计模式，例如工厂模式、观察者模式、策略模式等。这些设计模式通过定义一组通用的类和接口，并指导如何将它们组合起来以解决特定问题。它们帮助开发者遵循设计原则，如开闭原则、单一职责原则等，从而产生高质量的、可重用的代码。 核心思想：抽象出事物的共同点，封装成接口或抽象类，让子类实现具体的行为。 设计模式 策略模式策略模式（Strategy Pattern）是一种行为型设计模式，它定义了一系列的算法，将每个算法封装起来，使得它们可以相互替换。策略模式让算法的变化独立于使用它的客户端。 在策略模式中，有三个角色：上下文（Context）、策略（Strategy）和具体策略（ConcreteStrategy）。 上下文（Context）类持有一个策略对象，并在需要执行算法时调用策略对象的方法。上下文类不关心具体的算法实现，只关心调用合适的策略对象。 策略（Strategy）接口定义了算法的方法，具体的策略类实现了该接口，提供了不同的算法实现。 具体策略（ConcreteStrategy）类是策略的具体实现，封装了不同的算法实现。 策略模式的优点是： 策略模式将算法的定义和使用分离，使得算法可以独立于使用它的客户端进行变化和演化。 策略模式可以方便地扩展和添加新的算法，只需要实现新的具体策略类即可，无需修改上下文类。 策略模式可以减少大量的条件判断语句，提高代码的可读性和可维护性。 策略模式的缺点是： 客户端需要了解不同的策略类，选择合适的策略对象，增加了客户端的复杂性。 策略模式增加了对象的数量，每个具体策略类都需要一个独立的对象，导致系统中对象数量增加。 // 1. 定义策略接口interface Strategy int execute(int a, int b);// 2. 具体策略实现class AddStrategy implements Strategy @Override public int execute(int a, int b) return a + b; class SubtractStrategy implements Strategy @Override public int execute(int a, int b) return a - b; class MultiplyStrategy implements Strategy @Override public int execute(int a, int b) return a * b; // 3. 上下文类class Context private Strategy strategy; public void setStrategy(Strategy strategy) this.strategy = strategy; // 关键：保存策略实例 public int executeStrategy(int a, int b) return strategy.execute(a, b); // 关键：调用当前策略 // 4. 客户端代码class Main public static void main(String[] args) Context context = new Context(); // 设置加法策略 context.setStrategy(new AddStrategy()); int result = context.executeStrategy(5, 3); System.out.println(加法结果: + result); // 输出：8 // 设置减法策略 context.setStrategy(new SubtractStrategy()); result = context.executeStrategy(5, 3); System.out.println(减法结果: + result); // 输出：2 // 设置乘法策略 context.setStrategy(new MultiplyStrategy()); result = context.executeStrategy(5, 3); System.out.println(乘法结果: + result); // 输出：15 自定义配置注入动态刷新一般我们的配置信息都是放在配置文件application.yml中.然后SpringBoot就会自动加载配置文件中的信息到Environment中，再应用中访问这些配置也比较简单，常见的有@Value, @ConfigurationProperties 以及 environment.getProperty. SpringBoot应用中的配置读取简单，但是这些配置是不支持动态刷新的 本项目开发了一个对单体应用而言，非常实用的配置扩展，支持从自定义的数据源（MySQL）中获取配置，并注入到Environment中，且优先级最高，同时也需要支持配置的动态刷新 自定义数据源我们将自定义的配置存在数据库中，核心的字段就是配置的key, value 技术派中设计了一个非常简单的全局配置表 global_conf(还可以加上version和namespace作为拓展) 数据的加载 首先加载配置数据到内存 将配置注册到Environment中 敏感词过滤技术 敏感词校验算法：DFA(Deterministic Finite Automaton)DFA 是一种状态机，用于识别字符串中的敏感词。 一种基于mybatis拦截器的自定义数据库脱敏的技术方案. 相当于在数据库中维护一个敏感词表，然后通过DFA算法进行敏感词校验。 Hikari 连接池Hikari 作为 SpringBoot2.0 默认的连接池，在业界得到了普遍的认可，对于大部分业务场景，都可以实现快速高效的连接使用。","tags":["简历","知识点"],"categories":["Java问答笔记","2025-09"]},{"title":"2025.9.24学习日记","path":"/2025/09/24/学习日记25年9月/2025.9.24学习笔记/","content":"今日学习内容3DGS继续完成分块渲染的代码. 力扣每日一题今天的每日是一个模拟长除法的题目,思路就是哈希表记录余数和位置,然后如果重复出现相同的余数,那就说明出现了循环,直接返回. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.23学习日记","path":"/2025/09/23/学习日记25年9月/2025.9.23学习笔记/","content":"今日学习内容3DGS关于我目前的思路1.首先渲染一个高质量的基础模型.2.然后通过设计细节降级脚本,将基础模型降级到不同细节程度的模型.3.然后将3D空间进行分块,预先计算当相机位于该分块时的高斯激活表(高效的索引结构).4.渲染时,根据相机位置,读取对应的高斯激活表,然后根据高斯激活表,渲染对应细节程度的模型. 做PPT(两篇文章):Octree-GS- Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians LODGE: Level-of-DetailLarge-Scale Gaussian Splatting with Efficient Rendering PPT1这周我汇报的是这两篇论文,一篇是关于使用八叉树+分级渲染的3DGS,另一篇是谷歌25年提出的使用分级渲染的3DGS.都是关于在大规模场景对渲染可行性和帧率进行优化的论文. 2首先介绍这个八叉树分级渲染的3DGS.传统的3DGS虽然渲染质量高,但是在处理大规模场景时存在两个问题: ​渲染冗余​​,无论物体远近，所有高斯球都会被渲染，导致整体计算资源浪费,在距离相机远的时候会导致帧率下降严重. ​​缺乏细节层次,无法根据观察距离动态调整细节，导致远处细节过多而近处细节不足. 所以这篇文章通过引入​​层次化的八叉树结构​​和​​LOD机制​​，实现了动态选择不同尺度的高斯球，从而在保持视觉质量的同时显著提升渲染效率。 3首先介绍这篇文章设计的八叉树结构首先会将场景分成多层级的体素网格,每个体素中心定义一个锚点,每个锚点对应多个高斯球，较高级别的LOD对应较小体素大小,LOD越高细节越丰富,在第K级渲染的图像会光栅化从0到K的所有高斯球。左侧的公式可以看到,八叉树层数K由观测距离范围决定,其中d是剔除异常值后的最大最小值. 然后右侧公式是锚点的选择机制,对于给定的视角i. 4首先是文章的生长-剪枝策略:初始就是从低层级开始,然后在训练过程通过判断梯度大小来决定是否增加层级或者增加锚点,比如梯度很小,那就不变,梯度比较大,增加锚点,梯度非常大的话增加更高的层级.然后就是剪枝策略,通过可见频率和透明度剔除无效锚点,减少冗余和浮点伪影. 然后是渐进式的训练策略,采用由粗到细的训练方式,在优化过程中逐步激活更精细的LOD级别,并且使粗粒度锚点有更多时间学习远视角场景重建.可以看到使用渐进式的训练策略后,低层级的锚点会有更加清晰地轮廓. 5然后这个是额外的外观补偿模块,针对大规模场景中曝光补偿不一致的问题,集成生成潜在优化(GLO)来生成高斯基元的颜色为每个锚点引入可学习的个体外观代码允许对光照和外观进行详细控制 6然后就是这篇文章所做的实验,可以看到中间的本文的方法,在远距离可以使用更少的高斯球来表示,渲染帧率有明显的提升,并且点云的层次更加清晰. 7然后这是渲染效果图的对比. 8然后这个实验的表格.可以看到Octree-GS在保持视觉质量的同时，显著减少了渲染所需的高斯基元数量和存储需求。 9然后这是不同分级的渲染质量对比. 10然后这是在不同数据集上的渲染质量对比. 11然后这是渲染帧率的对比,可以看到在随着视角远离,文章的渲染帧率可以保持在比较高的渲染帧率.这篇文章使得渲染帧率不会随着高斯球规模增加而没有限度的降低的问题. 但是由于引入了复杂的八叉树结构,渲染过程会额外增加一个遍历树和LOD的选择过程,所以训练时间会有所增加. 12然后是第二篇,在解决大规模3DGS场景在内存显存受限设备（如移动设备）上的实时渲染问题。论文的核心贡献是引入了一种LOD表示方法，结合分块渲染，显著降低了渲染时间和GPU显存使用，同时保持了高质量的视觉输出 13左边可以看到​,展示了场景的多层次LOD表示，每个LOD级别包含不同细节的高斯分布。根据相机距离选择“活跃高斯”（active Gaussians），近距离使用高细节，远距离使用低细节。 ​​右边展示了文章的分块渲染策略。相机位置被聚类为多个块（chunks），每个块预计算其活跃高斯集合。渲染时，动态加载最近的两个块，并通过透明度混合（opacity blending）平滑过渡，避免视觉瑕疵。 14然后就是这个方法的整体结构,首先按照原始方法训练一个高质量的模型,然后通过第二个公式进行3D平滑滤波构建更加低细节度的模型.同时会对模型进行3D分块,并且预先计算相机位于每个分块时,对应的活跃高斯集合,也就是相当于预先计算相机位于不同位置时,哪些分块使用高细节模型,哪些使用低细节模型.通过映射表进行记录,使得渲染时的开销减小.并且在渲染时,会动态加载最近的两个分块,通过第三个公式,对分块边界的高斯进行透明度混合,避免视觉瑕疵. 第一个公式就是活跃高斯选择公式.相当高斯会根据其与相机的距离被分配到不同的LOD级别. 然后第二个公式,相当于相当于构建更低细节LOD所使用的公式,通过平滑滤波的方式来获取更低细节度的模型. 第三个公式,用于分块边界的高斯透明度混合. 15第一个图表示像素点需要处理的​​可见高斯数量.可以看到原版3DGS会存在渲染的严重不均衡,大量的像素会对应很多高斯球,导致渲染的性能下降.而文章的方法可以明显的减少长尾效应,需要处理极端数量高斯的像素点大大减少. 第二个图代表使用不同分级阈值下,渲染成本的热力图,可以通过计算平均每屏幕块需要处理的高斯数量,来避免手动选择阈值,获取最佳的阈值. 16这个是该方法的实验对比,可以看到比刚才的八叉树高斯的渲染帧率有更加明显的提升,这是因为八叉树方法渲染时需要进行遍历树的额外开销,而本方法通过预先计算相机位于不同位置的活跃高斯集合,使得渲染时可以跳过一些不必要的计算.相当于用空间换取时间. 17这些是渲染的效果图 18然后这是不同分级的一些实验 19然后这是进行的消融实验并且还可以看到在不同设备或者显存受限设备进行渲染的测试的可行性实验. 20总结的话,这两篇文章都是解决大规模场景的渲染帧率下降的问题,核心都是通过对远处的高斯进行降级,来减少渲染的开销. 力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.22学习日记","path":"/2025/09/22/学习日记25年9月/2025.9.22学习笔记/","content":"今日学习内容3DGS关于我目前的思路1.首先渲染一个高质量的基础模型.2.然后通过设计细节降级脚本,将基础模型降级到不同细节程度的模型.3.然后将3D空间进行分块,预先计算当相机位于该分块时的高斯激活表(高效的索引结构).4.渲染时,根据相机位置,读取对应的高斯激活表,然后根据高斯激活表,渲染对应细节程度的模型. 做PPT(两篇文章):Octree-GS- Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians LODGE: Level-of-DetailLarge-Scale Gaussian Splatting with Efficient Rendering 力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.20学习日记","path":"/2025/09/20/学习日记25年9月/2025.9.20学习笔记/","content":"今日学习内容3DGS力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub项目基本收尾,除了admin管理界面,派聪明实现ai的接口. 增强敏感词管理功能，添加对敏感词配置的空值检查，避免空指针异常。同时，新增聊天相关的WebSocket支持，包括聊天控制器、聊天助手和WebSocket配置，提升聊天功能的稳定性和用户体验。移除不再使用的字典控制器，优化代码结构。 JakicDong 2025920 20:52新增PaiWebConfig配置类和CustomizeErrorController自定义错误控制器，增强系统的Web配置和错误处理能力。PaiWebConfig实现了XML解析器的注册和消息转换器的配置，CustomizeErrorController提供了友好的错误页面响应。 JakicDong 2025920 20:33新增多个模块，包括AI聊天相关的枚举、数据传输对象、服务接口及实现，增强系统的AI聊天功能。同时新增支付相关的服务接口及实现，支持多种支付方式，提升支付管理能力。更新了多个数据访问层和映射器，完善了用户和文章的管理功能。 JakicDong 2025920 20:31新增动态配置管理模块，包括配置刷新事件监听器、动态配置绑定器、动态配置容器、Spring值处理器及相关工具类，增强系统的动态配置能力和灵活性。同时实现多数据源配置及切换功能，支持更复杂的数据库操作和配置管理。 JakicDong 2025920 19:05新增全局配置请求类、搜索全局配置请求类、全局配置数据传输对象、每天统计计数数据传输对象、用户搜索请求类、批量操作用户请求类、发布用户请求类及基本用户信息数据传输对象，增强系统的配置管理和用户信息处理能力。 JakicDong 2025920 18:51新增配置变更事件类、搜索配置请求参数类、聊天记录相关类及对话类，增强系统的事件处理和聊天记录管理能力，支持更灵活的配置和聊天功能。 JakicDong 2025920 18:42 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.19学习日记","path":"/2025/09/19/学习日记25年9月/2025.9.19学习笔记/","content":"今日学习内容3DGS实现一个分级渲染的3dgs,训练还是基于DashGS进行训练.首先将空间按照输入点云的规模进行3d空间的分块,比如说100000个点云,就将空间分成n4,n^3个空间块,然后重要的是,训练的过程是所有块同时进行的,相当于建立一个映射调配器,根据gs球的坐标来分配到不同的空间文件里面,注意我的电脑内存有128G,但是显存只有8G,所以训练过程几乎可以无限使用内存.然后训练过程中对每个分块都训练出低模中模高模三种,方式是同时DashGS进行训练时,低模在训练代数14时保存,中模在训练代数12时保存,高模在训练结束时保存.然后渲染模块也需要改,距离相机空间距离近的用高模,中距离用中模,远距离用低模.可以直接在原有代码上来改. python3 train_hierarchical.py -s /home/dong/3DGS/data/360_v2/bicycle -m ./output/lod_test_1 --iterations 30000 --low_model_iterations 5000 --mid_model_iterations 10000 --high_model_iterations 30000 --near_threshold 3.0 --far_threshold 10.0 力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.18学习日记","path":"/2025/09/18/学习日记25年9月/2025.9.18学习笔记/","content":"今日学习内容3DGS跑了昨天母点保护的代码,结果一坨,取消这个方法. 力扣每日一题构造题.是一个优先队列的构造题目,亮点是用到了一个懒删除的技巧,可以避免删除操作的复杂度.题目链接 class TaskManager // taskId - (priority, userId) private final MapInteger, int[] mp = new HashMap(); // (priority, taskId, userId) private final PriorityQueueint[] pq = new PriorityQueue((a, b) - a[0] != b[0] ? b[0] - a[0] : b[1] - a[1]); public TaskManager(ListListInteger tasks) for (ListInteger task : tasks) add(task.get(0), task.get(1), task.get(2)); public void add(int userId, int taskId, int priority) mp.put(taskId, new int[]priority, userId); pq.offer(new int[]priority, taskId, userId); public void edit(int taskId, int newPriority) // 懒修改 int userId = mp.get(taskId)[1]; add(userId, taskId, newPriority); public void rmv(int taskId) // 懒删除 mp.get(taskId)[0] = -1; public int execTop() while (!pq.isEmpty()) int[] top = pq.poll(); int priority = top[0], taskId = top[1], userId = top[2]; int[] p = mp.get(taskId); if (p[0] == priority p[1] == userId) rmv(taskId); return userId; // else 货不对板，堆顶和 mp 中记录的不一样，说明堆顶数据已被修改或删除，不做处理 return -1; JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub新增排行榜功能模块，包括排行榜请求参数类、排行榜信息类及其数据传输对象，完善用户活跃信息查询接口，增强系统的排行榜管理能力。同时新增排行榜控制器，支持前端展示活跃用户排行榜。 JakicDong 1 分钟之前新增站点地图功能模块，包括站点地图实体类、服务接口及实现、控制器，支持生成和刷新sitemap.xml，增强系统的SEO优化能力。同时更新统计服务，增加文章和用户的统计信息查询功能，提升数据统计的准确性和完整性。 JakicDong 19 分钟之前新增短链接功能模块，包括短链接请求对象、返回对象、数据库实体类、服务接口及实现，以及控制器。实现短链接的创建和获取，支持根据短链接重定向到原始URL，增强系统的链接管理能力。 JakicDong 今天 15:35 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.17学习日记","path":"/2025/09/17/学习日记25年9月/2025.9.17学习笔记/","content":"今日学习内容3DGS跑了昨天母点保护的代码,结果一坨,取消这个方法. 力扣每日一题构造题每日. JAVA并发编程篇Redis学习笔记简历制作大概初版已经差不多了. 项目-TecHub新增字典功能模块，包括字典实体类、数据访问层、转换器、服务接口及实现，以及控制器。实现字典数据的获取和转换，支持前端调用字典接口，增强系统的通用性和可扩展性。 JakicDong 昨天 18:37完成了Elasticsearch的搜索功能. 新增文章搜索功能，支持根据关键词查询文章列表和推荐文章。实现Elasticsearch配置及相关服务，优化搜索体验。同时更新前端控制器以支持搜索请求。 JakicDong 昨天 17:51 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.16学习日记","path":"/2025/09/16/学习日记25年9月/2025.9.16学习笔记/","content":"今日学习内容3DGS尝试了代际标签的训练,结果不明显,取消这个方法.加上母点保护代码. 力扣每日一题对于临项消除问题,并且先后顺序无关,可以考虑使用栈来实现.从左到右来执行. 最大公约数求法(辗转相除法): private int gcd(int a, int b) while (a != 0) int tmp = a; a = b % a; b = tmp; return b; 最小公倍数求法: // 先除后乘，避免溢出private int lcm(int a, int b) return a / gcd(a, b) * b; JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub正在实现ES的搜索功能.遇到的问题就是canal连接ES的安全问题,由于之前没有关闭ES的SSL,所以canal连接ES的时候会报错.解决办法就是修改ES配置文件,然后重启ES. 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.15学习日记","path":"/2025/09/15/学习日记25年9月/2025.9.15学习笔记/","content":"今日学习内容回学校上课. 力扣每日一题简单题. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHubRabbitMQ实现点赞通知消息:使用连接池来实现.连接池上十个连接,八个线程作为消费者线程进行监听. 优化RabbitMQ连接和连接池的实现，增加连接状态检查和异常处理，提升系统的稳定性和可用性。同时更新RabbitMQ自动配置类，增强消费者的启动和停止逻辑，支持多线程消息处理，提升消息消费效率。更新配置文件以调整连接池和消费者数量，适应不同的负载需求。 JakicDong 今天 11:13 下载了一个 TinyRDM Redis可视化工具,很好用. 使用kafka实现的点赞功能. 新增Kafka相关功能，包括Kafka主题常量、文章Kafka消息DTO、Kafka消费者监听器及Redis工具类，支持文章点赞、收藏、评论等操作的消息处理。同时新增操作记录注解和切面类，增强系统的消息记录和处理能力。 JakicDong 今天 20:35 梳理了git的功能. 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.13学习日记","path":"/2025/09/13/学习日记25年9月/2025.9.13学习笔记/","content":"今日学习内容3DGS力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.11学习日记","path":"/2025/09/11/学习日记25年9月/2025.9.12学习笔记/","content":"今日学习内容力扣每日一题一道脑筋急转弯题目,比较简单. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记557 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub今天对昨天的进行了总结. 新增异步执行切面类AsyncExecuteAspect，支持超时执行和自定义超时响应，优化异步任务管理。更新AsyncUtil类的注释，详细说明异步工具类的功能和设计。 JakicDong 片刻之前. 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.11学习日记","path":"/2025/09/11/学习日记25年9月/2025.9.11学习笔记/","content":"今日学习内容力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub加一点注释 JakicDong 昨天 19:16新增多个枚举类和请求响应对象，完善文章、分类、标签等相关功能的支持，包含文章事件、类型、聊天回答类型、聊天状态、奶油状态、操作文章状态、标签类型、配料状态、WebSocket连接状态等枚举定义。同时，新增文章消息事件、分页对象、文章请求和分类请求等数据传输对象，增强了文章管理模块的功能。 JakicDong 昨天 17:41更新文章编辑页，修改标题为”TecHub - 文章发表”。新增文章编辑功能，包含文章详情、分类列表和标签列表的处理逻辑，支持文章的编辑和新增操作。 JakicDong 昨天 14:51新增文章编辑页VO类，包含文章信息、分类列表和标签列表，支持文章编辑功能的数据传输。 JakicDong 昨天 14:51新增图片上传功能，完善图片相关配置类、OSS配置类及图片上传器接口，提供图片服务接口及实现，支持图片格式校验和上传，优化图片上传控制器及返回值对象，更新配置文件以包含图片相关路径设置。 JakicDong 昨天 13:48 项目-派聪明生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.10学习日记","path":"/2025/09/10/学习日记25年9月/2025.9.10学习笔记/","content":"今日学习内容力扣每日一题暴力枚举题. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub新增图片上传功能，包含图片相关配置类、OSS配置类、图片上传器接口及其本地存储实现，提供图片服务接口及实现，支持图片上传和格式校验，完善了图片上传控制器及返回值对象，配置文件中添加图片相关路径设置。 JakicDong 2 分钟之前 明天把文章发布相关的接口写一下.再把chat相关的完成,基本这个项目框架就搭好了. 项目-派聪明配好环境了,安装了ES MinIO Kafka部署教程 生活篇晚上教师节聚餐","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.9学习日记","path":"/2025/09/09/学习日记25年9月/2025.9.9学习笔记/","content":"今日学习内容今天学校第二周,回学校上课来了. 力扣每日一题暴力枚举题. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目-TecHub更新用户主页视图控制器，新增访问其他用户主页的功能，优化文章列表查询接口的注释，增加页面列表VO的注释以说明更多数据的存在。整合WebSocket长连接实现消息实时推送博客简单来说通过客户端首先通过端口来对服务端进行连接,在此过程中会进行身份验证和校验.然后客户端向服务端通信需要通过向broker节点发送消息,然后broker节点会向MessageMapping节点进行转发即刻进行处理.服务端发送给客户端消息,会直接转发到broker端口进行发送,客户端只需要对broker端口进行监听即可. debug实现了通知的listener. 新增评论查询功能，优化评论统计逻辑，完善通知消息监听器，支持评论、回复、点赞和关注等多种通知类型，提升了系统的通知处理能力。 JakicDong 1 分钟之前删除CommentDO实体类，新增NotifyMsgListener通知消息监听器，支持评论、回复、点赞、收藏等多种通知功能，优化了通知消息的处理逻辑。 JakicDong 1 分钟之前bug:notice界面查询不到数据. 新增用户相关功能，包括用户关系管理、用户信息保存及查询接口，优化了用户服务和控制器，支持用户关注、粉丝列表及文章列表的分页查询。 JakicDong 今天 14:43基本完成notice接口的功能.新增通知服务相关功能，包括用户未读消息统计、消息列表查询及已读状态更新，完善了通知控制器和视图控制器，支持前后端分离的消息通知展示。 JakicDong 今天 14:22debug:wsUserSessionCache未初始化导致出现NPE JakicDong 今天 09:46实现WebSocket消息推送功能，新增用户通知服务及相关拦截器、握手处理器和配置，支持用户间的消息推送和全局通知，优化了消息发送和接收的逻辑。 JakicDong 今天 09:19 项目-派聪明正在配环境,安装了ES和IK分词器插件. 生活篇晚上健身手臂和核心:强度大.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"简历存档","path":"/2025/09/09/杂项笔记/简历存档/","content":"董佳齐性别：男 | 年龄：23岁 | 籍贯：黑龙江 | 政治面貌：中共党员电话：133-5170-2257 | 邮箱：2062966830@qq.comGithub仓库：https://github.com/JakicDong/个人博客：https://jakicdong.github.io/当前状态：在读 | 求职意向：后端开发实习工程师 教育背景东北大学985 控制科学与工程 (A+) 硕士 (推免) 信息科学与工程学院 2024 - 2027 荣誉奖项证书： 研究生一等奖学金。天津工业大学双一流 自动化 本科 控制科学与工程学院 2020 - 2024 荣誉奖项证书： 2021-2022学年天津市优秀学生荣誉称号(每年度学院仅评1人)；CET-6；连续三年获校一等奖学金、校三好学生称号；桑麻基金会奖学金；全国大学生智能汽车竞赛国家级二等奖；大学生创新创业竞赛国家级项目负责人。 专业技能 Java基础：掌握Java基础，如：集合、注解、反射等相关知识。 JVM：熟悉JVM，如：JVM运行时内存区域，垃圾回收机制、类加载机制、对象创建过程等。 JUC：熟悉Java并发编程，如synchronized、ReentrantLock、同步器AQS、InheritableThreadLocal等原理。 MySQL：熟悉MySQL的使用，熟悉MySQL索引、事务、存储引擎、锁机制、两阶段提交等原理。 Redis：熟悉Redis数据结构使用场景，熟悉持久化，内存回收策略，熟悉缓存高并发常见场景，如：缓存穿透、缓存击穿、缓存雪崩。 框架：熟练使用Spring、Spring MVC、MyBatis、Spring Boot等开发框架，熟悉Spring IOC，AOP原理，熟悉Spring Boot自动装配原理，并自定义过Starter。 其他：了解Git、Maven等项目管理及构建工具。 实习经历欧思徕（沈阳）智能科技有限公司 - 后端开发实习生 - ( 2025.07 - 2025.10 ) 使用 Java 和 Spring Boot 完成了多个后端服务的开发，参与了核心业务的功能实现。 项目经验::: startTecHub技术社区:::核心开发人员:::2025.07 - 2025.09::: end仓库地址： https://github.com/JakicDong/TecHub项目描述： TecHub是一个前后端分离的用于企业内技术分享交流的社区项目，包括前端PC和管理后台，用于帮助开发者交流和分享技术的社区平台。技术栈：Spring Boot、MyBatis-Plus、MySQL、Redis、RabbitMQ、Docker。责任描述：作为核心开发人员，主要负责登录认证、消息通知、文章模块以及接入AI开发助手的工作。工作内容： 派聪明AI通过WebSocket实现前后端的及时通信，并通过异步流的方式实现消息动态输出的效果； 通过验证码和前端保持半长连接映射关系，当用户扫码关注公众号并输入验证码后，发起回调，识别用户信息并找到对应半长连接，实现系统自动登录； 基于 ThreadLocal 在登录校验拦截器中封装线程隔离的全局上下文，以便在线程内部存储用户信息，减少用户信息的数据库查询次数； 自定义实现 markdown 编辑器，通过复制粘贴即可将非同源的图片下载并上传到 OSS 实现智能化转链，并借助线程池和异步任务提升系统的高并发能力； 采用 HandleExceptionResolver 的全局异常处理策略，提高了代码的健壮性和可维护性，优化用户体验； 将用户评论、点赞、收藏、系统消息发送到RabbitMQ，实现消息异步解耦，提升系统效率和服务稳定性； 学习项目 - MyDB -（ 2025.10 - 2025.10 ） - https://github.com/JakicDong/TecHub 设计并实现了一个SQL数据库。 实现了分布式锁机制，确保订单处理的高一致性与数据的最终一致性。 使用 Kafka 进行消息队列处理，确保订单处理的高吞吐量。","tags":["简历"],"categories":["杂项笔记"]},{"title":"Elasticsearch(ES)下载使用指南","path":"/2025/09/09/项目笔记/Elasticsearch(ES)下载使用指南/","content":"下载ES下载地址:Elasticsearch(ES)下载地址下载所使用的8.10.0版本. 解压启动 Elasticsearch可以直接进入到 bin 目录，然后执行 .elasticsearch 启动 ES。 配置默认情况下，ES 默认是自动配置堆大小的，也就是没有设置固定的内存限制，所以 ES 会根据系统可用内存自动分配，我本机有时候能飙到 30 多个 G 的内存。如果你本机没有这么大的内存空间，你可以通过下面的命令运行： ES_JAVA_OPTS”-Xms5g -Xmx5g” .binelasticsearch -Xms 设置初始堆大小，-Xmx 设置最大堆大小，建议将 -Xms 和 -Xmx 设置为相同值，避免堆动态调整的开销。 ES 8.10.0 需要 JDK 17 的版本，大家在跑 ES 的时候尽量先配置 JDK17。 可以通过这个连接对比：https://www.elastic.co/cn/support/matrix#matrix_jvm 也可以直接在上一级目录执行下面的命令启动。 ./bin/elasticsearch 默认情况下，Elasticsearch 会在前台运行，并监听 9200端口。但由于 ES 从 8.x 版本开始，启用了安全功能，所以会有这样一段输出，注意保存一下。 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━✅ Elasticsearch security features have been automatically configured!✅ Authentication is enabled and cluster connections are encrypted.ℹ️ Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`): 9cpZafis4bqhWoMU9392ℹ️ HTTP CA certificate SHA-256 fingerprint: 44fd2a183a249d79fccb292c9fec4c40de76b9a4ca04b276d16d6ba874557f6bℹ️ Configure Kibana to use this cluster:• Run Kibana and click the configuration link in the terminal when Kibana starts.• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes): eyJ2ZXIiOiI4LjEwLjAiLCJhZHIiOlsiMTkyLjE2OC4zLjI1OjkyMDAiXSwiZmdyIjoiNDRmZDJhMTgzYTI0OWQ3OWZjY2IyOTJjOWZlYzRjNDBkZTc2YjlhNGNhMDRiMjc2ZDE2ZDZiYTg3NDU1N2Y2YiIsImtleSI6IjBVMXpMcGtCU05uZVpsdUp1N0IxOkI4ZHJ5TWwzU3hHbnViUHlfYjFKcFEifQ==ℹ️ Configure other nodes to join this cluster:• On this node: ⁃ Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`. ⁃ Uncomment the transport.host setting at the end of config/elasticsearch.yml. ⁃ Restart Elasticsearch.• On other nodes: ⁃ Start Elasticsearch with `bin/elasticsearch --enrollment-token token`, using the enrollment token that you generated.━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 如果 ES 没有按照要求加载对应的 JDK 版本，我们可以这样执行：ES_JAVA_HOME$JAVA_HOME .binelasticsearch -d Linux 服务器后台运行 如果你希望 Elasticsearch 在后台运行，可以使用 nohup：nohup .binelasticsearch elasticsearch.log 21 2. ES安全功能解除方法ES 的安全功能包括： HTTPS ：所有通信默认使用 HTTPS。身份验证 ：需要用户名和密码才能访问 Elasticsearch。证书生成 ：安装时会自动生成 TLSSSL 证书。 Elasticsearch 启动时默认启用了 HTTPS（加密通信），如果尝试通过 HTTP（明文通信）访问 Elasticsearch，会导致了以下错误： received plaintext http traffic on an https channel, closing connection 这表明你的客户端（如 curl 或浏览器）发送的是 HTTP 请求，而 Elasticsearch 配置为仅接受 HTTPS 请求 方法 1：使用 HTTPS 访问 Elasticsearch 默认情况下，Elasticsearch 会在安装目录下生成一个自签名证书，并启用 HTTPS。你可以通过以下步骤使用 HTTPS 访问 Elasticsearch。 步骤 1：找到生成的证书 在安装 Elasticsearch 时，系统会提示你保存以下信息： ①、CA 证书路径：通常位于 elasticsearch-8.10.0configcertshttp_ca.crt。 ②、 用户名和密码：默认用户是 elastic，密码会在启动时生成。 如果忘记了密码，可以通过以下命令重置： .binelasticsearch-reset-password -u elastic 步骤 2：使用 HTTPS 访问 回到解压目录（注意不是 bin 目录），运行以下命令，指定 CA 证书并使用 HTTPS 协议： curl –cacert configcertshttp_ca.crt -X GET “https://localhost:9200“ -u elastic 系统会提示输入密码，输入 elastic 用户的密码后即可访问。 注意⚠️，要记得把 ES 的密码配置到你本地代码的 application.yml 文件，和前面 ES 生成的密码是匹配的。 Enter host password for user elastic: name : Jakics-MacBook-Pro.local, cluster_name : elasticsearch, cluster_uuid : Ycs1J_a3SsGcMLNOz6-nbw, version : number : 8.10.0, build_flavor : default, build_type : tar, build_hash : e338da74c79465dfdc204971e600342b0aa87b6b, build_date : 2023-09-07T08:16:21.960703010Z, build_snapshot : false, lucene_version : 9.7.0, minimum_wire_compatibility_version : 7.17.0, minimum_index_compatibility_version : 7.0.0 , tagline : You Know, for Search IK分词器下载地址:https://release.infinilabs.com/analysis-ik/stable/ 启动指令ES_JAVA_HOME=$JAVA_HOME ./bin/elasticsearch -d 用法如何获取es的token 我来帮您获取Elasticsearch的token。在Elasticsearch 8.x中，有几种方式可以获取token： 首先让我检查Elasticsearch是否正在运行： Command Executed: ps aux | grep elasticsearch Output: mac 53510 0.0 0.0 410724320 1408 s003 S+ 11:26AM 0:00.00 grep elasticsearch Elasticsearch没有运行。让我先启动它，然后获取token： Command Executed: ES_JAVA_HOME=$JAVA_HOME ./bin/elasticsearch -d Output: 很好！Elasticsearch已经启动成功。现在我来获取token。等待几秒钟让Elasticsearch完全启动： Command Executed: sleep 10 Output: Command Executed: ./bin/elasticsearch-reset-password -u elastic -b Output: warning: ignoring JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home; using bundled JDKPassword for the [elastic] user successfully reset.New value: kjoXlrSJfOv_I0c=5zkJ 现在我获取用于Kibana的enrollment token： Command Executed: ./bin/elasticsearch-create-enrollment-token -s kibana Output: warning: ignoring JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home; using bundled JDKeyJ2ZXIiOiI4LjEwLjAiLCJhZHIiOlsiMTkyLjE2OC4zLjI1OjkyMDAiXSwiZmdyIjoiNDRmZDJhMTgzYTI0OWQ3OWZjY2IyOTJjOWZlYzRjNDBkZTc2YjlhNGNhMDRiMjc2ZDE2ZDZiYTg3NDU1N2Y2YiIsImtleSI6IjhleVJVSmtCRDlQVnJsMWR2YnA1Ol9NRnpPcGJZVG9lU1pJcUgwT3VDYWcifQ== 现在我也获取用于新节点的enrollment token： Command Executed: ./bin/elasticsearch-create-enrollment-token -s node Output: warning: ignoring JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home; using bundled JDKeyJ2ZXIiOiI4LjEwLjAiLCJhZHIiOlsiMTkyLjE2OC4zLjI1OjkyMDAiXSwiZmdyIjoiNDRmZDJhMTgzYTI0OWQ3OWZjY2IyOTJjOWZlYzRjNDBkZTc2YjlhNGNhMDRiMjc2ZDE2ZDZiYTg3NDU1N2Y2YiIsImtleSI6IjgteVJVSmtCRDlQVnJsMWQtcm9LOjIxR1hLdFp4U1hpTGFCZWMtWFVma0EifQ== 让我也检查一下CA证书的指纹： Command Executed: ./bin/elasticsearch-certutil cert-ca -cacert config/certs/http_ca.crt Output: warning: ignoring JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home; using bundled JDKSimplifies certificate creation for use with the Elastic StackNon-option arguments:command Option Description ------ ----------- -E KeyValuePair Configure a setting-h, --help Show help -s, --silent Show minimal output-v, --verbose Show verbose outputERROR: Unknown command [cert-ca], with exit code 64 让我用正确的命令检查CA证书指纹： Command Executed: openssl x509 -fingerprint -sha256 -in config/certs/http_ca.crt -noout Output: sha256 Fingerprint=44:FD:2A:18:3A:24:9D:79:FC:CB:29:2C:9F:EC:4C:40:DE:76:B9:A4:CA:04:B2:76:D1:6D:6B:A8:74:55:7F:6B 获取Elasticsearch Token的方法总结我已经为您获取了所有必要的Token和凭据信息： 1. 超级用户elastic的密码用户名: elastic密码: kjoXlrSJfOv_I0c=5zkJ 2. Kibana Enrollment TokeneyJ2ZXIiOiI4LjEwLjAiLCJhZHIiOlsiMTkyLjE2OC4zLjI1OjkyMDAiXSwiZmdyIjoiNDRmZDJhMTgzYTI0OWQ3OWZjY2IyOTJjOWZlYzRjNDBkZTc2YjlhNGNhMDRiMjc2ZDE2ZDZiYTg3NDU1N2Y2YiIsImtleSI6IjhleVJVSmtCRDlQVnJsMWR2YnA1Ol9NRnpPcGJZVG9lU1pJcUgwT3VDYWcifQ== 3. Node Enrollment Token（用于新节点加入集群）eyJ2ZXIiOiI4LjEwLjAiLCJhZHIiOlsiMTkyLjE2OC4zLjI1OjkyMDAiXSwiZmdyIjoiNDRmZDJhMTgzYTI0OWQ3OWZjY2IyOTJjOWZlYzRjNDBkZTc2YjlhNGNhMDRiMjc2ZDE2ZDZiYTg3NDU1N2Y2YiIsImtleSI6IjgteVJVSmtCRDlQVnJsMWQtcm9LOjIxR1hLdFp4U1hpTGFCZWMtWFVma0EifQ== 4. HTTPS CA证书指纹44:FD:2A:18:3A:24:9D:79:FC:CB:29:2C:9F:EC:4C:40:DE:76:B9:A4:CA:04:B2:76:D1:6D:6B:A8:74:55:7F:6B 5. Elasticsearch服务地址HTTPS: https://192.168.3.25:9200HTTP: http://127.0.0.1:9200（如果禁用了SSL） 常用的Token获取命令以后您可以使用这些命令自己获取Token： # 重置elastic用户密码./bin/elasticsearch-reset-password -u elastic -b# 创建Kibana enrollment token./bin/elasticsearch-create-enrollment-token -s kibana# 创建新节点 enrollment token ./bin/elasticsearch-create-enrollment-token -s node# 查看CA证书指纹openssl x509 -fingerprint -sha256 -in config/certs/http_ca.crt -noout 现在您可以使用这些信息来配置Kibana或连接到Elasticsearch集群了！ Canal连接ES SSL问题解决方案问题描述在使用Canal客户端连接Elasticsearch 8.x时，可能遇到以下错误： javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection? 问题原因 Elasticsearch 8.x默认启用了SSLTLS加密 Canal客户端配置不正确，SSL协议不匹配 客户端期望SSL连接但服务器返回明文响应 解决方案方案1：正确配置SSL连接（推荐生产环境） 配置Canal使用HTTPS协议 es.url = https://192.168.3.25:9200 配置SSL证书路径 es.ssl.certificate.path = /path/to/elasticsearch/config/certs/http_ca.crt 配置认证信息 es.username = elastices.password = kjoXlrSJfOv_I0c=5zkJ 方案2：禁用SSL（仅开发环境）如果不需要SSL加密，可以禁用ES的SSL功能： 编辑elasticsearch.yml配置文件 xpack.security.enabled: falsexpack.security.enrollment.enabled: falsexpack.security.http.ssl.enabled: falsexpack.security.transport.ssl.enabled: false 重启Elasticsearch ./bin/elasticsearch -d 配置Canal使用HTTP协议 es.url = http://192.168.3.25:9200 方案3：混合配置（开发环境）保持ES的SSL功能，但配置Canal使用HTTP： 修改elasticsearch.yml，仅禁用HTTP SSL xpack.security.http.ssl.enabled: false 重启ES并配置Canal使用HTTP es.url = http://192.168.3.25:9200es.username = elastices.password = kjoXlrSJfOv_I0c=5zkJ 验证连接使用curl测试连接： HTTPS连接测试： curl --cacert config/certs/http_ca.crt -X GET https://192.168.3.25:9200 -u elastic HTTP连接测试： curl -X GET http://192.168.3.25:9200 -u elastic 注意事项 生产环境建议使用HTTPS：保持SSL加密确保数据安全 开发环境可以使用HTTP：简化配置和调试 密码管理：定期更换elastic用户密码 证书管理：妥善保管CA证书文件","tags":["项目","ES","Elasticsearch"],"categories":["项目笔记"]},{"title":"2025.9.8学习日记","path":"/2025/09/08/学习日记25年9月/2025.9.8学习笔记/","content":"今日学习内容今天学校第二周,回学校上课来了. 力扣每日一题暴力枚举题. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 简历制作大概初版已经差不多了. 项目更新用户主页视图控制器，新增访问其他用户主页的功能，优化文章列表查询接口的注释，增加页面列表VO的注释以说明更多数据的存在。整合WebSocket长连接实现消息实时推送博客简单来说通过客户端首先通过端口来对服务端进行连接,在此过程中会进行身份验证和校验.然后客户端向服务端通信需要通过向broker节点发送消息,然后broker节点会向MessageMapping节点进行转发即刻进行处理.服务端发送给客户端消息,会直接转发到broker端口进行发送,客户端只需要对broker端口进行监听即可. 生活篇中午健身回到学校正好可以去办卡的健身房练.今日练腿:强度很大.","tags":["基础","日记","leetcode","项目","STOMP"],"categories":["学习日记","2025-09"]},{"title":"2025.9.6学习日记","path":"/2025/09/06/学习日记25年9月/2025.9.6学习笔记/","content":"今日学习内容3DGS标注数据 力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目继续完善专栏接口. 新增专栏文章DTO类及相关接口实现，包括获取专栏文章列表和专栏详情的功能，完善了ColumnService和ColumnDao的实现，更新了ColumnViewController以支持专栏文章的展示和SEO标签的注入。 新增用户相关的DTO、枚举和服务实现，包括关注用户信息、通知消息、用户关系服务及其实现，完善了用户中心的视图控制器和相关的前端展示逻辑。 下一步完善user接口和notify接口. 新增用户中心相关的DTO、枚举和服务实现，包括关注用户信息、通知消息、用户关系服务及其实现，完善了用户主页的视图控制器和SEO标签注入逻辑。","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.3学习日记","path":"/2025/09/03/学习日记25年9月/2025.9.3学习笔记/","content":"今日学习内容力扣每日一题JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目已完成专栏的一个接口,还需完善. JakicDong 202593 16:25已完成评论的删除功能.测试功能完毕. JakicDong 202593 15:53已完成保存,查看评论功能,已修复传参为null的bug,是因为导入的不是spring的@RequestBody导致解析错误.现在可以正常的放松评论. JakicDong 202593 15:20实现了保存评论接口,但存在bug:保存评论时进入到post接口,但是提示文章id为空,并且req为null. JakicDong 202593 14:56实现了保存评论接口,但存在bug:保存评论时进入到post接口,但是提示文章id为空,并且req为null. JakicDong 202593 13:55","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.2学习日记","path":"/2025/09/02/学习日记25年9月/2025.9.2学习笔记/","content":"今日学习内容今天搞了半天的选课…这学期还得刷三门选修课. 力扣每日一题今天的每日通过排序来满足条件,然后On2直接遍历做. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.9.1学习日记","path":"/2025/09/01/学习日记25年9月/2025.9.1学习笔记/","content":"今日学习内容今天回学校,继续学习! 力扣每日一题今天的每日一题是优先队列相关的题目,求一堆班级及格率的平均值最大,可以插入必过的同学.思路是通过优先队列,按照及格率的增量来进行排序,每次插入必过的同学到增量最大的班级当中,贪心的来计算最大平均值.思路很巧妙,实现就是简单的优先队列使用. 配置win台式的ssh台式电脑的ssh秘钥过期了,所以重新配置了一下,步骤简单贴在下面. 1. 直接使用Git Bash2. 打开Git Bash（不是PowerShell）3. 在开始菜单搜索Git Bash4. 或者在当前文件夹右键选择Git Bash Here5. 在Git Bash中执行以下命令：6. 然后在GitHub上添加SSH密钥：7. 登录GitHub8. 进入 Settings → SSH and GPG keys9. 点击 New SSH key10. 粘贴公钥内容（Ctrl+V）11. 给密钥起个名字，比如我的电脑12. 点击Add SSH key win10启动redis找到redis安装目录redis-server.exe redis.windows.conf 启动redisredis-cli.exe 启动redis客户端redis-cli.exe -p 6379 指定端口启动redis客户端redis-cli.exe -p 6379 -a 123456 指定端口和密码启动redis客户端redis-cli.exe -p 6379 -a 123456 -n 0 指定端口和密码和数据库启动redis客户端 更改git仓库为ssh连接:git remote set-url origin git@github.com:JakicDong/TecHub.git JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目还剩很多的crud的功能接口需要完成.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-09"]},{"title":"2025.8.30学习日记","path":"/2025/08/30/学习日记25年8月/2025.8.30学习笔记/","content":"今日学习内容最近每天只在做每日一题. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目还剩很多的crud的功能接口需要完成.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"Redis学习笔记","path":"/2025/08/30/杂项笔记/git秘钥过期处理/","content":"问题当git秘钥过期时,会导致无法使用git命令,无法进行git操作.我使用的是github仓库进行代码管理. 解决方法 检查现有密钥\tls -al ~/.ssh 备份旧密钥(可选)\tcp ~/.ssh/id_rsa* ~/ssh_backup/ 生成新密钥\tssh-keygen -t rsa -b 4096 -C 你的邮箱 启动 SSH 代理\teval $(ssh-agent -s) 添加新密钥到代理\tssh-add ~/.ssh/id_rsa 复制公钥\tcat ~/.ssh/id_rsa.pub GitHub 添加公钥\t网页操作 测试连接 更新仓库远程 URL\tgit remote set-url origin git@github.com:用户/仓库.git 验证操作\tgit push origin main nano ~.sshconfig 配置文件添加 Host github.com Hostname ssh.github.com Port 443 User git IdentityFile ~/.ssh/id_rsa # 如果您的私钥是其他名称或路径，请修改此处，例如 ~/.ssh/id_ed25519 将仓库地址改为ssh地址 git remote set-url origin git@github.com:JakicDong/Myblog_posts.git","tags":["基础","Java"],"categories":["杂项笔记"]},{"title":"2025.8.27学习日记","path":"/2025/08/27/学习日记25年8月/2025.8.27学习笔记/","content":"今日学习内容最近每天只在做每日一题. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目还剩很多的crud的功能接口需要完成.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.20学习日记","path":"/2025/08/20/学习日记25年8月/2025.8.20学习笔记/","content":"今日学习内容最近每天只在做每日一题. 休息的差不多了,今天开始继续学习. 力扣每日一题一道二维DP题目. JAVA并发编程篇1071 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目还剩很多的crud的功能接口需要完成.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.13学习日记","path":"/2025/08/13/学习日记25年8月/2025.8.13学习笔记/","content":"今日学习内容终于放假了. 力扣每日一题判断数是不是3的幂,直接temp*3判断即可. JAVA并发编程篇971 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目生活篇准备去秦皇岛旅行14-17号","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.12学习日记","path":"/2025/08/12/学习日记25年8月/2025.8.12学习笔记/","content":"今日学习内容3DGS继续做母点保护的实验. 力扣每日一题0-1背包问题 JAVA并发编程篇971 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目新增文章热门推荐功能，支持根据作者ID查询热门文章；新增评论相关功能，包括获取热门评论和评论点赞数统计；完善文章详情页的侧边栏信息展示。复习项目.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.11学习日记","path":"/2025/08/11/学习日记25年8月/2025.8.11学习笔记/","content":"今日学习内容3DGS继续做母点保护的实验. 力扣每日一题还是一道和2的幂相关的题目. JAVA并发编程篇971 学习中,正在完善笔记. Redis学习笔记357 学习中,正在完善笔记. 项目新增文章阅读扩展服务和文章详情页的SEO标签初始化功能，完善了文章阅读控制逻辑，支持不同阅读模式的控制。新增操作类型枚举类OperateTypeEnum、支付状态枚举类PayStatusEnum，以及自定义提示框解析器和扩展类，包含Markdown转换器、JSON工具类、支付相关服务和评论相关服务的实现。","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.9学习日记","path":"/2025/08/09/学习日记25年8月/2025.8.9学习笔记/","content":"今日学习内容3DGS做母点保护的实验. 力扣每日一题快速判断一个数是不是2的幂. return n 0 (n (n - 1)) == 0; JAVA并发编程篇971 学习中,正在完善笔记.git config –global http.postBuffer 524288000 增加缓冲区大小我需要测试一下 项目梳理项目.正在实现文章接口.新增文章相关DTO和评论相关DTO类，包括ArticleOtherDTO、ArticlePayInfoDTO、ColumnArticleFlipDTO、CommentSaveReq、BaseCommentDTO、SubCommentDTO、TopCommentDTO、ArticleDetailVo及ArticleViewController控制器。 代码随想录无 生活篇晚上健身晚上练的手臂,强度中等偏上.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"Java并发编程学习笔记","path":"/2025/08/08/Java问答笔记/Java并发编程学习笔记/","content":"基础1.并行跟并发有什么区别？ 并行是多核 CPU 上的多任务处理，多个任务在同一时间真正地同时执行。 并发是单核 CPU 上的多任务处理，多个任务在同一时间段内交替执行，通过时间片轮转实现交替执行，用于解决 IO 密集型任务的瓶颈。单线程处理IO密集型任务：时间轴：|--IO等待--|--CPU处理--|--IO等待--|--CPU处理--|CPU利用率： 空闲 忙碌 空闲 忙碌 线程1：|--IO等待--|--CPU处理--|线程2： |--IO等待--|--CPU处理--|线程3： |--IO等待--|--CPU处理--|CPU利用率：忙碌 忙碌 忙碌 忙碌 忙碌 忙碌 你是如何理解线程安全的？推荐阅读：多线程带来了哪些问题？ 如果一段代码块或者一个方法被多个线程同时执行，还能够正确地处理共享数据，那么这段代码块或者这个方法就是线程安全的。 可以从三个要素来确保线程安全： ①、原子性：一个操作要么完全执行，要么完全不执行，不会出现中间状态。 可以通过同步关键字 synchronized 或原子操作，如 AtomicInteger 来保证原子性。 AtomicInteger count = new AtomicInteger(0);count.incrementAndGet(); // 原子操作 ②、可见性：当一个线程修改了共享变量，其他线程能够立即看到变化。 可以通过 volatile 关键字来保证可见性。 private volatile String itwanger = 沉默王二; ③、有序性：要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行。 2.🌟说说进程和线程的区别？推荐阅读:进程与线程的区别是什么？ 进程说简单点就是我们在电脑上启动的一个个应用。它是操作系统分配资源的最小单位。 线程是进程中的独立执行单元。多个线程可以共享同一个进程的资源，如内存；每个线程都有自己独立的栈和寄存器。 如何理解协程？协程被视为比线程更轻量级的并发单元，可以在单线程中实现并发执行，由我们开发者显式调度。 协程是在用户态进行调度的，避免了线程切换时的内核态开销。 Java 自身是不支持携程的，我们可以使用 Quasar、Kotlin 等框架来实现协程。 fun main() = runBlocking launch delay(1000L) println(World!) println(Hello,) 线程间是如何进行通信的？原则上可以通过消息传递和共享内存两种方法来实现。Java 采用的是共享内存的并发模型。 这个模型被称为 Java 内存模型，简写为 JMM，它决定了一个线程对共享变量的写入，何时对另外一个线程可见。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。 用一句话来概括就是：共享变量存储在主内存中，每个线程的私有本地内存，存储的是这个共享变量的副本。 线程 A 与线程 B 之间如要通信，需要要经历 2 个步骤： 线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。 线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中。 3.🌟说说线程有几种创建方式？推荐阅读：室友打了一把王者就学会了 Java 多线程 有三种，分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口。 第一种需要重写父类 Thread 的 run() 方法，并且调用 start() 方法启动线程。 class ThreadTask extends Thread public void run() System.out.println(看完二哥的 Java 进阶之路，上岸了!); public static void main(String[] args) ThreadTask task = new ThreadTask(); task.start(); 这种方法的缺点是，如果 ThreadTask 已经继承了另外一个类，就不能再继承 Thread 类了，因为 Java 不支持多重继承。 第二种需要重写 Runnable 接口的 run() 方法，并将实现类的对象作为参数传递给 Thread 对象的构造方法，最后调用 start() 方法启动线程。 class RunnableTask implements Runnable public void run() System.out.println(看完二哥的 Java 进阶之路，上岸了!); public static void main(String[] args) RunnableTask task = new RunnableTask(); Thread thread = new Thread(task); thread.start(); 这种方法的优点是可以避免 Java 的单继承限制，并且更符合面向对象的编程思想，因为 Runnable 接口将任务代码和线程控制的代码解耦了。 第三种需要重写 Callable 接口的 call() 方法，然后创建 FutureTask 对象，参数为 Callable 实现类的对象；紧接着创建 Thread 对象，参数为 FutureTask 对象，最后调用 start() 方法启动线程。 class CallableTask implements CallableString public String call() return 看完二哥的 Java 进阶之路，上岸了!; public static void main(String[] args) throws ExecutionException, InterruptedException CallableTask task = new CallableTask(); FutureTaskString futureTask = new FutureTask(task); Thread thread = new Thread(futureTask); thread.start(); System.out.println(futureTask.get()); 这种方法的优点是可以获取线程的执行结果。 一个 8G 内存的系统最多能创建多少个线程?推荐阅读：深入理解 JVM 的运行时数据区 理论上大约 8000 个。 创建线程的时候，至少需要分配一个虚拟机栈，在 64 位操作系统中，默认大小为 1M，因此一个线程大约需要 1M 的内存。 但 JVM、操作系统本身的运行就要占一定的内存空间，所以实际上可以创建的线程数远比 8000 少。 详细解释一下。 可以通过 java -XX:+PrintFlagsFinal -version | grep ThreadStackSize 命令查看 JVM 栈的默认大小。 其中 ThreadStackSize 的单位是 KB，也就是说默认的 JVM 栈大小是 1024 KB，也就是 1M。 启动一个 Java 程序，你能说说里面有哪些线程吗？首先是 main 线程，这是程序执行的入口。 然后是垃圾回收线程，它是一个后台线程，负责回收不再使用的对象。 还有编译器线程，比如 JIT，负责把一部分热点代码编译后放到 codeCache 中。 可以通过下面的代码进行检测： class ThreadLister public static void main(String[] args) // 获取所有线程的堆栈跟踪 MapThread, StackTraceElement[] threads = Thread.getAllStackTraces(); for (Thread thread : threads.keySet()) System.out.println(Thread: + thread.getName() + (ID= + thread.getId() + )); 结果如下所示： Thread: Monitor Ctrl-Break (ID=5)Thread: Reference Handler (ID=2)Thread: main (ID=1)Thread: Signal Dispatcher (ID=4)Thread: Finalizer (ID=3) 简单解释下： Thread: main (ID=1) - 主线程，Java 程序启动时由 JVM 创建。 Thread: Reference Handler (ID=2) - 这个线程是用来处理引用对象的，如软引用、弱引用和虚引用。负责清理被 JVM 回收的对象。 Thread: Finalizer (ID=3) - 终结器线程，负责调用对象的 finalize 方法。对象在垃圾回收器标记为可回收之前，由该线程执行其 finalize 方法，用于执行特定的资源释放操作。 Thread: Signal Dispatcher (ID=4) - 信号调度线程，处理来自操作系统的信号，将它们转发给 JVM 进行进一步处理，例如响应中断、停止等信号。 Thread: Monitor Ctrl-Break (ID=5) - 监视器线程，通常由一些特定的 IDE 创建，用于在开发过程中监控和管理程序执行或者处理中断。 4.🌟调用 start 方法时会执行 run 方法，那怎么不直接调用 run方法？调用 start() 会创建一个新的线程，并异步执行 run() 方法中的代码。 直接调用 run() 方法只是一个普通的同步方法调用，所有代码都在当前线程中执行，不会创建新线程。没有新的线程创建，也就达不到多线程并发的目的。 通过敲代码体验一下。 class MyThread extends Thread public void run() System.out.println(Thread.currentThread().getName()); public static void main(String[] args) MyThread t1 = new MyThread(); t1.start(); // 正确的方式，创建一个新线程，并在新线程中执行 run() t1.run(); // 仅在主线程中执行 run()，没有创建新线程 来看输出结果： mainThread-0 也就是说，调用 start() 方法会通知 JVM，去调用底层的线程调度机制来启动新线程。 调用 start() 后，线程进入就绪状态，等待操作系统调度；一旦调度执行，线程会执行其 run() 方法中的代码。 5.线程有哪些常用的调度方法？比如说 start 方法用于启动线程并让操作系统调度执行；sleep 方法用于让当前线程休眠一段时间；wait 方法会让当前线程等待，notify 会唤醒一个等待的线程。 说说wait方法和notify方法？当线程 A 调用共享对象的 wait() 方法时，线程 A 会被阻塞挂起，直到： 线程 B 调用了共享对象的 notify() 方法或者 notifyAll() 方法； 其他线程调用线程 A 的 interrupt() 方法，导致线程 A 抛出 InterruptedException 异常。 线程 A 调用共享对象的 wait(timeout)方法后，没有在指定的 timeout 时间内被其它线程唤醒，那么这个方法会因为超时而返回。 当线程 A 调用共享对象的 notify() 方法后，会唤醒一个在这个共享对象上调用 wait 系列方法被挂起的线程。 共享对象上可能会有多个线程在等待，具体唤醒哪个线程是随机的。 如果调用的是 notifyAll 方法，会唤醒所有在这个共享变量上调用 wait 系列方法而被挂起的线程。 说说 sleep 方法？当线程 A 调用了 Thread 的 sleep 方法后，线程 A 会暂时让出指定时间的执行权。 指定的睡眠时间到了后该方法会正常返回，接着参与 CPU 调度，获取到 CPU 资源后可以继续执行。 说说yield方法？yield() 方法的目的是让当前线程让出 CPU 使用权，回到就绪状态。但是线程调度器可能会忽略。 说说interrupt方法？推荐阅读：interrupt方法 interrupt() 方法用于通知线程停止，但不会直接终止线程，需要线程自行处理中断标志。 常与 isInterrupted() 或 Thread.interrupted() 配合使用。 Thread thread = new Thread(() - while (!Thread.currentThread().isInterrupted()) System.out.println(Running); System.out.println(Interrupted););thread.start();thread.interrupt(); // 中断线程 说说 stop 方法？stop 方法用来强制停止线程，目前已经处于废弃状态，因为 stop 方法可能会在不一致的状态下释放锁，破坏对象的一致性。 6.🌟线程有几种状态？6 种。 new 代表线程被创建但未启动； runnable 代表线程处于就绪或正在运行状态，由操作系统调度； blocked 代表线程被阻塞，等待获取锁； waiting 代表线程等待其他线程的通知或中断； timed_waiting 代表线程会等待一段时间，超时后自动恢复； terminated 代表线程执行完毕，生命周期结束。 也就是说，线程的生命周期可以分为五个主要阶段：新建、就绪、运行、阻塞和终止。线程在运行过程中会根据状态的变化在这些阶段之间切换。 class ThreadStateExample public static void main(String[] args) throws InterruptedException Thread thread = new Thread(() - try Thread.sleep(2000); // TIMED_WAITING synchronized (ThreadStateExample.class) ThreadStateExample.class.wait(); // WAITING catch (InterruptedException e) Thread.currentThread().interrupt(); ); System.out.println(State after creation: + thread.getState()); // NEW thread.start(); System.out.println(State after start: + thread.getState()); // RUNNABLE Thread.sleep(500); System.out.println(State while sleeping: + thread.getState()); // TIMED_WAITING synchronized (ThreadStateExample.class) ThreadStateExample.class.notify(); // 唤醒线程 thread.join(); System.out.println(State after termination: + thread.getState()); // TERMINATED 用一个表格来做个总结： 如何强制终止线程？第一步，调用线程的 interrupt() 方法，请求终止线程。 第二步，在线程的 run() 方法中检查中断状态，如果线程被中断，就退出线程。 class MyTask implements Runnable @Override public void run() while (!Thread.currentThread().isInterrupted()) try System.out.println(Running...); Thread.sleep(1000); // 模拟工作 catch (InterruptedException e) // 捕获中断异常后，重置中断状态 Thread.currentThread().interrupt(); System.out.println(Thread interrupted, exiting...); break; public class Main public static void main(String[] args) throws InterruptedException Thread thread = new Thread(new MyTask()); thread.start(); Thread.sleep(3000); // 主线程等待3秒 thread.interrupt(); // 请求终止线程 中断结果： 7.什么是线程上下文切换？线程上下文切换是指 CPU 从一个线程切换到另一个线程执行时的过程。 在线程切换的过程中，CPU 需要保存当前线程的执行状态，并加载下一个线程的上下文。 之所以要这样，是因为 CPU 在同一时刻只能执行一个线程，为了实现多线程并发执行，需要不断地在多个线程之间切换。 为了让用户感觉多个线程是在同时执行的， CPU 资源的分配采用了时间片轮转的方式，线程在时间片内占用 CPU 执行任务。当线程使用完时间片后，就会让出 CPU 让其他线程占用。 线程可以被多核调度吗？多核处理器提供了并行执行多个线程的能力。每个核心可以独立执行一个或多个线程，操作系统的任务调度器会根据策略和算法，如优先级调度、轮转调度等，决定哪个线程何时在哪个核心上运行。 8.守护线程了解吗？了解，守护线程是一种特殊的线程，它的作用是为其他线程提供服务。 Java 中的线程分为两类，一种是守护线程，另外一种是用户线程。 JVM 启动时会调用 main 方法，main 方法所在的线程就是一个用户线程。在 JVM 内部，同时还启动了很多守护线程，比如垃圾回收线程。 守护线程和用户线程有什么区别呢？区别之一是当最后一个非守护线程束时， JVM 会正常退出，不管当前是否存在守护线程，也就是说守护线程是否结束并不影响 JVM 退出。 换而言之，只要有一个用户线程还没结束，正常情况下 JVM 就不会退出。 9.线程间有哪些通信方式？线程之间传递信息的方式有多种，比如说使用 volatile 和 synchronized 关键字共享对象、使用 wait() 和 notify() 方法实现生产者-消费者模式、使用 Exchanger 进行数据交换、使用 Condition 实现线程间的协调等。 简单说说 volatile 和 synchronized 的使用方式？多个线程可以通过 volatile 和 synchronized 关键字访问和修改同一个对象，从而实现信息的传递。 关键字 volatile 可以用来修饰成员变量，告知程序任何对该变量的访问均需要从共享内存中获取，并同步刷新回共享内存，保证所有线程对变量访问的可见性。 关键字 synchronized 可以修饰方法，或者同步代码块，确保多个线程在同一个时刻只有一个线程在执行方法或代码块。 class SharedObject private String message; private boolean hasMessage = false; public synchronized void writeMessage(String message) while (hasMessage) try wait(); catch (InterruptedException e) Thread.currentThread().interrupt(); this.message = message; hasMessage = true; notifyAll(); public synchronized String readMessage() while (!hasMessage) try wait(); catch (InterruptedException e) Thread.currentThread().interrupt(); hasMessage = false; notifyAll(); return message; public class Main public static void main(String[] args) SharedObject sharedObject = new SharedObject(); Thread writer = new Thread(() - sharedObject.writeMessage(Hello from Writer!); ); Thread reader = new Thread(() - String message = sharedObject.readMessage(); System.out.println(Reader received: + message); ); writer.start(); reader.start(); wait() 和 notify() 方法的使用方式了解吗？一个线程调用共享对象的 wait() 方法时，它会进入该对象的等待池，释放已经持有的锁，进入等待状态。 一个线程调用 notify() 方法时，它会唤醒在该对象等待池中等待的一个线程，使其进入锁池，等待获取锁。 class MessageBox private String message; private boolean empty = true; public synchronized void produce(String message) while (!empty) try wait(); catch (InterruptedException e) Thread.currentThread().interrupt(); empty = false; this.message = message; notifyAll(); public synchronized String consume() while (empty) try wait(); catch (InterruptedException e) Thread.currentThread().interrupt(); empty = true; notifyAll(); return message; public class Main public static void main(String[] args) MessageBox box = new MessageBox(); Thread producer = new Thread(() - box.produce(Message from producer); ); Thread consumer = new Thread(() - String message = box.consume(); System.out.println(Consumer received: + message); ); producer.start(); consumer.start(); Condition 也提供了类似的方法，await() 负责阻塞、signal() 和 signalAll() 负责通知。 通常与锁 ReentrantLock 一起使用，为线程提供了一种等待某个条件成真的机制，并允许其他线程在该条件变化时通知等待线程。 Exchanger 的使用方式了解吗？Exchanger 是一个同步点，可以在两个线程之间交换数据。一个线程调用 exchange() 方法，将数据传递给另一个线程，同时接收另一个线程的数据。 class Main public static void main(String[] args) ExchangerString exchanger = new Exchanger(); Thread thread1 = new Thread(() - try String message = Message from thread1; String response = exchanger.exchange(message); System.out.println(Thread1 received: + response); catch (InterruptedException e) Thread.currentThread().interrupt(); ); Thread thread2 = new Thread(() - try String message = Message from thread2; String response = exchanger.exchange(message); System.out.println(Thread2 received: + response); catch (InterruptedException e) Thread.currentThread().interrupt(); ); thread1.start(); thread2.start(); CompletableFuture 的使用方式了解吗？CompletableFuture 是 Java 8 引入的一个类，支持异步编程，允许线程在完成计算后将结果传递给其他线程。 class Main public static void main(String[] args) CompletableFutureString future = CompletableFuture.supplyAsync(() - // 模拟长时间计算 return Message from CompletableFuture; ); future.thenAccept(message - System.out.println(Received: + message); ); 10.🌟请说说 sleep 和 wait 的区别？（补充）sleep 会让当前线程休眠，不需要获取对象锁，属于 Thread 类的方法；wait 会让获得对象锁的线程等待，要提前获得对象锁，属于 Object 类的方法。 ①、所属类不同 sleep() 方法专属于 Thread 类。 wait() 方法专属于 Object 类。 ②、锁行为不同 如果一个线程在持有某个对象锁时调用了 sleep 方法，它在睡眠期间仍然会持有这个锁。 class SleepDoesNotReleaseLock private static final Object lock = new Object(); public static void main(String[] args) throws InterruptedException Thread sleepingThread = new Thread(() - synchronized (lock) System.out.println(Thread 1 会继续持有锁，并且进入睡眠状态); try Thread.sleep(5000); catch (InterruptedException e) e.printStackTrace(); System.out.println(Thread 1 醒来了，并且释放了锁); ); Thread waitingThread = new Thread(() - synchronized (lock) System.out.println(Thread 2 进入同步代码块); ); sleepingThread.start(); Thread.sleep(1000); waitingThread.start(); 输出结果： Thread 1 会继续持有锁，并且进入睡眠状态Thread 1 醒来了，并且释放了锁Thread 2 进入同步代码块 从输出中我们可以看到，waitingThread 必须等待 sleepingThread 完成睡眠后才能进入同步代码块。 而当线程执行 wait 方法时，它会释放持有的对象锁，因此其他线程也有机会获取该对象的锁。 class WaitReleasesLock private static final Object lock = new Object(); public static void main(String[] args) throws InterruptedException Thread waitingThread = new Thread(() - synchronized (lock) try System.out.println(Thread 1 持有锁，准备等待 5 秒); lock.wait(5000); System.out.println(Thread 1 醒来了，并且退出同步代码块); catch (InterruptedException e) e.printStackTrace(); ); Thread notifyingThread = new Thread(() - synchronized (lock) System.out.println(Thread 2 尝试唤醒等待中的线程); lock.notify(); System.out.println(Thread 2 执行完了 notify); ); waitingThread.start(); Thread.sleep(1000); notifyingThread.start(); 输出结果： Thread 1 持有锁，准备等待 5 秒Thread 2 尝试唤醒等待中的线程Thread 2 执行完了 notifyThread 1 醒来了，并且退出同步代码块 这表明 waitingThread 在调用 wait 后确实释放了锁。 ③、使用条件不同 sleep() 方法可以在任何地方被调用。wait() 方法必须在同步代码块或同步方法中被调用，这是因为调用 wait() 方法的前提是当前线程必须持有对象的锁。否则会抛出 IllegalMonitorStateException 异常。 ④、唤醒方式不同 调用 sleep 方法后，线程会进入 TIMED_WAITING 状态，即在指定的时间内暂停执行。当指定的时间结束后，线程会自动恢复到 RUNNABLE 状态，等待 CPU 调度再次执行。 调用 wait 方法后，线程会进入 WAITING 状态，直到有其他线程在同一对象上调用 notify 或 notifyAll 方法，线程才会从 WAITING 状态转变为 RUNNABLE 状态，准备再次获得 CPU 的执行权。 我们来通过代码再感受一下 sleep 和 wait 在用法上的区别，先看 sleep 的用法： class SleepExample public static void main(String[] args) Thread thread = new Thread(() - System.out.println(线程准备休眠 2 秒); try Thread.sleep(2000); // 线程将睡眠2秒 catch (InterruptedException e) e.printStackTrace(); System.out.println(线程醒来了); ); thread.start(); 再来看 wait() 的用法： class WaitExample public static void main(String[] args) final Object lock = new Object(); Thread thread = new Thread(() - synchronized (lock) try System.out.println(线程准备等待 2 秒); lock.wait(2000); // 线程会等待2秒，或者直到其他线程调用 lock.notify()/notifyAll() System.out.println(线程结束等待); catch (InterruptedException e) e.printStackTrace(); ); thread.start(); 11.🌟怎么保证线程安全？（补充）线程安全是指在并发环境下，多个线程访问共享资源时，程序能够正确地执行，而不会出现数据不一致的问题。 为了保证线程安全，可以使用 synchronized 关键字对方法加锁，对代码块加锁。线程在执行同步方法、同步代码块时，会获取类锁或者对象锁，其他线程就会阻塞并等待锁。 如果需要更细粒度的锁，可以使用 ReentrantLock 并发重入锁等。 如果需要保证变量的内存可见性，可以使用 volatile 关键字。 对于简单的原子变量操作，还可以使用 Atomic 原子类。 对于线程独立的数据，可以使用 ThreadLocal 来为每个线程提供专属的变量副本。 对于需要并发容器的地方，可以使用 ConcurrentHashMap、CopyOnWriteArrayList 等。 有个int的变量为0，十个线程轮流对其进行++操作（循环10000次），结果大于10 万还是小于等于10万，为什么？在这个场景中，最终的结果会小于 100000，原因是多线程环境下，++ 操作并不是一个原子操作，而是分为读取、加 1、写回三个步骤。 读取变量的值。 将读取到的值加 1。 将结果写回变量。 这样的话，就会有多个线程读取到相同的值，然后对这个值进行加 1 操作，最终导致结果小于 100000。 详细解释下。 多个线程在并发执行 ++ 操作时，可能出现以下竞态条件： 线程 1 读取变量值为 0。 线程 2 也读取变量值为 0。 线程 1 进行加法运算并将结果 1 写回变量。 线程 2 进行加法运算并将结果 1 写回变量，覆盖了线程 1 的结果。 可以通过 synchronized 关键字为 ++ 操作加锁。 class Main private static int count = 0; public static void main(String[] args) throws InterruptedException Runnable task = () - for (int i = 0; i 10000; i++) synchronized (Main.class) count++; ; ListThread threads = new ArrayList(); for (int i = 0; i 10; i++) Thread thread = new Thread(task); threads.add(thread); thread.start(); for (Thread thread : threads) thread.join(); System.out.println(Final count: + count); 或者使用 AtomicInteger 的 incrementAndGet() 方法来替代 ++ 操作，保证变量的原子性。 class Main private static AtomicInteger count = new AtomicInteger(0); public static void main(String[] args) throws InterruptedException Runnable task = () - for (int i = 0; i 10000; i++) count.incrementAndGet(); ; ListThread threads = new ArrayList(); for (int i = 0; i 10; i++) Thread thread = new Thread(task); threads.add(thread); thread.start(); for (Thread thread : threads) thread.join(); System.out.println(Final count: + count.get()); 场景:有一个 key 对应的 value 是一个json 结构，json 当中有好几个子任务，这些子任务如果对 key 进行修改的话，会不会存在线程安全的问题？会。 在单节点环境中，可以使用 synchronized 关键字或 ReentrantLock 来保证对 key 的修改操作是原子的。 class KeyManager private final ReentrantLock lock = new ReentrantLock(); private String key = \\tasks\\: [\\task1\\, \\task2\\]; public String readKey() lock.lock(); try return key; finally lock.unlock(); public void updateKey(String newKey) lock.lock(); try this.key = newKey; finally lock.unlock(); 在多节点环境中，可以使用分布式锁 Redisson 来保证对 key 的修改操作是原子的。 class DistributedKeyManager private final RedissonClient redisson; public DistributedKeyManager() Config config = new Config(); config.useSingleServer().setAddress(redis://127.0.0.1:6379); this.redisson = Redisson.create(config); public void updateKey(String key, String newValue) RLock lock = redisson.getLock(key); lock.lock(); try // 模拟读取和更新操作 String currentValue = readFromDatabase(key); // 假设读取 JSON 数据 String updatedValue = modifyJson(currentValue, newValue); // 修改 JSON writeToDatabase(key, updatedValue); // 写回数据库 finally lock.unlock(); private String readFromDatabase(String key) // 模拟从数据库读取 return \\tasks\\: [\\task1\\, \\task2\\]; private String modifyJson(String json, String newValue) // 使用 JSON 库解析并修改 return json.replace(task1, newValue); private void writeToDatabase(String key, String value) // 模拟写回数据库 说一个线程安全的使用场景？单例模式。在多线程环境下，如果多个线程同时尝试创建实例，单例类必须确保只创建一个实例，并提供一个全局访问点。 饿汉式是一种比较直接的实现方式，它通过在类加载时就立即初始化单例对象来保证线程安全。 class Singleton private static final Singleton instance = new Singleton(); private Singleton() public static Singleton getInstance() return instance; 懒汉式单例则在第一次使用时初始化单例对象，这种方式需要使用双重检查锁定来确保线程安全，volatile 关键字用来保证可见性，syncronized 关键字用来保证同步。 class LazySingleton private static volatile LazySingleton instance; private LazySingleton() public static LazySingleton getInstance() if (instance == null) // 第一次检查 synchronized (LazySingleton.class) if (instance == null) // 第二次检查 instance = new LazySingleton(); return instance; 能说一下 Hashtable 的底层数据结构吗？与 HashMap 类似，Hashtable 的底层数据结构也是一个数组加上链表的方式，然后通过 synchronized 加锁来保证线程安全。 二哥的Java 进阶之路：Hashtable源码 ThreadLocal推荐阅读：ThreadLocal 全面解析 12.🌟ThreadLocal 是什么？ThreadLocal 是一种用于实现线程局部变量的工具类。它允许每个线程都拥有自己的独立副本，从而实现线程隔离。 三分恶面渣逆袭：ThreadLocal线程副本 使用 ThreadLocal 通常分为四步： ①、创建 ThreadLocal //创建一个ThreadLocal变量public static ThreadLocalString localVariable = new ThreadLocal(); ②、设置 ThreadLocal 的值 //设置ThreadLocal变量的值localVariable.set(沉默王二是沙雕); ③、获取 ThreadLocal 的值 //获取ThreadLocal变量的值String value = localVariable.get(); ④、删除 ThreadLocal 的值 //删除ThreadLocal变量的值localVariable.remove(); 在 Web 应用中，可以使用 ThreadLocal 存储用户会话信息，这样每个线程在处理用户请求时都能方便地访问当前用户的会话信息。 在数据库操作中，可以使用 ThreadLocal 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题。 在格式化操作中，例如日期格式化，可以使用 ThreadLocal 存储 SimpleDateFormat 实例，避免多线程共享同一实例导致的线程安全问题。 ThreadLocal 有哪些优点？每个线程访问的变量副本都是独立的，避免了共享变量引起的线程安全问题。由于 ThreadLocal 实现了变量的线程独占，使得变量不需要同步处理，因此能够避免资源竞争。 ThreadLocal 可用于跨方法、跨类时传递上下文数据，不需要在方法间传递参数。 13.你在工作中用到过 ThreadLocal 吗？有用到过，用来存储用户信息。 技术派：ThreadLocal 技术派实战项目是典型的 MVC 架构，登录后的用户每次访问接口，都会在请求头中携带一个 token，在控制层可以根据这个 token，解析出用户的基本信息。 假如在服务层和持久层也要用到用户信息，就可以在控制层拦截请求把用户信息存入 ThreadLocal。 技术派实战源码：控制层拦截请求 这样我们在任何一个地方，都可以取出 ThreadLocal 中存的用户信息。 技术派实战源码：从ThreadLocal中取出信息 很多其它场景的 cookie、session 等等数据隔离都可以通过 ThreadLocal 去实现。 三分恶面渣逆袭：ThreadLoca存放用户上下文 14.🌟ThreadLocal 怎么实现的呢？当我们创建一个 ThreadLocal 对象并调用 set 方法时，其实是在当前线程中初始化了一个 ThreadLocalMap。 二哥的 Java 进阶之路：ThreadLocalMap ThreadLocalMap 是 ThreadLocal 的一个静态内部类，它内部维护了一个 Entry 数组，key 是 ThreadLocal 对象，value 是线程的局部变量，这样就相当于为每个线程维护了一个变量副本。 三分恶面渣逆袭：ThreadLoca结构图 Entry 继承了 WeakReference，它限定了 key 是一个弱引用，弱引用的好处是当内存不足时，JVM 会回收 ThreadLocal 对象，并且将其对应的 Entry.value 设置为 null，这样可以在很大程度上避免内存泄漏。 static class Entry extends WeakReferenceThreadLocal? /** The value associated with this ThreadLocal. */ Object value; //节点类 Entry(ThreadLocal? k, Object v) //key赋值 super(k); //value赋值 value = v; 总结一下： ThreadLocal 的实现原理是，每个线程维护一个 Map，key 为 ThreadLocal 对象，value 为想要实现线程隔离的对象。 1、通过 ThreadLocal 的 set 方法将对象存入 Map 中。 2、通过 ThreadLocal 的 get 方法从 Map 中取出对象。 3、Map 的大小由 ThreadLocal 对象的多少决定。 ThreadLocal 的结构 什么是弱引用，什么是强引用？我先说一下强引用，比如 User user = new User(沉默王二) 中，user 就是一个强引用，new User(沉默王二) 就是强引用对象。 当 user 被置为 null 时（user = null），new User(沉默王二) 对象就会被垃圾回收；否则即便是内存空间不足，JVM 也不会回收 new User(沉默王二) 这个强引用对象，宁愿抛出 OutOfMemoryError。 弱引用，比如说在使用 ThreadLocal 中，Entry 的 key 就是一个弱引用对象。 ThreadLocalUser userThreadLocal = new ThreadLocal();userThreadLocal.set(new User(沉默王二)); userThreadLocal 是一个强引用，new ThreadLocal() 是一个强引用对象； new User(沉默王二) 是一个强引用对象。 调用 set 方法后，会将 key = new ThreadLocal() 放入 ThreadLocalMap 中，此时的 key 是一个弱引用对象。当 JVM 进行垃圾回收时，如果发现了弱引用对象，就会将其回收。 三分恶面渣逆袭：ThreadLocal内存分配 其关系链就是： ThreadLocal 强引用 - ThreadLocal 对象。 Thread 强引用 - ThreadLocalMap。 ThreadLocalMap[i] 强引用了 - Entry。 Entry.key 弱引用 - ThreadLocal 对象。 Entry.value 强引用 - 线程的局部变量对象。 15.🌟ThreadLocal 内存泄露是怎么回事？ThreadLocalMap 的 Key 是 弱引用，但 Value 是强引用。 如果一个线程一直在运行，并且 value 一直指向某个强引用对象，那么这个对象就不会被回收，从而导致内存泄漏。 二哥的 Java 进阶之路：ThreadLocalMap 内存溢出 那怎么解决内存泄漏问题呢？很简单，使用完 ThreadLocal 后，及时调用 remove() 方法释放内存空间。 try threadLocal.set(value); // 执行业务操作 finally threadLocal.remove(); // 确保能够执行清理 remove() 会调用 ThreadLocalMap 的 remove 方法遍历哈希表，找到 key 等于当前 ThreadLocal 的 Entry，找到后会调用 Entry 的 clear 方法，将 Entry 的 value 设置为 null。 private void remove(ThreadLocal? key) Entry[] tab = table; int len = tab.length; // 计算 key 的 hash 值 int i = key.threadLocalHashCode (len-1); // 遍历数组，找到 key 为 null 的 Entry for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) if (e.get() == key) // 将该 Entry 的 key 置为 null（即 Entry 失效） e.clear(); // 清理过期的 entry expungeStaleEntry(i); return; public void clear() this.referent = null; 然后执行 expungeStaleEntry() 方法，清除 key 为 null 的 Entry。 二哥的Java进阶之路：expungeStaleEntry 那为什么 key 要设计成弱引用？弱引用的好处是，当内存不足的时候，JVM 能够及时回收掉弱引用的对象。 比如说： WeakReference key = new WeakReference(new ThreadLocal()); key 是弱引用，new WeakReference(new ThreadLocal()) 是弱引用对象，当 JVM 进行垃圾回收时，只要发现了弱引用对象，就会将其回收。 一旦 key 被回收，ThreadLocalMap 在进行 set、get 的时候就会对 key 为 null 的 Entry 进行清理。 二哥的 Java 进阶之路：清理 entry 总结一下，在 ThreadLocal 被垃圾收集后，下一次访问 ThreadLocalMap 时，Java 会自动清理那些键为 null 的 entry，这个过程会在执行 get()、set()、remove()时触发。 二哥的 Java 进阶之路：replaceStaleEntry方法 你了解哪些 ThreadLocal 的改进方案？在 JDK 20 Early-Access Build 28 版本中，出现了 ThreadLocal 的改进方案，即 ScopedValue。 还有 Netty 中的 FastThreadLocal，它是 Netty 对 ThreadLocal 的优化，内部维护了一个索引常量 index，每次创建 FastThreadLocal 中都会自动+1，用来取代 hash 冲突带来的损耗，用空间换时间。 private final int index;public FastThreadLocal() index = InternalThreadLocalMap.nextVariableIndex();public static int nextVariableIndex() int index = nextIndex.getAndIncrement(); if (index 0) nextIndex.decrementAndGet(); return index; 以及阿里的 TransmittableThreadLocal，不仅实现了子线程可以继承父线程 ThreadLocal 的功能，并且还可以跨线程池传递值。 TransmittableThreadLocalString context = new TransmittableThreadLocal();// 在父线程中设置context.set(value-set-in-parent);// 在子线程中可以读取，值是value-set-in-parentString value = context.get(); 16.ThreadLocalMap 的源码看过吗？有研究过。 ThreadLocalMap 虽然被叫做 Map，但它并没有实现 Map 接口，是一个简单的线性探测哈希表。 static class ThreadLocalMap static class Entry extends WeakReferenceThreadLocal? Object value; Entry(ThreadLocal? k, Object v) super(k); // 这里的 Key 是 WeakReference value = v; private Entry[] table; // 存储 ThreadLocal 变量的数组 private int size; // 当前 Entry 数量 private int threshold; // 触发扩容的阈值 底层的数据结构也是数组，数组中的每个元素是一个 Entry 对象，Entry 对象继承了 WeakReference，key 是 ThreadLocal 对象，value 是线程的局部变量。 三分恶面渣逆袭：ThreadLocalMap结构示意图 当调用 ThreadLocal.set(value) 时，会将 value 存入 ThreadLocalMap。 public void set(T value) Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); set() 方法是 ThreadLocalMap 的核心方法，通过 key 的哈希码与数组长度取模，计算出 key 在数组中的位置，这一点和 HashMap 的实现类似。 private void set(ThreadLocal? key, Object value) Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode (len - 1); // 计算索引 for (Entry e = tab[i]; e != null; e = tab[nextIndex(i, len)]) ThreadLocal? k = e.get(); if (k == key) // 如果 key 已存在，更新 value e.value = value; return; if (k == null) // Key 为 null，清理无效 Entry replaceStaleEntry(key, value, i); return; tab[i] = new Entry(key, value); // 直接插入 Entry size++; if (size = threshold) rehash(); threadLocalHashCode 的计算有点东西，每创建一个 ThreadLocal 对象，它就会新增一个黄金分割数，可以让哈希码分布的非常均匀。相当于在创建的时候,让哈希码直接跳过一个位置,这样就可以让哈希码分布的非常均匀. private static final int HASH_INCREMENT = 0x61c88647;private static int nextHashCode() return nextHashCode.getAndAdd(HASH_INCREMENT); 黄金分割数的具体使用场景： ThreadLocal 构造函数中： public class ThreadLocalT private final int threadLocalHashCode = nextHashCode(); public ThreadLocal() // threadLocalHashCode 在这里被赋值 每次创建 ThreadLocal 对象时： 第1个 ThreadLocal：hashCode 0 第2个 ThreadLocal：hashCode 0 + 0x61c88647 1640531527 第3个 ThreadLocal：hashCode 1640531527 + 0x61c88647 -1013904242 第4个 ThreadLocal：hashCode -1013904242 + 0x61c88647 628874885 以此类推… 在 ThreadLocalMap 中计算索引： // set() 方法中int i = key.threadLocalHashCode (len - 1);// getEntry() 方法中 int i = key.threadLocalHashCode (table.length - 1);// remove() 方法中int i = key.threadLocalHashCode (len - 1); 当调用 ThreadLocal.get() 时，会调用 ThreadLocalMap 的 getEntry() 方法，根据 key 的哈希码找到对应的线程局部变量。 private Entry getEntry(ThreadLocal? key) int i = key.threadLocalHashCode (table.length - 1); Entry e = table[i]; if (e != null e.get() == key) // 如果 key 存在，直接返回 return e; else return getEntryAfterMiss(key, i, e); // 继续查找 当调用 ThreadLocal.remove() 时，会调用 ThreadLocalMap 的 remove() 方法，根据 key 的哈希码找到对应的线程局部变量，将其清除，防止内存泄漏。 private void remove(ThreadLocal? key) Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode (len - 1); for (Entry e = tab[i]; e != null; e = tab[nextIndex(i, len)]) if (e.get() == key) e.clear(); // 清除 WeakReference e.value = null; // 释放 Value expungeStaleEntries(); return; 17.ThreadLocalMap 怎么解决 Hash 冲突的？开放定址法。 如果计算得到的槽位 i 已经被占用，ThreadLocalMap 会采用开放地址法中的线性探测来寻找下一个空闲槽位： 如果 i 位置被占用，尝试 i+1。 如果 i+1 也被占用，继续探测 i+2，直到找到一个空位。 如果到达数组末尾，则回到数组头部，继续寻找空位。 private static int nextIndex(int i, int len) return ((i + 1 len) ? i + 1 : 0); 为什么要用线性探测法而不是HashMap 的拉链法来解决哈希冲突？ThreadLocalMap 设计的目的是存储线程私有数据，不会有大量的 Key，所以采用线性探测更节省空间。 拉链法还需要单独维护一个链表，甚至红黑树，不适合 ThreadLocal 这种场景。 开放地址法了解吗？简单来说，就是这个坑被人占了，那就接着去找空着的坑。 三分恶面渣逆袭：ThreadLocalMap解决冲突 如果我们插入一个 value27 的数据，通过 hash 计算后应该落入第 4 个槽位，而槽位 4 已经有数据了，而且 key 和当前的不等。 此时就会线性向后查找，一直找到 Entry 为 null 的槽位才会停止。 18.ThreadLocalMap 扩容机制了解吗？了解。 与 HashMap 不同，ThreadLocalMap 并不会直接在元素数量达到阈值时立即扩容，而是先清理被 GC 回收的 key，然后在填充率达到四分之三时进行扩容。 private void rehash() // 清理被 GC 回收的 key expungeStaleEntries(); //扩容 if (size = threshold - threshold / 4) resize(); 清理过程会遍历整个数组，将 key 为 null 的 Entry 清除。 private void expungeStaleEntries() Entry[] tab = table; int len = tab.length; for (int j = 0; j len; j++) Entry e = tab[j]; // 如果 key 为 null，清理 Entry if (e != null e.get() == null) expungeStaleEntry(j); 阈值 threshold 的默认值是数组长度的三分之二。 private void setThreshold(int len) threshold = len * 2 / 3; 扩容时，会将数组长度翻倍，然后重新计算每个 Entry 的位置，采用线性探测法来寻找新的空位，然后将 Entry 放入新的数组中。 private void resize() Entry[] oldTab = table; int oldLen = oldTab.length; // 扩容为原来的两倍 int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; // 遍历老数组 for (int j = 0; j oldLen; ++j) Entry e = oldTab[j]; if (e != null) ThreadLocal? k = e.get(); if (k == null) e.value = null; // 释放 Value，防止内存泄漏 else // 重新计算位置 int h = k.threadLocalHashCode (newLen - 1); while (newTab[h] != null) // 线性探测寻找新位置 h = nextIndex(h, newLen); // 放入新数组 newTab[h] = e; count++; table = newTab; size = count; threshold = newLen * 2 / 3; // 重新计算扩容阈值 一句话总结：ThreadLocalMap 采用的是“先清理再扩容”的策略，扩容时，数组长度翻倍，并重新计算索引，如果发生哈希冲突，采用线性探测法来解决。 三分恶面渣逆袭：ThreadLocalMap扩容 19.父线程能用 ThreadLocal 给子线程传值吗？不能。 二哥的 Java 进阶之路：子线程无法获取父线程的 ThreadLocal 因为 ThreadLocal 变量存储在每个线程的 ThreadLocalMap 中，而子线程不会继承父线程的 ThreadLocalMap。 可以使用 InheritableThreadLocal来解决这个问题。 二哥的 Java 进阶之路：InheritableThreadLocal源码 子线程在创建的时候会拷贝父线程的 InheritableThreadLocal 变量。 二哥的 Java 进阶之路：Thread 源码 来看一下使用示例： class InheritableThreadLocalExample private static final InheritableThreadLocalString inheritableThreadLocal = new InheritableThreadLocal(); public static void main(String[] args) inheritableThreadLocal.set(父线程的值); new Thread(() - System.out.println(子线程获取的值： + inheritableThreadLocal.get()); // 继承了父线程的值 ).start(); InheritableThreadLocal的原理了解吗？了解。 在 Thread 类的定义中，每个线程都有两个 ThreadLocalMap： public class Thread /* 普通 ThreadLocal 变量存储的地方 */ ThreadLocal.ThreadLocalMap threadLocals = null; /* InheritableThreadLocal 变量存储的地方 */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; 普通 ThreadLocal 变量存储在 threadLocals 中，不会被子线程继承。 InheritableThreadLocal 变量存储在 inheritableThreadLocals 中，当 new Thread() 创建一个子线程时，Thread 的 init() 方法会检查父线程是否有 inheritableThreadLocals，如果有，就会拷贝 InheritableThreadLocal 变量到子线程： private void init(ThreadGroup g, Runnable target, String name, long stackSize) // 获取当前父线程 Thread parent = currentThread(); // 复制 InheritableThreadLocal 变量 if (parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); Java 内存模型20.🌟说一下你对 Java 内存模型的理解？推荐阅读：说说 Java 的内存模型 Java 内存模型是 Java 虚拟机规范中定义的一个抽象模型，用来描述多线程环境中共享变量的内存可见性。 深入浅出 Java 多线程：Java内存模型 共享变量存储在主内存中，每个线程都有一个私有的本地内存，存储了共享变量的副本。 当一个线程更改了本地内存中共享变量的副本，它需要 JVM 刷新到主内存中，以确保其他线程可以看到这些更改。 当一个线程需要读取共享变量时，它一版会从本地内存中读取。如果本地内存中的副本是过时的，JVM 会将主内存中的共享变量最新值刷新到本地内存中。 三分恶面渣逆袭：实际线程工作模型 为什么线程要用自己的内存？线程从主内存拷贝变量到工作内存，可以减少 CPU 访问 RAM 的开销。 每个线程都有自己的变量副本，可以避免多个线程同时修改共享变量导致的数据冲突。 21. i++是原子操作吗？不是，它包括三个步骤： 从内存中读取 i 的值。 对 i 进行加 1 操作。 将新的值写回内存。 说说你对原子性、可见性、有序性的理解？原子性要求一个操作是不可分割的，要么全部执行成功，要么完全不执行。 举个例子：就比如说 count++ 就不是一个原子操作，它包括读取 count 的值、加 1、写回 count 三个步骤，所以需要加锁或者使用AtomicInteger代替 int 来保证原子性。 可见性要求一个线程对共享变量的修改，能够被其他线程及时看见。 我通过下面的代码解释一下： private static boolean flag = true;public static void main(String[] args) new Thread(() - while (flag) // 线程 A 可能一直看不到 flag=false System.out.println(线程 A 退出); ).start(); try Thread.sleep(1000); catch (InterruptedException e) flag = false; // 线程 B 修改 flag 线程 A 会在本地内存中缓存 flag=true，虽然线程 B 修改了 flag=false，但不会立即同步到主内存以及线程 A 的本地内存，因此线程 A 会一直处于死循环。 解决办法就是通过 volatile 关键字来保证可见性。 有序性是指程序执行的顺序是否按照代码编写的顺序执行。 在单线程环境下，代码能够准确无误地按照编写顺序执行。但在多线程环境下，CPU 和编译器可能会进行指令重排，代码的执行顺序因此会发生变化。 我通过下面的代码解释一下： int a = 0, b = 0;boolean flag = false;void thread1() a = 1; flag = true; // 可能会被 CPU 优化，先执行void thread2() if (flag) System.out.println(a); // 可能打印 0，而不是 1 由于指令重排，flag = true 可能会在 a = 1 之前执行，导致 thread2() 读取 flag=true 后，a 仍然是 0，出现不符合代码逻辑的情况。 简要回答： 原子性保证操作不可中断，可见性保证变量修改后线程能看到最新值，有序性保证代码执行顺序一致，可以通过 volatile、synchronized 和 CAS 机制来保证这些特性。 下面的代码是原子操作吗？int i = 2;int j = i;i++;i = i + 1; 第 1 行代码是基本类型赋值，是原子性操作。 第 2 行先读 i 的值，再赋值给 j，不是原子操作。 第 3 和第 4 行都不是原子操作，都需要先读取 i 的值，再+1，然后再赋值给 i。 22.说说什么是指令重排？指令重排是指 CPU 或编译器为了提高程序的执行效率，改变代码执行顺序的一种优化技术。 从 Java 源代码到最终执行的指令序列，会经历 3 种重排序：编译器重排序、指令并行重排序、内存系统重排序。 三分恶面渣逆袭：多级指令重排 指令重排可能会导致双重检查锁失效，比如下面的单例模式代码： public class Singleton private static Singleton instance; public static Singleton getInstance() if (instance == null) // 第一次检查 synchronized (Singleton.class) if (instance == null) // 第二次检查 instance = new Singleton(); // 可能发生指令重排 return instance; 如果线程 A 执行了 instance = new Singleton();，但构造方法还没执行完，线程 B 可能会读取到一个未初始化的对象，导致出现空指针异常。 三分恶面渣逆袭：双重校验单例模式异常情形 正确的方式是给 instance 变量加上 volatile 关键字，禁止指令重排。 class Singleton private static volatile Singleton instance; public static Singleton getInstance() if (instance == null) synchronized (Singleton.class) if (instance == null) instance = new Singleton(); // 由于 volatile，禁止指令重排 return instance; 23.happens-before 了解吗？Happens-Before 是 Java 内存模型定义的一种保证线程间可见性和有序性的规则。 如果操作 A Happens-Before 操作 B，那么： 操作 A 的结果对操作 B 可见。 操作 A 在时间上先于操作 B 执行。 换句话说，如果 A Happens-Before B，那么 A 的修改必须对 B 可见，并且 B 不能重排序到 A 之前。 你知道哪些 Happens-Before 规则？三分恶面渣逆袭：happens-before六大规则 JMM 规定了 6 种 Happens-Before 规则，满足这些规则的操作不会被重排序，并且保证了数据的可见性。 ①、程序顺序规则：单线程内，代码按顺序执行；比如 a = 1; b = 2;，a 先于 b 执行。 ②、监视器锁定规则：unlock() Happens-Before lock()；比如 synchronized 释放锁后，获取锁的线程能够看到最新的数据。 ③、volatile 变量规则：写 volatile 变量 Happens-Before 读 volatile。 ④、传递性规则：A Happens-Before B 且 B Happens-Before C，则 A Happens-Before C。例如 a 1 先于 b 2，b 2 先于 c 3，则 a 1 先于 c 3。 ⑤、线程启动规则：线程 A 执行操作 ThreadB.start()，那么 A 线程的 ThreadB.start() 操作 happens-before 于线程 B 中的任意操作。 ⑥、线程终止规则：线程的所有操作 Happens-Before Thread.join()；例如 t.join(); 之后，主线程一定能看到 t 的修改。 24.as-if-serial 了解吗？As-If-Serial 规则允许 CPU 和编译器优化代码顺序，但不会改变单线程的执行结果。它只适用于单线程，多线程环境仍然可能发生指令重排，需要 volatile 和 synchronized 等机制来保证有序性。 来解释说明一下。 double pi = 3.14; // Adouble r = 1.0; // Bdouble area = pi * r * r; // C C 依赖于 A，同时 C 也依赖着 B。 二哥的 Java 进阶之路：as-if-serial 因此在最终执行的指令序列中，C 不能被重排序到 A 或者 B 的前面，否则就会出现错误。 但 A 和 B 之间没有依赖关系，因此编译器和处理器可以重排序 A 和 B 之间的执行顺序。 所以程序可能会有两种执行顺序： 三分恶面渣逆袭：两种执行结果 Happens-Before 规则保证了多线程环境下的有序性，防止指令重排导致的并发问题。As-If-Serial 规则保证了单线程代码不会因优化而执行错误。 25.🌟volatile 了解吗？推荐阅读：volatile 关键字解析 了解。 第一，保证可见性，线程修改 volatile 变量后，其他线程能够立即看到最新值；第二，防止指令重排，volatile 变量的写入不会被重排序到它之前的代码。 volatile 怎么保证可见性的？当线程对 volatile 变量进行写操作时，JVM 会在这个变量写入之后插入一个写屏障指令，这个指令会强制将本地内存中的变量值刷新到主内存中。 三分恶面渣逆袭：volatile写插入内存屏障后生成的指令序列示意图 StoreStore; // 保证写入之前的操作不会重排volatile_write(); // 写入 volatile 变量StoreLoad; // 保证写入后，其他线程立即可见 在 x86 架构下，通常会使用 lock 指令来实现写屏障，例如： mov [a], 2 ; 将值 2 写入内存地址 alock add [a], 0 ; lock 指令充当写屏障，确保内存可见性 当线程对 volatile 变量进行读操作时，JVM 会插入一个读屏障指令，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值。 三分恶面渣逆袭：volatile写插入内存屏障后生成的指令序列示意图 我们来声明一个 volatile 变量 x： volatile int x = 0 线程 A 对 x 写入后会将其最新的值刷新到主内存中，线程 B 读取 x 时由于本地内存中的 x 失效了，就会从主内存中读取最新的值。 三分恶面渣逆袭：volatile内存可见性 volatile 怎么保证有序性的？JVM 会在 volatile 变量的读写前后插入 “内存屏障”，以约束 CPU 和编译器的优化行为： StoreStore 屏障可以禁止volatile 写操作与普通写操作的重排 StoreLoad 屏障会禁止volatile 写与volatile 读重排 LoadLoad 屏障会禁止volatile 读与后续普通读操作重排 LoadStore 屏障会禁止volatile 读与后续普通写操作重排 volatile 和 synchronized 的区别？volatile 关键字用于修饰变量，确保该变量的更新操作对所有线程是可见的，即一旦某个线程修改了 volatile 变量，其他线程会立即看到最新的值。 synchronized 关键字用于修饰方法或代码块，确保同一时刻只有一个线程能够执行该方法或代码块，从而实现互斥访问。 volatile 加在基本类型和对象上的区别？当 volatile 用于基本数据类型时，能确保该变量的读写操作是直接从主内存中读取或写入的。 private volatile int count = 0; 当 volatile 用于引用类型时，能确保引用本身的可见性，即确保引用指向的对象地址是最新的。 但是，volatile 并不能保证引用对象内部状态的线程安全。 private volatile SomeObject obj = new SomeObject(); 虽然 volatile 确保了 obj 引用的可见性，但对 obj 引用的 new SomeObject() 对象并不受 volatile 保护。 如果需要保证引用对象内部状态的线程安全，需要使用 synchronized 或 ReentrantLock 等锁机制。 锁26.synchronized 用过吗？用过，频率还很高。 synchronized 在 JDK 1.6 之后，进行了锁优化，增加了偏向锁、轻量级锁，大大提升了 synchronized 的性能。 synchronized 上锁的对象是什么？synchronized 用在普通方法上时，上锁的是执行这个方法的对象。 public synchronized void increment() this.count++; synchronized 用在静态方法上时，上锁的是这个类的 Class 对象。 public static synchronized void increment() count++; synchronized 用在代码块上时，上锁的是括号中指定的对象，比如说当前对象 this。 public void increment() synchronized (this) this.count++; 27.synchronized 的实现原理了解吗？synchronized 依赖 JVM 内部的 Monitor 对象来实现线程同步。使用的时候不用手动去 lock 和 unlock，JVM 会自动加锁和解锁。 synchronized 加锁代码块时，JVM 会通过 monitorenter、monitorexit 两个指令来实现同步： 前者表示线程正在尝试获取 lock 对象的 Monitor； 后者表示线程执行完了同步代码块，正在释放锁。 使用 javap -c -s -v -l SynchronizedDemo.class 反编译 synchronized 代码块时，就能看到这两个指令。 三分恶面渣逆袭：monitorenter和monitorexit synchronized 修饰普通方法时，JVM 会通过 ACC_SYNCHRONIZED 标记符来实现同步。 三分恶面渣逆袭：synchronized修饰同步方法 你对 Monitor 了解多少？Monitor 是 JVM 内置的同步机制，每个对象在内存中都有一个对象头——Mark Word，用于存储锁的状态，以及 Monitor 对象的指针。 博客园Zebt：Java 对象头 synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁。 在 Hotspot 虚拟机中，Monitor 由 ObjectMonitor 实现： ObjectMonitor() _count = 0; // 记录线程获取锁的次数 _owner = NULL; // 指向持有ObjectMonitor对象的线程 _WaitSet = NULL; // 处于wait状态的线程，会被加入到_WaitSet _cxq = NULL ; _EntryList = NULL ; // 处于等待锁block状态的线程，会被加入到该列表 _owner：当前持有 ObjectMonitor 的线程，初始值为 null，表示没有线程持有锁。线程成功获取锁后，该值更新为线程 ID，释放锁后重置为 null。 _count：记录当前线程获取锁的次数（可重入锁），每次成功加锁 _count + 1，释放锁 _count - 1。 _WaitSet：等待队列，调用 wait() 方法后，线程会释放锁，并加入 _WaitSet，进入 WAITING 状态，等待 notify() 唤醒。 _cxq：阻塞队列，用于存放刚进入 Monitor 的线程（还未进入 _EntryList）。 _EntryList：竞争队列，所有等待获取锁的线程（BLOCKED 状态）会进入 _EntryList，等待锁释放后竞争执行权。 结构示意图： +----------------------+| ObjectMonitor || ---------------- || _owner = Thread-1 | // 当前持有锁的线程| _count = 1 | // 线程获取锁的次数| _WaitSet - T3,T4 | // 执行 wait() 的线程| _EntryList - T2,T5| // 竞争锁的线程| _cxq - T6,T7 | // 新进入的线程+----------------------+ 会不会牵扯到 os 层面呢？会，synchronized 升级为重量级锁时，依赖于操作系统的互斥量——mutex 来实现，mutex 用于保证任何给定时间内，只有一个线程可以执行某一段特定的代码段。 28.synchronized 怎么保证可见性？通过两步操作： 加锁时，线程必须从主内存读取最新数据。 释放锁时，线程必须将修改的数据刷回主内存，这样其他线程获取锁后，就能看到最新的数据。 线程 A 线程 B ┌────────────────────┐ │ synchronized(lock) │ │ x = 1; │ // 1. 线程 A 修改变量 x └────────────────────┘ ↓ 释放锁 （JVM 强制刷新 x 到主内存） （线程 B 获取锁） ┌────────────────────┐ │ synchronized(lock) │ │ print(x); │ // 2. 线程 B 读取最新 x=1 └────────────────────┘ synchronized 怎么保证有序性？synchronized 通过 JVM 指令 monitorenter 和 monitorexit，来确保加锁代码块内的指令不会被重排。 来解释一下，比如说对于： synchronized (lock) x = 1; flag = true; javap 反编译后的伪代码： monitorenter // 获取锁store x, 1 // 变量 x = 1store flag, true // 变量 flag = truemonitorexit // 释放锁 实际 javap 反编译后的结果： 二哥的 Java 进阶之路：javap 反编译后的synchronized 指令解释一下： 指令 作用 monitorenter 获取锁，进入同步代码块 iconst_1 将整数 1 压入操作数栈 istore_1 存储 1 到局部变量 x iconst_1 再次将整数 1 压入操作数栈 istore_2 存储 1 到局部变量 flag aload 4 加载 lock 对象引用 monitorexit 释放锁，退出同步代码块 synchronized 怎么实现可重入的呢？可重入意味着同一个线程可以多次获得同一个锁，而不会被阻塞。 美团技术博客：可重入锁 synchronized 之所以支持可重入，是因为 Java 的对象头包含了一个 Mark Word，用于存储对象的状态，包括锁信息。 当一个线程获取对象锁时，JVM 会将该线程的 ID 写入 Mark Word，并将锁计数器设为 1。 如果一个线程尝试再次获取已经持有的锁，JVM 会检查 Mark Word 中的线程 ID。如果 ID 匹配，表示的是同一个线程，锁计数器递增。 当线程退出同步块时，锁计数器递减。如果计数器值为零，JVM 将锁标记为未持有状态，并清除线程 ID 信息。 来解释一下： class ReentrantExample public synchronized void method1() System.out.println(Method1 acquired lock); method2(); // 线程已经持有锁，能继续调用 method2 public synchronized void method2() System.out.println(Method2 acquired lock); public static void main(String[] args) ReentrantExample example = new ReentrantExample(); example.method1(); 执行结果： Method1 acquired lockMethod2 acquired lock 因为 synchronized 支持可重入，所以 method1 获取锁后，method2 仍然可以获取锁。 底层是通过 Monitor 对象的 owner 和 count 字段实现的，owner 记录持有锁的线程，count 记录线程获取锁的次数。 +----------------------+| ObjectMonitor || ---------------- || _owner = Thread-1 | // 当前持有锁的线程| _count = 2 | // 线程重入了 2 次+----------------------+ 29.🌟synchronized 锁升级了解吗？推荐阅读：偏向锁、轻量级锁、重量级锁到底是什么？ JDK 1.6 的时候，为了提升 synchronized 的性能，引入了锁升级机制，从低开销的锁逐步升级到高开销的锁，以最大程度减少锁的竞争。 三分恶面渣逆袭：Mark Word变化 没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作；轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞；只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞。 了解 synchronized 四种锁状态吗？了解。 ①、无锁状态，对象未被锁定，Mark Word 存储对象的哈希码等信息。 ②、偏向锁，当线程第一次获取锁时，会进入偏向模式。Mark Word 会记录线程 ID，后续同一线程再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁。 博客园boluo1230：偏向锁 ③、轻量级锁，当多个线程在不同时段获取同一把锁，即不存在锁竞争的情况时，JVM 会采用轻量级锁来避免线程阻塞。 未持有锁的线程通过CAS 自旋等待锁释放。 TodoCoder：自旋和阻塞的区别 当线程进入 synchronized 加锁的代码时，如果对象的锁状态为偏向锁，也就是锁类型为“01”，偏向锁标记为“0”的状态。 博客园wadeluffy：Mark Word 然后采用 CAS 自旋的方式，尝试将对象头中的 Mark Word 替换为指向 Lock Record 的指针，并将 Lock Record 中的 owner 指针指向对象的 Mark Word。 博客园boluo1230：轻量级锁 如果这个替换动作成功了，线程就拥有了该对象的锁，对象头 Mark Word 的锁标志位会更新为“00”，表示对象处于轻量级锁状态。 ④、重量级锁，如果自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程进入 synchronized 加锁的代码时，轻量级锁就会升级为重量级锁。 此时，对象头的锁类型会更新为“10”，Mark Word 会存储指向 Monitor 对象的指针，其他等待锁的线程都会进入阻塞状态。 synchronized 做了哪些优化？在 JDK 1.6 之前，synchronized 是直接调用 ObjectMonitor 的 enter 和 exit 指令实现的，这种锁也被称为重量级锁，性能较差。 随着 JDK 版本的更新，synchronized 的性能得到了极大的优化： ①、偏向锁：同一个线程可以多次获取同一把锁，无需重复加锁。 ②、轻量级锁：当没有线程竞争时，通过 CAS 自旋等待锁，避免直接进入阻塞。 ③、锁消除：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销。 请详细说说锁升级的过程？懵逼状态下的回答：锁升级会从无锁升级为偏向锁，再升级为轻量级锁，最后升级为重量级锁。 三分恶面渣逆袭：锁升级简略过程 知道一点，但不深入的回答： 三分恶面渣逆袭：synchronized 锁升级过程 ①、偏向锁：当一个线程第一次获取锁时，JVM 会在对象头的 Mark Word 记录这个线程 ID，下次进入 synchronized 时，如果还是同一个线程，可以直接执行，无需额外加锁。 ②、轻量级锁：当多个线程尝试获取锁但不是同一个时段，偏向锁会升级为轻量级锁，等待锁的线程通过 CAS 自旋避免进入阻塞状态。 ③、重量级锁：如果自旋失败，锁会升级为重量级锁，等待锁的线程会进入阻塞状态，等待监视器 Monitor 进行调度。 详细解释一下： ①、从无锁到偏向锁： 当一个线程首次访问同步代码时，如果此对象处于无锁状态且偏向锁未被禁用，JVM 会将该对象头的锁标记改为偏向锁状态，并记录当前线程 ID。此时，对象头中的 Mark Word 中存储了持有偏向锁的线程 ID。 如果另一个线程尝试获取这个已被偏向的锁，JVM 会检查当前持有偏向锁的线程是否活跃。如果持有偏向锁的线程不活跃，可以将锁偏向给新的线程；否则撤销偏向锁，升级为轻量级锁。 ②、偏向锁的轻量级锁： 进行偏向锁撤销时，会遍历堆栈的所有锁记录，暂停拥有偏向锁的线程，并检查锁对象。如果这个过程中发现有其他线程试图获取这个锁，JVM 会撤销偏向锁，并将锁升级为轻量级锁。 当有两个或以上线程竞争同一个偏向锁时，偏向锁模式不再有效，此时偏向锁会被撤销，对象的锁状态会升级为轻量级锁。 ③、轻量级锁到重量级锁： 轻量级锁通过自旋来等待锁释放。如果自旋超过预定次数（自旋次数是可调的，并且是自适应的，失败次数多自旋次数就少），表明锁竞争激烈。 当自旋多次失败，或者有线程在等待队列中等待相同的轻量级锁时，轻量级锁会升级为重量级锁。在这种情况下，JVM 会在操作系统层面创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被阻塞，直到锁被释放。 30.🌟synchronized 和 ReentrantLock 的区别了解吗？两句话回答：synchronized 由 JVM 内部的 Monitor 机制实现，ReentrantLock基于 AQS 实现。 synchronized 可以自动加锁和解锁，ReentrantLock 需要手动 lock() 和 unlock()。 三分恶面渣逆袭：synchronized和ReentrantLock的区别 如果面试官还想知道更多，可以继续回答： ①、ReentrantLock 可以实现多路选择通知，绑定多个 Condition，而 synchronized 只能通过 wait 和 notify 唤醒，属于单路通知； ReentrantLock lock = new ReentrantLock();Condition condition = lock.newCondition(); ②、synchronized 可以在方法和代码块上加锁，ReentrantLock 只能在代码块上加锁，但可以指定是公平锁还是非公平锁。 // synchronized 修饰方法public synchronized void method() // 业务代码// synchronized 修饰代码块synchronized (this) // 业务代码// ReentrantLock 加锁ReentrantLock lock = new ReentrantLock();lock.lock();try // 业务代码 finally lock.unlock(); ③、ReentrantLock 提供了一种能够中断等待锁的线程机制，通过 lock.lockInterruptibly() 来实现。 ReentrantLock lock = new ReentrantLock();try lock.lockInterruptibly(); catch (InterruptedException e) // 处理中断异常 并发量大的情况下，使用 synchronized 还是 ReentrantLock？我更倾向于 ReentrantLock，因为： ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景。 ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争。 ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制。 Lock 了解吗？Lock 是 JUC 中的一个接口，最常用的实现类包括可重入锁 ReentrantLock、读写锁 ReentrantReadWriteLock 等。 ReentrantLock 的 lock() 方法实现逻辑了解吗？lock 方法的具体实现由 ReentrantLock 内部的 Sync 类来实现，涉及到线程的自旋、阻塞队列、CAS、AQS 等。 二哥的Java 进阶之路：Lock.lock() 方法源码 lock 方法会首先尝试通过 CAS 来获取锁。如果当前锁没有被持有，会将锁状态设置为 1，表示锁已被占用。否则，会将当前线程加入到 AQS 的等待队列中。 final void lock() if (compareAndSetState(0, 1)) // 尝试直接获取锁 setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); // 如果获取失败，进入AQS队列等待 31.AQS 了解多少？推荐阅读：到底什么是 AQS? AQS 是一个抽象类，它维护了一个共享变量 state 和一个线程等待队列，为 ReentrantLock 等类提供底层支持。 三分恶面渣逆袭：AQS抽象队列同步器 AQS 的思想是，如果被请求的共享资源处于空闲状态，则当前线程成功获取锁；否则，将当前线程加入到等待队列中，当其他线程释放锁时，从等待队列中挑选一个线程，把锁分配给它。 AQS 的源码阅读过吗？有研究过。 第一，状态 state 由 volatile 变量修饰，用于保证多线程之间的可见性； private volatile int state; ②、同步队列由内部定义的 Node 类实现，每个 Node 包含了等待状态、前后节点、线程的引用等，是一个先进先出的双向链表。 static final class Node static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile Node prev; volatile Node next; volatile Thread thread; AQS 支持两种同步方式： 独占模式下：每次只能有一个线程持有锁，例如 ReentrantLock。 共享模式下：多个线程可以同时获取锁，例如 Semaphore 和 CountDownLatch。 核心方法包括： acquire：获取锁，失败进入等待队列； release：释放锁，唤醒等待队列中的线程； acquireShared：共享模式获取锁； releaseShared：共享模式释放锁。 AQS 使用一个 CLH 队列来维护等待线程，CLH 是三个作者 Craig、Landin 和 Hagersten 的首字母缩写，是一种基于链表的自旋锁。 三分恶面渣逆袭：CLH队列 在 CLH 中，当一个线程尝试获取锁失败后，会被添加到队列的尾部并自旋，等待前一个节点的线程释放锁。 三分恶面渣逆袭：AQS变种CLH队列 CLH 的优点是，假设有 100 个线程在等待锁，锁释放之后，只会通知队列中的第一个线程去竞争锁。避免同时唤醒大量线程，浪费 CPU 资源。 32.🌟说说 ReentrantLock 的实现原理？ReentrantLock 是基于 AQS 实现的 可重入排他锁，使用 CAS 尝试获取锁，失败的话，会进入 CLH 阻塞队列，支持公平锁、非公平锁，可以中断、超时等待。 三分恶面渣逆袭：ReentrantLock 非公平锁加锁流程简图 内部通过一个计数器 state 来跟踪锁的状态和持有次数。当线程调用 lock() 方法获取锁时，ReentrantLock 会检查 state 的值，如果为 0，通过 CAS 修改为 1，表示成功加锁。否则根据当前线程的公平性策略，加入到等待队列中。 线程首次获取锁时，state 值设为 1；如果同一个线程再次获取锁时，state 加 1；每释放一次锁，state 减 1。 当线程调用 unlock() 方法时，ReentrantLock 会将持有锁的 state 减 1，如果 state = 0，则释放锁，并唤醒等待队列中的线程来竞争锁。 使用方式非常简单： class CounterWithLock private int count = 0; private final Lock lock = new ReentrantLock(); public void increment() lock.lock(); // 获取锁 try count++; finally lock.unlock(); // 释放锁 public int getCount() return count; new ReentrantLock() 默认创建的是非公平锁 NonfairSync。在非公平锁模式下，锁可能会授予刚刚请求它的线程，而不考虑等待时间。当切换到公平锁模式下，锁会授予等待时间最长的线程。 33.ReentrantLock 怎么创建公平锁？很简单，创建 ReentrantLock 的时候，传递参数 true 就可以了。 ReentrantLock lock = new ReentrantLock(true);// true 代表公平锁，false 代表非公平锁public ReentrantLock(boolean fair) sync = fair ? new FairSync() : new NonfairSync(); 怎么创建一个非公平锁呢？创建 ReentrantLock 时，不传递参数或者传递参数就好了。 非公平锁和公平锁有什么不同？两句话回答： 公平锁意味着在多个线程竞争锁时，获取锁的顺序与线程请求锁的顺序相同，即先来先服务。 非公平锁不保证线程获取锁的顺序，当锁被释放时，任何请求锁的线程都有机会获取锁，而不是按照请求的顺序。 公平锁的实现逻辑了解吗？公平锁的核心逻辑在 AQS 的 hasQueuedPredecessors() 方法中，该方法用于判断当前线程前面是否有等待的线程。 二哥的 Java 进阶之路：公平锁的源码 如果队列前面有等待线程，当前线程就不能抢占锁，必须按照队列顺序排队。如果队列前面没有线程，或者当前线程是队列头部的线程，就可以获取锁。 34.🌟CAS 了解多少？推荐阅读：一文彻底搞清楚 Java 实现 CAS 的原理 CAS 是一种乐观锁，用于比较一个变量的当前值是否等于预期值，如果相等，则更新值，否则重试。 CAS 原子性：博客园的紫薇哥哥 在 CAS 中，有三个值： V：要更新的变量(var) E：预期值(expected) N：新值(new) 先判断 V 是否等于 E，如果等于，将 V 的值设置为 N；如果不等，说明已经有其它线程更新了 V，当前线程就放弃更新。 这个比较和替换的操作需要是原子的，不可中断的。Java 中的 CAS 是由 Unsafe 类实现的。 AtomicInteger 类的 compareAndSet 就是一个 CAS 方法： AtomicInteger atomicInteger = new AtomicInteger(0);int expect = 0;int update = 1;atomicInteger.compareAndSet(expect, update); 它调用的是 Unsafe 的 compareAndSwapInt。 二哥的 Java 进阶之路：compareAndSwapInt 怎么保证 CAS 的原子性？CPU 会发出一个 LOCK 指令进行总线锁定，阻止其他处理器对内存地址进行操作，直到当前指令执行完成。 lock cmpxchg [esi], eax ; 比较 esi 地址中的值与 eax，如果相等则替换 总线锁定：博客园的紫薇哥哥 35.🌟CAS 有什么问题？CAS 存在三个经典问题，ABA 问题、自旋开销大、只能操作一个变量等。 三分恶面渣逆袭：CAS三大问题 什么是 ABA 问题？ABA 问题指的是，一个值原来是 A，后来被改为 B，再后来又被改回 A，这时 CAS 会误认为这个值没有发生变化。 线程 1：CAS(A → B)，修改变量 A → B线程 2：CAS(B → A)，变量又变回 A线程 3：CAS(A → C)，CAS 成功，但实际数据已被修改过！ 可以使用版本号时间戳的方式来解决 ABA 问题。 比如说，每次变量更新时，不仅更新变量的值，还更新一个版本号。CAS 操作时，不仅比较变量的值，还比较版本号。 class OptimisticLockExample private int version; private int value; public synchronized boolean updateValue(int newValue, int currentVersion) if (this.version == currentVersion) this.value = newValue; this.version++; return true; return false; Java 的 AtomicStampedReference 就增加了版本号，它会同时检查引用值和 stamp 是否都相等。 二哥的 Java 进阶之路：AtomicStampedReference 使用示例： class ABAFix private static AtomicStampedReferenceString ref = new AtomicStampedReference(100, 1); public static void main(String[] args) new Thread(() - int stamp = ref.getStamp(); ref.compareAndSet(100, 200, stamp, stamp + 1); ref.compareAndSet(200, 100, ref.getStamp(), ref.getStamp() + 1); ).start(); new Thread(() - try Thread.sleep(100); catch (InterruptedException e) int stamp = ref.getStamp(); System.out.println(CAS 结果： + ref.compareAndSet(100, 300, stamp, stamp + 1)); ).start(); 自旋开销大怎么解决？CAS 失败时会不断自旋重试，如果一直不成功，会给 CPU 带来非常大的执行开销。 可以加一个自旋次数的限制，超过一定次数，就切换到 synchronized 挂起线程。 int MAX_RETRIES = 10;int retries = 0;while (!atomicInt.compareAndSet(expect, update)) retries++; if (retries MAX_RETRIES) synchronized (this) // 超过次数，使用 synchronized 处理 if (atomicInt.get() == expect) atomicInt.set(update); break; 涉及到多个变量同时更新怎么办？可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新。 class Account static class Balance final int money; final int points; Balance(int money, int points) this.money = money; this.points = points; private AtomicReferenceBalance balance = new AtomicReference(new Balance(100, 10)); public void update(int newMoney, int newPoints) Balance oldBalance, newBalance; do oldBalance = balance.get(); newBalance = new Balance(newMoney, newPoints); while (!balance.compareAndSet(oldBalance, newBalance)); 36.Java 有哪些保证原子性的方法？三分恶面渣逆袭：Java保证原子性方法 比如说以 Atomic 开头的原子类，synchronized 关键字，ReentrantLock 锁等。 37.原子操作类了解多少？原子操作类是基于 CAS + volatile 实现的，底层依赖于 Unsafe 类，最常用的有 AtomicInteger、AtomicLong、AtomicReference 等。 三分恶面渣逆袭：原子操作类 像 AtomicIntegerArray 这种以 Array 结尾的，还可以原子更新数组里的元素。锁的粒度是数组里的元素。 class AtomicArrayExample public static void main(String[] args) AtomicIntegerArray atomicArray = new AtomicIntegerArray(new int[]1, 2, 3); atomicArray.incrementAndGet(1); // 对索引 1 进行自增 System.out.println(atomicArray.get(1)); // 输出 3 像 AtomicStampedReference 还可以通过版本号的方式解决 CAS 中的 ABA 问题。 class AtomicStampedReferenceExample public static void main(String[] args) AtomicStampedReferenceInteger ref = new AtomicStampedReference(100, 1); int stamp = ref.getStamp(); // 获取版本号 ref.compareAndSet(100, 200, stamp, stamp + 1); // A → B ref.compareAndSet(200, 100, ref.getStamp(), ref.getStamp() + 1); // B → A 38.AtomicInteger 的源码读过吗？有读过。 AtomicInteger 是基于 volatile 和 CAS 实现的，底层依赖于 Unsafe 类。核心方法包括 getAndIncrement、compareAndSet 等。 public final int getAndIncrement() return unsafe.getAndAddInt(this, valueOffset, 1); 39.线程死锁了解吗？死锁发生在多个线程相互等待对方释放锁时。比如说线程 1 持有锁 R1，等待锁 R2；线程 2 持有锁 R2，等待锁 R1。 The Java Trail：死锁 死锁发生的四个条件了解吗？三分恶面渣逆袭：死锁产生必备四条件 第一条件是互斥：资源不能被多个线程共享，一次只能由一个线程使用。如果一个线程已经占用了一个资源，其他请求该资源的线程必须等待，直到资源被释放。 第二个条件是持有并等待：一个线程已经持有一个资源，并且在等待获取其他线程持有的资源。 第三个条件是不可抢占：资源不能被强制从线程中夺走，必须等线程自己释放。 第四个条件是循环等待：存在一种线程等待链，线程 A 等待线程 B 持有的资源，线程 B 等待线程 C 持有的资源，直到线程 N 又等待线程 A 持有的资源。 该如何避免死锁呢？第一，所有线程都按照固定的顺序来申请资源。例如，先申请 R1 再申请 R2。 第二，如果线程发现无法获取某个资源，可以先释放已经持有的资源，重新尝试申请。 40.🌟死锁问题怎么排查呢？首先从系统级别上排查，比如说在 Linux 生产环境中，可以先使用 top ps 等命令查看进程状态，看看是否有进程占用了过多的资源。 接着，使用 JDK 自带的一些性能监控工具进行排查，比如说 使用 jps -l 查看当前进程，然后使用 jstack 进程号 查看当前进程的线程堆栈信息，看看是否有线程在等待锁资源。 也可以使用一些可视化的性能监控工具，比如说 JConsole、VisualVM 等，查看线程的运行状态、锁的竞争情况等。 三分恶面渣逆袭：线程死锁检测 我们来通过实际代码说明一下： class DeadLockDemo private static final Object lock1 = new Object(); private static final Object lock2 = new Object(); public static void main(String[] args) new Thread(() - synchronized (lock1) System.out.println(线程1获取到了锁1); try Thread.sleep(1000); catch (InterruptedException e) e.printStackTrace(); synchronized (lock2) System.out.println(线程1获取到了锁2); ).start(); new Thread(() - synchronized (lock2) System.out.println(线程2获取到了锁2); try Thread.sleep(1000); catch (InterruptedException e) e.printStackTrace(); synchronized (lock1) System.out.println(线程2获取到了锁1); ).start(); 创建两个线程，每个线程都试图按照不同的顺序获取两个锁（lock1 和 lock2）。 锁的获取顺序不一致很容易导致死锁。运行这段代码，会发现两个线程都无法继续执行，进入了死锁状态。 二哥的 Java 进阶之路：死锁发生了 运行 jstack pid 命令，可以看到死锁的线程信息。 jstack pid 查看死锁信息 编码时，尽量使用 tryLock() 代替 lock()，tryLock() 可以设置超时时间，避免线程一直等待。 同时，尽量避免一个线程同时获取多个锁，如果需要多个锁，可以按照固定的顺序获取。 推荐阅读： JVM 性能监控工具之命令行篇 JVM 性能监控工具之可视化篇 阿里开源的 Java 诊断神器 Arthas 41.聊聊线程同步和互斥？（补充）同步，意味着线程之间要密切合作，按照一定的顺序来执行任务。比如说，线程 A 先执行，线程 B 再执行。 互斥，意味着线程之间要抢占资源，同一时间只能有一个线程访问共享资源。比如说，线程 A 在访问共享资源时，线程 B 不能访问。 同步关注的是线程之间的协作，互斥关注的是线程之间的竞争。 如何实现同步和互斥？可以使用 synchronized 关键字或者 Lock 接口的实现类，如 ReentrantLock 来给资源加锁。 锁在操作系统层面的意思是 Mutex，某个线程进入临界区后，也就是获取到锁后，其他线程不能再进入临界区，要阻塞等待持有锁的线程离开临界区。 cxuan：使用临界区的互斥 锁要解决哪些问题？第一，谁可以拿到锁，可以是类对象，可以是当前的 this 对象，也可以是任何其他新建的对象。 synchronized (this) // 临界区 第二，抢占锁的规则，能不能抢占多次，自己能不能反复抢。 第三，抢不到怎么办，自旋？阻塞？或者超时放弃？ 第四，锁被释放了还在等待锁的线程怎么办？是通知所有线程一起抢或者只告诉一个线程抢？ 说说自旋锁？自旋锁是指当线程尝试获取锁时，如果锁已经被占用，线程不会立即阻塞，而是通过自旋，也就是循环等待的方式不断尝试获取锁。 线程1 线程2 | | | 获取锁成功 | 尝试获取锁 |------------|（锁已被占用，自旋等待） | 释放锁 | |------------| 获取锁成功 | | 适用于锁持有时间短的场景，ReentrantLock 的 tryLock 方法就用到了自旋锁。 二哥的 Java 进阶之路：tryLock中的自旋 自旋锁的优点是可以避免线程切换带来的开销，缺点是如果锁被占用时间过长，会导致线程空转，浪费 CPU 资源。 class SpinLock private AtomicBoolean lock = new AtomicBoolean(false); public void lock() while (!lock.compareAndSet(false, true)) // 自旋等待，不断尝试获取锁 public void unlock() lock.set(false); public static void main(String[] args) SpinLock spinLock = new SpinLock(); Runnable task = () - spinLock.lock(); try System.out.println(Thread.currentThread().getName() + 获取到锁); finally spinLock.unlock(); ; Thread t1 = new Thread(task); Thread t2 = new Thread(task); t1.start(); t2.start(); 默认情况下，自旋锁会一直等待，直到获取到锁为止。在实际开发中，需要设置自旋次数或者超时时间。如果超过阈值，线程可以放弃锁或者进入阻塞状态。 互斥和同步在时间上有要求吗？有。 互斥的核心是保证同一时刻只有一个线程能访问共享资源。 同步强调的是线程之间的执行顺序，特别是在多个线程需要依赖于彼此的执行结果时。 例如，在 CountDownLatch 中，主线程会等待多个子线程的任务完成。 class SyncExample public static void main(String[] args) throws InterruptedException CountDownLatch latch = new CountDownLatch(3); // 创建3个子线程 for (int i = 0; i 3; i++) new Thread(() - try Thread.sleep(1000); // 模拟任务 System.out.println(打完王者了.); catch (InterruptedException e) e.printStackTrace(); finally latch.countDown(); // 每个线程任务完成后计数器减1 ).start(); System.out.println(等打完三把王者就去睡觉...); latch.await(); // 主线程等待子线程完成 System.out.println(好，王者玩完了，可以睡了); 所有子线程完成后，主线程才会继续执行。 二哥的Java 进阶之路：CountDownLatch 42.🌟聊聊悲观锁和乐观锁？（补充）好的。 悲观锁认为每次访问共享资源时都会发生冲突，所在在操作前一定要先加锁，防止其他线程修改数据。 乐观锁认为冲突不会总是发生，所以在操作前不加锁，而是在更新数据时检查是否有其他线程修改了数据。如果发现数据被修改了，就会重试。 乐观锁发现有线程过来修改数据，怎么办？可以重新读取数据，然后再尝试更新，直到成功为止或达到最大重试次数。 读取数据 - 尝试更新 - 成功（返回成功） | - 失败 - 重试 - 达到最大次数 - 返回失败 写个代码演示一下： class CasRetryExample private static AtomicInteger counter = new AtomicInteger(0); private static final int MAX_RETRIES = 5; public static void main(String[] args) boolean success = false; int retries = 0; while (retries MAX_RETRIES) int currentValue = counter.get(); boolean updated = counter.compareAndSet(currentValue, currentValue + 1); if (updated) System.out.println(更新成功，当前值: + counter.get()); success = true; break; else retries++; System.out.println(更新失败，进行第 + retries + 次重试); if (!success) System.out.println(达到最大重试次数，操作失败); 并发工具类43.CountDownLatch 了解吗？推荐阅读：Semaphore、Exchanger、CountDownLatch、CyclicBarrier、Phaser，一网打尽 CountDownLatch 是 JUC 中的一个同步工具类，用于协调多个线程之间的同步，确保主线程在多个子线程完成任务后继续执行。 它的核心思想是通过一个倒计时计数器来控制多个线程的执行顺序。 class CountDownLatchExample public static void main(String[] args) throws InterruptedException int threadCount = 3; CountDownLatch latch = new CountDownLatch(threadCount); for (int i = 0; i threadCount; i++) new Thread(() - try Thread.sleep((long) (Math.random() * 1000)); // 模拟任务执行 System.out.println(Thread.currentThread().getName() + 执行完毕); catch (InterruptedException e) e.printStackTrace(); finally latch.countDown(); // 线程完成后，计数器 -1 ).start(); latch.await(); // 主线程等待 System.out.println(所有子线程执行完毕，主线程继续执行); 在使用的时候，我们需要先初始化一个 CountDownLatch 对象，指定一个计数器的初始值，表示需要等待的线程数量。 然后在每个子线程执行完任务后，调用 countDown() 方法，计数器减 1。 接着主线程调用 await() 方法进入阻塞状态，直到计数器为 0，也就是所有子线程都执行完任务后，主线程才会继续执行。 秦二爷：王者荣耀等待玩家确认 以王者荣耀为例，我们来创建五个线程，分别代表大乔、兰陵王、安其拉、哪吒和铠。每个玩家都调用 countDown() 方法，表示已就位。主线程调用 await() 方法，等待所有玩家就位。 public static void main(String[] args) throws InterruptedException CountDownLatch countDownLatch = new CountDownLatch(5); Thread daqiao = new Thread(() - System.out.println(大乔已就位！); countDownLatch.countDown(); ); Thread lanlingwang = new Thread(() - System.out.println(兰陵王已就位！); countDownLatch.countDown(); ); Thread anqila = new Thread(() - System.out.println(安其拉已就位！); countDownLatch.countDown(); ); Thread nezha = new Thread(() - System.out.println(哪吒已就位！); countDownLatch.countDown(); ); Thread kai = new Thread(() - System.out.println(铠已就位！); countDownLatch.countDown(); ); daqiao.start(); lanlingwang.start(); anqila.start(); nezha.start(); kai.start(); countDownLatch.await(); System.out.println(全员就位，开始游戏！); 五个玩家在倒计时结束后，一起出击。 private static void waitToFight(CountDownLatch countDownLatch, String name) try countDownLatch.await(); // 在此等待信号再继续 System.out.println(name + 收到，发起进攻！); catch (InterruptedException e) Thread.currentThread().interrupt(); System.out.println(name + 被中断); public static void main(String[] args) CountDownLatch countDownLatch = new CountDownLatch(1); Thread daqiao = new Thread(() - waitToFight(countDownLatch, 大乔), Thread-大乔); Thread lanlingwang = new Thread(() - waitToFight(countDownLatch, 兰陵王), Thread-兰陵王); Thread anqila = new Thread(() - waitToFight(countDownLatch, 安琪拉), Thread-安琪拉); Thread nezha = new Thread(() - waitToFight(countDownLatch, 哪吒), Thread-哪吒); Thread kai = new Thread(() - waitToFight(countDownLatch, 凯), Thread-凯); daqiao.start(); lanlingwang.start(); anqila.start(); nezha.start(); kai.start(); try Thread.sleep(5000); // 模拟准备时间 catch (InterruptedException e) Thread.currentThread().interrupt(); System.out.println(主线程被中断); System.out.println(敌军还有 5 秒到达战场，全军出击！); countDownLatch.countDown(); // 发出信号 场景题：假如要查10万多条数据，用线程池分成20个线程去执行，怎么做到等所有的线程都查找完之后，即最后一条结果查找结束了，才输出结果？很简单，可以使用 CountDownLatch 来实现。CountDownLatch 非常适合这个场景。 第一步，创建 CountDownLatch 对象，初始值设定为 20，表示 20 个线程需要完成任务。 第二步，创建线程池，每个线程执行查询操作，查询完毕后调用 countDown() 方法，计数器减 1。 第三步，主线程调用 await() 方法，等待所有线程执行完毕。 class DataQueryExample public static void main(String[] args) throws InterruptedException // 模拟10万条数据 int totalRecords = 100000; int threadCount = 20; int batchSize = totalRecords / threadCount; // 每个线程处理的数据量 // 创建线程池 ExecutorService executor = Executors.newFixedThreadPool(threadCount); CountDownLatch latch = new CountDownLatch(threadCount); // 模拟查询结果 ConcurrentLinkedQueueString results = new ConcurrentLinkedQueue(); for (int i = 0; i threadCount; i++) int start = i * batchSize; int end = (i == threadCount - 1) ? totalRecords : (start + batchSize); executor.execute(() - try // 模拟查询操作 for (int j = start; j end; j++) results.add(Data- + j); System.out.println(Thread.currentThread().getName() + 处理数据 + start + - + end); finally latch.countDown(); // 线程任务完成，计数器减1 ); // 等待所有线程完成 latch.await(); executor.shutdown(); // 输出结果 System.out.println(所有线程执行完毕，查询结果总数： + results.size()); 44.CyclicBarrier 了解吗？了解。 CyclicBarrier 的字面意思是可循环使用的屏障，用于多个线程相互等待，直到所有线程都到达屏障后再同时执行。 三分恶面渣逆袭：CyclicBarrier工作流程 在使用的时候，我们需要先初始化一个 CyclicBarrier 对象，指定一个屏障值 N，表示需要等待的线程数量。 然后每个线程执行 await() 方法，表示自己已经到达屏障，等待其他线程，此时屏障值会减 1。 当所有线程都到达屏障后，也就是屏障值为 0 时，所有线程会继续执行。 class CyclicBarrierExample private static final int THREAD_COUNT = 3; private static final CyclicBarrier barrier = new CyclicBarrier(THREAD_COUNT); public static void main(String[] args) for (int i = 0; i THREAD_COUNT; i++) new Thread(() - try System.out.println(Thread.currentThread().getName() + 到达屏障); barrier.await(); // 线程阻塞，直到所有线程都到达 System.out.println(Thread.currentThread().getName() + 继续执行); catch (InterruptedException | BrokenBarrierException e) e.printStackTrace(); ).start(); 45.CyclicBarrier 和 CountDownLatch 有什么区别？CyclicBarrier 让所有线程相互等待，全部到达后再继续；CountDownLatch 让主线程等待所有子线程执行完再继续。 对比项 CyclicBarrier CountDownLatch 主要用途 让所有线程相互等待，全部到达后再继续 让主线程等待所有子线程执行完 可重用性 ✅ 可重复使用，每次屏障打开后自动重置 ❌ 不可重复使用，计数器归零后不能恢复 是否可执行回调 ✅ 可以，所有线程到达屏障后可执行 barrierAction ❌ 不能 线程等待情况 所有线程互相等待，一个线程未到达，其他线程都会阻塞 主线程等待所有子线程完成，子线程执行完后可继续运行 适用场景 线程相互依赖，需要同步执行 主线程等待子线程完成 示例场景 计算任务拆分，所有线程都到达后才能继续 主线程等多个任务初始化完成 46.Semaphore 了解吗？Semaphore——信号量，用于控制同时访问某个资源的线程数量，类似限流器，确保最多只有指定数量的线程能够访问某个资源，超过的必须等待。 三分恶面渣逆袭：Semaphore 拿停车场来举例。 停车场的车位是有限的，如果有空位，显示牌需要显示剩余的车位，车辆就可以驶入；否则就会显示数字 0，新来的车辆就得排队等待。 如果有车离开，显示牌重新显示闲置的车位数量，等待的车辆按序驶入停车场。 三分恶面渣逆袭：停车场空闲车位提示 在使用 Semaphore 时，首先需要初始化一个 Semaphore 对象，指定许可证数量，表示最多允许多少个线程同时访问资源。 然后在每个线程访问资源前，调用 acquire() 方法获取许可证，如果没有可用许可证，则阻塞等待。 需要注意的是，访问完资源后，要调用 release() 方法释放许可证。 class SemaphoreExample private static final int THREAD_COUNT = 5; private static final Semaphore semaphore = new Semaphore(2); // 最多允许 2 个线程访问 public static void main(String[] args) for (int i = 0; i THREAD_COUNT; i++) new Thread(() - try semaphore.acquire(); // 获取许可（如果没有可用许可，则阻塞） System.out.println(Thread.currentThread().getName() + 访问资源...); Thread.sleep(2000); // 模拟任务执行 catch (InterruptedException e) e.printStackTrace(); finally semaphore.release(); // 释放许可 ).start(); Semaphore 可以用于流量控制，比如数据库连接池、网络连接池等。 假如有这样一个需求，要读取几万个文件的数据，因为都是 IO 密集型任务，我们可以启动几十个线程并发地读取。 但是在读到内存后，需要存储到数据库，而数据库连接数是有限的，比如说只有 10 个，那我们就必须控制线程的数量，保证同时只有 10 个线程在使用数据库连接。 这个时候，就可以使用 Semaphore 来做流量控制： class SemaphoreTest private static final int THREAD_COUNT = 30; private static ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_COUNT); private static Semaphore s = new Semaphore(10); public static void main(String[] args) for (int i = 0; i THREAD_COUNT; i++) threadPool.execute(new Runnable() @Override public void run() try s.acquire(); System.out.println(save data); s.release(); catch (InterruptedException e) ); threadPool.shutdown(); 47.Exchanger 了解吗？Exchanger——交换者，用于在两个线程之间进行数据交换。 三分恶面渣逆袭：英雄交换猎物 支持双向数据交换，比如说线程 A 调用 exchange(dataA)，线程 B 调用 exchange(dataB)，它们会在同步点交换数据，即 A 得到 B 的数据，B 得到 A 的数据。 如果一个线程先调用 exchange()，它会阻塞等待，直到另一个线程也调用 exchange()。 使用 Exchanger 的时候，需要先创建一个 Exchanger 对象，然后在两个线程中调用 exchange() 方法，就可以进行数据交换了。 class ExchangerExample private static final ExchangerString exchanger = new Exchanger(); public static void main(String[] args) new Thread(() - try String threadAData = 数据 A; System.out.println(线程 A 交换前的数据： + threadAData); String received = exchanger.exchange(threadAData); System.out.println(线程 A 收到的数据： + received); catch (InterruptedException e) e.printStackTrace(); ).start(); new Thread(() - try String threadBData = 数据 B; System.out.println(线程 B 交换前的数据： + threadBData); String received = exchanger.exchange(threadBData); System.out.println(线程 B 收到的数据： + received); catch (InterruptedException e) e.printStackTrace(); ).start(); Exchanger 可以用于遗传算法，也可以用于校对工作，比如我们将纸制银行流水通过人工的方式录入到电子银行时，为了避免错误，可以录入两遍，然后通过 Exchanger 来校对两次录入的结果。 class ExchangerTest private static final ExchangerString exgr = new ExchangerString(); private static ExecutorService threadPool = Executors.newFixedThreadPool(2); public static void main(String[] args) threadPool.execute(new Runnable() @Override public void run() try String A = 银行流水A; // A录入银行流水数据 exgr.exchange(A); catch (InterruptedException e) ); threadPool.execute(new Runnable() @Override public void run() try String B = 银行流水B; // B录入银行流水数据 String A = exgr.exchange(B); System.out.println(A和B数据是否一致： + A.equals(B) + ，A录入的是： + A + ，B录入是： + B); catch (InterruptedException e) ); threadPool.shutdown(); memo：2025 年 02 月 18 日修改至此。 48.🌟能说一下 ConcurrentHashMap 的实现吗？（补充）好的。ConcurrentHashMap 是 HashMap 的线程安全版本。 JDK 7 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁。不同的线程可以同时操作不同的段，从而实现并发。 初念初恋：JDK 7 ConcurrentHashMap JDK 8 使用了一种更加细粒度的锁——桶锁，再配合 CAS + synchronized 代码块控制并发写入，以最大程度减少锁的竞争。 初念初恋：JDK 8 ConcurrentHashMap 对于读操作，ConcurrentHashMap 使用了 volatile 变量来保证内存可见性。 对于写操作，ConcurrentHashMap 优先使用 CAS 尝试插入，如果成功就直接返回；否则使用 synchronized 代码块进行加锁处理。 说一下 JDK 7 中 ConcurrentHashMap 的实现原理？好的。 JDK 7 的 ConcurrentHashMap 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁，每个段类似一个 Hashtable。 三分恶面渣逆袭：ConcurrentHashMap示意图 每个段维护一个键值对数组 HashEntryK, V[] table，HashEntry 是一个单项链表。 static final class HashEntryK,V final int hash; final K key; volatile V value; final HashEntryK,V next; 段继承了 ReentrantLock，所以每个段都是一个可重入锁，不同的线程可以同时操作不同的段，从而实现并发。 static final class SegmentK,V extends ReentrantLock transient volatile HashEntryK,V[] table; transient int count; 说一下 JDK 7 中 ConcurrentHashMap 的 put 流程？put 流程和 HashMap 非常类似，只不过是先定位到具体的段，再通过 ReentrantLock 去操作而已。一共可以分为 4 个步骤： 第一步，计算 key 的 hash，定位到段，段如果是空就先初始化； 第二步，使用 ReentrantLock 进行加锁，如果加锁失败就自旋，自旋超过次数就阻塞，保证一定能获取到锁； 第三步，遍历段中的键值对 HashEntry，key 相同直接替换，key 不存在就插入。 第四步，释放锁。 三分恶面渣逆袭：JDK7 put 流程 说一下 JDK 7 中 ConcurrentHashMap 的 get 流程？get 就更简单了，先计算 key 的 hash 找到段，再遍历段中的键值对，找到就直接返回 value。 get 不用加锁，因为是 value 是 volatile 的，所以线程读取 value 时不会出现可见性问题。 说一下 JDK 8 中 ConcurrentHashMap 的实现原理？好的。 JDK 8 中的 ConcurrentHashMap 取消了分段锁，采用 CAS + synchronized 来实现更细粒度的桶锁，并且使用红黑树来优化链表以提高哈希冲突时的查询效率，性能比 JDK 7 有了很大的提升。 说一下 JDK 8 中 ConcurrentHashMap 的 put 流程？三分恶面渣逆袭：Java 8 put 流程 第一步，计算 key 的 hash，以确定桶在数组中的位置。如果数组为空，采用 CAS 的方式初始化，以确保只有一个线程在初始化数组。 // 计算 hashint hash = spread(key.hashCode());// 初始化数组if (tab == null || (n = tab.length) == 0) tab = initTable();// 计算桶的位置int i = (n - 1) hash; 第二步，如果桶为空，直接 CAS 插入节点。如果 CAS 操作失败，会退化为 synchronized 代码块来插入节点。 // CAS 插入节点if (tabAt(tab, i) == null) if (casTabAt(tab, i, null, new NodeK,V(hash, key, value, null))) break;// 否则，使用 synchronized 代码块插入节点else synchronized (f) // **只锁当前桶** if (tabAt(tab, i) == f) // 确保未被其他线程修改 if (f.hash = 0) // 链表处理 for (NodeK,V e = f;;) K ek; if (e.hash == hash ((ek = e.key) == key || (key != null key.equals(ek)))) e.val = value; break; e = e.next; else if (f instanceof TreeBin) // **红黑树处理** ((TreeBinK,V) f).putTreeVal(hash, key, value); 插入的过程中会判断桶的哈希是否小于 0（f.hash = 0），小于 0 说明是红黑树，大于等于 0 说明是链表。 这里补充一点：在 ConcurrentHashMap 的实现中，红黑树节点 TreeBin 的 hash 值固定为 -2。 二哥的 Java 进阶之路：TreeBin 的哈希值固定为 -2 第三步，如果链表长度超过 8，转换为红黑树。 if (binCount = TREEIFY_THRESHOLD) treeifyBin(tab, i); 第四步，在插入新节点后，会调用 addCount() 方法检查是否需要扩容。 addCount(1L, binCount); 说一下 JDK 8 中 ConcurrentHashMap 的 get 流程？get 也是通过 key 的 hash 进行定位，如果该位置节点的哈希匹配且键相等，则直接返回值。 二哥的 Java 进阶之路：HashMap 和 ConcurrentHashMap 的 get 方法 如果节点的哈希为负数，说明是个特殊节点，比如说如树节点或者正在迁移的节点，就调用find方法查找。 二哥的 Java 进阶之路：ForwardingNode和TreeNode的 find 方法 否则遍历链表查找匹配的键。如果都没找到，返回 null。 说一下 HashMap 和 ConcurrentHashMap 的区别？HashMap 是非线程安全的，多线程环境下应该使用 ConcurrentHashMap。 你项目中怎么使用 ConcurrentHashMap 的？在技术派实战项目中，很多地方都用到了 ConcurrentHashMap，比如说在异步工具类 AsyncUtil 中，就使用了 ConcurrentHashMap 来存储任务的名称和它们的运行时间，以便观察和分析任务的执行情况。 二哥的 Java 进阶之路：技术派的源码封装 ConcurrentHashMap 说一下 ConcurrentHashMap 对 HashMap 的改进？首先是 hash 的计算方法上，ConcurrentHashMap 的 spread 方法接收一个已经计算好的 hashCode，然后将这个哈希码的高 16 位与自身进行异或运算。 static final int spread(int h) return (h ^ (h 16)) HASH_BITS; 比 HashMap 的 hash 计算多了一个 HASH_BITS 的操作。这里的 HASH_BITS 是一个常数，值为 0x7fffffff，它确保结果是一个非负整数。 static final int hash(Object key) int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h 16); 另外，ConcurrentHashMap 对节点 Node 做了进一步的封装，比如说用 Forwarding Node 来表示正在进行扩容的节点。 static final class ForwardingNodeK,V extends NodeK,V final NodeK,V[] nextTable; ForwardingNode(NodeK,V[] tab) super(MOVED, null, null, null); this.nextTable = tab; 最后就是 put 方法，通过 CAS + synchronized 代码块来进行并发写入。 二哥的 Java 进阶之路：ConcurrentHashMap 的源码 为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而在 JDK 1.8 要用 synchronizedJDK 1.7 中的 ConcurrentHashMap 使用了分段锁机制，每个 Segment 都继承了 ReentrantLock，这样可以保证每个 Segment 都可以独立地加锁。 而在 JDK 1.8 中，ConcurrentHashMap 取消了 Segment 分段锁，采用了更加精细化的锁——桶锁，以及 CAS 无锁算法，每个桶都可以独立地加锁，只有在 CAS 失败时才会使用 synchronized 代码块加锁，这样可以减少锁的竞争，提高并发性能。 49.ConcurrentHashMap 怎么保证可见性？（补充）ConcurrentHashMap 中的 Node 节点中，value 和 next 都是 volatile 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到。 static class NodeK,V implements Map.EntryK,V final int hash; final K key; volatile V value; volatile NodeK,V next; 50.为什么 ConcurrentHashMap 比 Hashtable 效率高（补充）Hashtable 在任何时刻只允许一个线程访问整个 Map，是通过对整个 Map 加锁来实现线程安全的。比如 get 和 put 方法，是直接在方法上加的 synchronized 关键字。 public synchronized V put(K key, V value) if (value == null) throw new NullPointerException(); int hash = key.hashCode(); int index = (hash 0x7FFFFFFF) % table.length; ... return oldValue; 而 ConcurrentHashMap 在 JDK 8 中是采用 CAS + synchronized 实现的，仅在必要时加锁。 比如说 put 的时候优先使用 CAS 尝试插入，如果失败再使用 synchronized 代码块加锁。 get 的时候是完全无锁的，因为 value 是 volatile 变量 修饰的，保证了内存可见性。 public V get(Object key) int hash = spread(key.hashCode()); NodeK,V[] tab = table; int index = (tab.length - 1) hash; NodeK,V e = tabAt(tab, index); if (e != null) do if (e.hash == hash (e.key == key || (key != null key.equals(e.key)))) return e.value; // 读取 volatile 变量，保证可见性 while ((e = e.next) != null); return null; 51.能说一下 CopyOnWriteArrayList 的实现原理吗？（补充）CopyOnWriteArrayList 是 ArrayList 的线程安全版本，适用于读多写少的场景。它的核心思想是写操作时创建一个新数组，修改后再替换原数组，这样就能够确保读操作无锁，从而提高并发性能。 CL0610：最终一致性 内部使用 volatile 变量来修饰数组 array，以读操作的内存可见性。 private transient volatile Object[] array; 写操作的时候使用 ReentrantLock 来保证线程安全。 public boolean add(E e) final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try Object[] elements = getArray(); int len = elements.length; // 创建一个新数组 Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; // 替换原数组 setArray(newElements); return true; finally // 释放锁 lock.unlock(); 缺点就是写操作的时候会复制一个新数组，如果数组很大，写操作的性能会受到影响。 52. 能说一下 BlockingQueue 吗？（补充）BlockingQueue 是 JUC 包下的一个线程安全队列，支持阻塞式的“生产者-消费者”模型。 当队列容器已满，生产者线程会被阻塞，直到消费者线程取走元素后为止；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。 BlockingQueue 的实现类有很多，比如说 ArrayBlockingQueue、PriorityBlockingQueue 等。 实现类 数据结构 是否有界 特点 ArrayBlockingQueue 数组 ✅ 有界 基于数组，固定容量，FIFO LinkedBlockingQueue 链表 ✅ 可有界（默认 Integer.MAX_VALUE） 基于链表，吞吐量比 ArrayBlockingQueue 高 PriorityBlockingQueue 堆（优先队列） ❌ 无界 元素按优先级排序（非 FIFO） DelayQueue 优先队列（基于 Delayed 接口） ❌ 无界 元素到期后才能被取出 SynchronousQueue 无缓冲 ✅ 容量为 0 必须一对一交换数据，适用于高吞吐的任务提交 LinkedTransferQueue 链表 ❌ 无界 支持 tryTransfer()，数据立即交给消费者 阻塞队列是如何实现的？阻塞队列使用 ReentrantLock + Condition 来确保并发安全。 以 ArrayBlockingQueue 为例，它内部维护了一个数组，使用两个指针分别指向队头和队尾。 put 的时候先用 ReentrantLock 加锁，然后判断队列是否已满，如果已满就阻塞等待，否则插入元素。 final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull;public void put(E e) throws InterruptedException final ReentrantLock lock = this.lock; lock.lockInterruptibly(); // 🔹 加锁，确保线程安全 try while (count == items.length) // 🔹 队列满，阻塞 notFull.await(); enqueue(e); // 🔹 插入元素 finally lock.unlock(); // 🔹 释放锁 线程池53.🌟什么是线程池？线程池是用来管理和复用线程的工具，它可以减少线程的创建和销毁开销。 三分恶面渣逆袭：管理线程的池子 在 Java 中，ThreadPoolExecutor 是线程池的核心实现，它通过核心线程数、最大线程数、任务队列和拒绝策略来控制线程的创建和执行。 举个例子：就像你开了一家餐厅，线程池就相当于固定数量的服务员，顾客（任务）来了就安排空闲的服务员（线程）处理，避免了频繁招人和解雇的成本。 54.你在项目中有用到线程池吗？推荐阅读：线程池在美团业务中的应用 有，用到过很多次。 比如说在技术派实战项目当中， 我们就封装了一个异步工具类 AsyncUtil，内置了可配置的线程池，基于 ThreadPoolExecutor，适用于 IO 密集型任务。 技术派源码：AsyncUtil 其中 corePoolSize 为 CPU 核心数的两倍，因为技术派中的大多数任务都是 IO 密集型的，maxPoolSize 设置为 50，是一个比较理想的值，尤其是在本地环境中；阻塞队列为 SynchronousQueue，意味着任务被创建后可以直接提交给等待的线程处理。 55.🌟说一下线程池的工作流程？可以简单总结为： 任务提交 → 核心线程执行 → 任务队列缓存 → 非核心线程执行 → 拒绝策略处理。 第一步，线程池通过 submit() 提交任务。 ExecutorService threadPool = Executors.newFixedThreadPool(5);threadPool.submit(() - System.out.println(Thread.currentThread().getName() + \\t + 办理业务);); 第二步，线程池会先创建核心线程来执行任务。 if (workerCountOf(c) corePoolSize) if (addWorker(command, true)) return; 第三步，如果核心线程都在忙，任务会被放入任务队列中。 workQueue.offer(task); 第四步，如果任务队列已满，且当前线程数量小于最大线程数，线程池会创建新的线程来处理任务。 if (!addWorker(command, false)) 第五步，如果线程池中的线程数量已经达到最大线程数，且任务队列已满，线程池会执行拒绝策略。 handler.rejectedExecution(command, this); 另外一版回答。 第一步，创建线程池。 第二步，调用线程池的 execute()方法，准备执行任务。 如果正在运行的线程数量小于 corePoolSize，那么线程池会创建一个新的线程来执行这个任务； 如果正在运行的线程数量大于或等于 corePoolSize，那么线程池会将这个任务放入等待队列； 如果等待队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么线程池会创建新的线程来执行这个任务； 如果等待队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会执行拒绝策略。 三分恶面渣逆袭：线程池执行流程 第三步，线程执行完毕后，线程并不会立即销毁，而是继续保持在池中等待下一个任务。 第四步，当线程空闲时间超出指定时间，且当前线程数量大于核心线程数时，线程会被回收。 能用一个生活中的例子说明下吗？可以。有个名叫“你一定暴富”的银行，该银行有 6 个窗口，现在开放了 3 个窗口，坐着 3 个小姐姐在办理业务。 靓仔小二去办理业务，会遇到什么情况呢？ 第一情况，小二发现有个空闲的小姐姐，正在翘首以盼，于是小二就快马加鞭跑过去办理了。 三分恶面渣逆袭：直接办理 第二种情况，小姐姐们都在忙，接待员小美招呼小二去排队区区取号排队，让小二稍安勿躁。 三分恶面渣逆袭：排队等待 第三种情况，不仅小姐姐们都在忙，排队区也满了，小二着急用钱，于是脾气就上来了，和接待员小美对线了起来，要求开放另外 3 个空闲的窗口。 小美迫于小二的压力，开放了另外 3 个窗口，排队区的人立马就冲了过去。 三分恶面渣逆袭：排队区满 第四种情况，6 个窗口的小姐姐都在忙，排队区也满了。。。 三分恶面渣逆袭：等待区，排队区都满 接待员小美给了小二 4 个选项： 对不起，我们暴富银行系统瘫痪了。 没看忙着呢，谁叫你来办的你找谁去！ 靓仔，看你比较急，去队里偷偷加个塞。 不好意思，今天没办法，你改天再来吧。 这个流程和线程池不能说一模一样，简直就是一模一样： corePoolSize 对应营业窗口数 3 maximumPoolSize 对应最大窗口数 6 workQueue 对应排队区 handler 对应接待员小美 class ThreadPoolDemo public static void main(String[] args) // 创建一个线程池 ExecutorService threadPool = new ThreadPoolExecutor( 3, // 核心线程数 6, // 最大线程数 0, // 线程空闲时间 TimeUnit.SECONDS, // 时间单位 new LinkedBlockingQueue(10), // 等待队列 Executors.defaultThreadFactory(), // 线程工厂 new ThreadPoolExecutor.AbortPolicy() // 拒绝策略 ); // 模拟 10 个顾客来银行办理业务 try for (int i = 1; i = 10; i++) final int tempInt = i; threadPool.execute(() - System.out.println(Thread.currentThread().getName() + \\t + 办理业务 + tempInt); ); catch (Exception e) e.printStackTrace(); finally threadPool.shutdown(); 🌟56.线程池的主要参数有哪些？线程池有 7 个参数，需要重点关注的有核心线程数、最大线程数、等待队列、拒绝策略。 三分恶面渣逆袭：线程池参数 ①、corePoolSize：核心线程数，长期存活，执行任务的主力。 ②、maximumPoolSize：线程池允许的最大线程数。 ③、workQueue：任务队列，存储等待执行的任务。 ④、handler：拒绝策略，任务超载时的处理方式。也就是线程数达到 maximumPoolSiz，任务队列也满了的时候，就会触发拒绝策略。 ⑤、threadFactory：线程工厂，用于创建线程，可自定义线程名。 ⑥、keepAliveTime：非核心线程的存活时间，空闲时间超过该值就销毁。 ⑦、unit：keepAliveTime 参数的时间单位： TimeUnit.DAYS; 天 TimeUnit.HOURS; 小时 TimeUnit.MINUTES; 分钟 TimeUnit.SECONDS; 秒 TimeUnit.MILLISECONDS; 毫秒 TimeUnit.MICROSECONDS; 微秒 TimeUnit.NANOSECONDS; 纳秒 能简单说一下参数之间的关系吗？一句话：任务优先使用核心线程执行，满了进入等待队列，队列满了启用非核心线程备用，线程池达到最大线程数量后触发拒绝策略，非核心线程的空闲时间超过存活时间就被回收。 核心线程数不够会怎么进行处理？当提交的任务数超过了 corePoolSize，但是小于 maximumPoolSize 时，线程池会创建新的线程来处理任务。 当提交的任务数超过了 maximumPoolSize 时，线程池会根据拒绝策略来处理任务。 举个例子说一下这些参数的变化？假设一个场景，线程池的配置如下： corePoolSize = 5maximumPoolSize = 10keepAliveTime = 60秒workQueue = LinkedBlockingQueue（容量为100）handler = ThreadPoolExecutor.AbortPolicy() 场景一：当系统启动后，有 10 个任务提交到线程池。 前 5 个任务会立即执行，因为核心线程数足够容纳它们。 随后的 5 个任务会被放入等待队列。 场景二：如果此时再有 100 个任务提交到线程池。 工作队列已满，线程池会创建额外的线程来执行这些任务，直到线程总数达到 10。 如果任务继续增加，超过了工作队列+最大线程数的限制，新来的任务会被 AbortPolicy 拒绝，抛出 RejectedExecutionException 异常。 场景三：如果任务突然减少： 核心线程会一直运行，而超出核心线程数的线程，会在 60 秒后回收。 57.🌟线程池的拒绝策略有哪些？有四种： AbortPolicy：默认的拒绝策略，会抛 RejectedExecutionException 异常。 CallerRunsPolicy：让提交任务的线程自己来执行这个任务，也就是调用 execute 方法的线程。 DiscardOldestPolicy：等待队列会丢弃队列中最老的一个任务，也就是队列中等待最久的任务，然后尝试重新提交被拒绝的任务。 DiscardPolicy：丢弃被拒绝的任务，不做任何处理也不抛出异常。 三分恶面渣逆袭：四种策略 分别对应着小二去银行办理业务被经理“薄纱”的四个场景：“我们系统瘫痪了”、“谁叫你来办的你找谁去”、“看你比较急，去队里加个塞”、“今天没办法，不行你看改一天”。 当线程池无法接受新的任务时，也就是线程数达到 maximumPoolSize，任务队列也满了的时候，就会触发拒绝策略。 如果默认策略不能满足需求，可以通过实现 RejectedExecutionHandler 接口来定义自己的淘汰策略。例如：记录被拒绝任务的日志。 class CustomRejectedHandler public static void main(String[] args) // 自定义拒绝策略 RejectedExecutionHandler rejectedHandler = (r, executor) - System.out.println(Task + r.toString() + rejected. Queue size: + executor.getQueue().size()); ; // 自定义线程池 ThreadPoolExecutor executor = new ThreadPoolExecutor( 2, // 核心线程数 4, // 最大线程数 10, // 空闲线程存活时间 TimeUnit.SECONDS, new ArrayBlockingQueue(2), // 阻塞队列容量 Executors.defaultThreadFactory(), rejectedHandler // 自定义拒绝策略 ); for (int i = 0; i 10; i++) final int taskNumber = i; executor.execute(() - System.out.println(Executing task + taskNumber); try Thread.sleep(1000); // 模拟任务耗时 catch (InterruptedException e) e.printStackTrace(); ); executor.shutdown(); 58.线程池有哪几种阻塞队列？常用的有五种，有界队列 ArrayBlockingQueue；无界队列 LinkedBlockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列 SynchronousQueue。 三分恶面渣逆袭：线程池常用阻塞队列 ①、ArrayBlockingQueue：一个有界的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池。 ArrayBlockingQueueInteger blockingQueue = new ArrayBlockingQueueInteger(10, true); ②、LinkedBlockingQueue：底层是链表，如果不指定大小，默认大小是 Integer.MAX_VALUE，几乎相当于一个无界队列。 技术派实战项目中，就使用了 LinkedBlockingQueue 来配置 RabbitMQ 的消息队列。 技术派实战项目源码：RabbitMQ 的消息队列 ③、PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。任务按照其自然顺序或 Comparator 来排序。 适用于需要按照给定优先级处理任务的场景，比如优先处理紧急任务。 ④、DelayQueue：类似于 PriorityBlockingQueue，由二叉堆实现的无界优先级阻塞队列。 Executors 中的 newScheduledThreadPool() 就使用了 DelayQueue 来实现延迟执行。 public ScheduledThreadPoolExecutor(int corePoolSize) super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); ⑤、SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作。 Executors.newCachedThreadPool() 就使用了 SynchronousQueue，这个线程池会根据需要创建新线程，如果有空闲线程则会重复使用，线程空闲 60 秒后会被回收。 public static ExecutorService newCachedThreadPool() return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueueRunnable()); 59.线程池提交 execute 和 submit 有什么区别？execute 方法没有返回值，适用于不关心结果和异常的简单任务。 threadsPool.execute(new Runnable() @Override public void run() System.out.println(execute() 方法提交的任务); ); submit 有返回值，适用于需要获取结果或处理异常的场景。 FutureObject future = executor.submit(harReturnValuetask);try Object s = future.get(); catch (InterruptedException e | ExecutionException e) // 处理无法执行任务异常 finally // 关闭线程池 executor.shutdown(); 60.线程池怎么关闭知道吗？可以调用线程池的shutdown或shutdownNow方法来关闭线程池。 shutdown 不会立即停止线程池，而是等待所有任务执行完毕后再关闭线程池。 ExecutorService executor = Executors.newFixedThreadPool(3);executor.execute(() - System.out.println(Task 1));executor.execute(() - System.out.println(Task 2));executor.shutdown(); // 不会立刻关闭，而是等待所有任务执行完毕 shutdownNow 会尝试通过一系列动作来停止线程池，包括停止接收外部提交的任务、忽略队列里等待的任务、尝试将正在跑的任务 interrupt 中断。 ExecutorService executor = Executors.newFixedThreadPool(3);executor.execute(() - try Thread.sleep(5000); // 模拟长时间运行任务 System.out.println(Task executed); catch (InterruptedException e) System.out.println(任务被中断); );ListRunnable unexecutedTasks = executor.shutdownNow(); // 立即关闭线程池System.out.println(未执行的任务数: + unexecutedTasks.size()); 需要注意的是，shutdownNow 不会真正终止正在运行的任务，只是给任务线程发送 interrupt 信号，任务是否能真正终止取决于线程是否响应 InterruptedException。 61.线程池的线程数应该怎么配置？首先，我会分析线程池中执行的任务类型是 CPU 密集型还是 IO 密集型？ ①、对于 CPU 密集型任务，我的目标是尽量减少线程上下文切换，以优化 CPU 使用率。一般来说，核心线程数设置为处理器的核心数或核心数加一是较理想的选择。 +1 是为了以备不时之需，如果某线程因等待系统资源而阻塞时，可以有多余的线程顶上去，不至于影响整体性能。 ②、对于 IO 密集型任务，由于线程经常处于等待状态，等待 IO 操作完成，所以可以设置更多的线程来提高并发，比如说 CPU 核心数的两倍。 常见线程池参数配置方案-来源美团技术博客 核心数可以通过 Java 的Runtime.getRuntime().availableProcessors()方法获取。 最后，我会根据业务需求和系统资源来调整线程池的其他参数，比如最大线程数、任务队列容量、非核心线程的空闲存活时间等。 ThreadPoolExecutor executor = new ThreadPoolExecutor( cores, // 核心线程数设置为CPU核心数 cores * 2, // 最大线程数为核心数的两倍 60L, TimeUnit.SECONDS, // 非核心线程的空闲存活时间 new LinkedBlockingQueue(100) // 任务队列容量); 如何知道你设置的线程数多了还是少了？可以通过监控和调试来判断线程数是多还是少。 比如说通过 top 命令观察 CPU 的使用率，如果 CPU 使用率较低，可能是线程数过少；如果 CPU 使用率接近 100%，但吞吐量未提升，可能是线程数过多。 然后再通过 VisualVM 或 Arthas 分析线程运行情况，查看线程的状态、等待时间、运行时间等信息。 也可以使用 jstack 命令查看线程堆栈信息，查看线程是否处于阻塞状态。 jstack Java 进程 ID | grep -A 20 BLOCKED // 查看阻塞线程 如果有大量的 BLOCKED 线程，说明线程数可能过多，竞争比较激烈。 62.有哪几种常见的线程池？主要有四种： 固定大小的线程池 Executors.newFixedThreadPool(int nThreads);，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等。 缓存线程池 Executors.newCachedThreadPool();，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。 定时任务线程池 Executors.newScheduledThreadPool(int corePoolSize);，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等。 单线程线程池 Executors.newSingleThreadExecutor();，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。 63.能说一下四种常见线程池的原理吗？不管是 FixedThreadPool、CachedThreadPool，还是 SingleThreadExecutor 和 ScheduledThreadPoolExecutor，它们本质上都是 ThreadPoolExecutor 的不同配置。 说说固定大小线程池的原理？线程池大小是固定的，corePoolSize == maximumPoolSize，默认使用 LinkedBlockingQueue 作为阻塞队列，适用于任务量稳定的场景，如数据库连接池、RPC 处理等。 new ThreadPoolExecutor(4, 4, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); 新任务提交时，如果线程池有空闲线程，直接执行；如果没有，任务进入 LinkedBlockingQueue 等待。缺点是任务队列默认无界，可能导致任务堆积，甚至 OOM。 三分恶面渣逆袭：FixedThreadPool 说说缓存线程池的原理？线程池大小不固定，corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE。空闲线程超过 60 秒会被销毁，使用 SynchronousQueue 作为阻塞队列，适用于短时间内有大量任务的场景。 new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); 提交任务时，如果线程池没有空闲线程，直接新建线程执行任务；如果有，复用线程执行任务。线程空闲 60 秒后销毁，减少资源占用。缺点是线程数没有上限，在高并发情况下可能导致 OOM。 三分恶面渣逆袭：CachedThreadPool执行流程 说说单线程线程池的原理？线程池只有 1 个线程，保证任务按提交顺序执行，使用 LinkedBlockingQueue 作为阻塞队列，适用于需要按顺序执行任务的场景。 new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); 始终只创建 1 个线程，新任务必须等待前一个任务完成后才能执行，其他任务都被放入 LinkedBlockingQueue 排队执行。缺点是无法并行处理任务。 三分恶面渣逆袭：SingleThreadExecutor运行流程 说说定时任务线程池的原理？定时任务线程池的大小可配置，支持定时 周期性任务执行，使用 DelayedWorkQueue 作为阻塞队列，适用于周期性执行任务的场景。 public ScheduledThreadPoolExecutor(int corePoolSize) super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); 执行定时任务时，schedule() 方法可以将任务延迟一定时间后执行一次；scheduleAtFixedRate() 方法可以将任务延迟一定时间后以固定频率执行；scheduleWithFixedDelay() 方法可以将任务延迟一定时间后以固定延迟执行。 三分恶面渣逆袭：ScheduledThreadPool执行流程 缺点是，如果任务执行时间 设定时间间隔，scheduleAtFixedRate 可能会导致任务堆积。 三分恶面渣逆袭：ScheduledThreadPoolExecutor执行流程 使用无界队列的线程池会出现什么问题？如果线程获取一个任务后，任务的执行时间比较长，会导致队列的任务越积越多，导致内存使用不断飙升，最终出现 OOM。 64.线程池异常怎么处理知道吗？常见的处理方式有，使用 try-catch 捕获、使用 Future 获取异常、自定义ThreadPoolExecutor 重写 afterExecute 方法、使用 UncaughtExceptionHandler 捕获异常。 三分恶面渣逆袭：线程池异常处理 ①、try-catch 是最简单的方法。 executor.execute(() - try System.out.println(任务开始); int result = 1 / 0; // 除零异常 catch (Exception e) System.err.println(捕获异常： + e.getMessage()); ); ②、使用 Future 获取异常。 FutureObject future = executor.submit(() - System.out.println(任务开始); int result = 1 / 0; // 除零异常 return result;);try future.get(); catch (InterruptedException | ExecutionException e) System.err.println(捕获异常： + e.getMessage()); ③、自定义 ThreadPoolExecutor 重写 afterExecute 方法。 ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 2, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueueRunnable()) @Override protected void afterExecute(Runnable r, Throwable t) super.afterExecute(r, t); if (t != null) System.err.println(捕获异常： + t.getMessage()); ;executor.execute(() - System.out.println(任务开始); int result = 1 / 0; // 除零异常); ④、使用 UncaughtExceptionHandler 捕获异常。 ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 2, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueueRunnable());executor.setRejectedExecutionHandler(new ThreadPoolExecutor.AbortPolicy());executor.setThreadFactory(new ThreadFactory() @Override public Thread newThread(Runnable r) Thread thread = new Thread(r); thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() @Override public void uncaughtException(Thread t, Throwable e) System.err.println(捕获异常： + e.getMessage()); ); return thread; );executor.execute(() - System.out.println(任务开始); int result = 1 / 0; // 除零异常); 如果项目使用 execute()，不关心任务返回值，建议使用 UncaughtExceptionHandler： thread.setUncaughtExceptionHandler((t, e) - System.err.println(线程 + t.getName() + 捕获到异常： + e.getMessage())); 如果项目使用 submit()，关心任务返回值，建议使用 Future： Future? future = executor.submit(task);try future.get(); catch (ExecutionException e) System.err.println(捕获异常： + e.getCause()); 如果想要全局捕获所有任务异常，建议重写 afterExecute 方法： class MyThreadPoolExecutor extends ThreadPoolExecutor @Override protected void afterExecute(Runnable r, Throwable t) if (t == null r instanceof Future?) try ((Future?) r).get(); catch (Exception e) System.err.println(任务异常： + e.getCause()); 65.能说一下线程池有几种状态吗？有 5 种状态，它们的转换遵循严格的状态流转规则，不同状态控制着线程池的任务调度和关闭行为。 状态由 RUNNING → SHUTDOWN → STOP → TIDYING → TERMINATED 依次流转。 三分恶面渣逆袭：线程池状态切换图 RUNNING 状态的线程池可以接收新任务，并处理阻塞队列中的任务；SHUTDOWN 状态的线程池不会接收新任务，但会处理阻塞队列中的任务；STOP 状态的线程池不会接收新任务，也不会处理阻塞队列中的任务，并且会尝试中断正在执行的任务；TIDYING 状态表示所有任务已经终止；TERMINATED 状态表示线程池完全关闭，所有线程销毁。 状态 状态码 是否接收新任务 是否执行队列中的任务 是否中断正在执行的任务 RUNNING 111 ✅ 是 ✅ 是 ❌ 否 SHUTDOWN 000 ❌ 否 ✅ 是 ❌ 否 STOP 001 ❌ 否 ❌ 否 ✅ 是 TIDYING 010 ❌ 否 ❌ 否 ❌ 否 TERMINATED 011 ❌ 否 ❌ 否 ❌ 否 66.线程池如何实现参数的动态修改？线程池提供的 setter 方法就可以在运行时动态修改参数，比如说 setCorePoolSize 可以用来修改核心线程数、setMaximumPoolSize 可以用来修改最大线程数。 三分恶面渣逆袭：JDK 线程池参数设置 需要注意的是，调用 setCorePoolSize() 时如果新的核心线程数比原来的大，线程池会创建新的线程；如果更小，线程池不会立即销毁多余的线程，除非有空闲线程超过 keepAliveTime。 当然了，还可以利用 Nacos 配置中心，或者实现自定义的线程池，监听参数变化去动态调整参数。 三分恶面渣逆袭：动态修改线程池参数 67.🌟线程池调优了解吗？（补充）三分恶面渣逆袭：线程池调优 首先我会根据任务类型设置核心线程数参数，比如 IO 密集型任务会设置为 CPU 核心数*2 的经验值。 其次我会结合线程池动态调整的能力，在流量波动时通过 setCorePoolSize 平滑扩容，或者直接使用 DynamicTp 实现线程池参数的自动化调整。 最后，我会通过内置的监控指标建立容量预警机制。比如通过 JMX 监控线程池的运行状态，设置阈值，当线程池的任务队列长度超过阈值时，触发告警。 68.线程池在使用的时候需要注意什么？（补充）我认为有 3 个比较重要的关注点： 第一个，选择合适的线程池大小。过小的线程池可能会导致任务一直在排队；过大的线程池可能会导致大家都在竞争 CPU 资源，增加上下文切换的开销 第二个，选择合适的任务队列。使用有界队列可以避免资源耗尽的风险，但是可能会导致任务被拒绝；使用无界队列虽然可以避免任务被拒绝，但是可能会导致内存耗尽 比如在使用 LinkedBlockingQueue 的时候，可以传入参数来限制队列中任务的数量，这样就不会出现 OOM。 第三个，尽量使用自定义的线程池，而不是使用 Executors 创建的线程池。 因为 newFixedThreadPool 线程池由于使用了 LinkedBlockingQueue，队列的容量默认无限大，任务过多时会导致内存溢出；newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，导致服务器负载过高宕机。 69.🌟你能设计实现一个线程池吗？推荐阅读：三分恶线程池原理 线程池的主要目的是为了避免频繁地创建和销毁线程。 三分恶面渣逆袭：线程池主要实现流程 我会把线程池看作一个工厂，里面有一群“工人”，也就是线程了，专门用来做任务。 当任务来了，需要先判断有没有空闲的工人，如果有就把任务交给他们；如果没有，就把任务暂存到一个任务队列里，等工人忙完了再去处理。 如果队列满了，还没有空闲的工人，就要考虑扩容，让预备的工人过来干活，但不能超过预定的最大值，防止工厂被挤爆。 如果连扩容也没法解决，就需要一个拒绝策略，可能直接拒绝任务或者报个错。 核心线程池类（可参考）： class CustomThreadPoolExecutor private final int corePoolSize; private final int maximumPoolSize; private final long keepAliveTime; private final TimeUnit unit; private final BlockingQueueRunnable workQueue; private final RejectedExecutionHandler handler; private volatile boolean isShutdown = false; private int currentPoolSize = 0; // 构造方法 public CustomThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueueRunnable workQueue, RejectedExecutionHandler handler) this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.keepAliveTime = keepAliveTime; this.unit = unit; this.workQueue = workQueue; this.handler = handler; // 提交任务 public void execute(Runnable task) if (isShutdown) throw new IllegalStateException(ThreadPool is shutdown); synchronized (this) // 如果当前线程数小于核心线程数，直接创建新线程 if (currentPoolSize corePoolSize) new Worker(task).start(); currentPoolSize++; return; // 尝试将任务添加到队列中 if (!workQueue.offer(task)) if (currentPoolSize maximumPoolSize) new Worker(task).start(); currentPoolSize++; else // 调用拒绝策略 handler.rejectedExecution(task, null); // 关闭线程池 public void shutdown() isShutdown = true; // 工作线程 private class Worker extends Thread private Runnable task; Worker(Runnable task) this.task = task; @Override public void run() while (task != null || (task = getTask()) != null) try task.run(); finally task = null; // 从队列中获取任务 private Runnable getTask() try return workQueue.poll(keepAliveTime, unit); catch (InterruptedException e) return null; 拒绝策略： /** * 拒绝策略 */class CustomRejectedExecutionHandler // AbortPolicy 抛出异常 public static class AbortPolicy implements RejectedExecutionHandler public void rejectedExecution(Runnable r, ThreadPoolExecutor e) throw new RuntimeException(Task + r.toString() + rejected from + e.toString()); // DiscardPolicy 什么都不做 public static class DiscardPolicy implements RejectedExecutionHandler public void rejectedExecution(Runnable r, ThreadPoolExecutor e) // Do nothing // DiscardOldestPolicy 丢弃队列中最旧的任务 public static class CallerRunsPolicy implements RejectedExecutionHandler public void rejectedExecution(Runnable r, ThreadPoolExecutor e) if (!e.isShutdown()) r.run(); 使用示例： class ThreadPoolTest public static void main(String[] args) // 创建线程池 CustomThreadPoolExecutor executor = new CustomThreadPoolExecutor( 2, 4, 10, TimeUnit.SECONDS, new LinkedBlockingQueue(2), new CustomRejectedExecutionHandler.AbortPolicy()); // 提交任务 for (int i = 0; i 10; i++) final int index = i; executor.execute(() - System.out.println(Task + index + is running); try Thread.sleep(2000); catch (InterruptedException e) e.printStackTrace(); ); // 关闭线程池 executor.shutdown(); 执行结果： 二哥的 Java 进阶之路：自定义线程池 手写一个数据库连接池，可以吗？可以的，我的思路是这样的：数据库连接池主要是为了避免每次操作数据库时都去创建连接，因为那样很浪费资源。所以我打算在初始化时预先创建好固定数量的连接，然后把它们放到一个线程安全的容器里，后续有请求的时候就从队列里拿，使用完后再归还到队列中。 class SimpleConnectionPool // 配置 private String jdbcUrl; private String username; private String password; private int maxConnections; private BlockingQueueConnection connectionPool; // 构造方法 public SimpleConnectionPool(String jdbcUrl, String username, String password, int maxConnections) throws SQLException this.jdbcUrl = jdbcUrl; this.username = username; this.password = password; this.maxConnections = maxConnections; this.connectionPool = new LinkedBlockingQueue(maxConnections); // 初始化连接池 for (int i = 0; i maxConnections; i++) connectionPool.add(createNewConnection()); // 创建新连接 private Connection createNewConnection() throws SQLException return DriverManager.getConnection(jdbcUrl, username, password); // 获取连接 public Connection getConnection(long timeout, TimeUnit unit) throws InterruptedException, SQLException Connection connection = connectionPool.poll(timeout, unit); // 等待指定时间获取连接 if (connection == null) throw new SQLException(Timeout: Unable to acquire a connection.); return connection; // 归还连接 public void releaseConnection(Connection connection) throws SQLException if (connection != null) if (connection.isClosed()) // 如果连接已关闭，创建一个新连接补充到池中 connectionPool.add(createNewConnection()); else // 将连接归还到池中 connectionPool.offer(connection); // 关闭所有连接 public void closeAllConnections() throws SQLException for (Connection connection : connectionPool) if (!connection.isClosed()) connection.close(); // 测试用例 public static void main(String[] args) try SimpleConnectionPool pool = new SimpleConnectionPool( jdbc:mysql://localhost:3306/pai_coding, root, , 5 ); // 获取连接 Connection conn = pool.getConnection(5, TimeUnit.SECONDS); // 使用连接（示例查询） System.out.println(Connection acquired: + conn); Thread.sleep(2000); // 模拟查询 // 归还连接 pool.releaseConnection(conn); System.out.println(Connection returned.); // 关闭所有连接 pool.closeAllConnections(); catch (Exception e) e.printStackTrace(); 运行结果： 二哥的Java 进阶之路：数据库连接池 70.线程池执行中断电了应该怎么处理？线程池本身只能在内存中进行任务调度，并不会持久化，一旦断电，线程池里的所有任务和状态都会丢失。 我会考虑以下几个方面： 第一，持久化任务。可以将任务持久化到数据库或者消息队列中，等电恢复后再重新执行。 第二，任务幂等性，需要保证任务是幂等的，也就是无论执行多少次，结果都一致。 第三，恢复策略。当系统重启时，应该有一个恢复流程：检测上次是否有未完成的任务，将这些任务重新加载到线程池中执行，确保断电前的工作能够恢复。 并发容器和框架71.ForkJoin 框架了解吗？关于 ForkJoin 框架，我了解一些，它是 Java 7 引入的一个并行框架，主要用于分治算法的并行执行。这个框架通过将大的任务递归地分解成小任务，然后并行执行，最后再合并结果，以达到最高效率处理大量数据的目的。 三分恶面渣逆袭：ForkJoin分治算法 ForkJoin 框架的核心理念是分而治之，将大任务拆分为多个小任务并行处理，最后再将这些小任务的结果汇总。 就像是一个树形结构，根节点是一个大的任务，叶子节点是最小的子任务，每个任务都可能会被分裂成更小的子任务，直到达到某个临界点，任务再逐个执行。 具体来说，ForkJoin 包括两个主要的类： ForkJoinPool，一个特殊的线程池，底层使用了工作窃取算法，也就是当一个线程执行完自己的任务后，它可以窃取其他线程的任务，避免线程闲置。 三分恶面渣逆袭：工作窃取 RecursiveTask 和 RecursiveAction，分别用于有返回值和无返回值的任务，这两个类都继承自 ForkJoinTask。 class ForkJoinExample public static void main(String[] args) int[] arr = new int[100]; for (int i = 0; i 100; i++) arr[i] = i + 1; // 填充数据 1 到 100 // 创建 ForkJoinPool，默认使用可用的处理器核心数 ForkJoinPool pool = new ForkJoinPool(); // 创建 ForkJoin 任务 SumTask task = new SumTask(arr, 0, arr.length); // 执行任务 Integer result = pool.invoke(task); System.out.println(数组的和是: + result); // 自定义任务，继承 RecursiveTask static class SumTask extends RecursiveTaskInteger private int[] arr; private int start; private int end; public SumTask(int[] arr, int start, int end) this.arr = arr; this.start = start; this.end = end; @Override protected Integer compute() if (end - start = 10) // 如果任务足够小，就直接计算 int sum = 0; for (int i = start; i end; i++) sum += arr[i]; return sum; else // 否则拆分任务 int mid = (start + end) / 2; SumTask left = new SumTask(arr, start, mid); SumTask right = new SumTask(arr, mid, end); // 分别执行子任务 left.fork(); right.fork(); // 合并结果 int leftResult = left.join(); int rightResult = right.join(); return leftResult + rightResult; // 汇总结果","tags":["基础","并发编程"],"categories":["Java问答笔记"]},{"title":"2025.8.8学习日记","path":"/2025/08/08/学习日记25年8月/2025.8.8学习笔记/","content":"今日学习内容3DGS1.编写了一个频率能量测试脚本,输入图像可以生成不同分辨率的频率能量,可以用于定量分析图像结构. 2.对DashGS进行改写: 由于3DGS训练时仅依靠图像梯度来拟合真值图像,过度依赖当前的真值图像,使得空间中出现很多浮点.而原始输入的colmap点相对3DGS训练的点在空间上更加可靠(由多视角三角测量+全局优化得来),所以我增加原始输入点(母点)的复制能力,限制3DGS训练的子点的复制能力. 目前的实现方法是,DashGS的密度调度器会对高斯球增值的数量进行限制,而我通过控制母点和子点的分配比例来控制二者的复制数量,假设有100个点的容量,留给母点70个容量,剩下的给子点,来实现对复制点流向的控制.只做了一组实验,具体效果还需要进一步评估.原版Dash:SSIM ↑: 0.7548986PSNR ↑: 24.0219326LPIPS↓: 0.3954713改版Dash:SSIM ↑: 0.7556131PSNR ↑: 24.1246986LPIPS↓: 0.3952501 力扣每日一题JAVA并发编程篇371 学习中,并且正在完善笔记. 项目代码随想录生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.7学习日记","path":"/2025/08/07/学习日记25年8月/2025.8.7学习笔记/","content":"今日学习内容订阅了cursor的pro版本,功能真的强大. 3DGS力扣每日一题脑筋急转弯+DP java集合框架篇3030 完成学习,并且记了一篇笔记. 项目实现全局异常处理接口. 代码随想录生活篇战地6晚上玩了战地6b测.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"SSH远程调用开发环境","path":"/2025/08/06/3DGS/SSH远程调用开发环境/","content":"前言为了方便调试和开发,我需要用我的Mac笔记本远程调用Ubuntu20主机,来跑Ubuntu上的程序.采取的方案使用过VSCode的SSH远程连接来调用Ubuntu的环境. 环境一台Mac笔记本,一台Ubuntu20主机. 配置一、台式机（Ubuntu）基础配置​​ 启用SSH服务（若未安装） sudo apt updatesudo apt install openssh-server -ysudo systemctl enable --now sshsudo ufw allow 22 # 如果启用了防火墙sudo ufw disable # 关闭防火墙#sudo ufw status # 查看防火墙状态#sudo ufw enable # 开启防火墙 检查SSH服务状态 sudo systemctl status ssh 应看到Active: active (running) 获取本机IP（用于后续连接） ip a | grep inet | grep -v 127.0.0.1 输出示例：192.168.1.100或者使用ifconfig命令查看. ​​二、Mac端SSH密钥配置​​ 生成密钥对（在Mac终端执行） ssh-keygen -t ed25519 -C mac_to_ubuntu 按回车接受默认存储路径：/Users/你的用户名/.ssh/id_ed25519建议设置密码短语（可选但更安全） 查看公钥内容（复制后面要用） cat ~/.ssh/id_ed25519.pub 输出示例：ssh-ed25519 AAAAC3Nz... mac_to_ubuntu 将公钥上传到台式机 ssh-copy-id -i ~/.ssh/id_ed25519.pub 你的ubuntu用户名@台式机IP 示例：ssh-copy-id -i ~/.ssh/id_ed25519.pub user@192.168.1.100首次需要输入Ubuntu密码ssh-copy-id -i ~/.ssh/id_ed25519 dong@192.168.3.27 测试免密登录 ssh -i ~/.ssh/id_ed25519 你的ubuntu用户名@台式机IP ssh -i ~/.ssh/id_ed25519 dong@192.168.3.27成功应直接进入Ubuntu终端.退出终端使用exit命令. 三、VSCode配置 安装必要扩展在VS Code扩展市场搜索安装：Remote - SSHPython（Microsoft官方扩展） 创建SSH配置文件在Mac的~/.ssh/config中添加（没有则新建）：Host Ubuntu-Desktop HostName 192.168.3.27 User dong IdentityFile ~/.ssh/id_ed25519 IdentitiesOnly yes 3.点击左下角Remote-SSH: Connect to Host4.选择Ubuntu-Desktop即可开始快乐的远程调用了!","tags":["远程开发"],"categories":["3DGS"]},{"title":"2025.8.6学习日记","path":"/2025/08/06/学习日记25年8月/2025.8.6学习笔记/","content":"今日学习内容3DGS实现远程调用.做了DASH的注释. 力扣每日一题线段树二分的题目,做了线段树的笔记. java集合框架篇1730 项目实现全局异常处理接口. 代码随想录三道DP题目. 生活篇晚上健身胸,肩,腿,三头,强度大.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"Ubuntu20使用多个版本CUDA","path":"/2025/08/05/3DGS/Ubuntu20使用多个版本CUDA/","content":"1 前言由于项目需要,需要在不同版本CUDA之间切换,所以需要安装多个版本的CUDA. 2 下载需要的CUDA版本前往 CUDA 的下载官网：https://developer.nvidia.com/cuda-toolkit-archive 选择自己需要的 CUDA 版本，选择对应的系统进行下载，建议选择 .run 形式的安装器。以 root 权限运行 .run 安装器，根据提示进行安装即可。过程中需要注意取消勾选显卡驱动，不要重新安装驱动，因为我们上一步已经安装过了。 3 安装对应的 cuDNN 库前往 cuDNN 的下载官网：https://developer.nvidia.com/cudnn-archive 注意选择和上一步安装的 CUDA 版本适配的 cuDNN 版本，我们需要分别给每个版本的 CUDA 安装对应版本的 cuDNN 库。建议选择 .tar.xz 形式的压缩包。 下载好后，使用 tar -xvf xxx.tar.xz 解压 cuDNN 库，然后直接执行以下指令进行安装：（注意在正确位置填上对应的版本号，例如 cuda-12.3） sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda-版本号/includesudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda-版本号/lib64sudo chmod a+r /usr/local/cuda-版本号/include/cudnn*.h /usr/local/cuda-版本号/lib64/libcudnn* 4 配置切换脚本我们的切换方式原理是软链接。每个 CUDA 版本安装后，会在 usrlocal 创建对应版本的文件夹，例如 cuda-12.3，我们可以创建一个软链接 usrlocalcuda，让软链接指向对应版本的 CUDA，我们通过修改软链接的指向就能修改 CUDA 版本了。 我们通过文本编辑器打开用户文件夹下的 .bashrc，这里用 vim 为例：vim ~/.bashrc 在该文件尾部添加以下内容： export PATH=$PATH:/usr/local/cuda/binexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64function switch_cuda if [[ $1 =~ ^[0-9]+\\.[0-9]+$ ]]; then sudo rm /usr/local/cuda sudo ln -s /usr/local/cuda-$1 /usr/local/cuda nvcc --version else echo invalid version fi 注意，如果你的 .bashrc 内已经有类似于前两行的内容，那么需要删除它，替换为我的写法。 添加后保存该文件，然后使用 source .bashrc 刷新该文件。 5 测试多版本切换上述脚本在 bash 内注册了一个函数叫 switch_cuda，通过调用该函数即可快速切换 CUDA 版本。用法为：switch_cuda xx.x (版本号) 以下是切换的演示，首先将 CUDA 版本切换到了 11.6，然后再切回 12.3：","tags":["3DGS"],"categories":["3DGS"]},{"title":"2025.8.5学习日记","path":"/2025/08/05/学习日记25年8月/2025.8.5学习笔记/","content":"今日学习内容3DGS配置环境,训练了DashGaussian. 力扣每日一题一道简单的On2遍历题目. java集合框架篇830 项目学习Thymeleaf渲染引擎,写了一个渲染引擎包装类. 代码随想录两道DP题目. 生活篇晚上健身练手臂,强度中等.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.8.4学习日记","path":"/2025/08/04/学习日记25年8月/2025.8.4学习笔记/","content":"今日学习内容3DGS跑数据,EffectiveGS环境还是不行.然后跑了TamingGS的代码,并且渲染了一下. 力扣每日一题一道滑动窗口的题目. java集合框架篇830 项目学习Thymeleaf渲染引擎.并且做了一篇笔记. 代码随想录生活篇晚上健身练背,强度偏大.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"Thymeleaf模版引擎","path":"/2025/08/04/项目笔记/Thymeleaf模版引擎/","content":"参考文章:技术派Thymeleaf模板引擎 Thymeleaf是什么Thymeleaf 是一个优秀的、面向 Java 的 HTML 页面模板，具有丰富的标签语言和函数。在 JSP 被淘汰之后，Thymeleaf 取而代之成为了 Spring Boot 推荐的模板引擎。 Thymeleaf 在有网和没网的环境下都可以正常工作，既能让美工在浏览器中查看页面的静态效果，也能让程序员在服务器查看带数据的动态页面效果。 这是因为 Thymeleaf 支持 HTML 原型，在 HTML 标签里增加额外的属性来达到模板+数据的展示方式。 浏览器在解释 HTML 的时候会忽略未定义的标签属性，所以 Thymeleaf 可以静态地运行；当有数据返回页面时，Thymeleaf 标签会动态地替换静态内容。 下面列举一些 Thymeleaf 常用的表达式、标签和函数。 常用表达式1）常用表达式 $... 变量表达式 *... 选择表达式 #... 文字表达式 @... URL 表达式 #maps 对象表达式2）常用标签 th:action 定义服务器端控制器路径。 th:each 循环语句 th:field 表单字段 th:href URL 链接 th:id div 标签中的 ID th:if 条件判断 th:include 引入文件 th:fragment 定义代码片段 th:object 替换对象 th:src 图片地址 th:text 文本 th:value 属性值3）常用函数 #dates 日期函数 #lists 列表函数 #arrays 数组函数 #strings 字符串函数 #numbers 数字函数 #calendars 日历函数 #objects 对象函数 #bools 布尔函数 想要查看更多 Thymeleaf 表达式、标签、函数等内容，可以到 Thymeleaf 官网： https://www.thymeleaf.org/ 项目如何整合Thymeleaf第一步 引入依赖第一步，在 ui 模块 pom.xml 文件中添加 Thymeleaf 的 stater dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter-thymeleaf/artifactId/dependency 第二步 配置Thymeleaf第二步，在 web 模块 devapplication-web.yml 文件中添加 Thymeleaf 的配置 spring: thymeleaf: mode: HTML encoding: UTF-8 servlet: content-type: text/html cache: false mode: 模板模式，通常情况下设置为 “HTML”。 encoding: 模板文件的字符编码，通常设置为 “UTF-8”。 servlet.content-type: 在 Servlet 环境中，模板的 content type。一般设置为 “texthtml”，代表生成的内容是 HTML。 cache: 是否缓存模板。在开发环境中，通常设置为 false，这样每次修改模板文件后，不需要重启应用就可以看到效果。在生产环境中，通常设置为 true，以提高性能。可参照 prodapplication-web.yml 第三步 编写控制器第三步，我们直接来看首页吧，在 IndexController 中添加 index 方法，内容如下。 @GetMapping(path = /, , /index)public String index(Model model, HttpServletRequest request) String activeTab = request.getParameter(category);//获取当前激活的内容分类 IndexVo vo = indexRecommendHelper.buildIndexVo(activeTab);//根据当前激活的分类获取首页数据 model.addAttribute(vo, vo); return views/home/index; 我尝试大白话讲一下上面代码渲染模板的逻辑: 客户端发送请求到服务器会被controller接收到,然后在上面获取到了数据. 然后将这个数据添加到了Model中,最后返回”viewshomeindex”视图. 这个视图会被Thymeleaf引擎解析,然后将Model中的数据填充到视图中. 最后返回给客户端. 很好理解，当访问首页的时候，返回 “viewshomeindex” 视图，该视图是一个 Thymeleaf 页面。 不同的ControllerSpringMVC里面返回类型的区分主要基于下面: 控制器类型@Controller // 默认返回视图名称@RestController // = @Controller + @ResponseBody，直接返回JSON 方法级注解@GetMapping(/page)public String page(Model model) /* 返回视图 */ @ResponseBody@GetMapping(/api/data)public DataVo api() /* 返回JSON */ 内容协商机制GET /api/data.json // 返回JSONGET /api/data.xml // 返回XML 常见应用场景graph LRA[请求类型] -- B含.html/.ftl等后缀B --|是| C[返回视图模板]B --|否| D[返回JSON] 实际开发中建议:前后端分离项目统一使用@RestController服务端渲染页面使用@Controller混合模式可以通过produces属性明确指定： @GetMapping(value = /api/data, produces = application/json)public DataVo api() /* 返回JSON */ 第四步 编写Thymeleaf页面第四步，在 paicoding-ui 模块下新建 index.html 文件（文件名对应控制器中 index 方法返回的字符串）. 这里只截个图，简单解释一下里面用到 Thymeleaf 的几个关键点。 html lang=zh xmlns:th=http://www.thymeleaf.org 为 Thymeleaf 的命名空间，通过引入命名空间就可以在 HTML 文件中使用 Thymeleaf 标签语言，用关键字 “th”来标注。 th:replace: 用指定的模板片段替换当前的元素。比如 th:replace=components/layout/header :: head(~::title, ~, ~) 就是将当前的 div 元素替换为 components/layout/header 模板中的 head 片段。 th:text: 设置元素的文本内容。比如 th:text=$global.siteInfo.websiteName 就是将当前元素的文本内容设置为 global.siteInfo.websiteName 的值。 th:href: 设置元素的 href 属性。类似的还有 th:src，用于设置元素的 src 属性。 th:if: 设置元素的条件显示。如果表达式的值为 true，则显示元素，否则不显示。比如 th:if=$!#lists.isEmpty(vo.sideBarItems)，如果 vo.sideBarItems 不为空，就显示当前元素。 th:inline=javascript 表示下面的 script 标签中包含 Thymeleaf 表达式，需要进行处理。 第五步 测试第五步，启动主类，在浏览器地址栏里输入 http://127.0.0.1:8080/ 访问首页。 Spring Boot 是如何自动装配 Thymeleaf 的？Spring Boot 的核心功能就是自动配置，这在整合 Thymeleaf 时也不例外。当我们在项目依赖中加入 spring-boot-starter-thymeleaf 后，Spring Boot 就会自动配置 Thymeleaf。这包括创建 Thymeleaf 的模板解析器、模板引擎，以及视图解析器。 在 Spring Boot 的自动装配模块（spring-boot-autoconfigure）中，有一个名为 ThymeleafAutoConfiguration 的配置类，这个类负责 Thymeleaf 的自动配置。 大家可以扒开 ThymeleafAutoConfiguration 细致看一下，我这里帮大家梳理几个点： ①、@EnableConfigurationProperties(ThymeleafProperties.class): 启用 Thymeleaf 的配置属性类 ThymeleafProperties，这个类定义了 Thymeleaf 的各种配置项，如模板的位置、缓存策略等。 和我们前面提到的 Thymeleaf 配置项是呼应的。 ②、@ConditionalOnClass({ TemplateMode.class, SpringTemplateEngine.class }): 这个条件注解表示只有当类路径中存在 TemplateMode 类和 SpringTemplateEngine 类时，才会启用这个自动配置。 TemplateMode 是 Thymeleaf 的类。 SpringTemplateEngine 是 Spring 与 Thymeleaf 集成的类。 所以，这个配置类的启动条件是 Thymeleaf 和 Spring 都存在。 模版解析器在 ThymeleafAutoConfiguration 中，有一个名为 DefaultTemplateResolverConfiguration 的内部类，这个类负责创建 Thymeleaf 的模板解析器。 Thymeleaf 模板解析器负责解析模板文件，并将模板文件解析为 Thymeleaf 可以处理的内部结构。DefaultTemplateResolverConfiguration 模板解析器可以处理在 classpath:/templates/ 目录下的 .html 文件。 模板引擎模板引擎是 Thymeleaf 的核心，它负责执行模板的处理和渲染。在处理模板时，模板引擎会使用表达式执行引擎对 Thymeleaf 表达式进行求值，同时还会执行所有定义的处理器。 在 Thymeleaf 中，模板引擎的核心类是 org.thymeleaf.TemplateEngine。该类负责处理模板，并产生处理结果。SpringTemplateEngine 是 TemplateEngine 的子类，比如在 ThymeleafWebMvcConfiguration 和 ThymeleafWebFluxConfiguration 类中，SpringTemplateEngine 对象会被注入到 ThymeleafViewResolver 和 ThymeleafReactiveViewResolver 中。 TemplateEngineConfigurations.DefaultTemplateEngineConfiguration 和 TemplateEngineConfigurations.ReactiveTemplateEngineConfiguration 配置类负责具体的引擎配置。 视图解析器视图解析器的任务是将控制器方法返回的视图名解析为实际的视图对象。在 Spring Boot 自动配置的 Thymeleaf 中，视图名就是模板文件的名字。例如，视图名 “index” 对应的模板文件是 classpath:/templates/index.html。 在 ThymeleafAutoConfiguration 中，有一个名为 ThymeleafWebMvcConfiguration 的内部类，它内部又有一个内部类 ThymeleafViewResolverConfiguration，这个类负责创建 Thymeleaf 的视图解析器。 // 配置类注解，proxyBeanMethods=false表示不代理@Bean方法，优化启动性能@Configuration(proxyBeanMethods = false)static class ThymeleafViewResolverConfiguration // 定义Thymeleaf视图解析器Bean @Bean // 仅当容器中不存在名为thymeleafViewResolver的Bean时才创建 @ConditionalOnMissingBean(name = thymeleafViewResolver) ThymeleafViewResolver thymeleafViewResolver(ThymeleafProperties properties, SpringTemplateEngine templateEngine) ThymeleafViewResolver resolver = new ThymeleafViewResolver(); // 设置模板引擎 resolver.setTemplateEngine(templateEngine); // 从配置获取字符编码 resolver.setCharacterEncoding(properties.getEncoding().name()); // 组合内容类型和字符编码 resolver.setContentType( appendCharset(properties.getServlet().getContentType(), resolver.getCharacterEncoding())); // 设置是否在渲染过程中产生部分输出 resolver.setProducePartialOutputWhileProcessing( properties.getServlet().isProducePartialOutputWhileProcessing()); // 设置排除的视图名称 resolver.setExcludedViewNames(properties.getExcludedViewNames()); // 设置匹配的视图名称 resolver.setViewNames(properties.getViewNames()); // 设置解析器顺序(低优先级)，作为后备解析器 resolver.setOrder(Ordered.LOWEST_PRECEDENCE - 5); // 是否启用缓存 resolver.setCache(properties.isCache()); return resolver; 简单总结一下，Spring Boot通过ThymeleafAutoConfiguration类来自动装配Thymeleaf。该类定义了许多Bean（如SpringTemplateEngine，ThymeleafViewResolver等），并使用条件注解（如@ConditionalOnClass，@ConditionalOnMissingBean等）来实现条件装配。由于使用了@EnableConfigurationProperties(ThymeleafProperties.class)注解来注入和读取Thymeleaf相关属性，所以我们可以在 application.yml 中自定义 Thymeleaf 的配置项。 如何将后端数据返回给 Thymeleaf 呢？理解了 Thymeleaf 的自动装配机制后，我们再来思考一个问题：如果我们想把后端的数据传回给 Thymeleaf，该怎么办呢？ 其实也简单，我们可以通过 Model 的 addAttribute 方法来完成，比如说 model.addAttribute(vo, vo);。 这样就能将数据传回给 Thymeleaf 了。 那 Thymeleaf 页面中如何取出这些数据呢？在 HTML 中，可以通过 ${vo.xxxx} 的方式。 div th:replace=views/home/navbar/index :: navbar($vo.categories)/div 这一行代码表示，用模板文件viewshomenavbarindex中定义的navbar元素（可能是一个HTML的片段或整个元素）替换当前div标签。$vo.categories表示传递给navbar元素的参数。在模板文件viewshomenavbarindex中，这个参数可以使用Thymeleaf的表达式访问。 在 JavaScript 中，可以通过 const archiveId = [[$vo.yyyy]] 的方式。 script th:inline=javascript const archiveId = [[$vo.categoryId]] const category = [[$ vo.currentCategory ]] const params = category: category ? category : 全部, page: 2 /script [[$vo.categoryId]]是Thymeleaf的内联表达式，这表示在服务器端渲染页面时，这个表达式会被$vo.categoryId的值替代。 那如果是一些公共的属性，针对所有页面的全局信息，该怎么办呢？ 我们可以通过全局拦截器的方式，GlobalViewInterceptor 中的 postHandle（这个方法在请求处理完成后，但在视图渲染之前执行）来完成。 通过 modelAndView.getModel().put(global, globalInitService.globalAttr()); 来往视图中增加一些全局信息。 比如说把开发环境、测试环境还是生产环境放进来，然后通过 $global.env 的方式访问。 小结Spring Boot通过自动配置（Auto-configuration）可以轻松整合了Thymeleaf模板引擎。 只需要在项目中添加Spring Boot Thymeleaf Starter依赖，就能启用Thymeleaf的自动配置，该过程在ThymeleafAutoConfiguration类中实现。 自动配置会创建必要的SpringTemplateEngine，ThymeleafViewResolver等Bean。 在application.yml中，可以对Thymeleaf进行个性化配置，如模板的前缀和后缀，缓存策略等。 在Controller层，我们可以使用Model对象将数据传递到Thymeleaf中，并在模板中利用Thymeleaf的各种标签（如th:text，th:if等）进行数据渲染和交互逻辑处理。 如果需要一些全局信息，则可以通过拦截器来完成。 以上，尤其是 Thymeleaf 的自动装配和全局信息这里可以实操一下，会学到很多。","tags":["项目","模板引擎"],"categories":["项目笔记"]},{"title":"2025.8.2学习日记","path":"/2025/08/02/学习日记25年8月/2025.8.2学习笔记/","content":"今日学习内容3DGS标注数据 力扣每日一题一道困难题. java集合框架篇130 项目重新梳理了微信验证码登录的逻辑,基本已经清楚了.首先在TecHub网页点击登录按钮,前端会发送subscribe请求到后端,此时就会创建长连接,并且在后端进行缓存,通过设备号记录长连接和验证码.然后如果此时通过微信发送验证码之后,首先会通过微信平台将收到的消息转发到后端callback接口,然后在接口中对数据进行判断,如果收到的消息是验证码关键字,会进行判断,这个验证码是否对应了某个长连接,如果有的话就进行登录操作. 代码随想录生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"Java集合框架学习笔记","path":"/2025/08/01/Java问答笔记/Java集合框架学习笔记/","content":"引言1.🌟说说有哪些常见的集合框架？ 推荐阅读：二哥的 Java 进阶之路：Java 集合框架 推荐阅读：阻塞队列 BlockingQueue。 集合框架可以分为两条大的支线： ①、第一条支线 Collection，主要由 List、Set、Queue 组成： List 代表有序、可重复的集合，典型代表就是封装了动态数组的 ArrayList 和封装了链表的 LinkedList； Set 代表无序、不可重复的集合，典型代表就是 HashSet 和 TreeSet； Queue 代表队列，典型代表就是双端队列 ArrayDeque，以及优先级队列 PriorityQueue。 ②、第二条支线 Map，代表键值对的集合，典型代表就是 HashMap。 另外一个回答版本： ①、Collection 接口：最基本的集合框架表示方式，提供了添加、删除、清空等基本操作，它主要有三个子接口： List：一个有序的集合，可以包含重复的元素。实现类包括 ArrayList、LinkedList 等。 Set：一个不包含重复元素的集合。实现类包括 HashSet、LinkedHashSet、TreeSet 等。 Queue：一个用于保持元素队列的集合。实现类包括 PriorityQueue、ArrayDeque 等。 ②、Map 接口：表示键值对的集合，一个键映射到一个值。键不能重复，每个键只能对应一个值。Map 接口的实现类包括 HashMap、LinkedHashMap、TreeMap 等。 集合框架有哪几个常用工具类？集合框架位于 java.util 包下，提供了两个常用的工具类： Collections：提供了一些对集合进行排序、二分查找、同步的静态方法。 Arrays：提供了一些对数组进行排序、打印、和 List 进行转换的静态方法。 简单介绍一下队列Java 中的队列主要通过 Queue 接口和并发包下的 BlockingQueue 两个接口来实现。 优先级队列 PriorityQueue 实现了 Queue 接口，是一个无界队列，它的元素按照自然顺序排序或者 Comparator 比较器进行排序。 双端队列 ArrayDeque 也实现了 Queue 接口，是一个基于数组的，可以在两端插入和删除元素的队列。 LinkedList 实现了 Queue 接口的子类 Deque，所以也可以当做双端队列来使用。 用过哪些集合类，它们的优劣？我常用的集合类有 ArrayList、LinkedList、HashMap、LinkedHashMap。 ArrayList 可以看作是一个动态数组，可以在需要时动态扩容数组的容量，只不过需要复制元素到新的数组。优点是访问速度快，可以通过索引直接查找到元素。缺点是插入和删除元素可能需要移动或者复制元素。 LinkedList 是一个双向链表，适合频繁的插入和删除操作。优点是插入和删除元素的时候只需要改变节点的前后指针，缺点是访问元素时需要遍历链表。 HashMap 是一个基于哈希表的键值对集合。优点是可以根据键的哈希值快速查找到值，但有可能会发生哈希冲突，并且不保留键值对的插入顺序。 LinkedHashMap 在 HashMap 的基础上增加了一个双向链表来保持键值对的插入顺序。 队列和栈的区别了解吗？队列是一种先进先出（FIFO, First-In-First-Out）的数据结构，第一个加入队列的元素会成为第一个被移除的元素。 栈是一种后进先出（LIFO, Last-In-First-Out）的数据结构，最后一个加入栈的元素会成为第一个被移除的元素。 哪些是线程安全的容器？像 Vector、Hashtable、ConcurrentHashMap、CopyOnWriteArrayList、ConcurrentLinkedQueue、ArrayBlockingQueue、LinkedBlockingQueue 都是线程安全的。 Collection 继承了哪些接口？Collection 继承了 Iterable 接口，这意味着所有实现 Collection 接口的类都必须实现 iterator() 方法，之后就可以使用增强型 for 循环遍历集合中的元素了。 ListList 推荐阅读文章 2.🌟ArrayList 和 LinkedList 有什么区别？推荐阅读：二哥的 Java 进阶之路：ArrayList 和 LinkedList ArrayList 是基于数组实现的，LinkedList 是基于链表实现的。 ArrayList 和 LinkedList 的用途有什么不同？多数情况下，ArrayList 更利于查找，LinkedList 更利于增删。 ①、由于 ArrayList 是基于数组实现的，所以 get(int index) 可以直接通过数组下标获取，时间复杂度是 O(1)；LinkedList 是基于链表实现的，get(int index) 需要遍历链表，时间复杂度是 O(n)。 当然，get(E element) 这种查找，两种集合都需要遍历通过 equals 比较获取元素，所以时间复杂度都是 O(n)。 ②、ArrayList 如果增删的是数组的尾部，时间复杂度是 O(1)；如果 add 的时候涉及到扩容，时间复杂度会上升到 O(n)。 但如果插入的是中间的位置，就需要把插入位置后的元素向前或者向后移动，甚至还有可能触发扩容，效率就会低很多，变成 O(n)。 LinkedList 因为是链表结构，插入和删除只需要改变前置节点、后置节点和插入节点的引用，因此不需要移动元素。 如果是在链表的头部插入或者删除，时间复杂度是 O(1)；如果是在链表的中间插入或者删除，时间复杂度是 O(n)，因为需要遍历链表找到插入位置；如果是在链表的尾部插入或者删除，时间复杂度是 O(1)。 ArrayList 和 LinkedList 是否支持随机访问？①、ArrayList 是基于数组的，也实现了 RandomAccess 接口，所以它支持随机访问，可以通过下标直接获取元素。 ②、LinkedList 是基于链表的，所以它没法根据下标直接获取元素，不支持随机访问。 ArrayList 和 LinkedList 内存占用有何不同？ArrayList 是基于数组的，是一块连续的内存空间，所以它的内存占用是比较紧凑的；但如果涉及到扩容，就会重新分配内存，空间是原来的 1.5 倍。 LinkedList 是基于链表的，每个节点都有一个指向下一个节点和上一个节点的引用，于是每个节点占用的内存空间比 ArrayList 稍微大一点。 ArrayList 和 LinkedList 的使用场景有什么不同？ArrayList 适用于： 随机访问频繁：需要频繁通过索引访问元素的场景。 读取操作远多于写入操作：如存储不经常改变的列表。 末尾添加元素：需要频繁在列表末尾添加元素的场景。 LinkedList 适用于： 频繁插入和删除：在列表中间频繁插入和删除元素的场景。 不需要快速随机访问：顺序访问多于随机访问的场景。 队列和栈：由于其双向链表的特性，LinkedList 可以实现队列（FIFO）和栈（LIFO）。 链表和数组有什么区别？ 数组在内存中占用的是一块连续的存储空间，因此我们可以通过数组下标快速访问任意元素。数组在创建时必须指定大小，一旦分配内存，数组的大小就固定了。 链表的元素存储在于内存中的任意位置，每个节点通过指针指向下一个节点。 3.ArrayList 的扩容机制了解吗？了解。当往 ArrayList 中添加元素时，会先检查是否需要扩容，如果当前容量+1 超过数组长度，就会进行扩容。 扩容后的新数组长度是原来的 1.5 倍，然后再把原数组的值拷贝到新数组中。 private void grow(int minCapacity) // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity 1); if (newCapacity - minCapacity 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); 4.ArrayList 怎么序列化的知道吗？在 ArrayList 中，writeObject 方法被重写了，用于自定义序列化逻辑：只序列化有效数据，因为 elementData 数组的容量一般大于实际的元素数量，声明的时候也加了 transient 关键字。 为什么 ArrayList 不直接序列化元素数组呢？出于效率的考虑，数组可能长度 100，但实际只用了 50，剩下的 50 没用到，也就不需要序列化。 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException // 将当前 ArrayList 的结构进行序列化 int expectedModCount = modCount; s.defaultWriteObject(); // 序列化非 transient 字段 // 序列化数组的大小 s.writeInt(size); // 序列化每个元素 for (int i = 0; i size; i++) s.writeObject(elementData[i]); // 检查是否在序列化期间发生了并发修改 if (modCount != expectedModCount) throw new ConcurrentModificationException(); 5.快速失败fail-fast了解吗？fail—fast 是 Java 集合的一种错误检测机制。 在用迭代器遍历集合对象时，如果线程 A 遍历过程中，线程 B 对集合对象的内容进行了修改，就会抛出 Concurrent Modification Exception。 迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用 hashNext()/next()遍历下一个元素之前，都会检测 modCount 变量是否为 expectedmodCount 值，是的话就返回遍历；否则抛出异常，终止遍历。 异常的抛出条件是检测到 modCount！=expectedmodCount 这个条件。如果集合发生变化时修改 modCount 值刚好又设置为了 expectedmodCount 值，则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的 bug。 java.util 包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改），比如 ArrayList 类。 什么是安全失败（fail—safe）呢？采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。 原理：由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发 Concurrent Modification Exception。 缺点：基于拷贝内容的优点是避免了 Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 场景：java.util.concurrent 包下的容器都是安全失败，可以在多线程下并发使用，并发修改，比如 CopyOnWriteArrayList 类。 6.有哪几种实现 ArrayList 线程安全的方法？常用的有两种。 可以使用 Collections.synchronizedList() 方法，它可以返回一个线程安全的 List。 SynchronizedList list = Collections.synchronizedList(new ArrayList()); 内部是通过 synchronized 关键字加锁来实现的。 也可以直接使用 CopyOnWriteArrayList，它是线程安全的 ArrayList，遵循写时复制的原则，每当对列表进行修改时，都会创建一个新副本，这个新副本会替换旧的列表，而对旧列表的所有读取操作仍然在原有的列表上进行。 CopyOnWriteArrayList list = new CopyOnWriteArrayList(); 通俗的讲，CopyOnWrite 就是当我们往一个容器添加元素的时候，不直接往容器中添加，而是先复制出一个新的容器，然后在新的容器里添加元素，添加完之后，再将原容器的引用指向新的容器。多个线程在读的时候，不需要加锁，因为当前容器不会添加任何元素。这样就实现了线程安全。 ArrayList 和 Vector 的区别？Vector 属于 JDK 1.0 时期的遗留类，不推荐使用，仍然保留着是因为 Java 希望向后兼容。 ArrayList 是在 JDK 1.2 时引入的，用于替代 Vector 作为主要的非同步动态数组实现。因为 Vector 所有的方法都使用了 synchronized 关键字进行同步，所以单线程环境下效率较低。 7.CopyOnWriteArrayList 了解多少？CopyOnWriteArrayList 就是线程安全版本的 ArrayList。 CopyOnWrite——写时复制，已经明示了它的原理。 CopyOnWriteArrayList 采用了一种读写分离的并发策略。CopyOnWriteArrayList 容器允许并发读，读操作是无锁的。至于写操作，比如说向容器中添加一个元素，首先将当前容器复制一份，然后在新副本上执行写操作，结束之后再将原容器的引用指向新容器。 MapMap 中最重要的就是 HashMap 了，面试基本被问出包浆了，一定要好好准备。 8.🌟能说一下 HashMap 的底层数据结构吗？推荐阅读：二哥的 Java 进阶之路：详解 HashMap JDK 8 中 HashMap 的数据结构是数组+链表+红黑树。 数组用来存储键值对，每个键值对可以通过索引直接拿到，索引是通过对键的哈希值进行进一步的 hash() 处理得到的。 当多个键经过哈希处理后得到相同的索引时，需要通过链表来解决哈希冲突——将具有相同索引的键值对通过链表存储起来。 不过，链表过长时，查询效率会比较低，于是当链表的长度超过 8 时（且数组的长度大于 64），链表就会转换为红黑树。红黑树的查询效率是 O(logn)，比链表的 O(n) 要快。 hash() 方法的目标是尽量减少哈希冲突，保证元素能够均匀地分布在数组的每个位置上。 static final int hash(Object key) int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h 16); 如果键的哈希值已经在数组中存在，其对应的值将被新值覆盖。 HashMap 的初始容量是 16，随着元素的不断添加，HashMap 就需要进行扩容，阈值是capacity * loadFactor，capacity 为容量，loadFactor 为负载因子，默认为 0.75。 扩容后的数组大小是原来的 2 倍，然后把原来的元素重新计算哈希值，放到新的数组中。 9.你对红黑树了解多少？红黑树是一种自平衡的二叉查找树： 每个节点要么是红色，要么是黑色； 根节点永远是黑色； 所有的叶子节点都是是黑色的（下图中的 NULL 节点）； 红色节点的子节点一定是黑色的； 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 红黑树口诀:左根右 , 根叶黑 .不红红 , 黑路同. 为什么不用二叉树？二叉树是最基本的树结构，每个节点最多有两个子节点，但是二叉树容易出现极端情况，比如插入的数据是有序的，那么二叉树就会退化成链表，查询效率就会变成 O(n)。 为什么不用平衡二叉树？平衡二叉树比红黑树的要求更高，每个节点的左右子树的高度最多相差 1，这种高度的平衡保证了极佳的查找效率，但在进行插入和删除操作时，可能需要频繁地进行旋转来维持树的平衡，维护成本更高。 为什么用红黑树？链表的查找时间复杂度是 O(n)，当链表长度较长时，查找性能会下降。红黑树是一种折中的方案，查找、插入、删除的时间复杂度都是 O(log n)。 10.红黑树怎么保持平衡的？旋转和染色。 ①、通过左旋和右旋来调整树的结构，避免某一侧过深。 ②、染⾊，修复红黑规则，从而保证树的高度不会失衡。 11.🌟HashMap 的 put 流程知道吗？哈希寻址 → 处理哈希冲突（链表还是红黑树）→ 判断是否需要扩容 → 插入覆盖节点。 详细版： 第一步，通过 hash 方法进一步扰动哈希值，以减少哈希冲突。 static final int hash(Object key) int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h 16); 第二步，进行第一次的数组扩容；并使用哈希值和数组长度进行取模运算，确定索引位置。 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length;if ((p = tab[i = (n - 1) hash]) == null) tab[i] = newNode(hash, key, value, null); 如果当前位置为空，直接将键值对插入该位置；否则判断当前位置的第一个节点是否与新节点的 key 相同，如果相同直接覆盖 value，如果不同，说明发生哈希冲突。 如果是链表，将新节点添加到链表的尾部；如果链表长度大于等于 8，则将链表转换为红黑树。 public V put(K key, V value) return putVal(hash(key), key, value, false, true);final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) NodeK,V[] tab; NodeK,V p; int n, i; // 如果 table 为空，先进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 计算索引位置，并找到对应的桶 if ((p = tab[i = (n - 1) hash]) == null) tab[i] = newNode(hash, key, value, null); // 如果桶为空，直接插入 else NodeK,V e; K k; // 检查第一个节点是否匹配 if (p.hash == hash ((k = p.key) == key || (key != null key.equals(k)))) e = p; // 覆盖 // 如果是树节点，放入树中 else if (p instanceof TreeNode) e = ((TreeNodeK,V)p).putTreeVal(this, tab, hash, key, value); // 如果是链表，遍历插入到尾部 else for (int binCount = 0; ; ++binCount) if ((e = p.next) == null) p.next = newNode(hash, key, value, null); // 如果链表长度达到阈值，转换为红黑树 if (binCount = TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; if (e.hash == hash ((k = e.key) == key || (key != null key.equals(k)))) break; // 覆盖 p = e; if (e != null) // 如果找到匹配的 key，则覆盖旧值 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; ++modCount; // 修改计数器 if (++size threshold) resize(); // 检查是否需要扩容 afterNodeInsertion(evict); return null; 每次插入新元素后，检查是否需要扩容，如果当前元素个数大于阈值（capacity * loadFactor），则进行扩容，扩容后的数组大小是原来的 2 倍；并且重新计算每个节点的索引，进行数据重新分布。 只重写元素的 equals 方法没重写 hashCode，put 的时候会发生什么?如果只重写 equals 方法，没有重写 hashCode 方法，那么会导致 equals 相等的两个对象，hashCode 不相等，这样的话，两个对象会被 put 到数组中不同的位置，导致 get 的时候，无法获取到正确的值。 12.HashMap 怎么查找元素的呢？通过哈希值定位索引 → 定位桶 → 检查第一个节点 → 遍历链表或红黑树查找 → 返回结果。 13.HashMap 的 hash 函数是怎么设计的?先拿到 key 的哈希值，是一个 32 位的 int 类型数值，然后再让哈希值的高 16 位和低 16 位进行异或操作，这样能保证哈希分布均匀。 static final int hash(Object key) int h; // 如果 key 为 null，返回 0；否则，使用 hashCode 并进行扰动 return (key == null) ? 0 : (h = key.hashCode()) ^ (h 16); 14.为什么 hash 函数能减少哈希冲突？快速回答：哈希表的索引是通过 h (n-1) 计算的，n 是底层数组的容量；n-1 和某个哈希值做 运算，相当于截取了最低的四位。如果数组的容量很小，只取 h 的低位很容易导致哈希冲突。 通过异或操作将 h 的高位引入低位，可以增加哈希值的随机性，从而减少哈希冲突。 解释一下。 以初始长度 16 为例，16-115。2 进制表示是0000 0000 0000 0000 0000 0000 0000 1111。只取最后 4 位相等于哈希值的高位都丢弃了。 比如说 1111 1111 1111 1111 1111 1111 1111 1111，取最后 4 位，也就是 1111。 1110 1111 1111 1111 1111 1111 1111 1111，取最后 4 位，也是 1111。 不就发生哈希冲突了吗？ 这时候 hash 函数 (h = key.hashCode()) ^ (h 16) 就派上用场了。 将哈希值无符号右移 16 位，意味着原哈希值的高 16 位被移到了低 16 位的位置。这样，原始哈希值的高 16 位和低 16 位就可以参与到最终用于索引计算的低位中。 选择 16 位是因为它是 32 位整数的一半，这样处理既考虑了高位的信息，又没有完全忽视低位原本的信息，从而达到了一种微妙的平衡状态。 举个例子（数组长度为 16）。 第一个键值对的键：h1 0001 0010 0011 0100 0101 0110 0111 1000 第二个键值对的键：h2 0001 0010 0011 0101 0101 0110 0111 1000 如果没有 hash 函数，直接取低 4 位，那么 h1 和 h2 的低 4 位都是 1000，也就是说两个键值对都会放在数组的第 8 个位置。 来看一下 hash 函数的处理过程。 ①、对于第一个键h1的计算： 原始: 0001 0010 0011 0100 0101 0110 0111 1000右移: 0000 0000 0000 0000 0001 0010 0011 0100异或: ---------------------------------------结果: 0001 0010 0011 0100 0100 0100 0100 1100 ②、对于第二个键h2的计算： 原始: 0001 0010 0011 0101 0101 0110 0111 1000右移: 0000 0000 0000 0000 0001 0010 0011 0101异或: ---------------------------------------结果: 0001 0010 0011 0101 0100 0100 0100 1101 通过上述计算，我们可以看到h1和h2经过h ^ (h 16)操作后得到了不同的结果。 现在，考虑数组长度为 16 时（需要最低 4 位来确定索引）： 对于h1的最低 4 位是1100（十进制中为 12）对于h2的最低 4 位是1101（十进制中为 13）这样，h1和h2就会被分别放在数组的第 12 个位置和第 13 个位置上，从而避免了哈希冲突。 15.为什么 HashMap 的容量是 2 的幂次方？是为了快速定位元素在底层数组中的下标。 HashMap 是通过 hash (n-1) 来定位元素下标的，n 为数组的大小，也就是 HashMap 底层数组的容量。 数组长度-1 正好相当于一个“低位掩码”——掩码的低位最好全是 1，这样 运算才有意义，否则结果一定是 0。 2 幂次方刚好是偶数，偶数-1 是奇数，奇数的二进制最后一位是 1，也就保证了 hash (length-1) 的最后一位可能为 0，也可能为 1（取决于 hash 的值），这样可以保证哈希值的均匀分布。 换句话说， 操作的结果就是将哈希值的高位全部归零，只保留低位值。 ab 的结果是：a、b 中对应位同时为 1，则结果为 1，否则为 0。例如 531，5 的二进制是 0101，3 的二进制是 0011，5300011。 假设某哈希值的二进制为 10100101 11000100 00100101，用它来做 运算，我们来看一下结果。 已知 HashMap 的初始长度为 16，16-115，二进制是 00000000 00000000 00001111（高位用 0 来补齐）： 10100101 11000100 00100101\t00000000 00000000 00001111---------------------------------- 00000000 00000000 00000101 因为 15 的高位全部是 0，所以 运算后的高位结果肯定也是 0，只剩下 4 个低位 0101，也就是十进制的 5。 这样，哈希值为 10100101 11000100 00100101 的键就会放在数组的第 5 个位置上。 对数组长度取模定位数组下标，这块有没有优化策略？快速回答：HashMap 的策略是将取模运算 hash % table.length 优化为位运算 hash (length - 1)。 因为当数组的长度是 2 的 N 次幂时，hash (length - 1) = hash % length。 比如说 9 % 4 = 1，9 的二进制是 1001，4 - 1 = 3，3 的二进制是 0011，9 3 = 1001 0011 = 0001 = 1。 再比如说 10 % 4 = 2，10 的二进制是 1010，4 - 1 = 3，3 的二进制是 0011，10 3 = 1010 0011 = 0010 = 2。 当数组的长度不是 2 的 n 次方时，hash % length 和 hash (length - 1) 的结果就不一致了。 比如说 7 % 3 = 1，7 的二进制是 0111，3 - 1 = 2，2 的二进制是 0010，7 2 = 0111 0010 = 0010 = 2。 1110 (hash = 14) 0111 (length - 1 = 7) ------------------------------ 0110 (结果 = 6) 保留 14 的最低 3 位，高位被清零。 从此，两个运算 hash % length 和 hash (length - 1) 有了完美的闭环。在计算机中，位运算的速度要远高于取余运算，因为计算机本质上就是二进制嘛。 说说什么是取模运算？在 Java 中，通常使用 % 运算符来表示取余，用 Math.floorMod() 来表示取模。 当操作数都是正数的话，取模运算和取余运算的结果是一样的；只有操作数出现负数的情况下，结果才会不同。 取模运算的商向负无穷靠近；取余运算的商向 0 靠近。这是导致它们两个在处理有负数情况下，结果不同的根本原因。 当数组的长度是 2 的 n 次幂时，取模运算/取余运算可以用位运算来代替，效率更高，毕竟计算机本身只认二进制。 比如说，7 对 3 取余，和 7 对 3 取模，结果都是 1。因为两者都是基于除法运算的，7 3 的商是 2，余数是 1。 对于 HashMap 来说，它需要通过 hash % table.length 来确定元素在数组中的位置。 比如说，数组长度是 3，hash 是 7，那么 7 % 3 的结果就是 1，也就是此时可以把元素放在下标为 1 的位置。 当 hash 是 8，8 % 3 的结果就是 2，也就是可以把元素放在下标为 2 的位置。 当 hash 是 9，9 % 3 的结果就是 0，也就是可以把元素放在下标为 0 的位置上。 是不是很奇妙，数组的大小为 3，刚好 3 个位置都利用上了。 16.如果初始化 HashMap，传一个 17 的容量，它会怎么处理？HashMap 会将容量调整到大于等于 17 的最小的 2 的幂次方，也就是 32。 这是因为哈希表的大小最好是 2 的 N 次幂，这样可以通过 (n - 1) hash 高效计算出索引值。 解释一下。 在 HashMap 的初始化构造方法中，有这样⼀段代码： public HashMap(int initialCapacity, float loadFactor) ... this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); 阀值 threshold 会通过⽅法 tableSizeFor() 进⾏计算。 static final int tableSizeFor(int cap) int n = cap - 1; n |= n 1; n |= n 2; n |= n 4; n |= n 8; n |= n 16; return (n 0) ? 1 : (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; ①、int n = cap - 1; 避免刚好是 2 的幂次方时，容量直接翻倍。 ②、接下来通过不断右移（）并与自身进行或运算（|=），将 n 的二进制表示中的所有低位设置为 1。 n |= n 1; 将最高位的 1 扩展到下一位。 n |= n 2; 扩展到后两位。 依此类推，直到 n |= n 16;，扩展到后十六位，这样从最高位的 1 到最低位，就都变成了 1。 ③、如果 n 小于 0，说明 cap 是负数，直接返回 1。 如果 n 大于或等于 MAXIMUM_CAPACITY（通常是1 30），则返回 MAXIMUM_CAPACITY。 否则，返回 n + 1，这是因为 n 的所有低位都是 1，所以 n + 1 就是大于 cap 的最小的 2 的幂次方。 初始化 HashMap 的时候需要传入容量吗？如果预先知道 Map 将存储大量键值对，提前指定一个足够大的初始容量可以减少因扩容导致的重哈希操作。 因为每次扩容时，HashMap 需要将现有的元素插入到新的数组中，这个过程相对耗时，尤其是当 Map 中已有大量数据时。 当然了，过大的初始容量会浪费内存，特别是当实际存储的元素远少于初始容量时。如果不指定初始容量，HashMap 将使用默认的初始容量 16。 17.你还知道哪些哈希函数的构造方法呢？①、除留取余法：H(key)=key%p(p=N)，关键字除以一个不大于哈希表长度的正整数 p，所得余数为地址，当然 HashMap 里进行了优化改造，效率更高，散列也更均衡。 除此之外，还有这几种常见的哈希函数构造方法： ②、直接定址法：直接根据key来映射到对应的数组位置，例如 1232 放到下标 1232 的位置。 ③、数字分析法：取key的某些数字（例如十位和百位）作为映射的位置 ④、平方取中法：取key平方的中间几位作为映射的位置 ⑤、将key分割成位数相同的几段，然后把它们的叠加和作为映射的位置。 18.解决哈希冲突的方法有哪些？简版回答：我知道的有 3 种，再哈希法、开放地址法和拉链法。 什么是再哈希法？准备两套哈希算法，当发生哈希冲突的时候，使用另外一种哈希算法，直到找到空槽为止。对哈希算法的设计要求比较高。 什么是开放地址法？遇到哈希冲突的时候，就去寻找下一个空的槽。有 3 种方法： 什么是拉链法？也就是链地址法，当发生哈希冲突的时候，使用链表将冲突的元素串起来。HashMap 采用的正是拉链法。 怎么判断 key 相等呢？依赖于key的equals()方法和hashCode()方法。 if (e.hash == hash ((k = e.key) == key || (key != null key.equals(k)))) ①、hashCode() ：使用key的hashCode()方法计算key的哈希码。 ②、equals() ：当两个key的哈希码相同时，HashMap还会调用key的equals()方法进行精确比较。只有当equals()方法返回true时，两个key才被认为是完全相同的。 如果两个key的引用指向了同一个对象，那么它们的hashCode()和equals()方法都会返回true，所以在 equals 判断之前可以先使用==运算符判断一次。 19.为什么 HashMap 链表转红黑树的阈值为 8 呢？树化发生在 table 数组的长度大于 64，且链表的长度大于 8 的时候。 为什么是 8 呢？源码的注释也给出了答案。 红黑树节点的大小大概是普通节点大小的两倍，所以转红黑树，牺牲了空间换时间，更多的是一种兜底的策略，保证极端情况下的查找效率。 阈值为什么要选 8 呢？和统计学有关。理想情况下，使用随机哈希码，链表里的节点符合泊松分布，出现节点个数的概率是递减的，节点个数为 8 的情况，发生概率仅为0.00000006。 至于红黑树转回链表的阈值为什么是 6，而不是 8？是因为如果这个阈值也设置成 8，假如发生碰撞，节点增减刚好在 8 附近，会发生链表和红黑树的不断转换，导致资源浪费。 20.HashMap扩容发生在什么时候呢？当键值对数量超过阈值，也就是容量 * 负载因子时。 默认的负载因子是多少？0.75。 初始容量是多少？16。 1 左移 4 位，0000 0001 → 0001 0000，也就是 2 的 4 次方。 static final int DEFAULT_INITIAL_CAPACITY = 1 4; // aka 16 为什么使用 1 4 而不是直接写 16？写 1 4 主要是为了强调这个值是 2 的幂次方，而不是一个完全随机的选择。 无论 HashMap 是否扩容，其底层的数组长度都应该是 2 的幂次方，因为这样可以通过位运算快速计算出元素的索引。 为什么选择 0.75 作为 HashMap 的默认负载因子呢？这是一个经验值。如果设置得太低，如 0.5，会浪费空间；如果设置得太高，如 0.9，会增加哈希冲突。 0.75 是 JDK 作者经过大量验证后得出的最优解，能够最大限度减少 rehash 的次数。 21.🌟HashMap的扩容机制了解吗？扩容时，HashMap 会创建一个新的数组，其容量是原来的两倍。然后遍历旧哈希表中的元素，将其重新分配到新的哈希表中。 如果当前桶中只有一个元素，那么直接通过键的哈希值与数组大小取模锁定新的索引位置：e.hash (newCap - 1)。 如果当前桶是红黑树，那么会调用 split() 方法分裂树节点，以保证树的平衡。 如果当前桶是链表，会通过旧键的哈希值与旧的数组大小取模 (e.hash oldCap) == 0 来作为判断条件，如果条件为真，元素保留在原索引的位置；否则元素移动到原索引 + 旧数组大小的位置。 JDK 7 扩容的时候有什么问题？JDK 7 在扩容的时候使用头插法来重新插入链表节点，这样会导致链表无法保持原有的顺序。 详细解释一下。 JDK 7 是通过哈希值与数组大小-1 进行与运算确定元素下标的。 static int indexFor(int h, int length) return h (length-1); 我们来假设： 数组 table 的长度为 2 键的哈希值为 3、7、5 取模运算后，键发生了哈希冲突，它们都需要放到 table[1] 的桶上。那么扩容前就是这个样子： 假设负载因子 loadFactor 为 1，也就是当元素的个数大于 table 的长度时进行扩容。 扩容后的数组容量为 4。 key 3 取模（3%4）后是 3，放在 table[3] 上。 key 7 取模（7%4）后是 3，放在 table[3] 上的链表头部。 key 5 取模（5%4）后是 1，放在 table[1] 上。 可以看到，由于 JDK 采用的是头插法，7 跑到 3 的前面了，原来的顺序是 3、7、5，7 在 3 的后面。 for (EntryK,V e : oldTable) while (null != e) EntryK,V next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; 最好的情况就是，扩容后的 7 还在 3 的后面，保持原来的顺序。 JDK 8 是怎么解决这个问题的？JDK 8 改用了尾插法，并且当 (e.hash oldCap) == 0 时，元素保留在原索引的位置；否则元素移动到原索引 + 旧数组大小的位置。 NodeK,V loHead = null, loTail = null;NodeK,V hiHead = null, hiTail = null;NodeK,V next;do next = e.next; if ((e.hash oldCap) == 0) if (loTail == null) loHead = e; else loTail.next = e; loTail = e; else if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; while ((e = next) != null);if (loHead != null) newTab[j] = loHead;if (hiHead != null) newTab[j + oldCap] = hiHead; 由于扩容时，数组长度会翻倍，例如：16 → 32， 因此，新数组的索引范围是原索引范围的两倍。 原索引 index = (n - 1) hash，扩容后的新索引就是 index = (2n - 1) hash。 也就是说，如果 (e.hash oldCap) == 0，元素在新数组中的位置与旧位置相同；否则，元素在新数组中的位置是旧位置 + 旧数组大小。 这样可以避免重新计算所有元素的哈希值，只需检查高位的某一位，就可以快速确定新位置。 扩容的时候每个节点都要进行位运算吗？不需要。HashMap 会通过 (e.hash oldCap) 来判断节点是否需要移动，0 的话保留原索引；1 才需要移动到新索引（原索引 + oldCap）。 这样就避免了 hashCode 的重新计算，大大提升了扩容的性能。 所以，哪怕有几十万条数据，可能只有一半的数据才需要移动到新位置。另外，位运算的计算速度非常快，因此，尽管扩容操作涉及到遍历整个哈希表并对每个节点进行判断，但这部分操作的计算成本是相对较低的。 22.JDK 8 对 HashMap 做了哪些优化呢？①、底层数据结构由数组 + 链表改成了数组 + 链表或红黑树的结构。 如果多个键映射到了同一个哈希值，链表会变得很长，在最坏的情况下，当所有的键都映射到同一个桶中时，性能会退化到 O(n)，而红黑树的时间复杂度是 O(logn)。 ②、链表的插入方式由头插法改为了尾插法。头插法在扩容后容易改变原来链表的顺序。 ③、扩容的时机由插入时判断改为插入后判断，这样可以避免在每次插入时都进行不必要的扩容检查，因为有可能插入后仍然不需要扩容。 ④、哈希扰动算法也进行了优化。JDK 7 是通过多次移位和异或运算来实现的。 JDK 8 让 hash 值的高 16 位和低 16 位进行了异或运算，让高位的信息也能参与到低位的计算中，这样可以极大程度上减少哈希碰撞。 23.你能自己设计实现一个 HashMap 吗？(手撕HashMap)可以，我先说一下整体的设计思路： 第一步，实现一个 hash 函数，对键的 hashCode 进行扰动。 第二步，实现一个拉链法的方法来解决哈希冲突。 第三步，扩容后，重新计算哈希值，将元素放到新的数组中。 完整代码： 24.🌟HashMap 是线程安全的吗？推荐阅读：HashMap 详解 HashMap 不是线程安全的，主要有以下几个问题： ①、多线程下扩容会死循环。JDK7 中的 HashMap 使用的是头插法来处理链表，在多线程环境下扩容会出现环形链表，造成死循环。 不过，JDK 8 时通过尾插法修复了这个问题，扩容时会保持链表原来的顺序。 ②、多线程在进行 put 元素的时候，可能会导致元素丢失。因为计算出来的位置可能会被其他线程覆盖掉，比如说一个县城 put 3 的时候，另外一个线程 put 了 7，就把 3 给弄丢了。 ③、put 和 get 并发时，可能导致 get 为 null。线程 1 执行 put 时，因为元素个数超出阈值而扩容，线程 2 此时执行 get，就有可能出现这个问题。 因为线程 1 执行完 table newTab 之后，线程 2 中的 table 已经发生了改变，比如说索引 3 的键值对移动到了索引 7 的位置，此时线程 2 去 get 索引 3 的元素就 get 不到了。 25.🌟怎么解决 HashMap 线程不安全的问题呢？在早期的 JDK 版本中，可以用 Hashtable 来保证线程安全。Hashtable 在方法上加了 synchronized 关键字。 另外，可以通过 Collections.synchronizedMap 方法返回一个线程安全的 Map，内部是通过 synchronized 对象锁来保证线程安全的，比在方法上直接加 synchronized 关键字更轻量级。 更优雅的解决方案是使用并发工具包下的 ConcurrentHashMap，使用了CAS+ synchronized 关键字来保证线程安全。(分段锁+CAS) 26.HashMap 内部节点是有序的吗？无序的，根据 hash 值随机插入。 27.讲讲 LinkedHashMap 怎么实现有序的？LinkedHashMap 在 HashMap 的基础上维护了一个双向链表，通过 before 和 after 标识前置节点和后置节点。 从而实现插入的顺序或访问顺序。 28.讲讲 TreeMap 怎么实现有序的？TreeMap 通过 key 的比较器来决定元素的顺序，如果没有指定比较器，那么 key 必须实现 Comparable 接口。 TreeMap 的底层是红黑树，红黑树是一种自平衡的二叉查找树，每个节点都大于其左子树中的任何节点，小于其右子节点树种的任何节点。 插入或者删除元素时通过旋转和染色来保持树的平衡。 查找的时候从根节点开始，利用二叉查找树的特点，逐步向左子树或者右子树递归查找，直到找到目标元素。 29.TreeMap 和 HashMap 的区别①、HashMap 是基于数组+链表+红黑树实现的，put 元素的时候会先计算 key 的哈希值，然后通过哈希值计算出元素在数组中的存放下标，然后将元素插入到指定的位置，如果发生哈希冲突，会使用链表来解决，如果链表长度大于 8，会转换为红黑树。 ②、TreeMap 是基于红黑树实现的，put 元素的时候会先判断根节点是否为空，如果为空，直接插入到根节点，如果不为空，会通过 key 的比较器来判断元素应该插入到左子树还是右子树。 在没有发生哈希冲突的情况下，HashMap 的查找效率是 O(1)。适用于查找操作比较频繁的场景。 TreeMap 的查找效率是 O(logn)。并且保证了元素的顺序，因此适用于需要大量范围查找或者有序遍历的场景。 Set30.讲讲 HashSet 的底层实现？HashSet 是由 HashMap 实现的，只不过值由一个固定的 Object 对象填充，而键用于操作。 public class HashSetE extends AbstractSetE implements SetE, Cloneable, java.io.Serializable static final long serialVersionUID = -5024744406713321676L; private transient HashMapE,Object map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); // …… 实际开发中，HashSet 并不常用，比如，如果我们需要按照顺序存储一组元素，那么 ArrayList 和 LinkedList 更适合；如果我们需要存储键值对并根据键进行查找，那么 HashMap 可能更适合。 HashSet 主要用于去重，比如，我们需要统计一篇文章中有多少个不重复的单词，就可以使用 HashSet 来实现。 // 创建一个 HashSet 对象HashSetString set = new HashSet();// 添加元素set.add(沉默);set.add(王二);set.add(陈清扬);set.add(沉默);// 输出 HashSet 的元素个数System.out.println(HashSet size: + set.size()); // output: 3// 遍历 HashSetfor (String s : set) System.out.println(s); HashSet 会自动去重，因为它是用 HashMap 实现的，HashMap 的键是唯一的，相同键会覆盖掉原来的键，于是第二次 add 一个相同键的元素会直接覆盖掉第一次的键。 HashSet 和 ArrayList 的区别ArrayList 是基于动态数组实现的，HashSet 是基于 HashMap 实现的。 ArrayList 允许重复元素和 null 值，可以有多个相同的元素；HashSet 保证每个元素唯一，不允许重复元素，基于元素的 hashCode 和 equals 方法来确定元素的唯一性。 ArrayList 保持元素的插入顺序，可以通过索引访问元素；HashSet 不保证元素的顺序，元素的存储顺序依赖于哈希算法，并且可能随着元素的添加或删除而改变。 HashSet 怎么判断元素重复，重复了是否 putHashSet 的 add 方法是通过调用 HashMap 的 put 方法实现的： public boolean add(E e) return map.put(e, PRESENT)==null; 所以 HashSet 判断元素重复的逻辑底层依然是 HashMap 的底层逻辑： HashMap 在插入元素时，通常需要三步： 第一步，通过 hash 方法计算 key 的哈希值。 static final int hash(Object key) int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h 16); 第二步，数组进行第一次扩容。 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; 第三步，根据哈希值计算 key 在数组中的下标，如果对应下标正好没有存放数据，则直接插入。 if ((p = tab[i = (n - 1) hash]) == null) tab[i] = newNode(hash, key, value, null); 如果对应下标已经有数据了，就需要判断是否为相同的 key，是则覆盖 value，否则需要判断是否为树节点，是则向树中插入节点，否则向链表中插入数据。 else NodeK,V e; K k; if (p.hash == hash ((k = p.key) == key || (key != null key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNodeK,V)p).putTreeVal(this, tab, hash, key, value); else for (int binCount = 0; ; ++binCount) if ((e = p.next) == null) p.next = newNode(hash, key, value, null); if (binCount = TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; if (e.hash == hash ((k = e.key) == key || (key != null key.equals(k)))) break; p = e; 也就是说，HashSet 通过元素的哈希值来判断元素是否重复，如果重复了，会覆盖原来的值。 if (e != null) // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue;","tags":["基础","Java"],"categories":["Java问答笔记"]},{"title":"2025.8.1学习日记","path":"/2025/08/01/学习日记25年8月/2025.8.1学习笔记/","content":"今日学习内容3DGS标注数据 力扣每日一题一道dp杨辉三角简单题.补签了三天的题. java集合框架篇130 项目实现了微信扫码输入验证码的登录方式.服务号的二维码: 代码随想录六道dp题. 生活篇晚上健身","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-08"]},{"title":"2025.7.31学习日记","path":"/2025/07/31/学习日记25年7月/2025.7.31学习笔记/","content":"今日学习内容3DGS做明天汇报的PPT 力扣每日一题一道选或不选的题目. JavaSE八股5656 完结 Redis八股057 项目实现了微信公众号和服务器后端的连接,然后可以实现自动回复. 代码随想录六道dp题. 生活篇晚上健身主要是练腹,然后做拉伸,强度很低.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.30学习日记","path":"/2025/07/30/学习日记25年7月/2025.7.30学习笔记/","content":"今日学习内容3DGS做明天汇报的PPT 力扣每日一题一道选或不选的题目. JavaSE八股3956 Redis八股057 代码随想录生活篇晚上健身练背,强度适中.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"Natapp内网穿透使用","path":"/2025/07/30/项目笔记/Natapp/","content":"由于需要使用微信服务号来转发请求,所以需要将后端的接口暴露出来,但原本在dev本地进行开发,所以就需要使用内网穿透的方式来实现. Natapp项目地址 首先进行注册,然后下载对应的客户端.我下载的是Mac的客户端,下载完成后,cd到下载(~Downloads)的目录,执行以下命令: chmod +x natapp 然后通过以下命令启动: ./natapp -authtoken=xxxx 然后在界面中配置隧道,然后选择好本地的端口(可以再次更改).但是配置好后,还需要一个二级域名,所以还需要配置二级域名.然后启动后可以看到可以正常连接到后端服务了.我的域名是:http://techub.natapp1.cchttps://natapp.cn/tunnel/lists","tags":["项目","内网穿透"],"categories":["项目笔记"]},{"title":"2025.7.29学习日记","path":"/2025/07/29/学习日记25年7月/2025.7.29学习笔记/","content":"今日学习内容继续做汇报PPT,今天状态好一些了. 这周汇报的CityGaussian–实时大场景高质量的3DGS。 文章提出的问题是，目前大多数的3DGS都专注在小型场景或者物体的重建上，然而当重建的规模尺度变大之后，现有的3DGS方法会出现显存爆炸并且导致渲染时卡顿,性能出现非常严重的下降。 所以这篇文章针对上述问题提出了针对大规模场景的3dgs方法，文章的核心思想是提出了一种有效的分治策略，以并行方式重建大规模3DGS。利用所提出的LoD策略也就是细节的分级处理，以最小的质量损失实现了不同尺度下的实时大规模场景渲染。并且目前在测试中效果优于目前最好的算法。 这个就是这篇文章训练的流程图：CityGaussian首先会训练一个低分辨率的全局先验，也就是图中蓝色的点云，为后续的分块作为一个原始的依据,然后对边界进行约束.然后第二步，将整个场景分成多个区块，分块大小根据显存大小动态调整，并且对每个分块进行负载均衡。然后第三步，对数据进行动态分配，包括动态边界,剔除噪声点第四步是每个区块进行并行的训练。第五步是将所有分块融合输出。 首先这个就是预先训练的低分辨率全局图,直接利用全部的观测图训练colmap点进行30000次迭代,然后生成一个全局几何分布的粗略的描述. 然后就是对边界进行约束,图中粉色框对应单位立方体的边界，其内部点云为需精细化处理的前景.然后根据前景的尺度生成单位立方体,将立方体外的点云进行非线性压缩,最终使前景的点云分布在0到1的范围,而其他点云则压缩在1到2的范围.然后就得到了右侧进行边界约束的图. 然后是微调阶段,在这时会根据如图这两个公式,来判断需要激活哪几个区块,对需要激活的区块进行训练.B1代表的是通过计算移除该区块前后的SSIM指标的差异，判断该区块是否显著影响当前视角的渲染质量.然后B2代表通过相机所在位置,检查相机位置是否位于区块的预设3D边界内. 然后是渲染阶段,会将场景进行细节分级,每个区块都会有三个不同细节程度的层级,对于更近的区块,使用更加细致的层级,对于更选的区块,选择更加粗略的区块.并且由于每个区块的边界并不是固定的,并且在训练过程中,会出现一些噪点成为区块的边界,所以会通过下面的公式来去除这些误差.这个MAD代表中位数绝对偏差,然后第二行修正后最小边界,剔除远低于正常范围的异常值.然后最大上界同理. 然后这是渲染的效果图,可以看到在这种大场景下,本文的方法实现了很好的渲染效果. 可以看到在不同测试数据下,相比原版3DGS,本文的方法的渲染质量都有了很大的提升. 这个图标代表使用不同的层次策略下,渲染速度的对比.红色折线是本文的多层级渲染策略,可以保证质量的同时保持较高的渲染速度. 上面的表是文章做的消融实验,第三行是不使用边界收缩的情况下,第四行是没有去除边界噪声的情况,这两个部分对质量都有影响.下面的表格是不同的分块策略对渲染质量的影响,可以看到区块越大,质量越高,但是渲染速度越慢. 3DGS做明天汇报的PPT 力扣每日一题一道选或不选的题目. JavaSE八股3956 Redis八股057 代码随想录生活篇","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.28学习日记","path":"/2025/07/28/学习日记25年7月/2025.7.28学习笔记/","content":"今日学习内容3DGS做明天汇报的PPT 力扣每日一题dfs选或不选 JavaSE八股3956 Redis八股057 代码随想录生活篇坏肚子了,今天效率很低,休息为主吧.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.26学习日记","path":"/2025/07/26/学习日记25年7月/2025.7.26学习笔记/","content":"今日学习内容3DGS力扣每日一题JavaSE八股3756 Redis八股057 项目篇注册了一个微信服务号,用于扫码登录. 代码随想录","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"Go基础笔记","path":"/2025/07/25/Go基础/Go基础笔记/","content":"https://gobyexample-cn.github.io/","tags":["基础","Go"],"categories":["Go基础"]},{"title":"2025.7.25学习日记","path":"/2025/07/25/学习日记25年7月/2025.7.25学习笔记/","content":"今日学习内容3DGS今天主要学习CityGaussianV2论文.CityGaussianV2主要是在V1的基础上进行改进.1.几何重建引擎 将3D椭球压扁为2D圆盘（surfel）新增法向量约束：强制所有圆盘朝向建筑表面深度一致性损失：用Depth-Anything V2提供的深度图监督. 2.动态训练系统实时监测高斯点长宽比：η min(su,sv)max(su,sv)当η 0.2时冻结该点分裂能力有效防止”芝麻粒”现象 3.高效压缩管线特征蒸馏：相邻4点融合为1点球形谐波降维：SH系数从48维→27维向量树压缩：建立8192级视觉字典 这篇V2属于是在V1基础上,专门对城市级别的大规模场景进行的针对性优化,对于其他别的场景估计效果不会很理想,但他的压缩方法还是可以借鉴一部分的. 力扣每日一题一道简单题,遍历即可. JavaSE八股3256 Redis八股057 项目篇学习扫码登录方面的知识,然后主要在配置环境,然后做了一个扫码登录的简单demo. 算法做了两道dp困难题.整理了灵神的题单.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"微信扫码登录","path":"/2025/07/25/项目笔记/微信扫码登录/","content":"整体业务流程关系如图:","tags":["项目","TecHub"],"categories":["项目笔记"]},{"title":"灵茶山算法基础笔记","path":"/2025/07/25/算法笔记/灵茶山算法基础笔记/","content":"转载自大佬整理的题单,太强了🙌 : 灵茶山艾府 基础算法精讲·题目汇总大家好，我是 灵茶山艾府。 我制作了一系列算法教学视频，整理成合集【基础算法精讲】。以下是合集中的视频链接、配套题目和代码，代码包含 PythonJavaC++Go 等多种语言。 制作不易，欢迎点赞，也欢迎转发给你的朋友或刷题群！ 视频精讲 题目 代码 备注 相向双指针 1 167. 两数之和 II - 输入有序数组 代码 15. 三数之和 代码 包含两个优化 2824. 统计和小于目标的下标对数目 代码 *课后作业 16. 最接近的三数之和 代码 *课后作业 18. 四数之和 代码 *课后作业 611. 有效三角形的个数 代码 *课后作业 相向双指针 2 11. 盛最多水的容器 代码 42. 接雨水 代码 额外讲了前后缀分解 125. 验证回文串 代码 *课后作业 2105. 给植物浇水 II 代码 *课后作业 滑动窗口 209. 长度最小的子数组 代码 最短 3. 无重复字符的最长子串 代码 最长 713. 乘积小于 K 的子数组 代码 方案数 2958. 最多 K 个重复元素的最长子数组 代码 *课后作业 2730. 找到最长的半重复子字符串 代码 *课后作业 2779. 数组的最大美丽值 代码 *课后作业 1004. 最大连续 1 的个数 III 代码 *课后作业 2962. 统计最大元素出现至少 K 次的子数组 代码 *课后作业 2302. 统计得分小于 K 的子数组数目 代码 *课后作业 1658. 将 x 减到 0 的最小操作数 代码 *课后作业 1234. 替换子串得到平衡字符串 代码 *课后作业 76. 最小覆盖子串 代码 *课后作业 二分查找 34. 在排序数组中查找元素的第一个和最后一个位置 代码 三种写法 2529. 正整数和负整数的最大计数 代码 *课后作业 2300. 咒语和药水的成功对数 代码 *课后作业 2563. 统计公平数对的数目 代码 *课后作业 2080. 区间内查询数字的频率 代码 *课后作业 275. H 指数 II 代码 *课后作业 875. 爱吃香蕉的珂珂 代码 *课后作业 2187. 完成旅途的最少时间 代码 *课后作业 2861. 最大合金数 代码 *课后作业 2439. 最小化数组中的最大值 代码 *课后作业 2517. 礼盒的最大甜蜜度 代码 *课后作业 二分查找 - 变形 162. 寻找峰值 代码 153. 寻找旋转排序数组中的最小值 代码 33. 搜索旋转排序数组 代码 两种方法 74. 搜索二维矩阵 代码 *课后作业 1901. 寻找峰值 II 代码 *课后作业 154. 寻找旋转排序数组中的最小值 II 代码 *课后作业 链表 - 反转系列 206. 反转链表 代码 92. 反转链表 II 代码 25. K 个一组翻转链表 代码 24. 两两交换链表中的节点 代码 *课后作业 445. 两数相加 II 代码 *课后作业 2816. 翻倍以链表形式表示的数字 代码 *课后作业 链表 - 快慢指针 876. 链表的中间结点 代码 141. 环形链表 代码 142. 环形链表 II 代码 143. 重排链表 代码 234. 回文链表 代码 *课后作业 链表 - 删除系列 237. 删除链表中的节点 代码 脑筋急转弯 19. 删除链表的倒数第 N 个结点 代码 前后指针 83. 删除排序链表中的重复元素 代码 82. 删除排序链表中的重复元素 II 代码 203. 移除链表元素 代码 *课后作业 3217. 从链表中移除在数组中存在的节点 代码 *课后作业 2487. 从链表中移除节点 代码 *课后作业 二叉树与递归 - 深入理解 104. 二叉树的最大深度 代码 两种方法 111. 二叉树的最小深度 代码 *课后作业 112. 路径总和 代码 *课后作业 129. 求根节点到叶节点数字之和 代码 *课后作业 1448. 统计二叉树中好节点的数目 代码 *课后作业 987. 二叉树的垂序遍历 代码 *课后作业 二叉树与递归 - 灵活运用 100. 相同的树 代码 101. 对称二叉树 代码 110. 平衡二叉树 代码 199. 二叉树的右视图 代码 226. 翻转二叉树 代码 *课后作业 617. 合并二叉树 代码 *课后作业 1026. 节点与其祖先之间的最大差值 代码 *课后作业 1080. 根到叶路径上的不足节点 代码 *课后作业 二叉树与递归 - 前序中序后序 98. 验证二叉搜索树 代码 三种方法 938. 二叉搜索树的范围和 代码 *课后作业 2476. 二叉搜索树最近节点查询 代码 *课后作业 230. 二叉搜索树中第 K 小的元素 代码 *课后作业 1373. 二叉搜索子树的最大键值和 代码 *课后作业 105. 从前序与中序遍历序列构造二叉树 代码 *课后作业 106. 从中序与后序遍历序列构造二叉树 代码 *课后作业 889. 根据前序和后序遍历构造二叉树 代码 *课后作业 1110. 删点成林 代码 *课后作业 二叉树与递归 - 最近公共祖先 236. 二叉树的最近公共祖先 代码 235. 二叉搜索树的最近公共祖先 代码 1123. 最深叶节点的最近公共祖先 代码 *课后作业 二叉树 - BFS 102. 二叉树的层序遍历 代码 两种写法 103. 二叉树的锯齿形层序遍历 代码 两种写法 513. 找树左下角的值 代码 107. 二叉树的层序遍历 II 代码 *课后作业 116. 填充每个节点的下一个右侧节点指针 代码 *课后作业 117. 填充每个节点的下一个右侧节点指针 II 代码 *课后作业 2415. 反转二叉树的奇数层 代码 *课后作业 2641. 二叉树的堂兄弟节点 II 代码 *课后作业 回溯 - 子集型 17. 电话号码的字母组合 代码 引入回溯概念用 78. 子集 代码 两种写法 131. 分割回文串 代码 两种写法 2698. 求一个整数的惩罚数 代码 *课后作业 回溯 - 组合型与剪枝 77. 组合 代码 两种写法 216. 组合总和 III 代码 两种写法 22. 括号生成 代码 两种写法 39. 组合总和 代码 *课后作业 93. 复原 IP 地址 代码 *课后作业 回溯 - 排列型 46. 全排列 代码 精确计算搜索树的节点个数 51. N 皇后 代码 52. N 皇后 II 代码 2850. 将石头分散到网格图的最少移动次数 代码 *课后作业 动态规划 - 从记忆化搜索到递推 198. 打家劫舍 代码 包含空间优化 70. 爬楼梯 代码 *课后作业 746. 使用最小花费爬楼梯 代码 *课后作业 377. 组合总和 Ⅳ 代码 *课后作业 2466. 统计构造好字符串的方案数 代码 *课后作业 2266. 统计打字方案数 代码 *课后作业 213. 打家劫舍 II 代码 *课后作业 64. 最小路径和 代码 *课后作业 0-1 背包 完全背包 至多恰好至少 494. 目标和 代码 包含空间优化 322. 零钱兑换 代码 包含空间优化 2915. 和为目标值的最长子序列的长度 代码 *课后作业 416. 分割等和子集 代码 *课后作业 518. 零钱兑换 II 代码 *课后作业 279. 完全平方数 代码 *课后作业 最长公共子序列 LCS 1143. 最长公共子序列 代码 包含空间优化 72. 编辑距离 代码 包含空间优化 97. 交错字符串 代码 *课后作业 1092. 最短公共超序列 代码 *课后作业 最长递增子序列 LIS 300. 最长递增子序列 代码 包括贪心二分 + $O(1)$ 空间 1671. 得到山形数组的最少删除次数 代码 *课后作业 1626. 无矛盾的最佳球队 代码 *课后作业 状态机 DP - 买卖股票系列 122. 买卖股票的最佳时机 II 代码 309. 买卖股票的最佳时机含冷冻期 代码 188. 买卖股票的最佳时机 IV 代码 变形：恰好至少 714. 买卖股票的最佳时机含手续费 代码 *课后作业 2826. 将三个组排序 代码 *课后作业 2786. 访问数组中的位置使分数最大 代码 *课后作业 区间 DP 516. 最长回文子序列 代码 包含空间优化 1039. 多边形三角剖分的最低得分 代码 3040. 相同分数的最大操作数目 II 代码 *课后作业 1547. 切棍子的最小成本 代码 *课后作业 1771. 由子序列构造的最长回文串的长度 代码 *课后作业 1000. 合并石头的最低成本 代码 *课后作业 树形 DP - 直径系列 543. 二叉树的直径 代码 124. 二叉树中的最大路径和 代码 2246. 相邻字符不同的最长路径 代码 687. 最长同值路径 代码 *课后作业 3203. 合并两棵树后的最小直径 代码 *课后作业 1617. 统计子树中城市之间最大距离 代码 *课后作业 2538. 最大价值和与最小价值和的差值 代码 *课后作业 树形 DP - 最大独立集 337. 打家劫舍 III 代码 1377. T 秒后青蛙的位置 代码 *课后作业 2646. 最小化旅行的价格总和 代码 *课后作业 树形 DP - 最小支配集 968. 监控二叉树 代码 单调栈 739. 每日温度 代码 两种写法 42. 接雨水 代码 496. 下一个更大元素 I 代码 *课后作业 503. 下一个更大元素 II 代码 *课后作业 901. 股票价格跨度 代码 *课后作业 1019. 链表中的下一个更大节点 代码 *课后作业 1944. 队列中可以看到的人数 代码 *课后作业 84. 柱状图中最大的矩形 代码 *课后作业 1793. 好子数组的最大分数 代码 *课后作业 85. 最大矩形 代码 *课后作业 单调队列 239. 滑动窗口最大值 代码 2398. 预算内的最多机器人数目 代码 *课后作业 862. 和至少为 K 的最短子数组 代码 *课后作业 1499. 满足不等式的最大值 代码 *课后作业 1696. 跳跃游戏 VI 代码 *课后作业 2944. 购买水果需要的最少金币数 代码 *课后作业 其他尚未更新的 topic 请看 题解精选（已分类） 算法题单🔥如何科学刷题？ 滑动窗口与双指针（定长不定长单序列双序列三指针分组循环） 二分算法（二分答案最小化最大值最大化最小值第K小） 单调栈（基础矩形面积贡献法最小字典序） 网格图（DFSBFS综合应用） 位运算（基础性质拆位试填恒等式思维） 图论算法（DFSBFS拓扑排序最短路最小生成树二分图基环树欧拉路径） 🔥动态规划（入门背包状态机划分区间状压数位数据结构优化树形博弈概率期望） 常用数据结构（前缀和差分栈队列堆字典树并查集树状数组线段树） 数学算法（数论组合概率期望博弈计算几何随机算法） 贪心与思维（基本贪心策略反悔区间字典序数学思维脑筋急转弯构造） 链表、二叉树与回溯（前后指针快慢指针DFSBFS直径LCA一般树） 字符串（KMPZ函数Manacher字符串哈希AC自动机后缀数组子序列自动机） 周赛总结 如何科学上分（科学刷题） 从周赛中学算法 - 2023·下 从周赛中学算法 - 2023·上 从周赛中学算法 - 2022·下 从周赛中学算法 - 2022·上 其他 🔥从集合论到位运算，常见位运算技巧分类总结！ 模运算的世界：当加减乘除遇上取模 【简单题杀手】分组循环 【图解】一张图秒懂二维前缀和 相向双指针小tips:可以通过获取的信息量来衡量一个算法的效率. 双指针的精髓:用O1的时间,获取到On的信息.(因为每次移动都能够排除多个不可能的组合) 167. 两数之和 II - 输入有序数组比如这道题目暴力做法枚举每个数,然后和target作比较,O1的时间就获取到了O1的信息.而双指针的方法将剩下的数和最大的数对比,O1的时间获取到了On的信息. 线段树以这道题目水果成篮III为例,入门线段树. 线段树发明的动机虽然说数组无序,但是仍然可以通过二分的方式寻找需要的位置,如果左半边有就排除右半边,如果左半边没有就排除左半边.并且每次二分,都是原问题相同的更小子问题,也就是继续寻找左右半边最大的容器. 为什么要这么设计线段树通过线段树,可以维护所有需要的情况的数据.并且在查找后,需要重新修正节点.所以需要的操作就是:找到最左边大于等于x的位置;将该位置改成-1. 线段树模板class SegmentTree private final int[] max; public SegmentTree(int[] a) int n = a.length; //max是​​大于或等于 n的最小的 2 的幂次方​​ max = new int[2 (32 - Integer.numberOfLeadingZeros(n - 1))]; //建立线段树 build(a, 1, 0, n - 1); // 找区间内的第一个 = x 的数，并更新为 -1，返回这个数的下标（没有则返回 -1） //o为第几个位置 l为左下标 r为右下标 x为目标数 public int findFirstAndUpdate(int o, int l, int r, int x) if (max[o] x) // 区间没有 = x 的数 return -1; if (l == r) max[o] = -1; // 更新为 -1，表示不能放水果 return l; int m = (l + r) / 2; int i = findFirstAndUpdate(o * 2, l, m, x); // 先递归左子树 if (i 0) // 左子树没找到 i = findFirstAndUpdate(o * 2 + 1, m + 1, r, x); // 再递归右子树 maintain(o); return i; //在回溯过程更新节点 private void maintain(int o) max[o] = Math.max(max[o * 2], max[o * 2 + 1]); // 初始化线段树 private void build(int[] a, int o, int l, int r) if (l == r) max[o] = a[l]; return; int m = (l + r) / 2; build(a, o * 2, l, m); build(a, o * 2 + 1, m + 1, r); maintain(o); class Solution public int numOfUnplacedFruits(int[] fruits, int[] baskets) SegmentTree t = new SegmentTree(baskets); int n = baskets.length; int ans = 0; for (int x : fruits) if (t.findFirstAndUpdate(1, 0, n - 1, x) 0) ans++; return ans;","tags":["基础","leetcode","算法"],"categories":["算法笔记"]},{"title":"2025.7.24学习日记","path":"/2025/07/24/学习日记25年7月/2025.7.24学习笔记/","content":"今日学习内容3DGS今天主要学习CityGaussianV2论文. 力扣每日一题树的困难题目. JavaSE八股2356 Redis八股057 项目篇代码随想录生活篇晚上健身练背,核心,肩,三头,强度中等偏上.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.23学习日记","path":"/2025/07/23/学习日记25年7月/2025.7.23学习笔记/","content":"今日学习内容3DGS今天主要学习CityGaussian的代码. 力扣每日一题一道堆加贪心的中等题目. JavaSE八股2356 Redis八股057 项目篇代码随想录生活篇晚上健身练背,核心,肩,三头,强度中等偏上.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"JavaSE学习笔记","path":"/2025/07/22/Java问答笔记/JavaSE学习笔记/","content":"Java 概述1.🌟什么是 Java？Java 是一门面向对象的编程语言，由 Sun 公司的詹姆斯·高斯林团队于 1995 年推出。吸收了 C++ 语言中大量的优点，但又抛弃了 C++ 中容易出错的地方，如垃圾回收、指针。 同时，Java 又是一门平台无关的编程语言，即一次编译，处处运行。 只需要在对应的平台上安装 JDK，就可以实现跨平台，在 Windows、macOS、Linux 操作系统上运行。 Java 语言和 C 语言有哪些区别？Java 是一种跨平台的编程语言，通过在不同操作系统上安装对应版本的 JVM 以实现“一次编译，处处运行”的目的。而 C 语言需要在不同的操作系统上重新编译。 Java 实现了内存的自动管理，而 C 语言需要使用 malloc 和 free 来手动管理内存。 2.Java 语言有哪些特点？ Java 语言的特点有： ①、面向对象，主要是封装，继承，多态。 ②、平台无关性，“一次编写，到处运行”，因此采用 Java 语言编写的程序具有很好的可移植性。 ③、支持多线程。C++ 语言没有内置的多线程机制，因此必须调用操作系统的 API 来完成多线程程序设计，而 Java 却提供了封装好多线程支持； ④、支持 JIT 编译，也就是即时编译器，它可以在程序运行时将字节码转换为热点机器码来提高程序的运行速度。 3.JVM、JDK 和 JRE 有什么区别？ JVM：也就是 Java 虚拟机，是 Java 实现跨平台的关键所在，不同的操作系统有不同的 JVM 实现。JVM 负责将 Java 字节码转换为特定平台的机器码，并执行。 JRE：也就是 Java 运行时环境，包含了运行 Java 程序所必需的库，以及 JVM。 JDK：一套完整的 Java SDK，包括 JRE，编译器 javac、Java 文档生成工具 javadoc、Java 字节码工具 javap 等。为开发者提供了开发、编译、调试 Java 程序的一整套环境。 简单来说，JDK 包含 JRE，JRE 包含 JVM。 4.说说什么是跨平台？原理是什么所谓的跨平台，是指 Java 语言编写的程序，一次编译后，可以在多个操作系统上运行。 原理是增加了一个中间件 JVM，JVM 负责将 Java字节码转换为特定平台的机器码，并执行。 5.什么是字节码？采用字节码的好处是什么?所谓的字节码，就是 Java 程序经过编译后产生的 .class 文件。 Java 程序从源代码到运行需要经过三步： 编译：将源代码文件 .java 编译成 JVM 可以识别的字节码文件 .class 解释：JVM 执行字节码文件，将字节码翻译成操作系统能识别的机器码 执行：操作系统执行二进制的机器码 6.为什么有人说 Java 是“编译与解释并存”的语言？编译型语言是指编译器针对特定的操作系统，将源代码一次性翻译成可被该平台执行的机器码。 解释型语言是指解释器对源代码进行逐行解释，解释成特定平台的机器码并执行。 举个例子，我想读一本国外的小说，我有两种选择： 找个翻译，等翻译将小说全部都翻译成汉语，一次性读完。 找个翻译，翻译一段我读一段，慢慢把书读完。 之所以有人说 Java 是“编译与解释并存”的语言，是因为 Java 程序需要先将 Java 源代码文件编译字节码文件，再解释执行。 基础语法7.🌟Java 有哪些数据类型？Java 的数据类型可以分为两种：基本数据类型和引用数据类型。 基本数据类型有：①、数值型 整数类型（byte、short、int、long） 浮点类型（float、double） ②、字符型（char）③、布尔型（boolean）它们的默认值和占用大小如下所示： 引用数据类型有： 类（class） 接口（interface） 数组（[]） boolean 类型实际占用几个字节？推荐阅读：Java 进阶之路：基本数据类型篇 这要依据具体的 JVM 实现细节。Java 虚拟机规范中，并没有明确规定 boolean 类型的大小，只规定了 boolean 类型的取值 true 或 false。 boolean: The boolean data type has only two possible values: true and false. Use this data type for simple flags that track truefalse conditions. This data type represents one bit of information, but its “size” isn’t something that’s precisely defined. 我本机的 64 位 JDK 中，通过 JOL 工具查看单独的 boolean 类型，以及 boolean 数组，所占用的空间都是 1 个字节。 给Integer最大值+1，是什么结果？当给 Integer.MAX_VALUE 加 1 时，会发生溢出，变成 Integer.MIN_VALUE。 int maxValue = Integer.MAX_VALUE;System.out.println(Integer.MAX_VALUE = + maxValue); // Integer.MAX_VALUE = 2147483647System.out.println(Integer.MAX_VALUE + 1 = + (maxValue + 1)); // Integer.MAX_VALUE + 1 = -2147483648// 用二进制来表示最大值和最小值System.out.println(Integer.MAX_VALUE in binary: + Integer.toBinaryString(maxValue)); // Integer.MAX_VALUE in binary: 1111111111111111111111111111111System.out.println(Integer.MIN_VALUE in binary: + Integer.toBinaryString(Integer.MIN_VALUE)); // Integer.MIN_VALUE in binary: 10000000000000000000000000000000 这是因为 Java 的整数类型采用的是二进制补码表示法，溢出时值会变成最小值。 Integer.MAX_VALUE 的二进制表示是 01111111 11111111 11111111 11111111（32 位）。 加 1 后结果变成 10000000 00000000 00000000 00000000，即 -2147483648（Integer.MIN_VALUE）。 8.自动类型转换、强制类型转换了解吗？推荐阅读：聊聊基本数据类型的转换 当把一个范围较小的数值或变量赋给另外一个范围较大的变量时，会进行自动类型转换；反之，需要强制转换。 这就好像，小杯里的水倒进大杯没问题，但大杯的水倒进小杯就可能会溢出。 ①、float f=3.4，对吗？ 不正确。3.4 默认是双精度，将双精度赋值给浮点型属于下转型（down-casting，也称窄化）会造成精度丢失，因此需要强制类型转换float f =(float)3.4;或者写成float f =3.4F ②、short s1 = 1; s1 = s1 + 1；对吗？short s1 = 1; s1 += 1;对吗？ short s1 = 1; s1 = s1 + 1; 会编译出错，由于 1 是 int 类型，因此 s1+1 运算结果也是 int 型，需要强制转换类型才能赋值给 short 型。 而 short s1 = 1; s1 += 1;可以正确编译，因为 s1+= 1;相当于 s1 = (short(s1 + 1); 其中有隐含的强制类型转换。 9.什么是自动拆箱装箱？ 装箱：将基本数据类型转换为包装类型，例如 int 转换为 Integer。 拆箱：将包装类型转换为基本数据类型。 举例： Integer i = 10; //装箱int n = i; //拆箱 再换句话说，i 是 Integer 类型，n 是 int 类型；变量 i 是包装器类，变量 n 是基本数据类型。 10.和有什么区别？ 是 逻辑与。 是短路与运算。逻辑与跟短路与的差别是非常大的，虽然二者都要求运算符左右两端的布尔值都是 true，整个表达式的值才是 true。 之所以称为短路运算是因为，如果左边的表达式的值是 false，右边的表达式会直接短路掉，不会进行运算。 例如在验证用户登录时判定用户名不是 null 而且不是空字符串，应当写为username != null !username.equals()，二者的顺序不能交换，更不能用 运算符，因为第一个条件如果不成立，根本不能进行字符串的 equals 比较，会抛出 NullPointerException 异常。 注意：逻辑或运算符（|）和短路或运算符（||）的差别也是类似。 11.switch 语句能否用在 bytelongString 类型上？Java 5 以前 switch(expr) 中，expr 只能是 byte、short、char、int。 从 Java 5 开始，Java 中引入了枚举类型， expr 也可以是 enum 类型。 从 Java 7 开始，expr 还可以是字符串，但是长整型在目前所有的版本中都是不可以的。 12.break,continue,return 的区别及作用？ break 跳出整个循环，不再执行循环(结束当前的循环体) continue 跳出本次循环，继续执行下次循环(结束正在执行的循环 进入下一个循环条件) return 程序返回，不再执行下面的代码(结束当前的方法 直接返回) 13.用效率最高的方法计算 2 乘以 8？2 3。位运算，数字的二进制位左移三位相当于乘以 2 的三次方。 14.说说自增自减运算？在写代码的过程中，常见的一种情况是需要某个整数类型变量增加 1 或减少 1，Java 提供了一种特殊的运算符，用于这种表达式，叫做自增运算符（++）和自减运算符（–）。 ++和–运算符可以放在变量之前，也可以放在变量之后。 当运算符放在变量之前时(前缀)，先自增减，再赋值；当运算符放在变量之后时(后缀)，先赋值，再自增减。 例如，当 b = ++a 时，先自增（自己增加 1），再赋值（赋值给 b）；当 b = a++ 时，先赋值(赋值给 b)，再自增（自己增加 1）。也就是，++a 输出的是 a+1 的值，a++输出的是 a 值。 用一句口诀就是：“符号在前就先加减，符号在后就后加减”。 看一下这段代码运行结果？int i = 1;i = i++;System.out.println(i); 答案是 1。有点离谱对不对。 对于 JVM 而言，它对自增运算的处理，是会先定义一个临时变量来接收 i 的值，然后进行自增运算，最后又将临时变量赋给了值为 2 的 i，所以最后的结果为 1。 相当于这样的代码： int i = 1；int temp = i;i++；i = temp;System.out.println(i); 这段代码会输出什么？int count = 0;for(int i = 0;i 100;i++) count = count++;System.out.println(count = +count); 答案是 0。 和上面的题目一样的道理，同样是用了临时变量，count 实际是等于临时变量的值。 int autoAdd(int count) int temp = count; count = count + 1; return temp; 15.float 是怎么表示小数的？（补充）推荐阅读：计算机系统基础（四）浮点数 float类型的小数在计算机中是通过 IEEE 754 标准的单精度浮点数格式来表示的。 S：符号位，0 代表正数，1 代表负数； M：尾数部分，用于表示数值的精度；比如说 ；1.25 就是尾数； R：基数，十进制中的基数是 10，二进制中的基数是 2； E：指数部分，例如 中的 -1 就是指数。 这种表示方法可以将非常大或非常小的数值用有限的位数表示出来，但这也意味着可能会有精度上的损失。 单精度浮点数占用 4 字节（32 位），这 32 位被分为三个部分：符号位、指数部分和尾数部分。 符号位（Sign bit）：1 位 指数部分（Exponent）：10 位 尾数部分（Mantissa，或 Fraction）：21 位 按照这个规则，将十进制数 25.125 转换为浮点数，转换过程是这样的： 整数部分：25 转换为二进制是 11001； 小数部分：0.125 转换为二进制是 0.001； 用二进制科学计数法表示：25.125 符号位 S 是 0，表示正数；指数部分 E 是 4，转换为二进制是 100；尾数部分 M 是 1.001001。 使用浮点数时需要注意，由于精度的限制，进行数学运算时可能会遇到舍入误差，特别是连续运算累积误差可能会变得显著。 对于需要高精度计算的场景（如金融计算），可能需要考虑使用BigDecimal类来避免这种误差。 16.讲一下数据准确性高是怎么保证的？（补充）在金融计算中，保证数据准确性有两种方案，一种使用 BigDecimal，一种将浮点数转换为整数 int 进行计算。 肯定不能使用 float 和 double 类型，它们无法避免浮点数运算中常见的精度问题，因为这些数据类型采用二进制浮点数来表示，无法准确地表示，例如 0.1。 BigDecimal num1 = new BigDecimal(0.1);BigDecimal num2 = new BigDecimal(0.2);BigDecimal sum = num1.add(num2);System.out.println(Sum of 0.1 and 0.2 using BigDecimal: + sum); // 输出 0.3，精确计算 在处理小额支付或计算时，通过转换为较小的货币单位（如分），这样不仅提高了运算速度，还保证了计算的准确性。 int priceInCents = 199; // 商品价格199分int quantity = 3;int totalInCents = priceInCents * quantity; // 计算总价System.out.println(Total price in cents: + totalInCents); // 输出597分 面向对象17.⾯向对象和⾯向过程的区别?面向过程是以过程为核心，通过函数完成任务，程序结构是函数+步骤组成的顺序流程。 面向对象是以对象为核心，通过对象交互完成任务，程序结构是类和对象组成的模块化结构，代码可以通过继承、组合、多态等方式复用。 面向过程就是编年体,面向对象就是纪传体. 18.🌟面向对象编程有哪些特性？推荐阅读：深入理解 Java 三大特性 面向对象编程有三大特性：封装、继承、多态。 封装是什么？封装是指将数据（属性，或者叫字段）和操作数据的方法（行为）捆绑在一起，形成一个独立的对象（类的实例）。 class Nvshen private String name; private int age; public void setName(String name) this.name = name; public String getName() return name; public void setAge(int age) this.age = age; 可以看得出，女神类对外没有提供 age 的 getter 方法，因为女神的年龄要保密。 所以，封装是把一个对象的属性私有化，同时提供一些可以被外界访问的方法。 继承是什么？继承允许一个类（子类）继承现有类（父类或者基类）的属性和方法。以提高代码的复用性，建立类之间的层次关系。 同时，子类还可以重写或者扩展从父类继承来的属性和方法，从而实现多态。 class Person protected String name; protected int age; public void eat() System.out.println(吃饭); class Student extends Person private String school; public void study() System.out.println(学习); Student 类继承了 Person 类的属性（name、age）和方法（eat），同时还有自己的属性（school）和方法（study）。 什么是多态？多态允许不同类的对象对同一消息做出响应，但表现出不同的行为（即方法的多样性）。 多态其实是一种能力——同一个行为具有不同的表现形式；换句话说就是，执行一段代码，Java 在运行时能根据对象类型的不同产生不同的结果。 多态的前置条件有三个： 子类继承父类 子类重写父类的方法 父类引用指向子类的对象 //子类继承父类class Wangxiaoer extends Wanger public void write() // 子类重写父类方法 System.out.println(记住仇恨，表明我们要奋发图强的心智); public static void main(String[] args) // 父类引用指向子类对象 Wanger wanger = new Wangxiaoer(); wanger.write(); class Wanger public void write() System.out.println(王二是沙雕); 为什么Java里面要多组合少继承？继承适合描述“is-a”的关系，但继承容易导致类之间的强耦合，一旦父类发生改变，子类也要随之改变，违背了开闭原则（尽量不修改现有代码，而是添加新的代码来实现）。 组合适合描述“has-a”或“can-do”的关系，通过在类中组合其他类，能够更灵活地扩展功能。组合避免了复杂的类继承体系，同时遵循了开闭原则和松耦合的设计原则。 举个例子，假设我们采用继承，每种形状和样式的组合都会导致类的急剧增加： // 基类class Shape public void draw() System.out.println(Drawing a shape); // 圆形class Circle extends Shape @Override public void draw() System.out.println(Drawing a circle); // 带红色的圆形class RedCircle extends Circle @Override public void draw() System.out.println(Drawing a red circle); // 带绿色的圆形class GreenCircle extends Circle @Override public void draw() System.out.println(Drawing a green circle); // 类似的，对于矩形也要创建多个类class Rectangle extends Shape @Override public void draw() System.out.println(Drawing a rectangle); class RedRectangle extends Rectangle @Override public void draw() System.out.println(Drawing a red rectangle); 组合模式更加灵活，可以将形状和颜色分开，松耦合。 // 形状接口interface Shape void draw();// 颜色接口interface Color void applyColor(); 形状干形状的事情。 // 圆形的实现class Circle implements Shape private Color color; // 通过组合的方式持有颜色对象 public Circle(Color color) this.color = color; @Override public void draw() System.out.print(Drawing a circle with ); color.applyColor(); // 调用颜色的逻辑 // 矩形的实现class Rectangle implements Shape private Color color; public Rectangle(Color color) this.color = color; @Override public void draw() System.out.print(Drawing a rectangle with ); color.applyColor(); 颜色干颜色的事情。 // 红色的实现class RedColor implements Color @Override public void applyColor() System.out.println(red color); // 绿色的实现class GreenColor implements Color @Override public void applyColor() System.out.println(green color); 19.多态解决了什么问题？（补充）多态指同一个接口或方法在不同的类中有不同的实现，比如说动态绑定，父类引用指向子类对象，方法的具体调用会延迟到运行时决定。 举例，现在有一个父类 Wanger，一个子类 Wangxiaoer，都有一个 write 方法。现在有一个父类 Wanger 类型的变量 wanger，它在执行 wanger.write() 时，究竟调用父类 Wanger 的 write() 方法，还是子类 Wangxiaoer 的 write() 方法呢？ //子类继承父类class Wangxiaoer extends Wanger public void write() // 子类覆盖父类方法 System.out.println(记住仇恨，表明我们要奋发图强的心智); public static void main(String[] args) // 父类引用指向子类对象 Wanger[] wangers = new Wanger(), new Wangxiaoer() ; for (Wanger wanger : wangers) // 对象是王二的时候输出：勿忘国耻 // 对象是王小二的时候输出：记住仇恨，表明我们要奋发图强的心智 wanger.write(); class Wanger public void write() System.out.println(勿忘国耻); 答案是在运行时根据对象的类型进行后期绑定，编译器在编译阶段并不知道对象的类型，但是 Java 的方法调用机制能找到正确的方法体，然后执行，得到正确的结果，这就是多态的作用。 多态的实现原理是什么？多态通过动态绑定实现，Java 使用虚方法表存储方法指针，方法调用时根据对象实际类型从虚方法表查找具体实现。 20.重载和重写的区别？推荐阅读：方法重写 Override 和方法重载 Overload 有什么区别？ 如果一个类有多个名字相同但参数个数不同的方法，我们通常称这些方法为方法重载（overload）。如果方法的功能是一样的，但参数不同，使用相同的名字可以提高程序的可读性。 如果子类具有和父类一样的方法（参数相同、返回类型相同、方法名相同，但方法体可能不同），我们称之为方法重写（override）。方法重写用于提供父类已经声明的方法的特殊实现，是实现多态的基础条件。 方法重载发生在同一个类中，同名的方法如果有不同的参数（参数类型不同、参数个数不同或者二者都不同）。 方法重写发生在子类与父类之间，要求子类与父类具有相同的返回类型，方法名和参数列表，并且不能比父类的方法声明更多的异常，遵守里氏代换原则。 什么是里氏代换原则？里氏代换原则也被称为李氏替换原则（Liskov Substitution Principle, LSP），其规定任何父类可以出现的地方，子类也一定可以出现。 LSP 是继承复用的基石，只有当子类可以替换掉父类，并且单位功能不受到影响时，父类才能真正被复用，而子类也能够在父类的基础上增加新的行为。 这意味着子类在扩展父类时，不应改变父类原有的行为。例如，如果有一个方法接受一个父类对象作为参数，那么传入该方法的任何子类对象也应该能正常工作。 class Bird void fly() System.out.println(鸟正在飞); class Duck extends Bird @Override void fly() System.out.println(鸭子正在飞); class Ostrich extends Bird // Ostrich违反了LSP，因为鸵鸟不会飞，但却继承了会飞的鸟类 @Override void fly() throw new UnsupportedOperationException(鸵鸟不会飞); 在这个例子中，Ostrich（鸵鸟）类违反了 LSP 原则，因为它改变了父类 Bird 的行为（即飞行）。设计时应该更加谨慎地使用继承关系，确保遵守 LSP 原则。 除了李氏替换原则外，还有其他几个重要的面向对象设计原则，它们共同构成了 SOLID 原则，分别是： ①、单一职责原则（Single Responsibility Principle, SRP），指一个类应该只有一个引起它变化的原因，即一个类只负责一项职责。这样做的目的是使类更加清晰，更容易理解和维护。 ②、开闭原则（Open-Closed Principle, OCP），指软件实体应该对扩展开放，对修改关闭。这意味着一个类应该通过扩展来实现新的功能，而不是通过修改已有的代码来实现。 举个例子，在不遵守开闭原则的情况下，有一个需要处理不同形状的绘图功能类。 class ShapeDrawer public void draw(Shape shape) if (shape instanceof Circle) drawCircle((Circle) shape); else if (shape instanceof Rectangle) drawRectangle((Rectangle) shape); private void drawCircle(Circle circle) // 画圆形 private void drawRectangle(Rectangle rectangle) // 画矩形 每增加一种形状，就需要修改一次 draw 方法，这违反了开闭原则。正确的做法是通过继承和多态来实现新的形状类，然后在 ShapeDrawer 中添加新的 draw 方法。 // 抽象的 Shape 类abstract class Shape public abstract void draw();// 具体的 Circle 类class Circle extends Shape @Override public void draw() // 画圆形 // 具体的 Rectangle 类class Rectangle extends Shape @Override public void draw() // 画矩形 // 使用开闭原则的 ShapeDrawer 类class ShapeDrawer public void draw(Shape shape) shape.draw(); // 调用多态的 draw 方法 ③、接口隔离原则（Interface Segregation Principle, ISP），指客户端不应该依赖它不需要的接口。这意味着设计接口时应该尽量精简，不应该设计臃肿庞大的接口。 ④、依赖倒置原则（Dependency Inversion Principle, DIP），指高层模块不应该依赖低层模块，二者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。这意味着设计时应该尽量依赖接口或抽象类，而不是实现类。 21.访问修饰符 public、private、protected、以及默认时的区别？Java 中，可以使用访问控制符来保护对类、变量、方法和构造方法的访问。Java 支持 4 种不同的访问权限。 default （即默认，什么也不写）: 在同一包内可见，不使用任何修饰符。可以修饰在类、接口、变量、方法。 private : 在同一类内可见。可以修饰变量、方法。注意：不能修饰类（外部类） public : 对所有类可见。可以修饰类、接口、变量、方法 protected : 对同一包内的类和所有子类可见。可以修饰变量、方法。注意：不能修饰类（外部类）。 22.this 关键字有什么作用？this 是自身的一个对象，代表对象本身，可以理解为：指向对象本身的一个指针。 this 的用法在 Java 中大体可以分为 3 种： 普通的直接引用，this 相当于是指向当前对象本身 形参与成员变量名字重名，用 this 来区分： public Person(String name,int age) this.name=name; this.age=age; 引用本类的构造方法 23.🌟抽象类和接口有什么区别？一个类只能继承一个抽象类；但一个类可以实现多个接口。所以我们在新建线程类的时候一般推荐使用实现 Runnable 接口的方式，这样线程类还可以继承其他类，而不单单是 Thread 类。 抽象类符合 is-a 的关系，而接口更像是 has-a 的关系，比如说一个类可以序列化的时候，它只需要实现 Serializable 接口就可以了，不需要去继承一个序列化类。 抽象类更多地是用来为多个相关的类提供一个共同的基础框架，包括状态的初始化，而接口则是定义一套行为标准，让不同的类可以实现同一接口，提供行为的多样化实现。 抽象类可以定义构造方法吗？可以，抽象类可以有构造方法。 abstract class Animal protected String name; public Animal(String name) this.name = name; public abstract void makeSound();public class Dog extends Animal private int age; public Dog(String name, int age) super(name); // 调用抽象类的构造函数 this.age = age; @Override public void makeSound() System.out.println(name + says: Bark); 接口可以定义构造方法吗？不能，接口主要用于定义一组方法规范，没有具体的实现细节。 Java支持多继承吗？Java 不支持多继承，一个类只能继承一个类，多继承会引发菱形继承问题。 class A void show() System.out.println(A); class B extends A void show() System.out.println(B); class C extends A void show() System.out.println(C); // 如果 Java 支持多继承class D extends B, C // 调用 show() 方法时，D 应该调用 B 的 show() 还是 C 的 show()？ 接口可以多继承吗？接口可以多继承，一个接口可以继承多个接口，使用逗号分隔。 interface InterfaceA void methodA();interface InterfaceB void methodB();interface InterfaceC extends InterfaceA, InterfaceB void methodC();class MyClass implements InterfaceC public void methodA() System.out.println(Method A); public void methodB() System.out.println(Method B); public void methodC() System.out.println(Method C); public static void main(String[] args) MyClass myClass = new MyClass(); myClass.methodA(); myClass.methodB(); myClass.methodC(); 在上面的例子中，InterfaceA 和 InterfaceB 是两个独立的接口。 InterfaceC 继承了 InterfaceA 和 InterfaceB，并且定义了自己的方法 methodC。 MyClass 实现了 InterfaceC，因此需要实现 InterfaceA 和 InterfaceB 中的方法 methodA 和 methodB，以及 InterfaceC 中的方法 methodC。 继承和抽象的区别？继承是一种允许子类继承父类属性和方法的机制。通过继承，子类可以重用父类的代码。 抽象是一种隐藏复杂性和只显示必要部分的技术。在面向对象编程中，抽象可以通过抽象类和接口实现。 抽象类和普通类的区别？抽象类使用 abstract 关键字定义，不能被实例化，只能作为其他类的父类。普通类没有 abstract 关键字，可以直接实例化。 抽象类可以包含抽象方法和非抽象方法。抽象方法没有方法体，必须由子类实现。普通类只能包含非抽象方法。 abstract class Animal // 抽象方法 public abstract void makeSound(); // 非抽象方法 public void eat() System.out.println(This animal is eating.); class Dog extends Animal // 实现抽象方法 @Override public void makeSound() System.out.println(Woof); public class Test public static void main(String[] args) Dog dog = new Dog(); dog.makeSound(); // 输出 Woof dog.eat(); // 输出 This animal is eating. 抽象类使用 abstract 关键字定义，不能被实例化，只能作为其他类的父类。普通类没有 abstract 关键字，可以直接实例化。 抽象类可以包含抽象方法和非抽象方法。抽象方法没有方法体，必须由子类实现。普通类只能包含非抽象方法。 abstract class Animal // 抽象方法 public abstract void makeSound(); // 非抽象方法 public void eat() System.out.println(This animal is eating.); class Dog extends Animal // 实现抽象方法 @Override public void makeSound() System.out.println(Woof); public class Test public static void main(String[] args) Dog dog = new Dog(); dog.makeSound(); // 输出 Woof dog.eat(); // 输出 This animal is eating. 24.成员变量与局部变量的区别有哪些？ 从语法形式上看：成员变量是属于类的，⽽局部变量是在⽅法中定义的变量或是⽅法的参数；成员变量可以被 public , private , static 等修饰符所修饰，⽽局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储⽅式来看：如果成员变量是使⽤ static 修饰的，那么这个成员变量是属于类的，如果没有使⽤ static 修饰，这个成员变量是属于实例的。对象存于堆内存，如果局部变量类型为基本数据类型，那么存储在栈内存，如果为引⽤数据类型，那存放的是指向堆内存对象的引⽤或者是指向常量池中的地址。 从变量在内存中的⽣存时间上看：成员变量是对象的⼀部分，它随着对象的创建⽽存在，⽽局部变量随着⽅法的调⽤⽽⾃动消失。 成员变量如果没有被赋初值：则会⾃动以类型的默认值⽽赋值（⼀种情况例外:被 final 修饰的成员变量也必须显式地赋值），⽽局部变量则不会⾃动赋值。 25.static 关键字了解吗？推荐阅读：详解 Java static 关键字的作用 static 关键字可以用来修饰变量、方法、代码块和内部类，以及导入包。 静态变量和实例变量的区别？静态变量: 是被 static 修饰符修饰的变量，也称为类变量，它属于类，不属于类的任何一个对象，一个类不管创建多少个对象，静态变量在内存中有且仅有一个副本。 实例变量: 必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。 静态⽅法和实例⽅法有何不同?类似地。 静态方法：static 修饰的方法，也被称为类方法。在外部调⽤静态⽅法时，可以使⽤”类名.⽅法名“的⽅式，也可以使⽤”对象名.⽅法名“的⽅式。静态方法里不能访问类的非静态成员变量和方法。 实例⽅法：依存于类的实例，需要使用”对象名.⽅法名“的⽅式调用；可以访问类的所有成员变量和方法。 26.final 关键字有什么作用？①、当 final 修饰一个类时，表明这个类不能被继承。比如，String 类、Integer 类和其他包装类都是用 final 修饰的。 ②、当 final 修饰一个方法时，表明这个方法不能被重写（Override）。也就是说，如果一个类继承了某个类，并且想要改变父类中被 final 修饰的方法的行为，是不被允许的。 ③、当 final 修饰一个变量时，表明这个变量的值一旦被初始化就不能被修改。如果是基本数据类型的变量，其数值一旦在初始化之后就不能更改；如果是引用类型的变量，在对其初始化之后就不能再让其指向另一个对象。 但是引用指向的对象内容可以改变。 27.final、finally、finalize 的区别？①、final 是一个修饰符，可以修饰类、方法和变量。当 final 修饰一个类时，表明这个类不能被继承；当 final 修饰一个方法时，表明这个方法不能被重写；当 final 修饰一个变量时，表明这个变量是个常量，一旦赋值后，就不能再被修改了。 ②、finally 是 Java 中异常处理的一部分，用来创建 try 块后面的 finally 块。无论 try 块中的代码是否抛出异常，finally 块中的代码总是会被执行。通常，finally 块被用来释放资源，如关闭文件、数据库连接等。 ③、finalize 是Object 类的一个方法，用于在垃圾回收器将对象从内存中清除出去之前做一些必要的清理工作。 这个方法在垃圾回收器准备释放对象占用的内存之前被自动调用。我们不能显式地调用 finalize 方法，因为它总是由垃圾回收器在适当的时间自动调用。 28.和 equals 的区别？在 Java 中，== 操作符和 equals() 方法用于比较两个对象： ①、：用于比较两个对象的引用，即它们是否指向同一个对象实例。 如果两个变量引用同一个对象实例，== 返回 true，否则返回 false。 对于基本数据类型（如 int, double, char 等），== 比较的是值是否相等。 ②、equals() 方法：用于比较两个对象的内容是否相等。默认情况下，equals() 方法的行为与 == 相同，即比较对象引用，如在超类 Object 中： public boolean equals(Object obj) return (this == obj); 然而，equals() 方法通常被各种类重写。例如，String 类重写了 equals() 方法，以便它可以比较两个字符串的字符内容是否完全一样。 举个例子： String a = new String(沉默王二);String b = new String(沉默王二);// 使用 == 比较System.out.println(a == b); // 输出 false，因为 a 和 b 引用不同的对象// 使用 equals() 比较System.out.println(a.equals(b)); // 输出 true，因为 a 和 b 的内容相同 29.🌟为什么重写 equals 时必须重写 hashCode ⽅法？因为基于哈希的集合类（如 HashMap）需要基于这一点来正确存储和查找对象。 具体地说，HashMap 通过对象的哈希码将其存储在不同的“桶”中，当查找对象时，它需要使用 key 的哈希码来确定对象在哪个桶中，然后再通过 equals() 方法找到对应的对象。 如果重写了 equals()方法而没有重写 hashCode()方法，那么被认为相等的对象可能会有不同的哈希码，从而导致无法在 HashMap 中正确处理这些对象。 什么是 hashCode 方法？hashCode() 方法的作⽤是获取哈希码，它会返回⼀个 int 整数，定义在 Object 类中， 是一个本地⽅法。 public native int hashCode(); 为什么要有 hashCode 方法？hashCode 方法主要用来获取对象的哈希码，哈希码是由对象的内存地址或者对象的属性计算出来的，它是⼀个 int 类型的整数，通常是不会重复的，因此可以用来作为键值对的建，以提高查询效率。 例如 HashMap 中的 key 就是通过 hashCode 来实现的，通过调用 hashCode 方法获取键的哈希码，并将其与右移 16 位的哈希码进行异或运算。 static final int hash(Object key) int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h 16); 为什么两个对象有相同的 hashcode 值，它们也不⼀定相等？这主要是由于哈希码（hashCode）的本质和目的所决定的。 哈希码是通过哈希函数将对象中映射成一个整数值，其主要目的是在哈希表中快速定位对象的存储位置。 由于哈希函数将一个较大的输入域映射到一个较小的输出域，不同的输入值（即不同的对象）可能会产生相同的输出值（即相同的哈希码）。 这种情况被称为哈希冲突。当两个不相等的对象发生哈希冲突时，它们会有相同的 hashCode。 为了解决哈希冲突的问题，哈希表在处理键时，不仅会比较键对象的哈希码，还会使用 equals 方法来检查键对象是否真正相等。如果两个对象的哈希码相同，但通过 equals 方法比较结果为 false，那么这两个对象就不被视为相等。 if (p.hash == hash ((k = p.key) == key || (key != null key.equals(k)))) e = p; hashCode 和 equals 方法的关系？如果两个对象通过 equals 相等，它们的 hashCode 必须相等。否则会导致哈希表类数据结构（如 HashMap、HashSet）的行为异常。 在哈希表中，如果 equals 相等但 hashCode 不相等，哈希表可能无法正确处理这些对象，导致重复元素或键值冲突等问题。 30.Java 是值传递，还是引用传递？Java 是值传递，不是引用传递。 当一个对象被作为参数传递到方法中时，参数的值就是该对象的引用。引用的值是对象在堆中的地址。 对象是存储在堆中的，所以传递对象的时候，可以理解为把变量存储的对象地址给传递过去。 引用类型的变量有什么特点？引用类型的变量存储的是对象的地址，而不是对象本身。因此，引用类型的变量在传递时，传递的是对象的地址，也就是说，传递的是引用的值。 31.说说深拷贝和浅拷贝的区别?推荐阅读：深入理解 Java 浅拷贝与深拷贝 在 Java 中，深拷贝（Deep Copy）和浅拷贝（Shallow Copy）是两种拷贝对象的方式，它们在拷贝对象的方式上有很大不同。 浅拷贝会创建一个新对象，但这个新对象的属性（字段）和原对象的属性完全相同。如果属性是基本数据类型，拷贝的是基本数据类型的值；如果属性是引用类型，拷贝的是引用地址，因此新旧对象共享同一个引用对象。 浅拷贝的实现方式为：实现 Cloneable 接口并重写 clone() 方法。 class Person implements Cloneable String name; int age; Address address; public Person(String name, int age, Address address) this.name = name; this.age = age; this.address = address; @Override protected Object clone() throws CloneNotSupportedException return super.clone(); class Address String city; public Address(String city) this.city = city; public class Main public static void main(String[] args) throws CloneNotSupportedException Address address = new Address(河南省洛阳市); Person person1 = new Person(沉默王二, 18, address); Person person2 = (Person) person1.clone(); System.out.println(person1.address == person2.address); // true 深拷贝也会创建一个新对象，但会递归地复制所有的引用对象，确保新对象和原对象完全独立。新对象与原对象的任何更改都不会相互影响。 深拷贝的实现方式有：手动复制所有的引用对象，或者使用序列化与反序列化。 ①、手动拷贝 class Person String name; int age; Address address; public Person(String name, int age, Address address) this.name = name; this.age = age; this.address = address; public Person(Person person) this.name = person.name; this.age = person.age; this.address = new Address(person.address.city); class Address String city; public Address(String city) this.city = city; public class Main public static void main(String[] args) Address address = new Address(河南省洛阳市); Person person1 = new Person(沉默王二, 18, address); Person person2 = new Person(person1); System.out.println(person1.address == person2.address); // false ②、序列化与反序列化 import java.io.*;class Person implements Serializable String name; int age; Address address; public Person(String name, int age, Address address) this.name = name; this.age = age; this.address = address; public Person deepClone() throws IOException, ClassNotFoundException // 1. 创建字节数组输出流，用于将对象序列化为字节数组 ByteArrayOutputStream bos = new ByteArrayOutputStream(); // 2. 创建对象输出流，将对象写入到字节数组输出流中 ObjectOutputStream oos = new ObjectOutputStream(bos); // 3. 将当前对象(this)序列化并写入到对象输出流中 // 这一步将对象转换为字节流 oos.writeObject(this); // 4. 创建字节数组输入流，从字节数组中读取数据 // bos.toByteArray()获取序列化后的字节数组 ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); // 5. 创建对象输入流，从字节数组输入流中读取对象 ObjectInputStream ois = new ObjectInputStream(bis); // 6. 从对象输入流中读取对象并强制转换为Person类型 // 这一步将字节流转换回对象，完成深拷贝 return (Person) ois.readObject(); class Address implements Serializable String city; public Address(String city) this.city = city; public class Main public static void main(String[] args) throws IOException, ClassNotFoundException Address address = new Address(河南省洛阳市); Person person1 = new Person(沉默王二, 18, address); Person person2 = person1.deepClone(); System.out.println(person1.address == person2.address); // false 32.Java 创建对象有哪几种方式？ Java 有四种创建对象的方式： ①、new 关键字创建，这是最常见和直接的方式，通过调用类的构造方法来创建对象。 Person person = new Person(); ②、反射机制创建，反射机制允许在运行时创建对象，并且可以访问类的私有成员，在框架和工具类中比较常见。 Class clazz = Class.forName(Person);Person person = (Person) clazz.newInstance();// 新方式（推荐）//Person p2 = Person.class.getDeclaredConstructor().newInstance();// 调用有参构造方法//ConstructorPerson constructor = Person.class.getDeclaredConstructor(String.class, int.class);//Person p3 = constructor.newInstance(张三, 25); ③、clone 拷贝创建，通过 clone 方法创建对象，需要实现 Cloneable 接口并重写 clone 方法。 Person person = new Person();Person person2 = (Person) person.clone(); ④、序列化机制创建，通过序列化将对象转换为字节流，再通过反序列化从字节流中恢复对象。需要实现 Serializable 接口。 Person person = new Person();ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(person.txt));oos.writeObject(person);ObjectInputStream ois = new ObjectInputStream(new FileInputStream(person.txt));Person person2 = (Person) ois.readObject(); new 子类的时候，子类和父类静态代码块，构造方法的执行顺序在 Java 中，当创建一个子类对象时，子类和父类的静态代码块、构造方法的执行顺序遵循一定的规则。这些规则主要包括以下几个步骤： 首先执行父类的静态代码块（仅在类第一次加载时执行）。 接着执行子类的静态代码块（仅在类第一次加载时执行）。 再执行父类的构造方法。 最后执行子类的构造方法。 下面是一个详细的代码示例： class Parent // 父类静态代码块 static System.out.println(父类静态代码块); // 父类构造方法 public Parent() System.out.println(父类构造方法); class Child extends Parent // 子类静态代码块 static System.out.println(子类静态代码块); // 子类构造方法 public Child() System.out.println(子类构造方法); public class Main public static void main(String[] args) new Child(); 执行上述代码时，输出结果如下： 父类静态代码块子类静态代码块父类构造方法子类构造方法 静态代码块：在类加载时执行，仅执行一次，按父类-子类的顺序执行。 构造方法：在每次创建对象时执行，按父类-子类的顺序执行，先初始化块后构造方法。 String篇33.String 是 Java 基本数据类型吗？可以被继承吗？不是，String 是一个类，属于引用数据类型。Java 的基本数据类型包括八种：四种整型（byte、short、int、long）、两种浮点型（float、double）、一种字符型（char）和一种布尔型（boolean）。 String 类可以继承吗？不行。String 类使用 final 修饰，是所谓的不可变类，无法被继承。 String 有哪些常用方法？我常用的有： length() - 返回字符串的长度。 charAt(int index) - 返回指定位置的字符。 substring(int beginIndex, int endIndex) - 返回字符串的一个子串，从 beginIndex 到 endIndex-1。 contains(CharSequence s) - 检查字符串是否包含指定的字符序列。 equals(Object anotherObject) - 比较两个字符串的内容是否相等。 indexOf(int ch) 和 indexOf(String str) - 返回指定字符或字符串首次出现的位置。 replace(char oldChar, char newChar) 和 replace(CharSequence target, CharSequence replacement) - 替换字符串中的字符或字符序列。 trim() - 去除字符串两端的空白字符。 split(String regex) - 根据给定正则表达式的匹配拆分此字符串。 34.🌟String 和 StringBuilder、StringBuffer 的区别？推荐阅读：StringBuffer 和 StringBuilder 两兄弟 String、StringBuilder和StringBuffer在 Java 中都是用于处理字符串的，它们之间的区别是，String 是不可变的，平常开发用得最多，当遇到大量字符串连接时，就用 StringBuilder，它不会生成很多新的对象，StringBuffer 和 StringBuilder 类似，但每个方法上都加了 synchronized 关键字，所以是线程安全的。 请说说 String 的特点 String类的对象是不可变的。也就是说，一旦一个String对象被创建，它所包含的字符串内容是不可改变的。 每次对String对象进行修改操作（如拼接、替换等）实际上都会生成一个新的String对象，而不是修改原有对象。这可能会导致内存和性能开销，尤其是在大量字符串操作的情况下。 请说说 StringBuilder 的特点 StringBuilder提供了一系列的方法来进行字符串的增删改查操作，这些操作都是直接在原有字符串对象的底层数组上进行的，而不是生成新的 String 对象。 StringBuilder不是线程安全的。这意味着在没有外部同步的情况下，它不适用于多线程环境。 相比于String，在进行频繁的字符串修改操作时，StringBuilder能提供更好的性能。 Java 中的字符串连+操作其实就是通过StringBuilder实现的。 请说说 StringBuffer 的特点StringBuffer和StringBuilder类似，但StringBuffer是线程安全的，方法前面都加了synchronized关键字。 请总结一下使用场景 String：适用于字符串内容不会改变的场景，比如说作为 HashMap 的 key。 StringBuilder：适用于单线程环境下需要频繁修改字符串内容的场景，比如在循环中拼接或修改字符串，是 String 的完美替代品。 StringBuffer：现在已经不怎么用了，因为一般不会在多线程场景下去频繁的修改字符串内容。 35.String str1 new String(“abc”) 和 String str2 “abc” 的区别？直接使用双引号为字符串变量赋值时，Java 首先会检查字符串常量池中是否已经存在相同内容的字符串。 如果存在，Java 就会让新的变量引用池中的那个字符串；如果不存在，它会创建一个新的字符串，放入池中，并让变量引用它。 使用 new String(abc) 的方式创建字符串时，实际分为两步： 第一步，先检查字符串字面量 “abc” 是否在字符串常量池中，如果没有则创建一个；如果已经存在，则引用它。 第二步，在堆中再创建一个新的字符串对象，并将其初始化为字符串常量池中 “abc” 的一个副本。 也就是说： String s1 = 沉默王二;String s2 = 沉默王二;String s3 = new String(沉默王二);System.out.println(s1 == s2); // 输出 true，因为 s1 和 s2 引用的是字符串常量池中同一个对象。System.out.println(s1 == s3); // 输出 false，因为 s3 是通过 new 关键字显式创建的，指向堆上不同的对象。 String s new String(“abc”)创建了几个对象？字符串常量池中如果之前已经有一个，则不再创建新的，直接引用；如果没有，则创建一个。 堆中肯定有一个，因为只要使用了 new 关键字，肯定会在堆中创建一个。 36.String 是不可变类吗？字符串拼接是如何实现的？String 是不可变的，这意味着一旦一个 String 对象被创建，其存储的文本内容就不能被改变。这是因为： ①、不可变性使得 String 对象在使用中更加安全。因为字符串经常用作参数传递给其他 Java 方法，例如网络连接、打开文件等。 如果 String 是可变的，这些方法调用的参数值就可能在不知不觉中被改变，从而导致网络连接被篡改、文件被莫名其妙地修改等问题。 ②、不可变的对象因为状态不会改变，所以更容易进行缓存和重用。字符串常量池的出现正是基于这个原因。 当代码中出现相同的字符串字面量时，JVM 会确保所有的引用都指向常量池中的同一个对象，从而节约内存。 ③、因为 String 的内容不会改变，所以它的哈希值也就固定不变。这使得 String 对象特别适合作为 HashMap 或 HashSet 等集合的键，因为计算哈希值只需要进行一次，提高了哈希表操作的效率。 字符串拼接是如何实现的？因为 String 是不可变的，因此通过“+”操作符进行的字符串拼接，会生成新的字符串对象。 例如： String a = hello ;String b = world!;String ab = a + b; a 和 b 是通过双引号定义的，所以会在字符串常量池中，而 ab 是通过“+”操作符拼接的，所以会在堆中生成一个新的对象。 Java 8 时，JDK 对“+”号的字符串拼接进行了优化，Java 会在编译期基于 StringBuilder 的 append 方法进行拼接。 下面是通过 javap -verbose 命令反编译后的字节码，能清楚的看到 StringBuilder 的创建和 append 方法的调用。 //表示这个方法需要两个操作数栈深度 4个局部变量槽 参数数量为1stack=2, locals=4, args_size=1 //从常量池加载#2位置的常量hello到栈顶 0: ldc #2 // String hello //将栈顶的值存储到局部变量表slot 1(对应变量a) 2: astore_1 //从常量池加载#3位置的常量(world!)到栈顶 3: ldc #3 // String world! //将栈顶值存储到局部变量表slot 2(对应变量b) 5: astore_2 //创建StringBuilder对象 6: new #4 // class java/lang/StringBuilder //复制栈顶值(新创建的StringBuilder对象) 9: dup //调用StringBuilder的构造方法 10: invokespecial #5 // Method java/lang/StringBuilder.init:()V //加载局部变量slot 1的值(变量a)到栈顶 13: aload_1 //调用StringBuilder.append()方法拼接第一个字符串 14: invokevirtual #6 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; //加载局部变量slot 2的值(变量b)到栈顶 17: aload_2 //再次调用StringBuilder.append()方法拼接第二个字符串 18: invokevirtual #6 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; //调用StringBuilder.toString()方法生成最终字符串 21: invokevirtual #7 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; //将结果存储到局部变量slot 3(对应变量ab) 24: astore_3 //方法返回 25: return 也就是说，上面的代码相当于： String a = hello ;String b = world!;StringBuilder sb = new StringBuilder();sb.append(a);sb.append(b);String ab = sb.toString(); 因此，如果笼统地讲，通过加号拼接字符串时会创建多个 String 对象是不准确的。因为加号拼接在编译期还会创建一个 StringBuilder 对象，最终调用 toString() 方法的时候再返回一个新的 String 对象。 @Overridepublic String toString() // Create a copy, dont share the array return new String(value, 0, count); 那除了使用 + 号来拼接字符串，还有 StringBuilder.append()、String.join() 等方式。 推荐阅读：如何拼接字符串？ 如何保证 String 不可变？第一，String 类内部使用一个私有的字符数组来存储字符串数据。这个字符数组在创建字符串时被初始化，之后不允许被改变。 private final char value[]; 第二，String 类没有提供任何可以修改其内容的公共方法，像 concat 这些看似修改字符串的操作，实际上都是返回一个新创建的字符串对象，而原始字符串对象保持不变。 public String concat(String str) if (str.isEmpty()) return this; int len = value.length; int otherLen = str.length(); char buf[] = Arrays.copyOf(value, len + otherLen); str.getChars(buf, len); return new String(buf, true); 第三，String 类本身被声明为 final，这意味着它不能被继承。这防止了子类可能通过添加修改方法来改变字符串内容的可能性。 public final class String 37.intern 方法有什么作用？JDK 源码里已经对这个方法进行了说明： * p* When the intern method is invoked, if the pool already contains a* string equal to this @code String object as determined by* the @link #equals(Object) method, then the string from the pool is* returned. Otherwise, this @code String object is added to the* pool and a reference to this @code String object is returned.* p 意思也很好懂： 如果当前字符串内容存在于字符串常量池（即 equals()方法为 true，也就是内容一样），直接返回字符串常量池中的字符串 否则，将此 String 对象添加到池中，并返回 String 对象的引用 Integer篇38.判断几个Integer对象是否相等Integer a 127，Integer b 127；Integer c 128，Integer d 128；相等吗? 推荐阅读：IntegerCache 推荐阅读：深入浅出 Java 拆箱与装箱 a 和 b 相等，c 和 d 不相等。 这个问题涉及到 Java 的自动装箱机制以及Integer类的缓存机制。 对于第一对： Integer a = 127;Integer b = 127; a和b是相等的。这是因为 Java 在自动装箱过程中，会使用Integer.valueOf()方法来创建Integer对象。 Integer.valueOf()方法会针对数值在-128 到 127 之间的Integer对象使用缓存。因此，a和b实际上引用了常量池中相同的Integer对象。 对于第二对： Integer c = 128;Integer d = 128; c和d不相等。这是因为 128 超出了Integer缓存的范围(-128 到 127)。 因此，自动装箱过程会为c和d创建两个不同的Integer对象，它们有不同的引用地址。 可以通过==运算符来检查它们是否相等： System.out.println(a == b); // 输出trueSystem.out.println(c == d); // 输出false 要比较Integer对象的数值是否相等，应该使用equals方法，而不是==运算符： System.out.println(a.equals(b)); // 输出trueSystem.out.println(c.equals(d)); // 输出true 使用equals方法时，c和d的比较结果为true，因为equals比较的是对象的数值，而不是引用地址。 什么是 Integer 缓存？就拿 Integer 的缓存吃来说吧。根据实践发现，大部分的数据操作都集中在值比较小的范围，因此 Integer 搞了个缓存池，默认范围是 -128 到 127。 当我们使用自动装箱来创建这个范围内的 Integer 对象时，Java 会直接从缓存中返回一个已存在的对象，而不是每次都创建一个新的对象。这意味着，对于这个值范围内的所有 Integer 对象，它们实际上是引用相同的对象实例。 Integer 缓存的主要目的是优化性能和内存使用。对于小整数的频繁操作，使用缓存可以显著减少对象创建的数量。 可以在运行的时候添加 -Djava.lang.Integer.IntegerCache.high=1000 来调整缓存池的最大值。 引用是 Integer 类型， 右侧是 int 基本类型时，会进行自动装箱，调用的其实是 Integer.valueOf()方法，它会调用 IntegerCache。 public static Integer valueOf(int i) if (i = IntegerCache.low i = IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); IntegerCache 是一个静态内部类，在静态代码块中会初始化好缓存的值。 private static class IntegerCache …… static //创建Integer对象存储 for(int k = 0; k cache.length; k++) cache[k] = new Integer(j++); …… new Integer(10) new Integer(10) 相等吗在 Java 中，使用new Integer(10) == new Integer(10)进行比较时，结果是 false。 这是因为 new 关键字会在堆（Heap）上为每个 Integer 对象分配新的内存空间，所以这里创建了两个不同的 Integer 对象，它们有不同的内存地址。 当使用==运算符比较这两个对象时，实际上比较的是它们的内存地址，而不是它们的值，因此即使两个对象代表相同的数值（10），结果也是 false。 39.String 怎么转成 Integer 的？原理？String 转成 Integer，主要有两个方法： Integer.parseInt(String s)Integer.valueOf(String s)不管哪一种，最终还是会调用 Integer 类内中的parseInt(String s, int radix)方法。 抛去一些边界之类的看看核心代码： public static int parseInt(String s, int radix) throws NumberFormatException int result = 0; //是否是负数 boolean negative = false; //char字符数组下标和长度 int i = 0, len = s.length(); …… int digit; //判断字符长度是否大于0，否则抛出异常 if (len 0) …… while (i len) // Accumulating negatively avoids surprises near MAX_VALUE //返回指定基数中字符表示的数值。（此处是十进制数值） digit = Character.digit(s.charAt(i++),radix); //进制位乘以数值 result *= radix; result -= digit; //根据上面得到的是否负数，返回相应的值 return negative ? result : -result; 去掉枝枝蔓蔓（当然这些枝枝蔓蔓可以去看看，源码 cover 了很多情况），其实剩下的就是一个简单的字符串遍历计算，不过计算方式有点反常规，是用负的值累减。 Object40.Object 类的常见方法？在 Java 中，经常提到一个词“万物皆对象”，其中的“万物”指的是 Java 中的所有类，而这些类都是 Object 类的子类。 Object 主要提供了 11 个方法，大致可以分为六类： 对象比较：①、public native int hashCode() ：native 方法，用于返回对象的哈希码。 public native int hashCode(); 按照约定，相等的对象必须具有相等的哈希码。如果重写了 equals 方法，就应该重写 hashCode 方法。可以使用 Objects.hash() 方法来生成哈希码。 public int hashCode() return Objects.hash(name, age); ②、public boolean equals(Object obj)：用于比较 2 个对象的内存地址是否相等。 public boolean equals(Object obj) return (this == obj); 如果比较的是两个对象的值是否相等，就要重写该方法，比如 String 类、Integer 类等都重写了该方法。举个例子，假如有一个 Person 类，我们认为只要年龄和名字相同，就是同一个人，那么就可以这样重写 equals 方法： class Person1 private String name; private int age; // 省略 gettter 和 setter 方法 public boolean equals(Object obj) if (this == obj) return true; if (obj instanceof Person1) Person1 p = (Person1) obj; return this.name.equals(p.getName()) this.age == p.getAge(); return false; 对象拷贝：protected native Object clone() throws CloneNotSupportedException：native 方法，返回此对象的一个副本。默认实现只做浅拷贝，且类必须实现 Cloneable 接口。 Object 本身没有实现 Cloneable 接口，所以在不重写 clone 方法的情况下直接直接调用该方法会发生 CloneNotSupportedException 异常。 对象转字符串：public String toString()：返回对象的字符串表示。默认实现返回类名@哈希码的十六进制表示，但通常会被重写以返回更有意义的信息。 public String toString() return getClass().getName() + @ + Integer.toHexString(hashCode()); 比如说一个 Person 类，我们可以重写 toString 方法，返回一个有意义的字符串： public String toString() return Person + name= + name + \\ + , age= + age + ; 当然了，这项工作也可以直接交给 IDE，比如 IntelliJ IDEA，直接右键选择 Generate，然后选择 toString 方法，就会自动生成一个 toString 方法。 也可以交给 Lombok，使用 @Data 注解，它会自动生成 toString 方法。 数组也是一个对象，所以通常我们打印数组的时候，会看到诸如 [I@1b6d3586 这样的字符串，这个就是 int 数组的哈希码。 多线程调度：每个对象都可以调用 Object 的 wait/notify 方法来实现等待通知机制。我们来写一个例子： public class WaitNotifyDemo public static void main(String[] args) Object lock = new Object(); new Thread(() - synchronized (lock) System.out.println(线程1：我要等待); try lock.wait(); catch (InterruptedException e) e.printStackTrace(); System.out.println(线程1：我被唤醒了); ).start(); new Thread(() - synchronized (lock) System.out.println(线程2：我要唤醒); lock.notify(); System.out.println(线程2：我已经唤醒了); ).start(); 解释一下： 线程 1 先执行，它调用了 lock.wait() 方法，然后进入了等待状态。 线程 2 后执行，它调用了 lock.notify() 方法，然后线程 1 被唤醒了。 ①、public final void wait() throws InterruptedException：调用该方法会导致当前线程等待，直到另一个线程调用此对象的notify()方法或notifyAll()方法。 ②、public final native void notify()：唤醒在此对象监视器上等待的单个线程。如果有多个线程等待，选择一个线程被唤醒。 ③、public final native void notifyAll()：唤醒在此对象监视器上等待的所有线程。 ④、public final native void wait(long timeout) throws InterruptedException：等待 timeout 毫秒，如果在 timeout 毫秒内没有被唤醒，会自动唤醒。 ⑥、public final void wait(long timeout, int nanos) throws InterruptedException：更加精确了，等待 timeout 毫秒和 nanos 纳秒，如果在 timeout 毫秒和 nanos 纳秒内没有被唤醒，会自动唤醒。 反射：推荐阅读：二哥的 Java 进阶之路：掌握 Java 反射 public final native Class? getClass()：用于获取对象的类信息，如类名。比如说： public class GetClassDemo public static void main(String[] args) Person p = new Person(); Class? extends Person aClass = p.getClass(); System.out.println(aClass.getName()); 输出结果： com.itwanger.Person 垃圾回收：protected void finalize() throws Throwable：当垃圾回收器决定回收对象占用的内存时调用此方法。用于清理资源，但 Java 不推荐使用，因为它不可预测且容易导致问题，Java 9 开始已被弃用。 异常处理41.🌟Java 中异常处理体系?推荐阅读：一文彻底搞懂 Java 异常处理 Java 中的异常处理机制用于处理程序运行过程中可能发生的各种异常情况，通常通过 try-catch-finally 语句和 throw 关键字来实现。 Throwable 是 Java 语言中所有错误和异常的基类。它有两个主要的子类：Error 和 Exception，这两个类分别代表了 Java 异常处理体系中的两个分支。 Error 类代表那些严重的错误，这类错误通常是程序无法处理的。比如，OutOfMemoryError 表示内存不足，StackOverflowError 表示栈溢出。这些错误通常与 JVM 的运行状态有关，一旦发生，应用程序通常无法恢复。 Exception 类代表程序可以处理的异常。它分为两大类：编译时异常（Checked Exception）和运行时异常（Runtime Exception）。 ①、编译时异常（Checked Exception）：这类异常在编译时必须被显式处理（捕获或声明抛出）。 如果方法可能抛出某种编译时异常，但没有捕获它（try-catch）或没有在方法声明中用 throws 子句声明它，那么编译将不会通过。例如：IOException、SQLException 等。 ②、运行时异常（Runtime Exception）：这类异常在运行时抛出，它们都是 RuntimeException 的子类。对于运行时异常，Java 编译器不要求必须处理它们（即不需要捕获也不需要声明抛出）。 运行时异常通常是由程序逻辑错误导致的，如NullPointerException、IndexOutOfBoundsException 等。 42.异常的处理方式？ ①、遇到异常时可以不处理，直接通过throw 和 throws 抛出异常，交给上层调用者处理。 throws 关键字用于声明可能会抛出的异常，而 throw 关键字用于抛出异常。 public void test() throws Exception throw new Exception(抛出异常); ②、使用 try-catch 捕获异常，处理异常。 try //包含可能会出现异常的代码以及声明异常的方法catch(Exception e) //捕获异常并进行处理finally //可选，必执行的代码 catch和finally的异常可以同时抛出吗？如果 catch 块抛出一个异常，而 finally 块中也抛出异常，那么最终抛出的将是 finally 块中的异常。catch 块中的异常会被丢弃，而 finally 块中的异常会覆盖并向上传递。 public class Example public static void main(String[] args) try throw new Exception(Exception in try); catch (Exception e) throw new RuntimeException(Exception in catch); finally throw new IllegalArgumentException(Exception in finally); try 块首先抛出一个 Exception。 控制流进入 catch 块，catch 块中又抛出了一个 RuntimeException。 但是在 finally 块中，抛出了一个 IllegalArgumentException，最终程序抛出的异常是 finally 块中的 IllegalArgumentException。 虽然 catch 和 finally 中的异常不能同时抛出，但可以手动捕获 finally 块中的异常，并将 catch 块中的异常保留下来，避免被覆盖。常见的做法是使用一个变量临时存储 catch 中的异常，然后在 finally 中处理该异常： public class Example public static void main(String[] args) Exception catchException = null; try throw new Exception(Exception in try); catch (Exception e) catchException = e; throw new RuntimeException(Exception in catch); finally try throw new IllegalArgumentException(Exception in finally); catch (IllegalArgumentException e) if (catchException != null) System.out.println(Catch exception: + catchException.getMessage()); System.out.println(Finally exception: + e.getMessage()); 43.三道经典异常处理代码题题目 1public class TryDemo public static void main(String[] args) System.out.println(test()); public static int test() try return 1; catch (Exception e) return 2; finally System.out.print(3); 在test()方法中，首先有一个try块，接着是一个catch块（用于捕获异常），最后是一个finally块（无论是否捕获到异常，finally块总会执行）。 ①、try块中包含一条return 1;语句。正常情况下，如果try块中的代码能够顺利执行，那么方法将返回数字1。在这个例子中，try块中没有任何可能抛出异常的操作，因此它会正常执行完毕，并准备返回1。 ②、由于try块中没有异常发生，所以catch块中的代码不会执行。 ③、无论前面的代码是否发生异常，finally块总是会执行。在这个例子中，finally块包含一条System.out.print(3);语句，意味着在方法结束前，会在控制台打印出3。 当执行main方法时，控制台的输出将会是： 这是因为finally块确保了它包含的System.out.print(3);会执行并打印3，随后test()方法返回try块中的值1，最终结果就是31。 题目 2public class TryDemo public static void main(String[] args) System.out.println(test1()); public static int test1() try return 2; finally return 3; 执行结果：3。 try 返回前先执行 finally，结果 finally 里不按套路出牌，直接 return 了，自然也就走不到 try 里面的 return 了。 注意：finally 里面使用 return 仅存在于面试题中，实际开发这么写要挨吊的（😂）。 题目 3public class TryDemo public static void main(String[] args) System.out.println(test1()); public static int test1() int i = 0; try i = 2; return i; finally i = 3; 执行结果：2。 大家可能会以为结果应该是 3，因为在 return 前会执行 finally，而 i 在 finally 中被修改为 3 了，那最终返回 i 不是应该为 3 吗？ 但其实，在执行 finally 之前，JVM 会先将 i 的结果暂存起来，然后 finally 执行完毕后，会返回之前暂存的结果，而不是返回 i，所以即使 i 已经被修改为 3，最终返回的还是之前暂存起来的结果 2。 IO44.Java 中 IO 流分为几种?Java IO 流的划分可以根据多个维度进行，包括数据流的方向（输入或输出）、处理的数据单位（字节或字符）、流的功能以及流是否支持随机访问等。 按照数据流方向如何划分？ 输入流（Input Stream）：从源（如文件、网络等）读取数据到程序。 输出流（Output Stream）：将数据从程序写出到目的地（如文件、网络、控制台等）。 按处理数据单位如何划分？ 字节流（Byte Streams）：以字节为单位读写数据，主要用于处理二进制数据，如音频、图像文件等。 字符流（Character Streams）：以字符为单位读写数据，主要用于处理文本数据。 按功能如何划分？ 节点流（Node Streams）：直接与数据源或目的地相连，如 FileInputStream、FileOutputStream。 处理流（Processing Streams）：对一个已存在的流进行包装，如缓冲流 BufferedInputStream、BufferedOutputStream。 管道流（Piped Streams）：用于线程之间的数据传输，如 PipedInputStream、PipedOutputStream。 IO 流用到了什么设计模式？其实，Java 的 IO 流体系还用到了一个设计模式——装饰器模式。 Java 缓冲区溢出，如何预防Java 缓冲区溢出主要是由于向缓冲区写入的数据超过其能够存储的数据量。可以采用这些措施来避免： ①、合理设置缓冲区大小：在创建缓冲区时，应根据实际需求合理设置缓冲区的大小，避免创建过大或过小的缓冲区。 ②、控制写入数据量：在向缓冲区写入数据时，应该控制写入的数据量，确保不会超过缓冲区的容量。Java 的 ByteBuffer 类提供了remaining()方法，可以获取缓冲区中剩余的可写入数据量。 import java.nio.ByteBuffer;public class ByteBufferExample public static void main(String[] args) // 模拟接收到的数据 byte[] receivedData = 1, 2, 3, 4, 5; int bufferSize = 1024; // 设置一个合理的缓冲区大小 // 创建ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(bufferSize); // 写入数据之前检查容量是否足够 if (buffer.remaining() = receivedData.length) buffer.put(receivedData); else System.out.println(Not enough space in buffer to write data.); // 准备读取数据：将limit设置为当前位置，position设回0 buffer.flip(); // 读取数据 while (buffer.hasRemaining()) byte data = buffer.get(); System.out.println(Read data: + data); // 清空缓冲区以便再次使用 buffer.clear(); 45.既然有了字节流,为什么还要有字符流?其实字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还比较耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。 所以， IO 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 文本存储是字节流还是字符流，视频文件呢？在计算机中，文本和视频都是按照字节存储的，只是如果是文本文件的话，我们可以通过字符流的形式去读取，这样更方面的我们进行直接处理。 比如说我们需要在一个大文本文件中查找某个字符串，可以直接通过字符流来读取判断。 处理视频文件时，通常使用字节流（如 Java 中的FileInputStream、FileOutputStream）来读取或写入数据，并且会尽量使用缓冲流（如BufferedInputStream、BufferedOutputStream）来提高读写效率。 在技术派实战项目项目中，对于文本，比如说文章和教程内容，是直接存储在数据库中的，而对于视频和图片等大文件，是存储在 OSS 中的。 因此，无论是文本文件还是视频文件，它们在物理存储层面都是以字节流的形式存在。区别在于，我们如何通过 Java 代码来解释和处理这些字节流：作为编码后的字符还是作为二进制数据。 46.🌟BIO、NIO、AIO 之间的区别？推荐阅读：Java NIO 比传统 IO 强在哪里？ Java 常见的 IO 模型有三种：BIO、NIO 和 AIO。 BIO：采用阻塞式 IO 模型，线程在执行 IO 操作时被阻塞，无法处理其他任务，适用于连接数较少的场景。 NIO：采用非阻塞 I/O 模型，线程在等待 IO 时可执行其他任务，通过 Selector 监控多个 Channel 上的事件，适用于连接数多但连接时间短的场景。 AIO：使用异步 I/O 模型，线程发起 IO 请求后立即返回，当 IO 操作完成时通过回调函数通知线程，适用于连接数多且连接时间长的场景。 简单说一下 BIO？BIO，也就是传统的 IO，基于字节流或字符流（如 FileInputStream、BufferedReader 等）进行文件读写，基于 Socket 和 ServerSocket 进行网络通信。 对于每个连接，都需要创建一个独立的线程来处理读写操作。 简单说下 NIO？NIO，JDK 1.4 时引入，放在 java.nio 包下，提供了 Channel、Buffer、Selector 等新的抽象，基于 RandomAccessFile、FileChannel、ByteBuffer 进行文件读写，基于 SocketChannel 和 ServerSocketChannel 进行网络通信。 实际上，“旧”的 IO 包已经使用 NIO 重新实现过，所以在进行文件读写时，NIO 并无法体现出比 BIO 更可靠的性能。 NIO 的魅力主要体现在网络编程中，服务器可以用一个线程处理多个客户端连接，通过 Selector 监听多个 Channel 来实现多路复用，极大地提高了网络编程的性能。 缓冲区 Buffer 也能极大提升一次 IO 操作的效率。 简单说下 AIO？AIO 是 Java 7 引入的，放在 java.nio.channels 包下，提供了 AsynchronousFileChannel、AsynchronousSocketChannel 等异步 Channel。 它引入了异步通道的概念，使得 IO 操作可以异步进行。这意味着线程发起一个读写操作后不必等待其完成，可以立即进行其他任务，并且当读写操作真正完成时，线程会被异步地通知。 AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(Paths.get(test.txt), StandardOpenOption.READ);ByteBuffer buffer = ByteBuffer.allocate(1024);FutureInteger result = fileChannel.read(buffer, 0);while (!result.isDone()) // do something 序列化47.什么是序列化？什么是反序列化？序列化（Serialization）是指将对象转换为字节流的过程，以便能够将该对象保存到文件、数据库，或者进行网络传输。 反序列化（Deserialization）就是将字节流转换回对象的过程，以便构建原始对象。 Serializable 接口有什么用？Serializable接口用于标记一个类可以被序列化。 public class Person implements Serializable private String name; private int age; // 省略 getter 和 setter 方法 serialVersionUID 有什么用？serialVersionUID 是 Java 序列化机制中用于标识类版本的唯一标识符。它的作用是确保在序列化和反序列化过程中，类的版本是兼容的。 import java.io.Serializable;public class MyClass implements Serializable private static final long serialVersionUID = 1L; private String name; private int age; // getters and setters serialVersionUID 被设置为 1L 是一种比较省事的做法，也可以使用 Intellij IDEA 进行自动生成。 但只要 serialVersionUID 在序列化和反序列化过程中保持一致，就不会出现问题。 如果不显式声明 serialVersionUID，Java 运行时会根据类的详细信息自动生成一个 serialVersionUID。那么当类的结构发生变化时，自动生成的 serialVersionUID 就会发生变化，导致反序列化失败。 Java 序列化不包含静态变量吗？是的，序列化机制只会保存对象的状态，而静态变量属于类的状态，不属于对象的状态。 如果有些变量不想序列化，怎么办？可以使用transient关键字修饰不想序列化的变量。 public class Person implements Serializable private String name; private transient int age; // 省略 getter 和 setter 方法 能解释一下序列化的过程和作用吗？序列化过程通常涉及到以下几个步骤： 第一步，实现 Serializable 接口。 public class Person implements Serializable private String name; private int age; // 省略构造方法、getters和setters 第二步，使用 ObjectOutputStream 来将对象写入到输出流中。 ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(person.ser)); 第三步，调用 ObjectOutputStream 的 writeObject 方法，将对象序列化并写入到输出流中。 Person person = new Person(沉默王二, 18);out.writeObject(person); 48.说说有几种序列化方式？Java 序列化方式有很多，常见的有三种： Java 对象序列化 ：Java 原生序列化方法即通过 Java 原生流(InputStream 和 OutputStream 之间的转化)的方式进行转化，一般是对象输出流 ObjectOutputStream和对象输入流ObjectInputStream。 Json 序列化：这个可能是我们最常用的序列化方式，Json 序列化的选择很多，一般会使用 jackson 包，通过 ObjectMapper 类来进行一些操作，比如将对象转化为 byte 数组或者将 json 串转化为对象。 ProtoBuff 序列化：ProtocolBuffer 是一种轻便高效的结构化数据存储格式，ProtoBuff 序列化对象可以很大程度上将其压缩，可以大大减少数据传输大小，提高系统性能。 网络编程49.了解过Socket网络套接字吗？（补充）推荐阅读：Java Socket：飞鸽传书的网络套接字 Socket 是网络通信的基础，表示两台设备之间通信的一个端点。Socket 通常用于建立 TCP 或 UDP 连接，实现进程间的网络通信。 一个简单的 TCP 客户端： class TcpClient public static void main(String[] args) throws IOException Socket socket = new Socket(127.0.0.1, 8080); // 连接服务器 BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); PrintWriter out = new PrintWriter(socket.getOutputStream(), true); out.println(Hello, Server!); // 发送消息 System.out.println(Server response: + in.readLine()); // 接收服务器响应 socket.close(); TCP 服务端： class TcpServer public static void main(String[] args) throws IOException ServerSocket serverSocket = new ServerSocket(8080); // 创建服务器端Socket System.out.println(Server started, waiting for connection...); Socket socket = serverSocket.accept(); // 等待客户端连接 System.out.println(Client connected: + socket.getInetAddress()); BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); PrintWriter out = new PrintWriter(socket.getOutputStream(), true); String message; while ((message = in.readLine()) != null) System.out.println(Received: + message); out.println(Echo: + message); // 回送消息 socket.close(); serverSocket.close(); RPC框架了解吗？RPC是一种协议，允许程序调用位于远程服务器上的方法，就像调用本地方法一样。RPC 通常基于 Socket 通信实现。 RPC，Remote Procedure Call，远程过程调用 RPC 框架支持高效的序列化（如 Protocol Buffers）和通信协议（如 HTTP2），屏蔽了底层网络通信的细节，开发者只需关注业务逻辑即可。 常见的 RPC 框架包括： gRPC：基于 HTTP2 和 Protocol Buffers。 Dubbo：阿里开源的分布式 RPC 框架，适合微服务场景。 Spring Cloud OpenFeign：基于 REST 的轻量级 RPC 框架。 Thrift：Apache 的跨语言 RPC 框架，支持多语言代码生成。 泛型50.Java 泛型了解么？推荐阅读：手写Java泛型，彻底掌握它 泛型主要用于提高代码的类型安全，它允许在定义类、接口和方法时使用类型参数，这样可以在编译时检查类型一致性，避免不必要的类型转换和类型错误。 没有泛型的时候，像 List 这样的集合类存储的是 Object 类型，导致从集合中读取数据时，必须进行强制类型转换，否则会引发 ClassCastException。 List list = new ArrayList();list.add(hello);String str = (String) list.get(0); // 必须强制类型转换 泛型一般有三种使用方式:泛型类、泛型接口、泛型方法。 1.泛型类： //此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型//在实例化泛型类时，必须指定T的具体类型public class GenericT private T key; public Generic(T key) this.key = key; public T getKey() return key; 如何实例化泛型类： GenericInteger genericInteger = new GenericInteger(123456); 2.泛型接口 ： public interface GeneratorT public T method(); 实现泛型接口，指定类型： class GeneratorImplT implements GeneratorString @Override public String method() return hello; 3.泛型方法 ： public static E void printArray( E[] inputArray ) for ( E element : inputArray ) System.out.printf( %s , element ); System.out.println(); 使用： // 创建不同类型数组： Integer, Double 和 CharacterInteger[] intArray = 1, 2, 3 ;String[] stringArray = Hello, World ;printArray( intArray );printArray( stringArray ); 泛型常用的通配符有哪些？常用的通配符为： T，E，K，V，？ ？ 表示不确定的 java 类型 T (type) 表示具体的一个 java 类型 K V (key value) 分别代表 java 键值中的 Key Value E (element) 代表 Element 什么是泛型擦除？所谓的泛型擦除，官方名叫“类型擦除”。 Java 的泛型是伪泛型，这是因为 Java 在编译期间，所有的类型信息都会被擦掉。 也就是说，在运行的时候是没有泛型的。 例如这段代码，往一群猫里放条狗： LinkedListCat cats = new LinkedListCat();LinkedList list = cats; // 注意我在这里把范型去掉了，但是list和cats是同一个链表！list.add(new Dog()); // 完全没问题！ 因为 Java 的范型只存在于源码里，编译的时候给你静态地检查一下范型类型是否正确，而到了运行时就不检查了。上面这段代码在 JRE（Java运行环境）看来和下面这段没区别： LinkedList cats = new LinkedList(); // 注意：没有范型！LinkedList list = cats;list.add(new Dog()); 为什么要类型擦除呢？主要是为了向下兼容，因为 JDK5 之前是没有泛型的，为了让 JVM 保持向下兼容，就出了类型擦除这个策略。 注解51.说一下你对注解的理解？Java 注解本质上是一个标记，可以理解成生活中的一个人的一些小装扮，比如戴什么什么帽子，戴什么眼镜。 注解可以标记在类上、方法上、属性上等，标记自身也可以设置一些值，比如帽子颜色是绿色。 有了标记之后，我们就可以在编译或者运行阶段去识别这些标记，然后搞一些事情，这就是注解的用处。 例如我们常见的 AOP，使用注解作为切点就是运行期注解的应用；比如 lombok，就是注解在编译期的运行。 注解生命周期有三大类，分别是： RetentionPolicy.SOURCE：给编译器用的，不会写入 class 文件 RetentionPolicy.CLASS：会写入 class 文件，在类加载阶段丢弃，也就是运行的时候就没这个信息了 RetentionPolicy.RUNTIME：会写入 class 文件，永久保存，可以通过反射获取注解信息 所以我上文写的是解析的时候，没写具体是解析啥，因为不同的生命周期的解析动作是不同的。 像常见的： 就是给编译器用的，编译器编译的时候检查没问题就 over 了，class 文件里面不会有 Override 这个标记。 再比如 Spring 常见的 Autowired ，就是 RUNTIME 的，所以在运行的时候可以通过反射得到注解的信息，还能拿到标记的值 required 。 反射52.🌟什么是反射？应用？原理？反射允许 Java 在运行时检查和操作类的方法和字段。通过反射，可以动态地获取类的字段、方法、构造方法等信息，并在运行时调用方法或访问字段。 比如创建一个对象是通过 new 关键字来实现的： Person person = new Person(); Person 类的信息在编译时就确定了，那假如在编译期无法确定类的信息，但又想在运行时获取类的信息、创建类的实例、调用类的方法，这时候就要用到反射。 反射功能主要通过 java.lang.Class 类及 java.lang.reflect 包中的类如 Method, Field, Constructor 等来实现。 比如说我们可以装来动态加载类并创建对象： String className = java.util.Date;Class? cls = Class.forName(className);Object obj = cls.newInstance();System.out.println(obj.getClass().getName()); 比如说我们可以这样来访问字段和方法： // 加载并实例化类Class? cls = Class.forName(java.util.Date);Object obj = cls.newInstance();// 获取并调用方法Method method = cls.getMethod(getTime);Object result = method.invoke(obj);System.out.println(Time: + result);// 访问字段Field field = cls.getDeclaredField(fastTime);field.setAccessible(true); // 对于私有字段需要这样做System.out.println(fastTime: + field.getLong(obj)); 反射有哪些应用场景？①、Spring 框架就大量使用了反射来动态加载和管理 Bean。 Class? clazz = Class.forName(com.example.MyClass);Object instance = clazz.newInstance(); ②、Java 的动态代理（Dynamic Proxy）机制就使用了反射来创建代理类。代理类可以在运行时动态处理方法调用，这在实现 AOP 和拦截器时非常有用。 //创建一个处理器实例,负责实际方法的调用逻辑InvocationHandler handler = new MyInvocationHandler();MyInterface proxyInstance = (MyInterface) Proxy.newProxyInstance( MyInterface.class.getClassLoader(), new Class?[] MyInterface.class , handler); ③、JUnit 和 TestNG 等测试框架使用反射机制来发现和执行测试方法。反射允许框架扫描类，查找带有特定注解（如 @Test）的方法，并在运行时调用它们。 Method testMethod = testClass.getMethod(testSomething);testMethod.invoke(testInstance); 反射的原理是什么？Java 程序的执行分为编译和运行两步，编译之后会生成字节码(.class)文件，JVM 进行类加载的时候，会加载字节码文件，将类型相关的所有信息加载进方法区，反射就是去获取这些信息，然后进行各种操作。 JDK1.8新特性JDK 已经出到 17 了，但是你迭代你的版本，我用我的 8。JDK1.8 的一些新特性，当然现在也不新了，其实在工作中已经很常用了。 53.JDK 1.8 都有哪些新特性？JDK 1.8 新增了不少新的特性，如 Lambda 表达式、接口默认方法、Stream API、日期时间 API、Optional 类等。 ①、Java 8 允许在接口中添加默认方法和静态方法。 public interface MyInterface default void myDefaultMethod() System.out.println(My default method); static void myStaticMethod() System.out.println(My static method); ②、Lambda 表达式描述了一个代码块（或者叫匿名方法），可以将其作为参数传递给构造方法或者普通方法以便后续执行。 public class LamadaTest public static void main(String[] args) new Thread(() - System.out.println(沉默王二)).start(); 《Effective Java》的作者 Josh Bloch 建议使用 Lambda 表达式时，最好不要超过 3 行。否则代码可读性会变得很差。 ③、Stream 是对 Java 集合框架的增强，它提供了一种高效且易于使用的数据处理方式。 ListString list = new ArrayList();list.add(中国加油);list.add(世界加油);list.add(世界加油);long count = list.stream().distinct().count();System.out.println(count); ④、Java 8 引入了一个全新的日期和时间 API，位于java.time包中。这个新的 API 纠正了旧版java.util.Date类中的许多缺陷。 LocalDate today = LocalDate.now();System.out.println(Todays Local date : + today);LocalTime time = LocalTime.now();System.out.println(Local time : + time);LocalDateTime now = LocalDateTime.now();System.out.println(Current DateTime : + now); ⑤、引入 Optional 是为了减少空指针异常。 OptionalString optional = Optional.of(沉默王二);optional.isPresent(); // trueoptional.get(); // 沉默王二optional.orElse(沉默王三); // bamoptional.ifPresent((s) - System.out.println(s.charAt(0))); // 沉 54.Lambda 表达式了解多少？Lambda 表达式主要用于提供一种简洁的方式来表示匿名方法，使 Java 具备了函数式编程的特性。 比如说我们可以使用 Lambda 表达式来简化线程的创建： new Thread(() - System.out.println(Hello World)).start(); 这比以前的匿名内部类要简洁很多。 所谓的函数式编程，就是把函数作为参数传递给方法，或者作为方法的结果返回。比如说我们可以配合 Stream 流进行数据过滤： ListInteger numbers = Arrays.asList(1, 2, 3, 4, 5, 6);ListInteger evenNumbers = numbers.stream() .filter(n - n % 2 == 0) .collect(Collectors.toList()); 其中 n - n % 2 == 0 就是一个 Lambda 表达式。表示传入一个参数 n，返回 n % 2 == 0 的结果。 Java8 有哪些内置函数式接口？JDK 1.8 API 包含了很多内置的函数式接口。其中就包括我们在老版本中经常见到的 Comparator 和 Runnable，Java 8 为他们都添加了 @FunctionalInterface 注解，以用来支持 Lambda 表达式。 除了这两个之外，还有 Callable、Predicate、Function、Supplier、Consumer 等等。 55.Optional 了解吗？Optional是用于防范NullPointerException。 可以将 Optional 看做是包装对象（可能是 null, 也有可能非 null）的容器。当我们定义了 一个方法，这个方法返回的对象可能是空，也有可能非空的时候，我们就可以考虑用 Optional 来包装它，这也是在 Java 8 被推荐使用的做法。 OptionalString optional = Optional.of(bam);optional.isPresent(); // trueoptional.get(); // bamoptional.orElse(fallback); // bamoptional.ifPresent((s) - System.out.println(s.charAt(0))); // b 56.Stream 流用过吗？Stream 流，简单来说，使用 java.util.Stream 对一个包含一个或多个元素的集合做各种操作。这些操作可能是 中间操作 亦或是 终端操作。 终端操作会返回一个结果，而中间操作会返回一个 Stream 流。 Stream 流一般用于集合，我们对一个集合做几个常见操作： ListString stringCollection = new ArrayList();stringCollection.add(ddd2);stringCollection.add(aaa2);stringCollection.add(bbb1);stringCollection.add(aaa1);stringCollection.add(bbb3);stringCollection.add(ccc);stringCollection.add(bbb2);stringCollection.add(ddd1);Filter 过滤stringCollection .stream() .filter((s) - s.startsWith(a)) .forEach(System.out::println);// aaa2, aaa1 Sorted 排序 stringCollection .stream() .sorted() .filter((s) - s.startsWith(a)) .forEach(System.out::println);// aaa1, aaa2 Map 转换 stringCollection .stream() .map(String::toUpperCase) .sorted((a, b) - b.compareTo(a)) .forEach(System.out::println);// DDD2, DDD1, CCC, BBB3, BBB2, AAA2, AAA1 Match 匹配 // 验证 list 中 string 是否有以 a 开头的, 匹配到第一个，即返回 trueboolean anyStartsWithA = stringCollection .stream() .anyMatch((s) - s.startsWith(a));System.out.println(anyStartsWithA); // true// 验证 list 中 string 是否都是以 a 开头的boolean allStartsWithA = stringCollection .stream() .allMatch((s) - s.startsWith(a));System.out.println(allStartsWithA); // false// 验证 list 中 string 是否都不是以 z 开头的,boolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) - s.startsWith(z));System.out.println(noneStartsWithZ); // true Count 计数count 是一个终端操作，它能够统计 stream 流中的元素总数，返回值是 long 类型。 // 先对 list 中字符串开头为 b 进行过滤，让后统计数量long startsWithB = stringCollection .stream() .filter((s) - s.startsWith(b)) .count();System.out.println(startsWithB); // 3 ReduceReduce 中文翻译为：减少、缩小。通过入参的 Function，我们能够将 list 归约成一个值。它的返回类型是 Optional 类型。 OptionalString reduced = stringCollection .stream() .sorted() .reduce((s1, s2) - s1 + # + s2);reduced.ifPresent(System.out::println);// aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2 以上是常见的几种流式操作，还有其它的一些流式操作，可以帮助我们更便捷地处理集合数据。","tags":["基础","Java"],"categories":["Java问答笔记"]},{"title":"Redis学习笔记","path":"/2025/07/22/Java问答笔记/Redis学习笔记/","content":"基础1.🌟说说什么是 Redis?Redis 是一种基于键值对的 NoSQL 数据库。 它主要的特点是把数据放在内存当中，相比直接访问磁盘的关系型数据库，读写速度会快很多，基本上能达到微秒级的响应。 所以在一些对性能要求很高的场景，比如缓存热点数据、防止接口爆刷，都会用到 Redis。 不仅如此，Redis 还支持持久化，可以将内存中的数据异步落盘，以便服务宕机重启后能恢复数据。 Redis 和 MySQL 的区别？Redis 属于非关系型数据库，数据是通过键值对的形式放在内存当中的；MySQL 属于关系型数据库，数据以行和列的形式存储在磁盘当中。 实际开发中，会将 MySQL 作为主存储，Redis 作为缓存，通过先查 Redis，未命中再查 MySQL 并写回Redis 的方式来提高系统的整体性能。 TecHub项目里哪里用到了 Redis？在TecHub实战项目当中，有很多地方都用到了 Redis，比如说用户活跃排行榜用到了 zset，作者白名单用到了 set。还有用户登录后的 Session、站点地图 SiteMap，分别用到了 Redis 的字符串和哈希表两种数据类型。其中比较有挑战性的一个应用是，通过 Lua 脚本封装 Redis 的 setnex 命令来实现分布式锁，以保证在高并发场景下，热点文章在短时间内的高频访问不会击穿 MySQL。 部署过 Redis 吗？第一种回答版本： 我只在本地部署过单机版，下载 Redis 的安装包，解压后运行 redis-server 命令即可。 第二种回答版本： 我有在生产环境中部署单机版 Redis，从官网下载源码包解压后执行 make make install 编译安装。然后编辑 redis.conf 文件，开启远程访问、设置密码、限制内存、设置内存过期淘汰策略、开启 AOF 持久化等： bind 0.0.0.0 # 允许远程访问requirepass your_password # 设置密码maxmemory 4gb # 限制内存，避免 OOMmaxmemory-policy allkeys-lru # 内存淘汰策略appendonly yes # 开启 AOF 持久化 第三种回答版本： 我有使用 Docker 拉取 Redis 镜像后进行容器化部署。 docker run -d --name redis -p 6379:6379 redis:7.0-alpine Redis 的高可用方案有部署过吗？有部署过哨兵机制，这是一个相对成熟的高可用解决方案，我们生产环境部署的是一主两从的 Redis 实例，再加上三个 Sentinel 节点监控它们。Sentinel 的配置相对简单，主要设置了故障转移的判定条件和超时阈值。 主节点配置： port 6379appendonly yes/code 从节点配置： replicaof 192.168.1.10 6379/code 哨兵节点配置： sentinel monitor mymaster 192.168.1.10 6379 2sentinel down-after-milliseconds mymaster 5000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 当主节点发生故障时，Sentinel 能够自动检测并协商选出新的主节点，这个过程大概需要 10-15 秒。 另一个大型项目中，我们使用了 Redis Cluster 集群方案。该项目数据量大且增长快，需要水平扩展能力。我们部署了 6 个主节点，每个主节点配备一个从节点，形成了一个 3主3从 的初始集群。Redis Cluster 的设置比Sentinel 复杂一些，需要正确配置集群节点间通信、分片映射等。 redis-server redis-7000.confredis-server redis-7001.conf...# 使用 redis-cli 创建集群# Redis 会自动将 key 哈希到 16384 个槽位# 主节点均分槽位，从节点自动跟随redis-cli --cluster create \\ 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 \\ 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \\ --cluster-replicas 1 Redis Cluster 最大的优势是数据自动分片，我们可以通过简单地增加节点来扩展集群容量。此外，它的故障转移也很快，通常在几秒内就能完成。 对于一些轻量级应用，我也使用过主从复制加手动故障转移的方案。主节点负责读写操作，从节点负责读操作。手动故障转移时，我们会先将从节点提升为主节点，然后重新配置其他从节点。 # 1\\. 取消从节点身份redis-cli -h slave-ip slaveof no one# 2\\. 将其他从节点指向新的主节点redis-cli -h other-slave-ip slaveof new-master-ip port 2.Redis 可以用来干什么？Redis 可以用来做缓存，比如说把高频访问的文章详情、商品信息、用户信息放入 Redis 当中，并通过设置过期时间来保证数据一致性，这样就可以减轻数据库的访问压力。 Redis 的 Zset 还可以用来实现积分榜、热搜榜，通过 score 字段进行排序，然后取前 N 个元素，就能实现 TOPN 的榜单功能。 利用 Redis 的 SETNX 命令或者 Redisson 还可以实现分布式锁，确保同一时间只有一个节点可以持有锁；为了防止出现死锁，可以给锁设置一个超时时间，到期后自动释放；并且最好开启一个监听线程，当任务尚未完成时给锁自动续期。 如果是秒杀接口，还可以使用 Lua 脚本来实现令牌桶算法，限制每秒只能处理 N 个请求。 -- KEYS[1]: 令牌桶的key-- ARGV[1]: 桶容量-- ARGV[2]: 令牌生成速率（每秒）-- ARGV[3]: 当前时间戳（秒）-- 从Redis哈希表中获取当前令牌桶的状态local bucket = redis.call(HMGET, KEYS[1], tokens, timestamp)-- 当前令牌数（如果不存在则使用默认容量）local tokens = tonumber(bucket[1]) or ARGV[1]-- 上次更新时间（如果不存在则使用当前时间）local last_time = tonumber(bucket[2]) or ARGV[3]-- 从参数中获取配置值local rate = tonumber(ARGV[2]) -- 令牌生成速率（每秒）local capacity = tonumber(ARGV[1]) -- 桶容量local now = tonumber(ARGV[3]) -- 当前时间戳（秒）-- 计算距离上次更新经过的时间差local delta = math.max(0, now - last_time)-- 计算应添加的令牌数（速率*时间差）local add_tokens = delta * rate-- 更新令牌数（不超过桶容量）tokens = math.min(capacity, tokens + add_tokens)-- 记录本次更新时间last_time = now-- 默认不允许通过（0）local allowed = 0-- 如果令牌数≥1，则消耗一个令牌并允许通过（1）if tokens = 1 then tokens = tokens - 1 allowed = 1end-- 更新Redis中的令牌桶状态redis.call(HMSET, KEYS[1], tokens, tokens, timestamp, last_time)-- 设置键的过期时间（防止长期不用的键占用内存）redis.call(EXPIRE, KEYS[1], 3600) -- 过期时间可自定义-- 返回是否允许通过（1=允许，0=拒绝）return allowed 在 Java 中调用 Lua 脚本： // 令牌桶参数int capacity = 10; // 桶容量int rate = 2; // 每秒2个令牌long now = System.currentTimeMillis() / 1000;String key = token_bucket:user:123;// 调用 Lua 脚本，返回 1 表示通过，0 表示被限流Long allowed = (Long) redis.eval(luaScript, 1, key, String.valueOf(capacity), String.valueOf(rate), String.valueOf(now)); 3.🌟Redis有哪些数据类型？Redis 支持五种基本数据类型，分别是字符串、列表、哈希、集合和有序集合。 还有三种扩展数据类型，分别是用于位级操作的 Bitmap、用于基数估算的 HyperLogLog、支持存储和查询地理坐标的 GEO。 详细介绍下字符串？字符串是最基本的数据类型，可以存储文本、数字或者二进制数据，最大容量是 512 MB。 适合缓存单个对象，比如验证码、token、计数器等。 详细介绍下列表？列表是一个有序的元素集合，支持从头部或尾部插入删除元素，常用于消息队列或任务列表。 详细介绍下哈希？哈希是一个键值对集合，适合存储对象，如商品信息、用户信息等。比如说 value = name: 沉默王二, age: 18。 详细介绍下集合？集合是无序且不重复的，支持交集、并集操作，查询效率能达到 O(1) 级别，主要用于去重、标签、共同好友等场景。 详细介绍下有序集合？有序集合的元素按分数进行排序，支持范围查询，适用于排行榜或优先级队列。 详细介绍下Bitmap？Bitmap 可以把一组二进制位紧凑地存储在一块连续内存中，每一位代表一个对象的状态，比如是否签到、是否活跃等。 比如用户 0 的已签到 1、用户 1 未签到 0、用户 2 已签到，Redis 就会把这些状态放进一个连续的二进制串 101，1 亿用户签到仅需 100,000,000 8 1024 ≈ 12MB 的空间，真的省到离谱。 详细介绍下HyperLogLog？HyperLogLog 是一种用于基数统计的概率性数据结构，可以在仅有 12KB 的内存空间下，统计海量数据集中不重复元素的个数，误差率仅 0.81%。 底层基于 LogLog 算法改进，先把每个元素哈希成一个二进制串，然后取前 14 位进行分组，放到 16384 个桶中，记录每组最大的前导零数量，最后用一个近似公式推算出总体的基数。 可以发现，哈希值越长前导零越多，也就说明集合里的元素越多。 大型网站 UV 统计系统示例： public class UVCounter private Jedis jedis; public void recordVisit(String date, String userId) String key = uv: + date; jedis.pfadd(key, userId); public long getUV(String date) return jedis.pfcount(uv: + date); public long getUVBetween(String startDate, String endDate) ListString keys = getDateKeys(startDate, endDate); return jedis.pfcount(keys.toArray(new String[0])); 详细介绍下GEO？GEO 用于存储和查询地理位置信息，可以用来计算两点之间的距离，查找某位置半径内的其他元素。 常见的应用场景包括：附近的人或者商家、计算外卖员和商家的距离、判断用户是否进入某个区域等。 底层基于 ZSet 实现，通过 Geohash 算法把经纬度编码成 score。 比如说查询附近的商家时，Redis 会根据中心点经纬度反推可能的 Geohash 范围， 在 ZSet 上做范围查询，拿到候选点后，用 Haversine 公式精确计算球面距离，筛选出最终符合要求的位置。 public class NearbyShopService private Jedis jedis; private static final String SHOP_KEY = shops:geo; // 添加商铺 public void addShop(String shopId, double longitude, double latitude) jedis.geoadd(SHOP_KEY, longitude, latitude, shopId); // 查询附近的商铺 public ListGeoRadiusResponse getNearbyShops( double longitude, double latitude, double radiusKm) return jedis.georadius(SHOP_KEY, longitude, latitude, radiusKm, GeoUnit.KM, GeoRadiusParam.geoRadiusParam() .withCoord() .withDist() .sortAscending() .count(20)); // 计算两个商铺之间的距离 public double getShopDistance(String shop1Id, String shop2Id) return jedis.geodist(SHOP_KEY, shop1Id, shop2Id, GeoUnit.KILOMETERS); 为什么使用 hash 类型而不使用 string 类型序列化存储？为什么使用 hash 类型而不使用 string 类型序列化存储？Hash 可以只读取或者修改某一个字段，而 String 需要一次性把整个对象取出来。 比如说有一个用户对象 user {name: ‘沉默王二’, age: 18}，如果使用 Hash 存储，可以直接修改 age 字段： redis.hset(user:1, age, 19); 如果使用 String 存储，需要先取出整个对象，修改后再存回去： String userJson = redis.get(user:1);User user = JSON.parseObject(userJson, User.class);user.setAge(19);redis.set(user:1, JSON.toJSONString(user)); 4.🌟Redis 为什么快呢？第一，Redis 的所有数据都放在内存中，而内存的读写速度本身就比磁盘快几个数量级。 第二，Redis 采用了基于 IO 多路复用技术的事件驱动模型来处理客户端请求和执行 Redis 命令。 其中的 IO 多路复用技术可以在只有一个线程的情况下，同时监听成千上万个客户端连接，解决传统 IO 模型中每个连接都需要一个独立线程带来的性能开销。 Redis 会根据操作系统选择最优的 IO 多路复用技术，比如 Linux 下使用 epoll，macOS 下使用 kqueue 等。 // epoll 的创建和使用int epfd = epoll_create(1024); // 创建 epoll 实例struct epoll_event ev, events[MAX_EVENTS];// 添加监听事件ev.events = EPOLLIN;ev.data.fd = listen_sock;epoll_ctl(epfd, EPOLL_CTL_ADD, listen_sock, ev);// 等待事件发生while (1) int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1); for (int i = 0; i nfds; i++) // 处理就绪的文件描述符 在 Redis 6.0 之前，包括连接建立、请求读取、响应发送，以及命令执行都是在主线程中顺序执行的，这样可以避免多线程环境下的锁竞争和上下文切换，因为 Redis 的绝大部分操作都是在内存中进行的，性能瓶颈主要是内存操作和网络通信，而不是 CPU。 为了进一步解决网络 IO 的性能瓶颈，Redis 6.0 引入了多线程机制，把网络 IO 和命令执行分开，网络 IO 交给线程池来处理，而命令执行仍然在主线程中进行，这样就可以充分利用多核 CPU 的性能。 主线程专注于命令执行，网络IO 由其他线程分担，在多核 CPU 环境下，Redis 的性能可以得到显著提升。 (有点像单片机的中断机制,保持主线程专注核心任务，让IO操作在后台异步处理，既保证了性能又保证了数据一致性。) 第三，Redis 对底层数据结构做了极致的优化，比如说 String 的底层数据结构动态字符串支持动态扩容、预分配冗余空间，能够减少内存碎片和内存分配的开销。 总结: 5.能详细说一下IO多路复用吗？IO 多路复用是一种允许单个进程同时监视多个文件描述符的技术，使得程序能够高效处理多个并发连接而无需创建大量线程。 Journey-C：IO 多路复用 IO 多路复用的核心思想是：让单个线程可以等待多个文件描述符就绪，然后对就绪的描述符进行操作。这样可以在不使用多线程或多进程的情况下处理并发连接。 蛮荆：IO 多路复用和多线程 主要的实现机制包括 select、poll、epoll、kqueue 和 IOCP 等。 请说说 select、poll、epoll、kqueue 和 IOCP 的区别？(用户想知道内核中有哪些文件描述符有数据可读，内核会返回一个就绪的文件描述符列表，用户只需要遍历这个列表，就可以知道哪些文件描述符有数据可读。)文件的描述符就是一个整数,代表一个打开的文件,或者一个网络连接。 select 的缺点是单个进程能监视的文件描述符数量有限，一般为 1024 个，且每次调用都需要将文件描述符集合从用户态复制到内核态，然后遍历找出就绪的描述符，性能较差。 // select 的基本使用int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);// 示例代码fd_set readfds;FD_ZERO(readfds); // 清空集合FD_SET(sockfd, readfds); // 添加监听套接字select(sockfd + 1, readfds, NULL, NULL, NULL);if (FD_ISSET(sockfd, readfds)) // 检查是否就绪 // 处理读事件 poll 的优点是没有最大文件描述符数量的限制，但是每次调用仍然需要将文件描述符集合从用户态复制到内核态，依然需要遍历，性能仍然较差。 // poll 的基本使用int poll(struct pollfd *fds, nfds_t nfds, int timeout);// 示例代码struct pollfd fds[MAX_EVENTS];fds[0].fd = sockfd;fds[0].events = POLLIN; // 监听读事件poll(fds, 1, -1);if (fds[0].revents POLLIN) // 处理读事件 epoll 是 Linux 特有的 IO 多路复用机制，支持大规模并发连接，使用事件驱动模型，性能更高。其工作原理是将文件描述符注册到内核中，然后通过事件通知机制来处理就绪的文件描述符，不需要轮询，也不需要数据拷贝，更没有数量限制，所以性能非常高。 // epoll 的基本使用int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);// 示例代码int epfd = epoll_create(1);struct epoll_event ev, events[MAX_EVENTS];ev.events = EPOLLIN;ev.data.fd = sockfd;epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, ev);while (1) int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1); for (int i = 0; i nfds; i++) if (events[i].data.fd == sockfd) // 处理读事件 kqueue 是 BSDmacOS 系统下的 IO 多路复用机制，类似于 epoll，支持大规模并发连接，使用事件驱动模型。 int kqueue(void);int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout); IOCP 是 Windows 系统下的 IO 多路复用机制，使用使用完成端口模型而非事件通知。 HANDLE CreateIoCompletionPort(HANDLE FileHandle, HANDLE ExistingCompletionPort, ULONG_PTR CompletionKey, DWORD NumberOfConcurrentThreads); 举个例子说一下 IO 多路复用？比如说我是一名数学老师，上课时提出了一个问题：“今天谁来证明一下勾股定律？” 同学小王举手，我就让小王回答；小李举手，我就让小李回答；小张举手，我就让小张回答。 这种模式就是 IO 多路复用，我只需要在讲台上等，谁举手谁回答，不需要一个一个去问。 有盐先生：IO 多路复用 Redis 就是使用 epoll 这样的 IO 多路复用机制，在单线程模型下实现高效的网络 IO，从而支持高并发的请求处理。 举例子说一下阻塞 IO和 IO 多路复用的差别？假设我是一名老师，让学生解答一道题目。 我的第一种选择：按顺序逐个检查，先检查 A同学，然后是 B，之后是 C、D。。。这中间如果有一个学生卡住，全班都会被耽误。 这种就是阻塞 IO，不具有并发能力。 阻塞 IO和 IO多路复用差别 我的第二种选择，我站在讲台上等，谁举手我去检查谁。C、D 举手，我去检查 C、D 的答案，然后继续回到讲台上等。此时 E、A 又举手，然后去处理 E 和 A。 select、poll 和 epoll 的实现原理？select 和 poll 都是通过把所有文件描述符传递给内核，由内核遍历判断哪些就绪。 select 将文件描述符 FD 通过 BitsMap 传入内核，轮询所有的 FD，通过调用 file-poll 函数查询是否有对应事件，没有就将 task 加入 FD 对应 file 的待唤醒队列，等待事件来临被唤醒。 journey-c：select poll 改进了连接数上限问题，不再用 BitsMap 来传入 FD，取而代之的是动态数组 pollfd，但本质上仍是线性遍历，性能没有提升太多。 journey-c：poll select和poll的模式都是，一次将参数拷贝到内核空间，等有结果了再一次拷贝出去。 epoll 将监听的 FD 注册进内核的红黑树，由内核在事件触发时将就绪的 FD 放入 ready list。应用程序通过 epoll_wait 获取就绪的 FD，从而避免遍历所有连接的开销。 journey-c：epoll epoll 最大的优点是：支持事件驱动 + 边缘触发(类似于上升沿)，ADD 时拷贝一次，epoll_wait 时利用 MMAP 和用户共享空间，直接拷贝数据到用户空间，因此在高并发场景下性能远高于 select 和 poll。 6.Redis为什么早期选择单线程？第一，单线程模型不需要考虑复杂的锁机制，不存在多线程环境下的死锁、竞态条件等问题，开发起来更快，也更容易维护。 wsh-study.com：Redis的单线程模型 第二，Redis 是IO 密集型而非 CPU 密集型，主要受内存和网络 IO 限制，而非 CPU 的计算能力，单线程可以避免线程上下文切换的开销。 哪怕我们在一个普通的 Linux 服务器上启动 Redis 服务，它也能在 1s 内处理 1000000 个用户请求。 第三，单线程可以保证命令执行的原子性，无需额外的同步机制。 官方单线程解释 Redis 虽然最初采用了单线程设计，但后续的版本中也在特定方面引入了多线程，比如说 Redis 4.0 就引异步多线程，用于清理脏数据、释放无用连接、删除大 Key 等。 /* 从数据库中删除一个键、值以及相关的过期条目（如果有的话）。 * 如果释放值对象需要大量的内存分配操作，该对象可能会被放入 * 延迟释放列表中，而不是同步释放。延迟释放列表将在 * bio.c 的另一个线程中进行回收。 */#define LAZYFREE_THRESHOLD 64int dbAsyncDelete(redisDb *db, robj *key) /* 从过期字典中删除条目不会释放键的 sds， * 因为它与主字典共享。 */ if (dictSize(db-expires) 0) dictDelete(db-expires,key-ptr); /* 如果值对象只包含少量的内存分配，使用延迟释放方式 * 实际上会更慢... 所以在一定阈值以下，我们就直接 * 同步释放对象。 */ dictEntry *de = dictUnlink(db-dict,key-ptr); if (de) robj *val = dictGetVal(de); // 计算value的回收收益 size_t free_effort = lazyfreeGetFreeEffort(val); /* 如果释放对象的工作量太大，就通过将对象添加到延迟释放列表 * 在后台进行处理。 * 注意，如果对象是共享的，现在就回收它是不可能的。这种情况 * 很少发生，但是有时 Redis 核心的某些实现部分可能会调用 * incrRefCount() 来保护对象，然后调用 dbDelete()。在这种 * 情况下，我们会继续执行并到达 dictFreeUnlinkedEntry() * 调用，这相当于仅仅调用 decrRefCount()。 */ // 只有回收收益超过一定值，才会执行异步删除，否则还是会退化到同步删除 if (free_effort LAZYFREE_THRESHOLD val-refcount == 1) atomicIncr(lazyfree_objects,1); bioCreateBackgroundJob(BIO_LAZY_FREE,val,NULL,NULL); dictSetVal(db-dict,de,NULL); /* 释放键值对，如果我们将 val 字段设置为 NULL 以便稍后 * 延迟释放，那么就只释放键。 */ if (de) dictFreeUnlinkedEntry(db-dict,de); if (server.cluster_enabled) slotToKeyDel(key-ptr); return 1; else return 0; 官方解释：https://redis.io/topics/faq 7.Redis 6.0 使用多线程是怎么回事?Redis 6.0 的多线程仅用于处理网络 IO，包括网络数据的读取、写入，以及请求解析。 │ 单线程执行命令 │ │ ↑ ↓ │┌─────────┐ ┌─┴────────────┴──┐│ I/O线程1 │ ←→ │ │├─────────┤ │ ││ I/O线程2 │ ←→ │ 主线程 │├─────────┤ │ ││ I/O线程3 │ ←→ │ │└─────────┘ └─────────────────┘ 而命令的执行依然是单线程，这种设计被称为“IO 线程化”，能够在高负载的情况下，最大限度地提升 Redis 的响应速度。 三分恶面渣逆袭：Redis6.0多线程 -— 这部分面试中可以不背，方便理解 start —- 这一变化主要是因为随着网络带宽和服务器性能的提升，Redis 的瓶颈从 CPU 逐渐转移到了网络 IO： 带宽从 10Gbps 提升到 100Gbps，甚至更高。 请求的并发数从几千到几万，甚至几十万。 单线程在高负载场景下处理网络 IO 出现了明显的性能瓶颈，Redis 的开发团队通过研究发现，在处理大数据包时，单线程 Redis 有超过 80% 的 CPU 时间花在网络 IO 上，而实际命令执行仅占 20% 左右。 wsh-study.com：Redis 6.0的多线程网络模型 Redis 6.0 的多线程 IO 模型主要包含三个核心步骤： 仍然由主线程负责接收客户端的连接请求。 主线程将连接请求分发给多个 IO 线程进行处理，主线程负责解析和执行命令。 命令执行完毕后，由多个 IO 线程将结果返回给客户端。 // Redis 主事件循环（简化版）void beforeSleep(struct aeEventLoop *eventLoop) // 1. 主线程分派读任务给 I/O 线程 handleClientsWithPendingReadsUsingThreads(); // 2. 等待 I/O 线程完成读取 waitForIOThreads(); // 3. 主线程处理命令 processInputBuffer(); // 4. 主线程分派写任务给 I/O 线程 handleClientsWithPendingWritesUsingThreads(); Redis 6.0 默认仍然使用单线程模式，但可以通过配置文件或命令行参数启用多线程模式。 # 启用多线程模式io-threads 4# 启用多线程写入（Redis 6.0 默认只开启多线程读取）io-threads-do-reads yes 建议将 IO 线程数设置为 CPU 核心数的一半，一般不建议超过 8 个。 经过多次测试，Redis 6.0 在处理 1-200 字节的小数据包时，性能提升 1.5-2 倍；在处理 1KB 以上的大数据包时提升约 3-5 倍。 -—这部分面试中可以不背，方便理解 end —- 8.说说 Redis 的常用命令（补充） 2024 年 04 月 11 日增补 一句话回答（也不用全部都背，挑三个就行）： Redis 支持多种数据结构，常用的命令也比较多，比如说操作字符串可以用 SET/GET/INCR，操作哈希可以用 HSET/HGET/HGETALL，操作列表可以用 LPUSH/LPOP/LRANGE，操作集合可以用 SADD/SISMEMBER，操作有序集合可以用 ZADD/ZRANGE/ZINCRBY等，通用命令有 EXPIRE/DEL/KEYS 等。 -—这部分面试中可以不背，方便理解 start—- ①、操作字符串的命令有： 命令 作用 示例 SET key value 设置字符串键值 SET name jack GET key 获取字符串值 GET name INCR key 数值自增 1 INCR count DECR key 数值自减 1 DECR stock INCRBY key N 增加 N INCRBY views 10 APPEND key value 追加字符串 APPEND log done GETRANGE key start end 获取子串 GETRANGE name 0 3 MSET k1 v1 k2 v2 批量设置多个键值 MSET a 1 b 2 ②、操作列表的命令有： LPUSH key value：将一个值插入到列表 key 的头部。 RPUSH key value：将一个值插入到列表 key 的尾部。 LPOP key：移除并返回列表 key 的头元素。 RPOP key：移除并返回列表 key 的尾元素。 LRANGE key start stop：获取列表 key 中指定范围内的元素。 ③、操作集合的命令有： SADD key member：向集合 key 添加一个元素。 SREM key member：从集合 key 中移除一个元素。 SMEMBERS key：返回集合 key 中的所有元素。 ④、操作有序集合的命令有： ZADD key score member：向有序集合 key 添加一个成员，或更新其分数。 ZRANGE key start stop [WITHSCORES]：按照索引区间返回有序集合 key 中的成员，可选 WITHSCORES 参数返回分数。 ZREVRANGE key start stop [WITHSCORES]：返回有序集合 key 中，指定区间内的成员，按分数递减。 ZREM key member：移除有序集合 key 中的一个或多个成员。 ⑤、操作哈希的命令有： HSET key field value：向键为 key 的哈希表中设置字段 field 的值为 value。 HGET key field：获取键为 key 的哈希表中字段 field 的值。 HGETALL key：获取键为 key 的哈希表中所有的字段和值。 HDEL key field：删除键为 key 的哈希表中的一个或多个字段。 详细说说 set 命令？SET 命令用于设置字符串的 key，支持过期时间和条件写入，常用于设置缓存、实现分布式锁、延长 Session 等场景。 SET key value [EX seconds | PX milliseconds | EXAT timestamp | PXAT timestamp-milliseconds | KEEPTTL] [NX | XX] [GET] 默认情况下，SET 会覆盖键已有的值。 支持多种设置过期时间的方式，比如说 EX 设置秒级过期时间，PX 设置毫秒过期时间。 支持条件写入，使其可以实现原子性操作，比如说 NX 仅在键不存在时设置值，XX 仅在键存在时设置值。 二哥的 Java 进阶之路：set 命令 缓存实现： SET user:profile:userid JSON数据 EX 3600 # 存储用户资料，并设置1小时过期 实现分布式锁： SET lock:resource_name random_value EX 10 NX # 获取锁，10秒后自动释放 存储 Session： SET session:sessionid session_data EX 1800 # 存储用户会话，30分钟过期 sadd 命令的时间复杂度是多少？SADD 支持一次添加多个元素，返回值为实际添加成功的元素数量，时间复杂度为 O(N)。 redis-cli SADD myset apple banana orange incr命令了解吗？INCR 是一个原子命令，可以将指定键的值加 1，如果 key 不存在，会先将其设置为 0，再执行加 1 操作。 二哥的Java进阶之路：INCR 常用于网站访问量、文章点赞数等计数器的实现；结合过期时间实现限流器；生成分布式唯一 ID；库存扣减等。 # 限制用户每分钟最多访问10次FUNCTION limit_api_call(user_id) current = INCR(rate:+user_id) IF current == 1 THEN EXPIRE(rate:+user_id, 60) END IF current 10 THEN RETURN false # 超出限制 ELSE RETURN true # 允许访问 ENDEND 9.单线程的Redis QPS 能到多少？(补充) 2024 年 4 月 14 日增补 根据官方的基准测试，一个普通服务器的 Redis 实例通常可以达到每秒十万左右的 QPS。 每秒请求数能达到10 万级 -—这部分面试中可以不背，方便理解 start —- Redis 的 QPS（每秒请求数）性能取决于多种因素，包括硬件配置、网络延迟、数据结构、命令类型等。 可以通过 redis-benchmark 命令进行基准测试： redis-benchmark -h 127.0.0.1 -p 6379 -c 50 -n 10000 -h：指定 Redis 服务器的地址，默认是 127.0.0.1。 -p：指定 Redis 服务器的端口，默认是 6379。 -c：并发连接数，即同时有多少个客户端在进行测试。 -n：请求总数，即测试过程中总共要执行多少个请求。 2023 年前，我用的是一台 macOS，4 GHz 四核 Intel Core i7，32 GB 1867 MHz DDR3，测试结果如下： 二哥的 Java 进阶之路：Redis 的基准测试 可以看得出，每秒能处理超过 10 万次请求。 QPS = 总请求数 / 总耗时 = 10000 / 0.09 ≈ 111111 QPS 延迟也非常低，99% 的请求都在 0.3ms 以内完成了。 -—这部分面试中可以不背，方便理解 end —- 持久化10.🌟Redis的持久化方式有哪些？主要有两种，RDB 和 AOF。RDB 通过创建时间点快照来实现持久化，AOF 通过记录每个写操作命令来实现持久化。 三分恶面渣逆袭：Redis持久化的两种方式 这两种方式可以单独使用，也可以同时使用。这样就可以保证 Redis 服务器在重启后不丢失数据，通过 RDB 和 AOF 文件来恢复内存中原有的数据。 Gaurav：RDB 和 AOF 详细说一下 RDB？RDB 持久化机制可以在指定的时间间隔内将 Redis 某一时刻的数据保存到磁盘上的 RDB 文件中，当 Redis 重启时，可以通过加载这个 RDB 文件来恢复数据。 Animesh Gaitonde：RDB RDB 持久化可以通过 save 和 bgsave 命令手动触发，也可以通过配置文件中的 save 指令自动触发。 三分恶面渣逆袭：save和bgsave save 命令会阻塞 Redis 进程，直到 RDB 文件创建完成。二哥的 Java 进阶之路：手动执行 RDB bgsave 命令会在后台 fork 一个子进程来执行 RDB 持久化操作，主进程不会被阻塞。Mr于：Redis bgsave 什么情况下会自动触发 RDB 持久化？第一种，在 Redis 配置文件中设置 RDB 持久化参数 save seconds changes，表示在指定时间间隔内，如果有指定数量的键发生变化，就会自动触发 RDB 持久化。 save 900 1 # 900 秒（15 分钟）内有 1 个 key 发生变化，触发快照save 300 10 # 300 秒（5 分钟）内有 10 个 key 发生变化，触发快照save 60 10000 # 60 秒内有 10000 个 key 发生变化，触发快照 第二种，主从复制时，当从节点第一次连接到主节点时，主节点会自动执行 bgsave 生成 RDB 文件，并将其发送给从节点。 达摩院的BLOG：Redis 主从复制时 RDB 自动生成 第三种，如果没有开启 AOF，执行 shutdown 命令时，Redis 会自动保存一次 RDB 文件，以确保数据不会丢失。 详细说一下 AOF？AOF 通过记录每个写操作命令，并将其追加到 AOF 文件来实现持久化，Redis 服务器宕机后可以通过重新执行这些命令来恢复数据。 Animesh Gaitonde：AOF 当 Redis 执行写操作时，会将写命令追加到 AOF 缓冲区；Redis 会根据同步策略将缓冲区的数据写入到 AOF 文件。 三分恶面渣逆袭：AOF工作流程 当 AOF 文件过大时，Redis 会自动进行 AOF 重写，剔除多余的命令，比如说多次对同一个 key 的 set 和 del，生成一个新的 AOF 文件；当 Redis 重启时，读取 AOF 文件中的命令并重新执行，以恢复数据。 AOF 的刷盘策略了解吗？Redis 将 AOF 缓冲区的数据写入到 AOF 文件时，涉及两个系统调用：write 将数据写入到操作系统的缓冲区，fsync 将 OS 缓冲区的数据刷新到磁盘。 这里的刷盘涉及到三种策略：always、everysec 和 no。 bytebytego：Redis AOF 的刷盘策略 always：每次写命令执行完，立即调用 fsync 同步到磁盘，这样可以保证数据不丢失，但性能较差。 everysec：每秒调用一次 fsync，将多条命令一次性同步到磁盘，性能较好，数据丢失的时间窗口为 1 秒。 no：不主动调用 fsync，由操作系统决定，性能最好，但数据丢失的时间窗口不确定，依赖于操作系统的缓存策略，可能会丢失大量数据。 可以通过配置文件中的 appendfsync 参数进行设置。 appendfsync everysec # 每秒 fsync 一次 说说AOF的重写机制？由于 AOF 文件会随着写操作的增加而不断增长，为了解决这个问题， Redis 提供了重写机制来对 AOF 文件进行压缩和优化。 pdai.tech：AOF 文件瘦身 AOF 重写可以通过两种方式触发，第一种是手动执行 BGREWRITEAOF 命令，适用于需要立即减小AOF文件大小的场景。 第二种是在 Redis 配置文件中设置自动重写参数，比如说 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size，表示当 AOF 文件大小超过指定值时，自动触发重写。 auto-aof-rewrite-percentage 100 # 默认值100，表示当前AOF文件大小相比上次重写后大小增长了多少百分比时触发重写auto-aof-rewrite-min-size 64mb # 默认值64MB，表示AOF文件至少要达到这个大小才会考虑重写 AOF 重写的具体过程是怎样的？Redis 在收到重写指令后，会创建一个子进程，并 fork 一份与父进程完全相同的数据副本，然后遍历内存中的所有键值对，生成重建它们所需的最少命令。 云烟成雨：Redis 的 AOF 重写机制 比如说多个 RPUSH 命令可以合并为一个带有多个参数的 RPUSH； 比如说一个键被设置后又被删除，这个键的所有操作都不会被写入新 AOF。 比如说使用 SADD key member1 member2 member3 代替多个单独的 SADD key memberX。 子进程在执行 AOF 重写的同时，主进程可以继续处理来自客户端的命令。 为了保证数据一致性，Redis 使用了 AOF 重写缓冲区机制，主进程在执行写操作时，会将命令同时写入旧的 AOF 文件和重写缓冲区。 等子进程完成重写后，会向主进程发送一个信号，主进程收到后将重写缓冲区中的命令追加到新的 AOF 文件中，然后调用操作系统的 rename，将旧的 AOF 文件替换为新的 AOF 文件。 主进程（fork） │ ├─→ 子进程（生成新的 AOF 文件） │ │ │ ├─→ 内存快照 │ ├─→ 写入临时 AOF 文件 │ ├─→ 通知主进程完成 │ ├─→ 主进程（追加缓冲区到新 AOF 文件） ├─→ 替换旧 AOF 文件 ├─→ 重写完成 AOF 重写期间，Redis 服务器会处于特殊状态： aof_child_pid 不为 0，表示有子进程在执行 AOF 重写 aof_rewrite_buf_blocks 链表不为空，存储 AOF 重写缓冲区内容 如果在配置文件中设置 no-appendfsync-on-rewrite 为 yes，那么重写期间可能会暂停 AOF 文件的 fsync 操作。 appendonly yes # 开启AOFappendfilename appendonly.aof # AOF文件名appendfsync everysec # 写入磁盘策略no-appendfsync-on-rewrite no # 重写期间是否临时关闭fsyncauto-aof-rewrite-percentage 100 # AOF文件增长到原来多少百分比时触发重写auto-aof-rewrite-min-size 64mb # AOF文件最小多大时才允许重写 AOF 文件存储的是什么类型的数据？AOF 文件存储的是 Redis 服务器接收到的写命令数据，以 Redis 协议格式保存。 这种格式的特点是，每个命令以*开头，后跟参数的数量，每个参数前用$符号，后跟参数字节长度，然后是参数的实际内容。 二哥的Java 进阶之路：AOF文件内容格式 AOF重写期间命令可能会写入两次，会造成什么影响？AOF 重写期间命令会同时写入现有AOF文件和重写缓冲区，这种机制是有意设计的，并不会导致数据重复或不一致问题。 UStarGao：AOF 双写机制 因为新旧文件是分离的，现有命令写入当前 AOF 文件，重写缓冲区的命令最终写入新的 AOF 文件，完成后，新文件通过原子性的 rename 操作替换旧文件。两个文件是完全分离的，不会导致同一个 AOF 文件中出现重复命令。 11.RDB 和 AOF 各自有什么优缺点？RDB 通过 fork 子进程在特定时间点对内存数据进行全量备份，生成二进制格式的快照文件。其最大优势在于备份恢复效率高，文件紧凑，恢复速度快，适合大规模数据的备份和迁移场景。 缺点是可能丢失两次快照期间的所有数据变更。 dfordebugging：rdb vs aof AOF 会记录每一条修改数据的写命令。这种日志追加的方式让 AOF 能够提供接近实时的数据备份，数据丢失风险可以控制在 1 秒内甚至完全避免。 缺点是文件体积较大，恢复速度慢。 来个表格对比一下： 对比项 RDB（快照） AOF（命令日志） 数据完整性 ❌ 可能丢失几分钟数据 ✅ 最多丢 1 秒数据 恢复速度 ✅ 快（直接加载二进制快照） ❌ 慢（逐条 replay） 文件大小 ✅ 小（压缩后） ❌ 大（命令追加） 性能影响 ✅ 低（fork 后保存） ❌ 较高（每次写都记录） 写入方式 定期全量写 每次写命令就记录 适用场景 冷备份，灾难恢复 实时持久化，数据安全 默认状态 默认启用 Redis 7 默认也启用 重写机制 无 有（BGREWRITEAOF） 混合支持 Redis 4.0 后支持结合使用（aof-use-rdb-preamble） 12.RDB 和 AOF 如何选择？在选择 Redis 持久化方案时，我会从业务需求和技术特性两个维度来考虑。 如果是缓存场景，可以接受一定程度的数据丢失，我会倾向于选择 RDB 或者完全不使用持久化。RDB 的快照方式对性能影响小，而且恢复速度快，非常适合这类场景。 洒脱的耿：Redis 做缓存 但如果是处理订单或者支付这样的核心业务，数据丢失将造成严重后果，那么 AOF 就成为必然选择。通过配置每秒同步一次，可以将潜在的数据丢失风险限制在可接受范围内。 极客时间：reids 在秒杀中的应用 在实际的项目当中，我更偏向于使用 RDB + AOF 的混合模式。 appendonly yes # 开启 AOFappendfsync everysec # 每秒刷盘一次aof-use-rdb-preamble yes # 开启混合持久化，重启时优先加载 RDB，RDB 作为冷备，AOF 作为实时同步 13.Redis如何恢复数据？当 Redis 服务重启时，它会优先查找 AOF 文件，如果存在就通过重放其中的命令来恢复数据；如果不存在或未启用 AOF，则会尝试加载 RDB 文件，直接将二进制数据载入内存来恢复。 三分恶面渣逆袭：Redis启动加载数据 如果 AOF 文件损坏的话，Redis 会尝试通过 redis-check-aof 工具来修复 AOF 文件，或者直接使用 --repair 参数来修复。 redis-check-aof --repair appendonly.aof 虽然 Redis 还提供了 redis-check-rdb 工具来检查 RDB 文件的完整性，但它并不支持修复 RDB 文件，只能用来验证文件的完整性。 redis-check-rdb dump.rdb 14.🌟Redis 4.0 的混合持久化了解吗？是的。 混合持久化结合了 RDB 和 AOF 两种方式的优点，解决了它们各自的不足。在 Redis 4.0 之前，我们要么面临 RDB 可能丢失数据的风险，要么承受 AOF 恢复慢的问题，很难两全其美。 Animesh Gaitonde：aof-use-rdb-preamble 混合持久化的工作原理非常巧妙：在 AOF 重写期间，先以 RDB 格式将内存中的数据快照保存到 AOF 文件的开头，再将重写期间的命令以 AOF 格式追加到文件末尾。 三分恶面渣逆袭：混合持久化 这样，当需要恢复数据时，Redis 先加载 RDB 格式的数据来快速恢复大部分的数据，然后通过重放命令恢复最近的数据，这样就能在保证数据完整性的同时，提升恢复速度。 如何设置持久化模式？启用混合持久化的方式非常简单，只需要在配置文件中设置 aof-use-rdb-preamble yes 就可以了。 aof-use-rdb-preamble yes 你在开发中是怎么配置 RDB 和 AOF 的？对于大多数生产环境，我倾向于使用混合持久化方式，结合 RDB 和 AOF 的优点。 # 启用AOFappendonly yes# 使用混合持久化aof-use-rdb-preamble yes# 每秒同步一次AOF，平衡性能和安全性appendfsync everysec# AOF重写触发条件：文件增长100%且至少达到64MBauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# RDB备份策略save 900 1 # 15分钟内有1个修改save 300 10 # 5分钟内有10个修改save 60 10000 # 1分钟内有10000个修改 对于单纯的缓存场景，或者本地开发，我会只启用 RDB，关闭 AOF： # 禁用AOFappendonly no# 较宽松的RDB策略save 3600 1 # 1小时内有1个修改save 300 100 # 5分钟内有100个修改 而对于金融类等高一致性的系统，我通常会在关键时间窗口动态将 appendfsync 设置为 always： # 启用AOFappendonly yes# 使用混合持久化aof-use-rdb-preamble yes# 每个命令都同步（谨慎使用，性能影响大）# 通常我会在关键时间窗口动态修改为alwaysappendfsync always# 更频繁的RDB快照save 300 1 # 5分钟内有1个修改save 60 100 # 1分钟内有100个修改 另外，对于高并发场景，应该设置no-appendfsync-on-rewrite yes，避免 AOF 重写影响主进程性能；对于大型实例，也应该设置 rdb-save-incremental-fsync yes 来减少大型 RDB 保存对性能的影响。 # AOF重写期间不fsync，AOF 重写期间，主进程不会对新写入的 AOF 缓冲区执行 fsync 操作（即不强制刷盘），而是等重写结束后再统一刷盘。no-appendfsync-on-rewrite yes# RDB 快照保存时采用增量 fsync，即每写入一定量的数据就执行一次 fsync，将数据分批同步到磁盘。rdb-save-incremental-fsync yes 高可用15.主从复制了解吗？主从复制允许从节点维护主节点的数据副本。在这种架构中，一个主节点可以连接多个从节点，从而形成一主多从的结构。主节点负责处理写操作，从节点自动同步主节点的数据变更，并处理读请求，从而实现读写分离。 三分恶面渣逆袭：Redis主从复制简图 主从复制的主要作用是什么?第一，主节点负责处理写请求，从节点负责处理读请求，从而实现读写分离，减轻主节点压力的同时提升系统的并发能力。 pdai.tech：主从复制的读写分离 第二，从节点可以作为主节点的数据备份，当主节点发生故障时，可以快速将从节点提升为新的主节点，从而保证系统的高可用性。 系统运维：Redis主从+Sentinel集群 什么情况下会出现主从复制数据不一致？Redis 的主从复制是异步进行的，因此在主节点宕机、网络波动或复制延迟较高时会出现从节点数据不同步的情况。 ningg.top：主从复制异步进行 比如主节点写入数据后宕机，但从节点还未来得及复制，就会出现数据不一致。 时间线：→客户端 → 向主节点 SET user:1 二哥 → 主节点处理成功 ✅ ↓ 正准备推送给从节点（异步复制）... 但还没推送完 ❌ ↓ —— 突然主节点宕机（机器死机、断网） 💥 —— ↓ Sentinel 监测到故障，failover：将从节点提升为新主节点 🧠 ↓客户端继续请求：GET user:1 ❓→ 从节点返回：空 ❌（数据没同步过来） 另一个容易被忽视的因素是主节点内存压力。当主节点内存接近上限并启用了淘汰策略时，某些键可能被自动删除，而这些删除操作如果未能及时同步，就会造成从节点保留了主节点已经不存在的数据。 图片来源于网络：主从不一致 主从复制数据不一致的解决方案有哪些？首先是网络层面的优化，理想情况下，主从节点应该部署在同一个网络区域内，避免跨区域的网络延迟。 其次是配置层面的调整，比如说适当增大复制积压缓冲区的大小和存活时间，以便从节点重连后进行增量同步而不是全量同步，以最大程度减少主从同步的延迟。 repl-backlog-size 1mb # 默认值 1MB，表示主节点的复制缓冲区大小repl-backlog-ttl 3600 # 默认值 3600 秒，表示主节点的复制缓冲区存活时间 第三是引入监控和自动修复机制，定期检查主从节点的数据一致性。 比如说通过比较主从的 offset 差值判断从库是否落后。一旦超过设定阈值，就将从节点剔除，并重新进行全量同步。 极客时间：Redis 核心技术与实战 Java 面试指南（付费）收录的得物面经同学 1 面试原题：Redis 分布式，主从，一个节点挂掉怎么办 Java 面试指南（付费）收录的小米面经同学 F 面试原题：redis 的主从架构和主从哨兵区别 Java 面试指南（付费）收录的收钱吧面经同学 1 Java 后端一面面试原题：Redis解决单点故障主要靠什么？主从模式用的是异步还是同步？ memo：2025 年 5 月 7 日修改至此，今天在修改球友简历时，收到了球友对简历修改的认可：“现在这份简历应该比较完美了”，完美这个词我觉得褒奖的有点多了，哈哈，不过我还是很开心的。 球友对简历的认可 16.Redis主从有几种常见的拓扑结构？主要有三种。 最基础的是一主一从，这种模式适合小型项目。一个主节点负责写入，一个从节点负责读和数据备份。这种结构虽然简单，但维护成本低。 三分恶面渣逆袭：一主一从 随着业务增长，读请求增多，可以考虑扩展为一主多从结构。主节点负责写入，多个从节点还可以分摊压力。 三分恶面渣逆袭：一主多从结构 在跨地域部署场景中，树状主从结构可以有效降低主节点负载和需要传送给从节点的数据量。通过引入复制中间层，从节点不仅可以复制主节点数据，同时可以作为其他从节点的主节点继续向下层复制。 三分恶面渣逆袭：树状主从结构 17.Redis的主从复制原理了解吗？了解。 Redis 的主从复制是指通过异步复制将主节点的数据变更同步到从节点，从而实现数据备份和读写分离。这个过程大致可以分为三个阶段：建立连接、同步数据和传播命令。 pdai.tech：Redis主从复制原理 在建立连接阶段，从节点通过执行 replicaof 命令连接到主节点。连接建立后，从节点向主节点发送 psync 命令，请求数据同步。这时主节点会为该从节点创建一个连接和复制缓冲区。 MainWoods：复制缓冲区 同步数据阶段分为全量同步和增量同步。当从节点首次连接主节点时，会触发全量同步。 ningG：增量同步和全量同步 在这个过程中，主节点会 fork 一个子进程生成 RDB 文件，同时将文件生成期间收到的写命令缓存到复制缓冲区。然后将 RDB 文件发送给从节点，从节点清空自己的数据并加载这个 RDB 文件。等 RDB 传输完成后，主节点再将缓存的写命令发送给从节点执行，确保数据完全一致。 博客园多少幅度：主从数据复制过程 主从完成全量同步后，主要依靠传播命令阶段来保持数据的增量同步。主节点会将每次执行的写命令实时发送给所有从节点。 ningG：命令传播 Redis 2.8 版本后，主节点会为每个从节点维护一个复制积压缓冲区，用于存储最近的写命令。 MainWoods：复制积压缓冲区 增量复制时，主节点会把要同步的写命令暂存一份到复制积压缓冲区。这样当从节点和主节点发生网络断连，从节点重新连接后，可以从复制积压缓冲区中复制尚未同步的写命令。 仁扬：增量同步 memo：2025 年 5 月 8 日修改至此，今天有球友在星球里发帖说拿到了腾讯的实习 offer，真的要恭喜了。面经，我看题目主要集中在技术派项目和MySQL、计算机网络的八股上。 球友拿到腾讯暑期实习 offer 18.详细说说全量同步和增量同步？全量同步会将主节点的完整数据集传输给从节点，通常发生在从节点首次连接主节点时。 三分恶面渣逆袭：全量同步 此时，从节点发送 psync ? -1 命令请求同步。? 表示从节点没有主节点 ID，-1 表示没有偏移量。主节点收到后会回复 FULLRESYNC响应从节点。同时也会包含主库 runid 和复制偏移量 offset 两个参数。 然后 fork 一个子进程生成 RDB 文件，并将新的写命令存入复制缓冲区。 从库收到 RDB 文件后，清空旧数据并加载新的 RDB 文件。加载完成后，从节点会向主节点回复确认消息，主节点再将复制缓冲区中的数据发送给从节点，确保从节点的数据与主节点一致。 全量同步的代价很高，因为完整的 RDB 文件在生成时会占用大量的 CPU 和磁盘 IO；在网络传输时还会消耗掉不少带宽。 于是 Redis 在 2.8 版本后引入了增量同步的概念，目的是在断线重连后避免全量同步。 增量依赖三个关键要素： ①、复制偏移量：主从节点分别维护一个复制偏移量，记录传输的字节数。主节点每传输 N 个字节数据，自身的复制偏移量就会增加 N；从节点每收到 N 个字节数据，也会相应增加自己的偏移量。 ②、主节点 ID：每个主节点都有一个唯一 ID，即复制 ID，用于标识主节点的数据版本。当主节点发生重启或者角色变化时，ID 会改变。 ③、复制积压缓冲区：主节点维护的一个固定长度的先进先出队列，默认大小为 1M。主节点在向从节点发送命令的同时，也会将命令写入这个缓冲区。 当从节点与主节点断开重连后，会发送 psyncrunIdoffset 命令，带上之前记录的主节点 ID 和复制偏移量。 三分恶面渣逆袭：增量同步 主节点收到这个命令后，会检查 runId 和 offset： 如果主节点 ID 与从节点提供的 runId 不匹配，说明主节点已经变化，必须进行全量同步。 如果 ID 匹配，主节点会查找从节点请求的偏移量之后的数据是否还在复制积压缓冲区。 如果在，只发送从该偏移量开始的增量数据，这就是增量同步；否则说明断线时间太长，积压缓冲区已经覆盖了这部分数据，需要全量同步。 码哥字节：复制积压缓冲区 增量同步的优势显而易见：只传输断线期间的命令数据，大大减少了网络传输量和主从节点的负载，从节点也不需要清空重载数据，能更快地跟上主节点状态。 对于写入频繁或网络不稳定的环境，应该增大复制积压缓冲区的大小，确保短时间断线后能进行增量同步而不是全量同步。 repl-backlog-size 1mb # 默认值 1MB，表示主节点的复制缓冲区大小repl-backlog-ttl 3600 # 默认值 3600 秒，表示主节点的复制缓冲区存活时间 memo：2025 年 5 月 9 日修改至此，今天在修改球友简历时，碰到一个河北大学硕东华理工大学本的球友，希望这个庭能给带来更多的帮助和支持。 河北大学的球友 19.主从复制存在哪些问题呢？Redis 主从复制的最大挑战来自于它的异步特性，主节点处理完写命令后会立即响应客户端，而不会等待从节点确认，这就导致在某些情况下可能出现数据不一致。 leonsh：主从同步 另一个常见问题是全量同步对系统的冲击。全量同步会占用大量的 CPU 和 IO 资源，尤其是在大数据量的情况下，会导致主节点的性能下降。 脑裂问题了解吗？在 Redis 的哨兵架构中，脑裂的典型表现为：主节点与哨兵、从节点之间的网络发生故障了，但与客户端的连接是正常的，就会出现两个“主节点”同时对外提供服务。 哨兵认为主节点已经下线了，于是会将一个从节点选举为新的主节点。但原主节点并不知情，仍然在继续处理客户端的请求。 橡 皮 人：脑裂问题 等主节点网络恢复正常了，发现已经有新的主节点了，于是原主节点会自动降级为从节点。在降级过程中，它需要与新主节点进行全量同步，此时原主节点的数据会被清空。导致客户端在原主节点故障期间写入的数据全部丢失。 极客时间：脑裂问题导致数据丢失 为了防止这种数据丢失，Redis 提供了 min-slaves-to-write 和 min-slaves-max-lag 参数。 这两个参数可以设置最少需要多少个从节点在线，以及从节点的最大延迟时间。 # 设置主节点能进行数据同步的最少从节点数量min-slaves-to-write 1# 设置主从节点间进行数据同步时，从节点给主节点发送 ACK 消息的最大延迟（以秒为单位）min-slaves-max-lag 10 设置这两个参数后，如果主节点连接不到指定数量的从节点，或者从节点响应超时，主节点会拒绝写入请求，从而避免脑裂期间的数据冲突。 具体来说，当网络分区发生，主节点与从节点、哨兵之间的连接断开，但主节点与客户端的连接正常时，由于主节点无法再连接到任何从节点，或者延迟超过了设定值，比如说配置了min-slaves-to-write 1，主节点就会自动拒绝所有写请求。 同时在网络的另一侧，哨兵会检测到主节点”下线”，选举一个从节点成为新的主节点。由于原主节点已经停止接受写入，所以不会产生新的数据变更，等网络恢复后，即使原主节点降级为从节点并进行全量同步，也不会丢失网络分区期间的写入数据，因为根本就没有新的写入发生。 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：主从复制有什么缺点呢？redis的脑裂问题 memo：2025 年 5 月 10 日今天把新项目的前置环境也配的七七八八了，还差一个 Kafka 的安装教程。日拱一卒，争取秋招前给球友们见面。 星球新项目的进度：前置环境的教程 20.Redis哨兵机制了解吗？Redis 中的哨兵用于监控主从集群的运行状态，并在主节点故障时自动进行故障转移。 三分恶面渣逆袭：Redis Sentinel 核心功能包括监控、通知和自动故障转移。哨兵会定期检查主从节点是否按预期工作，当检测到主节点故障时，就在从节点中选举出一个新的主节点，并通知客户端连接到新的主节点。 # 监控的主节点信息 + 多少个哨兵同意才算宕机sentinel monitor mymaster 127.0.0.1 6379 2# 多久不响应就标记为“主观下线”sentinel down-after-milliseconds mymaster 5000# 故障转移超时时间sentinel failover-timeout mymaster 60000# 同时允许多少个从节点同步新主节点数据sentinel parallel-syncs mymaster 1 Java 面试指南（付费）收录的比亚迪面经同学 1 面试原题：Redis 的哨兵机制了解吗？ 21.Redis哨兵的工作原理知道吗？哨兵的工作原理可以概括为 4 个关键步骤：定时监控、主观下线、领导者选举和故障转移。 首先，哨兵会定期向所有 Redis 节点发送 PING 命令来检测它们是否可达。如果在指定时间内没有收到回复，哨兵会将该节点标记为“主观下线”。 原野漫步：sentinel 当一个哨兵判断主节点主观下线后，会询问其他哨兵的意见，如果达到配置的法定人数，主节点会被标记为“客观下线”。 三分恶面渣逆袭：主观下线和客观下线 然后开始故障转移，这个过程中，哨兵会先选举出一个领导者，领导者再从从节点中选择一个最适合的节点作为新的主节点，选择标准包括复制偏移量、优先级等因素。 围龙小子：领导者选举 确定新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令使其升级为主节点，然后向其他从节点发送 SLAVEOF 命令指向新主节点，最后通过发布订阅机制通知客户端主节点已经发生变化。 一泽涟漪：Redis Sentinel故障转移 在实际部署中，为了保证哨兵机制的可靠性，通常建议至少部署三个哨兵节点，并且这些节点应分布在不同的物理机器上，降低单点故障风险。 守株阁：哨兵故障转移 同时，法定人数的设置也非常关键，一般建议设置为哨兵数量的一半加一，既能确保在少数哨兵故障时系统仍能正常工作，又能避免网络分区导致的脑裂问题。 Java 面试指南（付费）收录的 OPPO 面经同学 1 面试原题：Redis的Sentinel和Cluster怎么理解？说一下大概原理 memo：贴一个读者对 Java 进阶之路的美赞吧，我也是人，也需要的情绪共鸣，哈哈，就让赞美多一点吧😄 读者对 Java 进阶之路的美赞 22.Redis领导者选举了解吗？Redis 使用 Raft 算法实现领导者选举，目的是在主节点故障时，选出一个哨兵来负责执行故障转移操作。 二哥的 Java 进阶之路：领导者选举 选举过程是这样的： ①、当一个哨兵确认主节点客观下线后，会向其他哨兵节点发送请求，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。候选者会先给自己先投 1 票，然后等待其他哨兵节点的投票结果。 // sentinel.c中的sentinelAskMasterStateToOtherSentinels函数void sentinelAskMasterStateToOtherSentinels(sentinelRedisInstance *master) dictIterator *di; dictEntry *de; di = dictGetIterator(master-sentinels); while((de = dictNext(di)) != NULL) sentinelRedisInstance *sentinel = dictGetVal(de); int retval; // 只有在进入领导者选举阶段才发送投票请求 if (master-failover_state == SENTINEL_FAILOVER_STATE_SELECT_LEADER) // 发送特殊的is-master-down-by-addr命令请求投票 retval = redisAsyncCommand(sentinel-cc, sentinelReceiveVoteFromSentinel, sentinel, SENTINEL is-master-down-by-addr %s %d %llu %s, master-addr-ip, master-addr-port, (unsigned long long)master-failover_epoch, // 这里发送自己的runid请求投票 sentinelGetMyRunID()); else // 否则只询问主节点状态，不请求投票 retval = redisAsyncCommand(sentinel-cc, sentinelReceiveIsMasterDownReply, sentinel, SENTINEL is-master-down-by-addr %s %d %llu *, master-addr-ip, master-addr-port, (unsigned long long)0); dictReleaseIterator(di); ②、收到请求的哨兵节点进行判断，如果候选者的日志和自己的一样新，任期号也小于自己，且之前没有投票过，就会投同意票 Y。否则回复 N。 // sentinel.c中的sentinelCommand函数部分(处理SENTINEL命令)// 处理is-master-down-by-addr命令else if (!strcasecmp(c-argv[1]-ptr,is-master-down-by-addr)) /* SENTINEL IS-MASTER-DOWN-BY-ADDR ip port current-epoch runid */ sentinelRedisInstance *ri; char *master_ip = c-argv[2]-ptr; int master_port = atoi(c-argv[3]-ptr); long long req_epoch = strtoull(c-argv[4]-ptr,NULL,10); char *req_runid = c-argv[5]-ptr; int isdown = 0; char *leader = *; long long leader_epoch = -1; ri = sentinelGetMasterByAddress(master_ip, master_port); if (ri) isdown = ri-flags SRI_S_DOWN; // 判断是否是投票请求 if (req_runid[0] != *) // 检查是否已经在当前配置纪元中投过票 if (req_epoch sentinel.current_epoch) // 更新自己的配置纪元 sentinel.current_epoch = req_epoch; // 如果我们觉得主节点下线了，且在这个epoch还没投过票，则投票 if (isdown sentinel.current_epoch == req_epoch sentinel.leader_epoch req_epoch) // 记录投票信息 sentinel.leader_epoch = req_epoch; sentinel.leader = sdsnew(req_runid); leader = req_runid; leader_epoch = req_epoch; // 返回投票结果 addReplyMultiBulkLen(c,3); addReplyLongLong(c, isdown); addReplyBulkCString(c, leader); addReplyLongLong(c, leader_epoch); ③、候选者收到投票后会统计自己的得票数，如果获得了集群中超过半数节点的投票，它就会当选为领导者。 // sentinel.c中的sentinelReceiveVoteFromSentinel函数void sentinelReceiveVoteFromSentinel(redisAsyncContext *c, void *reply, void *privdata) sentinelRedisInstance *sentinel = privdata; sentinelRedisInstance *master = sentinel-master; redisReply *r = reply; char *leader = NULL; // 处理回复 if (r-type == REDIS_REPLY_ARRAY r-elements == 3) // 解析回复中的leader信息 if (r-element[1]-type == REDIS_REPLY_STRING) leader = r-element[1]-str; // 检查是否投给了我们 if (leader strcmp(leader, sentinelGetMyRunID()) == 0) // 记录获得一票 dictAdd(master-sentinels_voted, sdsnew(sentinel-runid), sentinel); // 检查是否获得多数票 if (master-failover_state == SENTINEL_FAILOVER_STATE_SELECT_LEADER) int voters = dictSize(master-sentinels) + 1; // +1是因为包括自己 int votes = dictSize(master-sentinels_voted) + 1; // 自己也算一票 // 如果获得多数票(大于一半) if (votes = voters/2+1) // 成为领导者，开始执行故障转移 sentinelEvent(LL_WARNING, +elected-leader, master, %@); master-failover_state = SENTINEL_FAILOVER_STATE_FAILOVER_IN_PROGRESS; sentinelFailoverSelectSlave(master); ④、如果没有哨兵在这一轮投票中获得超过半数的选票，这次选举就会失败，然后进行下一轮的选举。为了防止无限制的选举失败，每个哨兵都会有一个选举超时时间，且是随机的。 // sentinel.c中的sentinelFailoverSelectLeader函数void sentinelFailoverSelectLeader(sentinelRedisInstance *master) // 检查选举是否超时 mstime_t election_timeout = SENTINEL_ELECTION_TIMEOUT * 2; if (mstime() - master-failover_start_time election_timeout) // 选举超时，重置状态 sentinelEvent(LL_WARNING, -failover-abort-timeout, master, %@); sentinelAbortFailover(master); return; // ... 其他选举逻辑 ... // 如果没有足够票数且未超时，则继续等待 这里 SENTINEL_ELECTION_TIMEOUT_MIN 通常为 0，SENTINEL_ELECTION_TIMEOUT_MAX 通常为 2000 毫秒。这样每个哨兵会在 0-2 秒的随机时间后开始选举，减少选举冲突。 推荐阅读：Raft算法的选主过程详解 Java 面试指南（付费）收录的8 后端开发秋招一面面试原题：raft主节点挂了怎么选从节点 memo：2025 年 5 月 12 日修改至此，今天有球友发微信说拿到了三个大厂的 offer，分别是蚂蚁、美团和腾讯，真的是太优秀了呀。 球友拿到了蚂蚁、美团和腾讯的 offer 23.新的主节点是怎样被挑选出来的？哨兵在挑选新的主节点时，非常精细化。 三分恶面渣逆袭：新主节点的挑选过程 首先，哨兵会对所有从节点进行一轮基础筛选，排除那些不满足基本条件的节点。比如说已下线的节点、网络连接不稳定的节点，以及优先级设为 0 明确不参与挑选的节点。 // 第一轮筛选：排除不满足基本条件的从节点for (int i = 0; i numslaves; i++) sentinelRedisInstance *slave = slaves[i]; // 排除已下线的从节点 if (slave-flags (SRI_S_DOWN|SRI_O_DOWN)) continue; // 排除断开连接的从节点 if (slave-link-disconnected) continue; // 排除近期（5秒内）断过连的从节点 if (mstime() - slave-link-last_avail_time 5000) continue; // 排除未建立主从复制的节点 if (slave-slave_priority == 0) continue; // 找到第一个满足条件的从节点 selected = i; break; 然后，哨兵会对剩下的从节点进行排序，选出最合适的主节点。 // sentinel.c中的compareSlaves函数int compareSlaves(sentinelRedisInstance *a, sentinelRedisInstance *b) // 1. 首先比较用户设置的优先级，值越小优先级越高 if (a-slave_priority != b-slave_priority) return (a-slave_priority b-slave_priority) ? 1 : 2; // 2. 如果优先级相同，比较复制偏移量，偏移量越大数据越新 if (a-slave_repl_offset b-slave_repl_offset) return 1; else if (a-slave_repl_offset b-slave_repl_offset) return 2; // 3. 如果复制偏移量也相同，比较运行ID的字典序 return (strcmp(a-runid, b-runid) 0) ? 1 : 2; 排序的标准有三个： ①、从节点优先级： slave-priority 的值越小优先级越高，优先级为 0 的从节点不会被选中。 ②、复制偏移量： 偏移量越大意味着从节点的数据越新，复制的越完整。 ③、运行 ID： 如果优先级和偏移量都相同，就比较运行 ID 的字典序，字典序小的优先。 选出新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令将其提升为主节点。 // sentinel.c中的sentinelFailoverPromoteSlave函数void sentinelFailoverPromoteSlave(sentinelRedisInstance *master) // ... 选择最佳从节点的逻辑 ... // 向选中的从节点发送SLAVEOF NO ONE命令，使其成为主节点 retval = redisAsyncCommand(slave-link-cc, sentinelReceivePromotionResponseFromSlave, master, SLAVEOF NO ONE); // 更新状态 master-promoted_slave = slave; slave-flags |= SRI_PROMOTED; // 记录日志 sentinelEvent(LL_WARNING, +promoted-slave, slave, %@); sentinelEvent(LL_WARNING, +failover-state-wait-promotion, master, %@); 之后，哨兵会等待新主节点的角色转换完成，通过发送 INFO 命令检查其角色是否已变为 master 来确认。确认成功后，会更新所有从节点的复制目标，指向新的主节点。 SLAVEOF new-master-ip new-master-port memo：2025 年 5 月 13 日，今天有球友发微信说拿到了携程的 offer，携程现在也是第二梯队的互联网大厂了，值得一手恭喜啊。 球友拿到了携程的实习 offer 24.Redis集群了解吗？主从复制实现了读写分离和数据备份，哨兵机制实现了主节点故障时自动进行故障转移。 三分恶面渣逆袭：Redis集群示意图 集群架构是对前两种方案的进一步扩展和完善，通过数据分片解决 Redis 单机内存大小的限制，当用户基数从百万增长到千万级别时，我们只需简单地向集群中添加节点，就能轻松应对不断增长的数据量和访问压力。 比如说我们可以将单实例模式下的数据平均分为 5 份，然后启动 5 个 Redis 实例，每个实例保存 5G 的数据，从而实现集群化。 极客时间：切片集群架构图 25.请详细说一说Redis Cluster？（补充） 2024 年 04 月 26 日新增 Redis Cluster 是 Redis 官方提供的一种分布式集群解决方案。其核心理念是去中心化，采用 P2P 模式，没有中心节点的概念。每个节点都保存着数据和整个集群的状态，节点之间通过 gossip 协议交换信息。 Rajat Pachauri：Redis Cluster 在数据分片方面，Redis Cluster 使用哈希槽机制将整个集群划分为 16384 个单元。 aditya goel：哈希槽分片 例如，如果我们有 4 个 Redis 实例，那么每个实例会负责 4000 多个哈希槽。 Rajat Pachauri：分片结果 在计算哈希槽编号时，Redis Cluster 会通过 CRC16 算法先计算出键的哈希值，再对这个哈希值进行取模运算，得到一个 0 到 16383 之间的整数。 slot = CRC16(key) mod 16384 这种方式可以将数据均匀地分布到各个节点上，避免数据倾斜的问题。 三分恶面渣逆袭：槽 当需要存储或查询一个键值对时，Redis Cluster 会先计算这个键的哈希槽编号，然后根据哈希槽编号找到对应的节点进行操作。 推荐阅读：Redis Cluster Java 面试指南（付费）收录的字节跳动面经同学 1 Java 后端技术一面面试原题：Redis 切片集群？数据和实例之间的如何进行映射？ Java 面试指南（付费）收录的快手面经同学 1 部门主站技术部面试原题：Redis 的 cluster 集群如何实现？ memo：2025 年 5 月 14 日，今天有球友发微信说拿到了百度和美团的暑期实习 offer，果然五月也是一个开花结果的季节。 球友拿到了美团和百度的暑期实习 offer 26.集群中数据如何分区？常见的数据分区有三种：节点取余、一致性哈希和哈希槽。 节点取余分区简单明了，通过计算键的哈希值，然后对节点数量取余，结果就是目标节点的索引。 target_node = hash(key) % N // N为节点数量 三分恶面渣逆袭：节点取余分区 缺点是增加一个新节点后，节点数量从 N 变为 N+1，几乎所有的取余结果都会改变，导致大部分缓存失效。 为了解决节点变化导致的大规模数据迁移问题，一致性哈希分区出现了：它将整个哈希值空间想象成一个环，节点和数据都映射到这个环上。数据被分配到顺时针方向上遇到的第一个节点。 三分恶面渣逆袭：一致性哈希分区 这种设计的巧妙之处在于，当节点数量变化时，只有部分数据需要重新分配。比如说我们从 5 个节点扩容到 8 个节点，理论上只有约 38 的数据需要迁移，大大减轻了扩容时的系统压力。 但一致性哈希仍然有一个问题：数据分布不均匀。比如说在上面的例子中，节点 1 和节点 2 的数据量差不多，但节点 3 的数据量却远远小于它们。 Redis Cluster 的哈希槽分区在一致性哈希和节点取余的基础上，做了一些改进。 Dan Palmer：哈希槽 它将整个哈希值空间划分为 16384 个槽位，每个节点负责一部分槽，数据通过 CRC16 算法计算后对 16384 取模，确定它属于哪个槽。 slot = CRC16(key) % 16384 Dan Palmer：确定槽 假设系统中有 4 个节点，为其分配了 16 个槽(0-15)； 槽 0-3 位于节点 node1； 槽 4-7 位于节点 node2； 槽 8-11 位于节点 node3； 槽 12-15 位于节点 node4。 如果此时删除 node2，只需要将槽 4-7 重新分配即可，例如将槽 4-5 分配给 node1，槽 6 分配给 node3，槽 7 分配给 node4，数据在节点上的分布仍然较为均衡。 如果此时增加 node5，也只需要将一部分槽分配给 node5 即可，比如说将槽 3、槽 7、槽 11、槽 15 迁移给 node5，节点上的其他槽位保留。 因为槽的个数刚好是 2 的 14 次方，和 HashMap 中数组的长度必须是 2 的幂次方有着异曲同工之妙。它能保证扩容后，大部分数据停留在扩容前的位置，只有少部分数据需要迁移到新的槽上。 Java 面试指南（付费）收录的小米暑期实习同学 E 一面面试原题：你知道 Redis 的一致性 hash 吗 Java 面试指南（付费）收录的字节跳动面经同学 1 Java 后端技术一面面试原题：Redis 扩容之后，哈希槽的位置是否发生变化？ Java 面试指南（付费）收录的字节跳动面经同学 8 Java 后端实习一面面试原题：redis 分片集群，如何分片的，有什么好处 memo：2025 年 5 月 15 日，今天有球友发微信说加了星球后，算一算，踩着点拿到了滴滴的实习 offer，我看了一下时间线，也就一个月时间不到，真的太强了。 球友拿到了滴滴的 offer 27.能说说 Redis 集群的原理吗？Redis 集群的搭建始于节点的添加和握手。每个节点通过设置 cluster-enabled yes 来开启集群模式。然后通过 CLUSTER MEET 进行握手，将对方添加到各自的节点列表中。 三分恶面渣逆袭：节点和握手 这个过程设计的非常精巧：节点 A 发送 MEET 消息，节点 B 回复 PONG 并发送 PING，节点 A 回复 PONG，于是双向的通信链路就建立完成了。 happen：cluster meet 有趣的是，由于采用了 Gossip 协议，我们不需要让每对节点都执行握手。在一个多节点集群的部署中，仅需要让第一个节点与其他节点握手，其余节点就能通过信息传播自动发现并连接彼此。 程序员历小冰：Gossip 握手完成后，可以通过 CLUSTER ADDSLOTS 命令为主节点分配哈希槽。当 16384 个槽全部分配完毕，集群正式进入就绪状态。 三分恶面渣逆袭：分配槽 故障检测和恢复是保障 Redis 集群高可用的关键。每秒钟，节点会向一定数量的随机节点发送 PING 消息，当发现某个节点长时间未响应 PING 消息，就会将其标记为主观下线。 三分恶面渣逆袭：主观下线 当半数以上的主节点都认为某节点主观下线时，这个节点就会被标记为“客观下线”。 三分恶面渣逆袭：主观下线和客观下线 如果下线的是主节点，它的从节点之一将被选举为新的主节点，接管原主节点负责的哈希槽。 三分恶面渣逆袭：选举投票 部署 Redis 集群至少需要几个物理节点？部署一个生产环境可用的 Redis 集群，从技术角度来说，至少需要 3 个物理节点。 这个最小节点数的设定并非 Redis 技术上的硬性要求，而是基于高可用原则的实践考量。 从实践角度看，最经典的 Redis 集群配置是 3 主 3 从，共 6 个 Redis 实例。考虑到需要 3 个主节点和 3 个从节点，并且每对主从不能在同一物理机上，那么至少需要 3 个物理节点，每个物理节点上运行 1 个主节点和另一个主节点的从节点。 物理节点1：主节点A + 从节点B’ 物理节点2：主节点B + 从节点C’ 物理节点3：主节点C + 从节点A’ 这种交错部署方式可以确保任何一个物理节点故障时，最多只影响一个主节点和一个不同主节点的从节点。 memo：2025 年 5 月 16 日，今天在修改简历的时候，碰到一个河南理工本科，郑州大学硕士的球友，也是希望这个社群能够帮助到更多的同学，无论来自哪里，都能在这里找回那个渴望进步，渴望拿到优质 offer 的自己。 河南理工大学本、郑州大学硕的球友 28.说说Redis集群的动态伸缩？Redis 集群动态伸缩的核心机制是通过重新分配哈希槽实现的。 三分恶面渣逆袭：集群的伸缩 当需要扩容时，首先通过 CLUSTER MEET 命令将新节点加入集群；然后使用 reshard 命令将部分哈希槽重新分配给新节点。 三分恶面渣逆袭：扩容实例 -—这部分面试中可以不背start—- 准备新的节点： # redis.confport 6382cluster-enabled yescluster-config-file nodes-6382.confcluster-node-timeout 5000appendonly yes 然后启动新的节点： redis-server /path/to/redis-6382.conf 接下来，使用 CLUSTER MEET 命令将新节点加入集群： redis-cli -p 6379 cluster meet 127.0.0.1 6382 检查新节点是否加入： redis-cli -p 6379 cluster nodes 然后，重新分配哈希槽： redis-cli --cluster reshard 127.0.0.1:6379 在提示中输入要迁移的哈希槽范围。 # 输入要迁移的槽数量，比如 4096（平均分配的话，16384/4=4096）。How many slots do you want to move (from 16384 total slots)? 4096# 输入 6382 节点的 ID（可通过 cluster nodes 命令查到）。What is the receiving node ID? 6382的节点ID# 输入 all（表示从所有节点平均迁移）。Source node IDs? all# 输入 yes（表示确认迁移）。Do you want to proceed with the proposed reshard plan (yes/no)? yes 检查检查槽分配情况： redis-cli -p 6379 cluster slots 验证集群的状态： redis-cli -p 6382 cluster info 也可以直接一步到位： redis-cli --cluster reshard 127.0.0.1:6379 --cluster-from all --cluster-to 6382的节点ID --cluster-slots 4096 --cluster-yes -—这部分面试中可以不背end—- 缩容则是反向操作：先将要下线节点负责的所有槽迁移到其他节点，再通过 CLUSTER FORGET 命令将节点从集群中移除。 整个伸缩过程支持在线操作，无需停机，得益于 Redis 集群的 MOVED 和 ASK 重定向机制。当客户端访问的键不在当前节点时，会收到重定向响应，指引它连接到正确的节点。 MOVED 和 ASK 重定向的区别？MOVED 重定向反映的是哈希槽的永久性变更。当客户端请求一个键，但键所在的槽不在当前节点时，节点会返回 MOVED 响应，告诉客户端这个槽现在归属于哪个节点。通常发生在集群完成重新分片后，槽的分配关系已经稳定。 Aaron Zhu：MOVED 重定向 比如说某个槽从节点 A 移动到节点 B 后，如果客户端仍向节点 A 请求该槽中的键，会收到 MOVED 响应，提示应该连接节点 B。 ASK 重定向出现在槽迁移过程中，表示请求的键可能已经从源节点迁移到了目标节点，但迁移尚未完成。 Aaron Zhu：ASK 重定向 嗨嗨嗨，时隔两年，面渣逆袭第二版 PDF 终于可以下载了。我们做了大量的优化： 对于高频题：会标注在《Java 面试指南（付费）中出现的位置，哪家公司，原题是什么；如果你想节省时间的话，可以优先背诵这些题目，尽快做到知彼知己，百战不殆。 结合项目：包括技术派、mydb、pmhub来组织语言，让面试官最大程度感受到你的诚意，而不是机械化的背诵。 修复问题：第一版中出现的问题，包括球友们的私信反馈，网站留言区的评论，以及 GitHub 仓库中的 issue，让这份面试指南更加完善。 优化排版：增加手绘图，重新组织答案，使其更加口语化，从而更贴近面试官的预期。 你可以扫下面的二维码（或者长按自动识别）关注【沉默王二】公众号，发送关键字 222 来获取 PDF 版本，如果面渣逆袭真的对你有帮助，希望能给二哥的公众号加一个星标，满足我那一丁点虚荣心，这将是我更新下去的最强动力。 面渣逆袭的整理工作真的太不容易了，花了我好多好多的时间和精力，内容完全免费，但质量却有口皆碑，就是为了做一点真正有意义的、纯粹的事情。 memo：2025 年 5 月 17 日，今天有球友发微信说拿到了一个国企子公司的 Java 后端开发和一个小米安卓的 offer，问我该怎么选择？ 球友拿到了一个国企子公司和小米的 offer 缓存设计29.🌟什么是缓存击穿？缓存击穿是指某个热点数据缓存过期时，大量请求就会穿透缓存直接访问数据库，导致数据库瞬间承受的压力巨大。 fengkui.net：缓存击穿 解决缓存击穿有两种常用的策略： 第一种是加互斥锁。当缓存失效时，第一个访问的线程先获取锁并负责重建缓存，其他线程等待或重试。 三分恶面渣逆袭：加锁更新 这种策略虽然会导致部分请求延迟，但实现起来相对简单。在技术派实战项目中，我们就使用了 Redisson 的分布式锁来确保只有一个服务实例能更新缓存。 String cacheKey = product:: + productId;RLock lock = redissonClient.getLock(lock:: + productId);if (lock.tryLock(10, TimeUnit.SECONDS)) try String result = cache.get(cacheKey); if (result == null) result = database.queryProductById(productId); cache.set(cacheKey, result, 60 * 1000); // 设置缓存 finally lock.unlock(); 第二种是永不过期策略。缓存项本身不设置过期时间，也就是永不过期，但在缓存值中维护一个逻辑过期时间。当缓存逻辑上过期时，返回旧值的同时，异步启动一个线程去更新缓存。 public String getData(String key) CacheItem item = cache.get(key); if (item == null) // 缓存不存在，同步加载 String data = db.query(key); cache.set(key, new CacheItem(data, System.currentTimeMillis() + expireTime)); return data; else if (item.isLogicalExpired()) // 逻辑过期，异步刷新 asyncRefresh(key); // 返回旧数据 return item.getData(); return item.getData();// 异步刷新缓存private void asyncRefresh(final String key) threadPool.execute(() - // 重新查询数据库 String newData = db.query(key); // 更新缓存 cache.set(key, new CacheItem(newData, System.currentTimeMillis() + expireTime)); ); memo：2025 年 5 月 18 日修改至此，今天给球友改简历时，碰到一个西北工业大学的球友，这又是一所 985 院校，希望这个社群能把所有的 985 院校集齐，也希望去帮助到更多院校的同学，希望都能拿到一个满意的 offer。 西北工业大学的球友 什么是缓存穿透？缓存穿透是指查询的数据在缓存中没有命中，因为数据压根不存在，所以请求会直接落到数据库上。如果这种查询非常频繁，就会给数据库造成很大的压力。 fengkui.net：缓存穿透 缓存击穿是因为单个热点数据缓存失效导致的，而缓存穿透是因为查询的数据不存在，原因可能是自身的业务代码有问题，或者是恶意攻击造成的，比如爬虫。 常用的解决方案有两种：第一种是布隆过滤器，它是一种空间效率很高的数据结构，可以用来判断一个元素是否在集合中。 我们可以将所有可能存在的数据哈希到布隆过滤器中，查询时先检查布隆过滤器，如果布隆过滤器认为该数据不存在，就直接返回空；否则再去查询缓存，这样就可以避免无效的缓存查询。 酒剑仙：布隆过滤器解决缓存穿透 代码示例： public String getData(String key) // 缓存中不存在该key String cacheResult = cache.get(key); if (cacheResult != null) return cacheResult; // 布隆过滤器判断key是否可能存在 if (!bloomFilter.mightContain(key)) return null; // 一定不存在，直接返回 // 可能存在，查询数据库 String dbResult = db.query(key); // 将结果放入缓存，包括空值 cache.set(key, dbResult != null ? dbResult : , expireTime); return dbResult; 布隆过滤器存在误判，即可能会认为某个数据存在，但实际上并不存在。但绝不会漏判，即如果布隆过滤器认为某个数据不存在，那它一定不存在。因此它可以有效拦截不存在的数据查询，减轻数据库压力。 第二种是缓存空值。对于不存在的数据，我们将空值写入缓存，并设置一个合理的过期时间。这样下次相同的查询就能直接从缓存返回，而不再访问数据库。 三分恶面渣逆袭：缓存空值默认值 代码示例： public String getData(String key) String cacheResult = cache.get(key); // 缓存命中，包括空值 if (cacheResult != null) // 特殊值表示空结果 if (cacheResult.equals()) return null; return cacheResult; // 缓存未命中，查询数据库 String dbResult = db.query(key); // 写入缓存，空值也缓存，但设置较短的过期时间 int expireTime = dbResult == null ? EMPTY_EXPIRE_TIME : NORMAL_EXPIRE_TIME; cache.set(key, dbResult != null ? dbResult : , expireTime); return dbResult; 缓存空值的方法实现起来比较简单，但需要给空值设置一个合理的过期时间，以免数据库中新增了这些数据后，缓存仍然返回空值。 在实际的项目当中，还需要在接口层面做一些处理，比如说对参数进行校验，拦截明显不合理的请求；或者对疑似攻击的 IP 进行限流和封禁。 memo：2025 年 5 月 19 日，今天有球友发微信说拿到了滴滴的测开实习 offer，目前还想继续找，问我该继续学点什么，我的回复说，暑期能拿到 offer，秋招继续就行了，加上滴滴的实习经历就很硬核了。在准备暑期和秋招的时候，也不要太焦虑，保持一个好的学习习惯，秋招没问题的。 滴滴的测开offer 什么是缓存雪崩？缓存雪崩是指在某一时间段，大量缓存同时失效或者缓存服务突然宕机了，导致大量请求直接涌向数据库，导致数据库压力剧增，甚至引发系统崩溃的现象。 三分恶面渣逆袭：缓存雪崩 缓存击穿是单个热点数据失效导致的，缓存穿透是因为请求不存在的数据，而缓存雪崩是因为大范围的缓存失效。 缓存雪崩主要有三种成因和应对策略。 第一种，大量缓存同时过期，解决方法是添加随机过期时间。 public void setCache(String key, String value) // 基础过期时间，例如30分钟 int baseExpireSeconds = 1800; // 增加随机过期时间，范围0-300秒 int randomSeconds = new Random().nextInt(300); // 最终过期时间为基础时间加随机时间 cache.set(key, value, baseExpireSeconds + randomSeconds); 第二种，缓存服务崩溃，解决方法是使用高可用的缓存集群。 比如说使用 Redis Cluster 构建多节点集群，确保数据在多个节点上有备份，并且支持自动故障转移。 Rajat Pachauri：Redis Cluster 对于一些高频关键数据，可以配置本地缓存作为二级缓存，缓解 Redis 的压力。在技术派实战项目中，我们就采用了多级缓存的策略，其中就包括使用本地缓存 Caffeine 来作为二级缓存，当 Redis 出现问题时自动切换到本地缓存。 技术派教程：Caffeine本地缓存 这个过程称为“缓存降级”，保证 Redis 发生故障时，系统能够继续提供服务。 LoadingCacheString, UserPermissions permissionsCache = Caffeine.newBuilder() .maximumSize(1000) .expireAfterWrite(10, TimeUnit.MINUTES) .build(this::loadPermissionsFromRedis);public UserPermissions loadPermissionsFromRedis(String userId) try return redisClient.getPermissions(userId); catch (Exception ex) // Redis 异常处理，尝试从本地缓存获取 return permissionsCache.getIfPresent(userId); 第三种，缓存服务正常但并发请求量超过了缓存服务的承载能力，这种情况下可以采用限流和降级措施。 public String getData(String key) try // 尝试从缓存获取数据 return cache.get(key); catch (Exception e) // 缓存服务异常，触发熔断 if (circuitBreaker.shouldTrip()) // 直接从数据库获取，并进入降级模式 circuitBreaker.trip(); return getFromDbDirectly(key); throw e; private String getFromDbDirectly(String key) // 实施限流保护 if (!rateLimit.tryAcquire()) // 超过限流阈值，返回兜底数据或默认值 return getDefaultValue(key); // 限流通过，从数据库查询 return db.query(key); Java 面试指南（付费）收录的腾讯面经同学 22 暑期实习一面面试原题：缓存雪崩，如何解决 Java 面试指南（付费）收录的快手面经同学 7 Java 后端技术一面面试原题：说一下 缓存穿透、缓存击穿、缓存雪崩 Java 面试指南（付费）收录的字节跳动同学 7 Java 后端实习一面的原题：Redis 宕机会不会对权限系统有影响？ Java 面试指南（付费）收录的字节跳动同学 7 Java 后端实习一面的原题：说一下 Redis 雪崩、穿透、击穿等场景的解决方案 Java 面试指南（付费）收录的小米同学 F 面试原题：缓存常见问题和解决方案（引申到多级缓存），多级缓存（redis，nginx，本地缓存）的实现思路 Java 面试指南（付费）收录的TP联洲同学 5 Java 后端一面的原题：如何解决缓存穿透 Java 面试指南（付费）收录的理想汽车面经同学 2 一面面试原题：如何理解缓存雪崩、缓存击穿和缓存穿透？ memo：2025 年 5 月 20 日，今天有球友发微信说项目用的技术派，八股背的面渣，春招拿到了四个 offer，其中包括泰隆银行和交通银行，问我该怎么选择，说实话我看完后觉得挺难选的，😄不过还是值得恭喜一手。在准备春招的时候也不要着急，付出总会有回报的。 球友春招拿到了四个 offer 30.🌟能说说布隆过滤器吗？布隆过滤器是一种空间效率极高的概率性数据结构，用于快速判断一个元素是否在一个集合中。它的特点是能够以极小的内存消耗，判断一个元素“一定不在集合中”或“可能在集合中”，常用来解决 Redis 缓存穿透的问题。 三分恶面渣逆袭：布隆过滤器 -—这部分面试中可以不背start—- 布隆过滤器的核心由一个很长的二进制向量和一系列哈希函数组成。 初始化的时候，创建一个长度为 m 的位数组，初始值全为 0，同时选择 k 个不同的哈希函数 当添加一个元素时，用 k 个哈希函数计算出 k 个哈希值，然后对 m 取模，得到 k 个位置，将这些位置的二进制位都设为 1 当需要判断一个元素是否在集合中时，同样用 k 个哈希函数计算出 k 个位置，如果这些位置的二进制位有任何一个为 0，该元素一定不在集合中；如果全部为 1，则该元素可能在集合中 public class BloomFilterT private BitSet bitSet; private int bitSetSize; private int numberOfHashFunctions; public BloomFilter(double falsePositiveProbability, int expectedNumberOfElements) // 根据预期元素数量和期望的误判率，计算最优的位数组大小和哈希函数个数 this.bitSetSize = calculateOptimalBitSetSize(expectedNumberOfElements, falsePositiveProbability); this.numberOfHashFunctions = calculateOptimalNumberOfHashFunctions(expectedNumberOfElements, bitSetSize); this.bitSet = new BitSet(bitSetSize); public void add(T element) int[] hashes = createHashes(element); for (int hash : hashes) bitSet.set(Math.abs(hash % bitSetSize), true); public boolean mightContain(T element) int[] hashes = createHashes(element); for (int hash : hashes) if (!bitSet.get(Math.abs(hash % bitSetSize))) return false; // 如果任何一位为0，元素一定不存在 return true; // 所有位都为1，元素可能存在 // 其他辅助方法，如计算哈希值，计算最优参数等 -—这部分面试中可以不背end—- 布隆过滤器存在误判吗？是的，布隆过滤器存在误判。它可能会错误地认为某个元素在集合中，而元素实际上并不在集合中。 勇哥：布隆过滤器 但如果布隆过滤器认为某个元素不存在于集合中，那么它一定不存在。 误判产生的原因是因为哈希冲突。在布隆过滤器中，多个不同的元素可能映射到相同的位置。随着向布隆过滤器中添加的元素越来越多，位数组中的 1 也越来越多，发生哈希冲突的概率随之增加，误判率也就随之上升。 勇哥：布隆过滤器的误判 误判率取决于以下 3 个因素： 位数组的大小（m）：m 决定了可以存储的标志位数量。如果位数组过小，那么哈希碰撞的几率就会增加，从而导致更高的误判率。 哈希函数的数量（k）：k 决定了每个元素在位数组中标记的位数。哈希函数越多，碰撞的概率也会相应变化。如果哈希函数太少，过滤器很快会变得不精确；如果太多，误判率也会升高，效率下降。 存入的元素数量（n）：n 越多，哈希碰撞的几率越大，从而导致更高的误判率。 要降低误判率，可以增加位数组的大小或者减少插入的元素数量。 要彻底解决布隆过滤器的误判问题，可以在布隆过滤器返回”可能存在”时，再通过数据库进行二次确认。 布隆过滤器支持删除吗？布隆过滤器并不支持删除操作，这是它的一个重要限制。 当我们添加一个元素时，会将位数组中的 k 个位置设置为 1。由于多个不同元素可能共享相同的位，如果我们尝试删除一个元素，将其对应的 k 个位重置为 0，可能会错误地影响到其他元素的判断结果。 例如，元素 A 和元素 B 都将位置 5 设为 1，如果删除元素 A 时将位置 5 重置为 0，那么对元素 B 的查询就会产生错误的”不存在”结果，这违背了布隆过滤器的基本特性。 如果想要实现删除操作，可以使用计数布隆过滤器，它在每个位置上存储一个计数器而不是单一的位。这样可以通过减少计数器的值来实现删除操作，但会增加内存开销。 public class CountingBloomFilterT private int[] counters; private int size; private int hashFunctions; public CountingBloomFilter(int size, int hashFunctions) this.size = size; this.hashFunctions = hashFunctions; this.counters = new int[size]; public void add(T element) int[] positions = getHashPositions(element); for (int position : positions) counters[position]++; public void remove(T element) int[] positions = getHashPositions(element); for (int position : positions) if (counters[position] 0) counters[position]--; public boolean mightContain(T element) int[] positions = getHashPositions(element); for (int position : positions) if (counters[position] == 0) return false; return true; private int[] getHashPositions(T element) // 计算哈希位置的代码 为什么不能用哈希表而是用布隆过滤器？布隆过滤器最突出的优势是内存效率。 假如我们要判断 10 亿个用户 ID 是否曾经访问过特定页面，使用哈希表至少需要 10G 内存（每个 ID 至少需要8字节），而使用布隆过滤器只需要 1.2G 内存。 m ≈ -n*ln(p)/ln(2)² ≈ -10⁹*ln(0.01)/ln(2)² ≈ 9.6 billion bits ≈ 1.2GB Java 面试指南（付费）收录的字节跳动同学 7 Java 后端实习一面的原题：有了解过布隆过滤器吗？ Java 面试指南（付费）收录的TP联洲同学 5 Java 后端一面的原题：布隆过滤器原理，这种方式下5%的错误率可接受？ Java 面试指南（付费）收录的美团同学 9 一面面试原题：布隆过滤器？布隆过滤器优点？为什么不能用哈希表要用布隆过滤器？ Java 面试指南（付费）收录的理想汽车面经同学 2 一面面试原题：追问：说明一下布隆过滤器 memo：2025 年 5 月 20 日，今天有球友发贴说拿到了滴滴的暑期 offer，特意来感谢了一下面渣逆袭。 拿到了滴滴的 offer 31.🌟如何保证缓存和数据库的数据⼀致性？在技术派实战项目中，对于文章标签这种允许短暂不一致的数据，我会采用 Cache Aside + TTL 过期机制来保证缓存和数据库的一致性。 技术派教程：MySQL 和 Redis 一致性 具体做法是读取时先查 Redis，未命中再查 MySQL，同时为缓存设置一个合理的过期时间；更新时先更新 MySQL，再删除 Redis。 // 读取逻辑public UserInfo getUser(String userId) // 先查缓存 UserInfo user = cache.get(user: + userId); if (user != null) return user; // 缓存未命中，查数据库 user = database.selectUser(userId); if (user != null) // 放入缓存，设置合理的过期时间 cache.set(user: + userId, user, 3600); return user;// 更新逻辑public void updateUser(UserInfo user) // 先更新数据库 database.updateUser(user); // 删除缓存 cache.delete(user: + user.getId()); 这种方式简单有效，适用于读多写少的场景。TTL 过期时间也能够保证即使更新操作失败，未能及时删除缓存，过期时间也能确保数据最终一致。 那再来说说为什么要删除缓存而不是更新缓存？最初设计缓存策略时，我也考虑过直接更新缓存，但通过实践发现，删除缓存是更优的选择。 技术派：更新 Redis 而不是删除 Redis 最主要的原因是在并发环境下，假设我们有两个并发的更新操作，如果采用更新缓存的策略，就可能出现这样的时序问题： 操作 A 和操作 B 同时发生，A 先更新 MySQL 将值改为 10，B 后更新 MySQL 将值改为 11。但在缓存更新时，可能 B 先执行将缓存设为 11，然后 A 才执行将缓存设为10。这样就会造成 MySQL 是 11 但 Redis 是 10 的不一致状态。 而采用删除策略，无论 A 和 B 谁先删除缓存，后续的读取操作都会从 MySQL 获取最新值。 另外，相对而言，删除缓存的速度比更新缓存的速度快得多。 三分恶面渣逆袭：删除缓存和更新缓存 因为删除操作只是简单的 DEL 命令，而更新可能需要重新序列化整个对象再写入缓存。 那再说说为什么要先更新数据库，再删除缓存？这个操作顺序的选择也是我在实际项目中踩过坑才深刻理解的。假设我们采用先删缓存再更新数据库的策略，在高并发场景下就可能出现这样的问题： 线程 A 要更新用户信息，先删除了缓存 线程 B 恰好此时要读取该用户信息，发现缓存为空，于是查询数据库，此时还是旧值 线程 B 将查到的旧值重新放入缓存 线程 A 完成数据库更新 结果就是数据库是新的值，但缓存中还是旧值。 技术派：先删 Redis 再更新 MySQL 而采用先更新数据库再删缓存的策略，即使出现类似的并发情况，最坏的情况也只是短暂地从缓存中读取到了旧值，但缓存删除后的请求会直接从数据库中获取最新值。 另外，如果先删缓存再更新数据库，当数据库更新失败时，缓存已经被删除了。这会导致短期内所有读请求都会穿透到数据库，对数据库造成额外的压力。 三分恶面渣逆袭：先更数据库还是先删缓存 而先更新数据库再删缓存，如果数据库更新失败，缓存保持原状，系统仍然能继续正常提供服务。 public void updateUser(User user) try // 先更新数据库 database.updateUser(user); // 再删除缓存 cache.delete(user: + user.getId()); catch (DatabaseException e) // 数据库更新失败，缓存保持原状，系统仍可正常提供服务 log.error(Database update failed, e); throw e; catch (CacheException e) // 缓存删除失败，数据库已更新，数据会在TTL后自动一致 log.warn(Cache deletion failed, will be eventually consistent, e); // 可以选择不抛异常，因为有TTL兜底 memo：2025 年 5 月 22 日，今天给球友修改简历时，碰到一个西北工业大学本、电子科技大学硕的球友，一下子 985 高校又集齐了两所。如果球友们在星球里有所收获，也请给学弟学妹们一个口碑，让都能因此受益，拿到更好的 offer。 西北工业大学本、电子科技大学硕的球友 那假如对缓存数据库一致性要求很高，该怎么办呢？当业务对缓存与数据库的一致性要求很高时，比如支付系统、库存管理等场景，我会采用多种策略来保证强一致性。 二哥的 Java 进阶之路：缓存强一致性 第一种，引入消息队列来保证缓存最终被删除，比如说在数据库更新的事务中插入一条本地消息记录，事务提交后异步发送给 MQ 进行缓存删除。 三分恶面渣逆袭：消息队列保证key被删除 即使缓存删除失败，消息队列的重试机制也能保证最终一致性。 @Transactionalpublic void updateUser(UserInfo user) // 在事务中更新数据库 database.updateUser(user); // 在同一事务中记录需要删除的缓存信息 LocalMessage message = new LocalMessage(CACHE_DELETE, user: + user.getId()); database.insertLocalMessage(message); // 显式发布事件，供监听器捕获 eventPublisher.publishEvent(new UserUpdateEvent(this, user: + user.getId()));// 事务提交后发送MQ消息@TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT)public void sendCacheDeleteMessage(UserUpdateEvent event) messageQueue.send(cache-delete-topic, event.getCacheKey()); 第二种，使用 Canal 监听 MySQL 的 binlog，在数据更新时，将数据变更记录到消息队列中，消费者消息监听到变更后去删除缓存。 三分恶面渣逆袭：数据库订阅+消息队列保证key被删除 这种方案的优势是完全解耦了业务代码和缓存维护逻辑。 @CanalListenerpublic class CacheUpdateListener @EventHandler public void handleUserUpdate(UserUpdateEvent event) // 从binlog事件中提取变更信息 String userId = event.getUserId(); // 发送缓存删除消息 CacheDeleteMessage message = new CacheDeleteMessage(); message.setCacheKey(user: + userId); messageQueue.send(cache-delete-topic, message); // 消费者监听消息队列@KafkaListener(topics = cache-delete-topic)public void handleCacheDeleteMessage(CacheDeleteMessage message) // 删除缓存 cache.delete(message.getCacheKey()); 当然了，如果说业务比较简单，不需要上消息队列，可以通过延迟双删策略降低缓存和数据库不一致的时间窗口，在第一次删除缓存之后，过一段时间之后，再次尝试删除缓存。 三分恶面渣逆袭：延时双删 这种方式主要针对缓存不存在，但写入了脏数据的情况。 public void updateUser(UserInfo user) // 第一次删除缓存，减少不一致时间窗口 cache.delete(user: + user.getId()); // 更新数据库 database.updateUser(user); // 立即删除缓存 cache.delete(user: + user.getId()); // 延时删除，应对可能的并发读取 CompletableFuture.runAsync(() - try Thread.sleep(1000); // 延时时间根据主从同步延迟调整 cache.delete(user: + user.getId()); catch (InterruptedException e) Thread.currentThread().interrupt(); ); 最后，无论采用哪种策略，最好为缓存设置一个合理的过期时间作为最后的保障。即使所有的主动删除机制都失败了，TTL 也能确保数据最终达到一致： // 根据数据的重要程度设置不同的TTLpublic void setCache(String key, Object value, DataImportance importance) int ttl; switch (importance) case HIGH: // 关键数据，短TTL ttl = 300; // 5分钟 break; case MEDIUM: // 一般数据 ttl = 1800; // 30分钟 break; case LOW: // 不太重要的数据 ttl = 3600; // 1小时 break; cache.setWithTTL(key, value, ttl); 这种方式虽然简单，但能确保即使出现极端情况，数据不一致的影响也是可控的。 Java 面试指南（付费）收录的华为面经同学 8 技术二面面试原题：怎样保证数据的最终一致性？ Java 面试指南（付费）收录的腾讯面经同学 23 QQ 后台技术一面面试原题：数据一致性问题 Java 面试指南（付费）收录的微众银行同学 1 Java 后端一面的原题：MySQL 和缓存一致性问题了解吗？ Java 面试指南（付费）收录的美团面经同学 3 Java 后端技术一面面试原题：如何保证 redis 缓存与数据库的一致性，为什么这么设计 Java 面试指南（付费）收录的比亚迪面经同学 12 Java 技术面试原题：怎么解决redis和mysql的缓存一致性问题 Java 面试指南（付费）收录的字节跳动同学 17 后端技术面试原题：双写一致性怎么解决的 Java 面试指南（付费）收录的京东面经同学 9 面试原题：redis的数据和缓存不一致应该处理 memo：2025 年 5 月 23 日修改至此，今天在修改球友简历时，看到一条非常温暖的感谢信，球友说改完后的简历，每一句都比之前的好很多，真的很欣慰，感觉自己的付出得到了回报。😄 球友对简历修改的认可 32.如何保证本地缓存和分布式缓存的一致？在技术派实战项目中，为了减轻 Redis 的负载压力，我又追加了一层本地缓存 Caffeine。 三分恶面渣逆袭：本地缓存+分布式缓存 为了保证 Caffeine 和 Redis 缓存的一致性，我采用的策略是当数据更新时，通过 Redis 的 pubsub 机制向所有应用实例发送缓存更新通知，收到通知后的实例立即更新或者删除本地缓存。 三分恶面渣逆袭：本地缓存分布式缓存保持一致 @Servicepublic class CacheService private final RedisTemplate redisTemplate; private final CaffeineCache localCache; public void updateData(String key, Object value) // 更新数据库 database.update(key, value); // 更新分布式缓存 redisTemplate.opsForValue().set(key, value, 30, TimeUnit.MINUTES); // 发送缓存更新通知 CacheUpdateMessage message = new CacheUpdateMessage(key, UPDATE, value); redisTemplate.convertAndSend(cache-update-channel, message); @EventListener public void handleCacheUpdate(CacheUpdateMessage message) if (UPDATE.equals(message.getAction())) localCache.put(message.getKey(), message.getValue()); else if (DELETE.equals(message.getAction())) localCache.invalidate(message.getKey()); 考虑到消息可能丢失，我还会引入版本号机制作为补充。每次从 Redis 获取数据时添加一个最新的版本号。从本地缓存获取数据前，先检查自己的版本号是否是最新的，如果发现版本落后，就主动从 Redis 中获取最新数据。 @Componentpublic class VersionBasedCacheManager @Autowired private StringRedisTemplate redisTemplate; // 使用 Caffeine 构建本地缓存：最多 1000 项，写入后 10 分钟过期 private final CacheString, VersionedData localCache = Caffeine.newBuilder() .maximumSize(1000) .expireAfterWrite(10, TimeUnit.MINUTES) .build(); /** * 获取缓存数据，优先使用本地缓存，必要时从 Redis 加载 */ public Object get(String key) VersionedData cached = localCache.getIfPresent(key); // 从本地缓存取出 // 从 Redis 获取版本号 String versionStr = redisTemplate.opsForValue().get(key + :version); // 如果 Redis 中没找到版本号，说明可能数据已失效，强制刷新 if (versionStr == null) return loadAndCache(key); long remoteVersion = Long.parseLong(versionStr); // 如果本地没有缓存，或版本落后于 Redis，强制刷新 if (cached == null || cached.getVersion() remoteVersion) return loadAndCache(key); // 命中本地缓存且版本最新，直接返回 return cached.getData(); /** * 从 Redis 加载数据和版本，并写入本地缓存 */ private Object loadAndCache(String key) Object data = redisTemplate.opsForValue().get(key); String versionStr = redisTemplate.opsForValue().get(key + :version); if (data != null versionStr != null) long version = Long.parseLong(versionStr); localCache.put(key, new VersionedData(data, version)); return data; 如果在项目中多个地方都要使用到二级缓存的逻辑，如何设计这一块？我的思路是将二级缓存抽象成一个统一的组件。设计一个 CacheManager 作为核心入口，提供 get、put、evict 等基本操作，执行先查本地缓存，再查分布式缓存，最后查数据库的完整流程。 public class CacheManager private final LocalCache localCache; private final RedisCache redisCache; private final Database database; public CacheManager(LocalCache localCache, RedisCache redisCache, Database database) this.localCache = localCache; this.redisCache = redisCache; this.database = database; public Object get(String key) // 先查本地缓存 Object value = localCache.get(key); if (value != null) return value; // 再查分布式缓存 value = redisCache.get(key); if (value != null) // 更新本地缓存 localCache.put(key, value); return value; // 最后查数据库 value = database.get(key); if (value != null) // 更新分布式缓存和本地缓存 redisCache.put(key, value); localCache.put(key, value); return value; 本地缓存和 Redis 的区别了解吗？Redis 可以部署在多个节点上，支持数据分片、主从复制和集群。而本地缓存只能在单个服务器上使用。 对于读取频率极高、数据相对稳定、允许短暂不一致的数据，我优先选择本地缓存。比如系统配置信息、用户权限数据、商品分类信息等。 而对于需要实时同步、数据变化频繁、多个服务需要共享的数据，我会选择 Redis。比如用户会话信息、购物车数据、实时统计信息等。 Java 面试指南（付费）收录的字节跳动同学 7 Java 后端实习一面的原题：怎么保证二级缓存和 Redis 缓存的数据一致性？ Java 面试指南（付费）收录的华为面经同学 11 面试原题：使用的 guava cache 和 redis 是如何组合使用的？如果在项目中多个地方都要使用到二级缓存的逻辑，如何设计这一块？ Java 面试指南（付费）收录的去哪儿同学 1 技术二面的原题：redis 和本地缓存的区别，哪个效率高 Java 面试指南（付费）收录的拼多多面经同学 8 一面面试原题：缓存一致性如何保证 33.什么是热Key？所谓的热 Key，就是指在很短时间内被频繁访问的键。比如电商大促期间爆款商品的详情信息，流量明星爆瓜时的个人资料、热门话题等，都可能成为热Key。 由于 Redis 是单线程模型，大量请求集中到同一个键会导致该 Redis 节点的 CPU 使用率飙升，响应时间变长。 在 Redis 集群环境下，热Key 还会导致数据分布不均衡，某个节点承受的压力过大而其他节点相对空闲。 飞猪开放平台：热 Key 造成缓存击穿 更严重的情况是，当热Key 过期或被误删时，会引发缓存击穿问题。 那怎么监控热Key 呢？临时的方案可以使用 redis-cli --hotkeys 命令来监控 Redis 中的热 Key。 redis-cli -h address -p port -apassword — hotkey 飞猪开放平台：发现热点数据 或者在访问缓存时，在本地维护一个计数器，当某个键的访问次数在一分钟内超过设定阈值，就将其标记为热Key。 @Componentpublic class HotKeyDetector private final ConcurrentHashMapString, AtomicLong accessCounter = new ConcurrentHashMap(); private final int HOT_KEY_THRESHOLD = 1000; public boolean isHotKey(String key) long count = accessCounter.computeIfAbsent(key, k - new AtomicLong(0)) .incrementAndGet(); return count HOT_KEY_THRESHOLD; 34.那怎么处理热Key 呢？最有效的解决方法是增加本地缓存，将热 Key 缓存到本地内存中，这样请求就不需要访问 Redis 了。 三分恶面渣逆袭：热key处理 对于一些特别热的 Key，可以将其拆分成多个子 Key，然后随机分布到不同的 Redis 节点上。比如将 hot_product:12345 拆分成 hot_product:12345:1、hot_product:12345:2 等多个副本，读取时随机选择其中一个。 Jerry’s Notes：处理热 Key public String getHotData(String key) if (isHotKey(key)) // 随机选择一个副本 int replica = ThreadLocalRandom.current().nextInt(HOT_KEY_REPLICAS); return redis.get(key + : + replica); return redis.get(key); 35.怎么处理大 Key 呢？大Key 是指占用内存空间较大的缓存键，比如超过 10M 的键值对。常见的大Key 类型包括：包含大量元素的 List、Set、Hash 结构，存储大文件的 String 类型，以及包含复杂嵌套对象的 JSON 数据等。 在内存有限的情况下，可能导致 Redis 内存不足。另外，大Key 还会导致主从复制同步延迟，甚至引发网络拥塞。 可以通过 redis-cli --bigkeys 命令来监控 Redis 中的大 Key。 二哥的 Java 进阶之路：bigkeys 或者编写脚本进行全量扫描： @Componentpublic class BigKeyScanner private final RedisTemplate redisTemplate; private final int BIG_KEY_THRESHOLD = 1024 * 1024; // 1MB public ListBigKeyInfo scanBigKeys() ListBigKeyInfo bigKeys = new ArrayList(); // 使用SCAN命令遍历所有键 ScanOptions options = ScanOptions.scanOptions().count(1000).build(); Cursorbyte[] cursor = redisTemplate.executeWithStickyConnection( connection - connection.scan(options) ); while (cursor.hasNext()) String key = new String(cursor.next()); long memory = getKeyMemoryUsage(key); if (memory BIG_KEY_THRESHOLD) bigKeys.add(new BigKeyInfo(key, memory, getKeyType(key))); return bigKeys; private long getKeyMemoryUsage(String key) // 使用MEMORY USAGE命令获取键的内存占用 return redisTemplate.execute((RedisCallbackLong) connection - connection.memoryUsage(key.getBytes()) ); 对于大 Key 问题，最根本的解决方案是拆分大 Key，将其拆分成多个小 Key 存储。比如将一个包含大量用户信息的 Hash 拆分成多个小 Hash。 三分恶面渣逆袭：大key处理 public void splitBigKey(String bigKey) MapString, String bigData = redisTemplate.opsForHash().entries(bigKey); // 将大 Key 拆分成多个小 Key for (Map.EntryString, String entry : bigData.entrySet()) String smallKey = bigKey + : + entry.getKey(); redisTemplate.opsForValue().set(smallKey, entry.getValue()); // 删除原始大 Key redisTemplate.delete(bigKey); 另外，对于 JSON 数据，可以进行 Gzip 压缩后再存储，虽然会增加一些 CPU 开销，但在内存敏感的场景在是值得的。 public void setCompressedData(String key, Object data) try String json = objectMapper.writeValueAsString(data); byte[] compressed = compress(json.getBytes()); redisTemplate.opsForValue().set(key, compressed); catch (Exception e) log.error(Failed to compress data, e); private byte[] compress(byte[] data) throws IOException ByteArrayOutputStream out = new ByteArrayOutputStream(); try (GZIPOutputStream gzip = new GZIPOutputStream(out)) gzip.write(data); return out.toByteArray(); 推荐阅读： 阿里：发现并处理 Redis 的大 Key 和热 Key 董宗磊：Redis 热 Key 发现以及解决办法 Java 面试指南（付费）收录的华为 OD 的面试中出现过该题：讲一讲 Redis 的热 Key 和大 Key memo：2025 年 5 月 24 日，今天球友发私信说，拿到了荣耀通软的实习 offer，恭喜他！🎉 荣耀通软的实习 offer 36.缓存预热怎么做呢？缓存预热是指在系统启动或者特定时间点，提前将热点数据加载到缓存中，避免冷启动时大量请求直接打到数据库。 geeksforgeeks.org：缓存预热 缓存预热的方法有多种，在技术派实战项目中，我会在项目启动时将热门文章提前加载到 Redis 中，在每天凌晨定时将最新的站点地图更新到 Redis中，以确保用户在第一次访问时就能获取到缓存数据，从而减轻数据库的压力。 /** * 采用定时器方案，每天5:15分刷新站点地图，确保数据的一致性 */@Scheduled(cron = 0 15 5 * * ?)public void autoRefreshCache() log.info(开始刷新sitemap.xml的url地址，避免出现数据不一致问题!); refreshSitemap(); log.info(刷新完成！);@Overridepublic void refreshSitemap() initSiteMap();private synchronized void initSiteMap() long lastId = 0L; RedisClient.del(SITE_MAP_CACHE_KEY); while (true) ListSimpleArticleDTO list = articleDao.getBaseMapper().listArticlesOrderById(lastId, SCAN_SIZE); // 刷新站点地图信息，放到 Redis 当中 MapString, Long map = list.stream().collect(Collectors.toMap(s - String.valueOf(s.getId()), s - s.getCreateTime().getTime(), (a, b) - a)); RedisClient.hMSet(SITE_MAP_CACHE_KEY, map); if (list.size() SCAN_SIZE) break; lastId = list.get(list.size() - 1).getId(); Java 面试指南（付费）收录的字节跳动面经同学 1 技术二面面试原题：什么是缓存预热？如何解决？ 37.无底洞问题听说过吗？如何解决？无底洞问题的核心在于，随着缓存节点数量的增加，虽然总的存储容量和理论吞吐量都在增长，但是单个请求的响应时间反而变长了。 这个问题的根本原因是网络通信开销的增加。当节点数量从几十个增长到几千个时，客户端需要与更多的节点进行通信。 其次就是数据分布的碎片化。随着节点增多，数据分散得更加细碎，原本可以在一个节点获取的相关数据，现在可能分散在多个节点上。 针对这个问题，可以采取以下几种解决方案： 第一，可以将同一节点的多个请求合并成一个批量请求，减少网络往返次数。 public MapString, Object batchGet(ListString keys) // 按节点分组keys MapString, ListString nodeKeysMap = groupKeysByNode(keys); MapString, Object results = new ConcurrentHashMap(); // 并发访问各个节点 ListCompletableFutureVoid futures = nodeKeysMap.entrySet().stream() .map(entry - CompletableFuture.runAsync(() - String node = entry.getKey(); ListString nodeKeys = entry.getValue(); // 批量获取该节点的数据 MapString, Object nodeResults = getFromNode(node, nodeKeys); results.putAll(nodeResults); )) .collect(Collectors.toList()); // 等待所有请求完成 CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join(); return results; 第二，可以使用一致性哈希算法来优化数据分布，减少数据迁移和重分布的开销。 public class LocalityAwareSharding public String getNodeForKey(String key, String category) // 相同类别的数据尽量分配到相同节点 String shardKey = category + : + (key.hashCode() % SHARDS_PER_CATEGORY); return consistentHash.getNode(shardKey); // 用户相关数据尽量在同一个节点 public String getUserDataNode(String userId) return user_cluster_ + (userId.hashCode() % USER_CLUSTERS); 嗨嗨嗨，时隔两年，面渣逆袭第二版 PDF 终于可以下载了。我们做了大量的优化： 对于高频题：会标注在《Java 面试指南（付费）中出现的位置，哪家公司，原题是什么；如果你想节省时间的话，可以优先背诵这些题目，尽快做到知彼知己，百战不殆。 结合项目：包括技术派、mydb、pmhub来组织语言，让面试官最大程度感受到你的诚意，而不是机械化的背诵。 修复问题：第一版中出现的问题，包括球友们的私信反馈，网站留言区的评论，以及 GitHub 仓库中的 issue，让这份面试指南更加完善。 优化排版：增加手绘图，重新组织答案，使其更加口语化，从而更贴近面试官的预期。 你可以扫下面的二维码（或者长按自动识别）关注【沉默王二】公众号，发送关键字 222 来获取 PDF 版本，如果面渣逆袭真的对你有帮助，希望能给二哥的公众号加一个星标，满足我那一丁点虚荣心，这将是我更新下去的最强动力。 面渣逆袭的整理工作真的太不容易了，花了我好多好多的时间和精力，内容完全免费，但质量却有口皆碑，就是为了做一点真正有意义的、纯粹的事情。 Redis 运维38.Redis 报内存不足怎么处理？Redis 报内存不足时，通常是因为 Redis 占用的物理内存已经接近或者超过了配置的最大内存限制。这时可以采取以下几种步骤来处理： 第一，使用 INFO memory 命令查看 Redis 的内存使用情况，看看是否真的达到了最大内存限制。 redis-cli INFO memory 二哥的 Java 进阶之路：INFO memory 第二，如果服务器还有可用内存的话，修改 redis.conf 中的 maxmemory 参数，增加 Redis 的最大内存限制。比如将最大内存设置为 8GB： maxmemory 8gb 第三，修改 maxmemory-policy 参数来调整内存淘汰策略。比如可以选择 allkeys-lru 策略，让 Redis 自动删除最近最少使用的键。 maxmemory-policy allkeys-lru memo：2025 年 5 月 25 日修改至此，今天在修改球友简历时，碰到一个西安交通大学本、上海交通大学硕的球友，985 本硕学历真的非常顶了，我会竭尽所能去帮助他，在秋招中斩获一个 SSP offer，冲！ 西安交通大学本、上海交通大学硕 39.Redis key过期策略有哪些？Redis 主要采用了两种过期删除策略来保证过期的 key 能够被及时删除，包括惰性删除和定期删除。 二哥的 Java 进阶之路：Redis 的过期淘汰策略 惰性删除是最基本的策略，当客户端访问一个 key 时，Redis 会检查该 key 是否已过期，如果过期就会立即删除并返回 nil。 // 模拟惰性删除的逻辑public Object get(String key) RedisKey redisKey = getKeyFromMemory(key); if (redisKey != null isExpired(redisKey)) // key已过期，删除并返回null deleteKey(key); return null; return redisKey != null ? redisKey.getValue() : null; 这种策略的优点是不会有额外的 CPU 开销，只在访问 key 时才检查。但问题是如果一个过期的 key 永远不被访问，它就会一直占用内存。 java技术小馆：key 过期策略 于是就有了定期删除策略，Redis 会定期随机选择一些设置了过期时间的 key 进行检查，删除其中已过期的 key。这个过程默认每秒执行 10 次，每次随机选择 20 个 key 进行检查。 -—这部分面试中可以不背 start—- 可以通过 config get hz 命令查看 Redis 内部定时任务的频率。 二哥的 Java 进阶之路：config get hz hz 的值为“10”意味着 Redis 每秒执行 10 次定时任务 。可以通过 CONFIG SET hz 20 进行调整。 二哥本地 Redis 的配置文件路径和 hz 的默认值 -—这部分面试中可以不背 end—- Java 面试指南（付费）收录的腾讯面经同学 22 暑期实习一面面试原题：Redis key 删除策略 Java 面试指南（付费）收录的去哪儿面经同学 1 技术 2 面面试原题：redis 内存淘汰和过期策略 Java 面试指南（付费）收录的京东面经同学 5 Java 后端技术一面面试原题：redis key过期策略 40.🌟Redis有哪些内存淘汰策略？当内存使用接近 maxmemory 限制时，Redis 会依据内存淘汰策略来决定删除哪些 key 以缓解内存压力。 码哥字节：内存淘汰策略 常用的内存淘汰策略有八种，分别是默认的 noeviction，内存不足时不会删除任何 key，直接返回错误信息，生产环境下基本上不会使用。 然后是针对所有 key 的 allkeys-lru、allkeys-lfu 和 allkeys-random。lru 会删除最近最少使用的 key，在纯缓存场景中最常用，能自动保留热点数据；lfu 会删除访问频率最低的 key，更适合长期运行的系统；random 会随机删除一些 key，一般不推荐使用。 其次是针对设置了过期时间的 key，有 volatile-lru、volatile-lfu、volatile-ttl 和 volatile-random。 lru 在混合存储场景中经常使用。 @Servicepublic class HybridStorageService // 重要数据不设置过期时间，临时数据设置过期时间 public void storeData(String key, Object data, DataImportance importance) if (importance == DataImportance.HIGH) // 重要数据不设置过期时间，在volatile-*策略下不会被淘汰 redisTemplate.opsForValue().set(key, data); else // 临时数据设置过期时间，可以被volatile-*策略淘汰 redisTemplate.opsForValue().set(key, data, Duration.ofHours(1)); lfu 适合需要保护某些重要数据不被淘汰的场景；ttl 优先删除即将过期的 key，在用户会话管理系统中推荐使用；random 仍然很少用。 Java 面试指南（付费）收录的小米春招同学 K 一面面试原题：为什么 redis 快，淘汰策略 持久化 Java 面试指南（付费）收录的去哪儿面经同学 1 技术 2 面面试原题：redis 内存淘汰和过期策略 Java 面试指南（付费）收录的作业帮面经同学 1 Java 后端一面面试原题：redis内存淘汰策略 41.LRU 和 LFU 的区别是什么？LRU 是 Least Recently Used 的缩写，基于时间维度，淘汰最近最少访问的键。 LFU 是 Least Frequently Used 的缩写，基于次数维度，淘汰访问频率最低的键。 假设缓存中有三个数据 A、B、C，在 LRU 场景下，如果访问顺序是 A→B→C→A，那么此时的 LRU 顺序是B→C→A，如果需要淘汰，会先删除 B。 但在 LFU 场景下，如果 A 被访问了 5 次，B 被访问了 2 次，C 被访问了 1 次，那么无论最近的访问顺序如何，都会优先淘汰 C，因为它的访问频率最低。 LRU 更适合有明显时间局部性的场景，比如在新闻网站中，用户更关心最新的新闻，而昨天的新闻访问量会急剧下降。这种情况下，LRU 能很好地保留用户当前关心的热点内容。 LFU 则更适合有长期访问模式的场景，更强调“热度”，比如在电商平台中，某些商品可能长期保持热销状态，即使它们的访问时间间隔较长，但由于访问频率高，LFU 会优先保留这些商品的信息。 Java 面试指南（付费）收录的阿里系面经同学 19 饿了么面试原题：redis内存淘汰机制 延伸到LRU LFU memo：2025 年 5 月 27 日，今天球友发私信说，拿到了哈啰和得物的实习 offer，恭喜他！🎉 还特意感谢了一下之前对他简历的修改和学习上的建议。 球友拿到了得物和哈啰的 offer 42.Redis发生阻塞了怎么解决？Redis 发生阻塞在生产环境中是比较严重的问题，当发现 Redis 变慢时，我会先通过 monitor 命令查看当前正在执行的命令，或者使用 slowlog 命令查看慢查询日志。 # 查看当前正在执行的命令redis-cli MONITOR# 查看慢查询日志redis-cli SLOWLOG GET 10# 检查客户端连接状况redis-cli CLIENT LIST 通常情况下，大Key 是导致 Redis 阻塞的主要原因之一。比如说直接 DEL 一个包含几百万个元素的 Set，就会导致 Redis 阻塞几秒钟甚至更久。 这时候可以用 UNLINK 命令替代 DEL 来异步删除，避免阻塞主线程。 # 使用 UNLINK 异步删除大 Keyredis-cli UNLINK big_key 对于非常大的集合，可以使用 SCAN 命令分批删除。 public void safeBatchProcess(String key) ScanOptions options = ScanOptions.scanOptions().count(1000).build(); CursorString cursor = redisTemplate.opsForSet().scan(key, options); while (cursor.hasNext()) String member = cursor.next(); // 分批处理，避免阻塞 processElement(member); 另外，当 Redis 使用的内存超过物理内存时，操作系统会将部分内存交换到磁盘，这时候会导致 Redis 响应变慢。我的处理方式是： 使用 free -h 检查内存的使用情况 ；确认 Redis 的 maxmemory 设置是否合理；如果发生了内存交换，立即调整 maxmemory 并清理一些不重要的数据。 大量的客户端连接也可能会导致阻塞，这时候最好检查一下连接池的配置。 @Configurationpublic class RedisConnectionConfig @Bean public JedisConnectionFactory jedisConnectionFactory() JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(200); // 最大连接数 poolConfig.setMaxIdle(50); // 最大空闲连接 poolConfig.setMinIdle(10); // 最小空闲连接 poolConfig.setMaxWaitMillis(3000); // 获取连接最大等待时间 poolConfig.setTestOnBorrow(true); // 获取连接时检测有效性 return new JedisConnectionFactory(poolConfig); 嗨嗨嗨，时隔两年，面渣逆袭第二版 PDF 终于可以下载了。我们做了大量的优化： 对于高频题：会标注在《Java 面试指南（付费）中出现的位置，哪家公司，原题是什么；如果你想节省时间的话，可以优先背诵这些题目，尽快做到知彼知己，百战不殆。 结合项目：包括技术派、mydb、pmhub来组织语言，让面试官最大程度感受到你的诚意，而不是机械化的背诵。 修复问题：第一版中出现的问题，包括球友们的私信反馈，网站留言区的评论，以及 GitHub 仓库中的 issue，让这份面试指南更加完善。 优化排版：增加手绘图，重新组织答案，使其更加口语化，从而更贴近面试官的预期。 你可以扫下面的二维码（或者长按自动识别）关注【沉默王二】公众号，发送关键字 222 来获取 PDF 版本，如果面渣逆袭真的对你有帮助，希望能给二哥的公众号加一个星标，满足我那一丁点虚荣心，这将是我更新下去的最强动力。 面渣逆袭的整理工作真的太不容易了，花了我好多好多的时间和精力，内容完全免费，但质量却有口皆碑，就是为了做一点真正有意义的、纯粹的事情。 Redis 应用43.Redis如何实现异步消息队列？Redis 实现异步消息队列是一个很实用的技术方案，最简单的方式是使用 List 配合 LPUSH 和 RPOP 命令。 三分恶面渣逆袭：list作为队列 @Servicepublic class SimpleRedisQueue private final RedisTemplateString, Object redisTemplate; // 生产者：向队列发送消息 public void sendMessage(String queueName, Object message) redisTemplate.opsForList().leftPush(queueName, message); // 消费者：从队列获取消息 public Object receiveMessage(String queueName) return redisTemplate.opsForList().rightPop(queueName); // 阻塞式消费，避免轮询 public Object blockingReceive(String queueName, int timeoutSeconds) ListObject result = redisTemplate.opsForList() .rightPop(queueName, timeoutSeconds, TimeUnit.SECONDS); return result != null !result.isEmpty() ? result.get(0) : null; 另外就是用 Redis 的 PubSub 来实现简单的消息广播和订阅。 @Servicepublic class RedisPubSubService private final RedisTemplateString, Object redisTemplate; // 发布消息到指定频道 public void publish(String channel, Object message) redisTemplate.convertAndSend(channel, message); // 订阅频道 @PostConstruct public void subscribe() redisTemplate.setMessageListener((message, pattern) - System.out.println(Received message: + message); ); redisTemplate.getConnectionFactory().getConnection().subscribe( new ChannelTopic(myChannel).getTopic().getBytes() ); 发布者将消息发布到指定的频道，订阅该频道的客户端就能收到消息。 三分恶面渣逆袭：pubsub 但是这两种方式都是不可靠的，因为没有 ACK 机制所以不能保证订阅者一定能收到消息，也不支持消息持久化。 44.Redis如何实现延时消息队列?延时消息队列在实际业务中很常见，比如订单超时取消、定时提醒等场景。Redis 虽然不是专业的消息队列，但可以很好地实现延时队列功能。 核心思路是利用 ZSet 的有序特性，将消息作为 member，把消息的执行时间作为 score。这样消息就会按照执行时间自动排序，我们只需要定期扫描当前时间之前的消息进行处理就可以了。 三分恶面渣逆袭：zset实现延时队列 @Servicepublic class DelayedMessageQueue private final RedisTemplateString, Object redisTemplate; // 发送延时消息 public void sendDelayedMessage(String queueName, Object message, long delaySeconds) // 计算消息的执行时间 long executeTime = System.currentTimeMillis() + (delaySeconds * 1000); // 将消息加入ZSet，以执行时间作为score redisTemplate.opsForZSet().add(queueName, message, executeTime); log.info(发送延时消息: , 延时: 秒, message, delaySeconds); // 消费延时消息 @Scheduled(fixedDelay = 1000) // 每秒扫描一次 public void consumeDelayedMessages() String queueName = delayed:queue; long currentTime = System.currentTimeMillis(); // 获取已到期的消息（score = 当前时间） SetObject messages = redisTemplate.opsForZSet() .rangeByScore(queueName, 0, currentTime); for (Object message : messages) try // 处理消息 processMessage(message); // 处理成功后从队列中移除 redisTemplate.opsForZSet().remove(queueName, message); log.info(处理延时消息成功: , message); catch (Exception e) log.error(处理延时消息失败: , message, e); // 可以实现重试机制 handleFailedMessage(queueName, message); 具体实现上，我会在生产者发送延时消息时，计算消息应该执行的时间戳，然后用 ZADD 命令将消息添加到 ZSet 中。 ZADD delay_queue 1617024000 task1 消费者通过定时任务，使用 ZRANGEBYSCORE 命令获取当前时间之前的所有消息。 ZREMRANGEBYSCORE delay_queue -inf 1617024000 处理完成后再用 ZREM 删除消息。 ZREM delay_queue task1 在技术派实战项目中，我就用这种方式实现了文章定时发布的功能。作者在发布文章时，可以选择一个未来的时间节点，比如说 30 分钟后，系统就会向延时队列发送一条延时消息，然后定时任务就会在 30 分钟后将这条消息从延时队列中取出并发布文章。 Java 面试指南（付费）收录的腾讯面经同学 23 QQ 后台技术一面面试原题：Redis 实现延迟队列 Java 面试指南（付费）收录的字节跳动面经同学 8 Java 后端实习一面面试原题：redis 数据结构，用什么结构实现延迟消息队列 memo：2025 年 5 月 28 日修改至此，今天有球友在 VIP群里发消息说拿到了荣耀的暑期实习 offer，虽然时间节点已经不早了，但越是到这个时候，确实容易捡漏。 球友的荣耀实习 OC了 45.🌟Redis支持事务吗？是的，Redis 支持简单的事务，可以将 multi、exec、discard 和 watch 命令打包，然后一次性的按顺序执行。 Redis设计与实现：事务 基本流程是用 multi 开启事务，然后执行一系列命令，最后用 exec 提交。这些命令会被放入队列，在 exec 时批量执行。 二哥的 Java 进阶之路：Redis 事务 当客户端处于非事务状态时，所有发送给 Redis 服务的命令都会立即执行；但当客户端进入事务状态之后，这些命令会被放入一个事务队列中，然后立即返回 QUEUED，表示命令已入队。 Redis设计与实现：事务和非事务的区别 当 exec 命令执行时，Redis 会将事务队列中的所有命令按先进先出的顺序执行。当事务队列里的命令全部执行完毕后，Redis 会返回一个数组，包含每个命令的执行结果。 discard 命令用于取消一个事务，它会清空事务队列并退出事务状态。 二哥的 Java 进阶之路：discard watch 命令用于监视一个或者多个 key，如果这个 key 在事务执行之前 被其他命令改动，那么事务将会被打断。 码哥字节：watch 但 Redis 的事务与 MySQL 的有很大不同，它并不支持回滚，也不支持隔离级别。 说一下 Redis 事务的原理？Redis 事务的原理并不复杂，核心就是一个”先排队，后执行”的机制。 小生凡一：Redis事务 当执行 MULTI 命令时，Redis 会给这个客户端打一个事务的标记，表示这个客户端后面发送的命令不会被立即执行，而是被放到一个队列里排队等着。 小生凡一：MULTI 当 Redis 收到 EXEC 命令时，它会把队列里的命令一个个拿出来执行。因为 Redis 是单线程的，所以这个过程不会被其他命令打断，这就保证了Redis 事务的原子性。 小生凡一：WATCH 当执行 WATCH 命令时，Redis 会将 key 添加到全局监视字典中；只要这些 key 在 EXEC 前被其他客户端修改，Redis 就会给相关客户端打上脏标记，EXEC 时发现事务已被干扰就会直接取消整个事务。 // 全局监视字典dict *watched_keys;typedef struct watchedKey robj *key; redisDb *db; watchedKey; DISCARD 做的事情很简单直接，首先检查客户端是否真的在事务状态，如果不在就报错；如果在事务状态，就清空事务队列并退出事务状态。 void discardCommand(client *c) if (!(c-flags CLIENT_MULTI)) addReplyError(c,DISCARD without MULTI); return; discardTransaction(c); addReply(c,shared.ok); Redis 事务有哪些注意点？最重要的的一点是，Redis 事务不支持回滚，一旦 EXEC 命令被调用，所有命令都会被执行，即使有些命令可能执行失败。 Redis事务为什么不支持回滚？Redis 的核心设计理念是简单、高效，而不是完整的 ACID 特性。而实现回滚需要在执行过程中保存大量的状态信息，并在发生错误时逆向执行命令以恢复原始状态。这会增加 Redis 的复杂性和性能开销。 redis.io：不支持事务回滚 Redis事务满足原子性吗？要怎么改进？Redis 的事务不能满足标准的原子性，因为它不支持事务回滚，也就是说，假如某个命令执行失败，整个事务并不会自动回滚到初始状态。 // 一个转账事务redisTemplate.multi();redisTemplate.opsForValue().decrement(user:1:balance, 100); // 成功redisTemplate.opsForList().leftPush(user:1:balance, log); // 类型错误，失败redisTemplate.opsForValue().increment(user:2:balance, 100); // 还是会执行ListObject results = redisTemplate.exec();// 结果：用户1被扣了钱，用户2也收到了钱，但中间的日志操作失败了// 这符合Redis的原子性定义，但不符合业务期望 可以使用 Lua 脚本来替代事务，脚本运行期间，Redis 不会处理其他命令，并且我们可以在脚本中处理整个业务逻辑，包括条件检查和错误处理，保证要么执行成功，要么保持最初的状态，不会出现一个命令执行失败、其他命令执行成功的情况。 @Servicepublic class ImprovedTransactionService public boolean atomicTransfer(String fromUser, String toUser, int amount) String luaScript = local from_key = KEYS[1] + local to_key = KEYS[2] + local amount = tonumber(ARGV[1]) + // 检查转出账户余额 local from_balance = redis.call(GET, from_key) + if not from_balance then return -1 end + from_balance = tonumber(from_balance) + if from_balance amount then return -2 end + // 检查转入账户是否存在 if redis.call(EXISTS, to_key) == 0 then return -3 end + // 所有检查通过，执行转账 redis.call(DECRBY, from_key, amount) + redis.call(INCRBY, to_key, amount) + // 记录转账日志 local log = from_key .. : .. to_key .. : .. amount + redis.call(LPUSH, transfer:log, log) + return 1; DefaultRedisScriptLong script = new DefaultRedisScript(); script.setScriptText(luaScript); script.setResultType(Long.class); Long result = redisTemplate.execute(script, Arrays.asList(user: + fromUser + :balance, user: + toUser + :balance), amount); return result != null result == 1; Redis 事务的 ACID 特性如何体现？单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务在执行过程中如果某个命令失败了，其他命令还是会继续执行，不会回滚。 小生凡一：Redis 事务的原子性 一致性指的是，如果数据在执行事务之前是一致的，那么在事务执行之后，无论事务是否执行成功，数据也应该是一致的。但 Redis 事务并不保证一致性，因为如果事务中的某个命令失败了，其他命令仍然会执行，就会出现数据不一致的情况。 Redis 是单线程执行事务的，并且不会中断，直到执行完所有事务队列中的命令为止。因此，我认为 Redis 的事务具有隔离性的特征。 小生凡一：Redis 事务的隔离性 Redis 事务的持久性完全依赖于 Redis 本身的持久化机制，如果开启了 AOF，那么事务中的命令会作为一个整体记录到 AOF 文件中，当然也要看 AOF 的 fsync 策略。 如果只开启了 RDB，事务中的命令可能会在下次快照前丢失。如果两个都没有开启，肯定是不满足持久性的。 Java 面试指南（付费）收录的华为一面原题：说下 Redis 事务 二哥编程星球球友枕云眠美团 AI 面试原题：什么是 redis 的事务，它的 ACID 属性如何体现 Java 面试指南（付费）收录的快手同学 4 一面原题：Redis事务满足原子性吗？要怎么改进？ memo：2025 年 5 月 29 日，今天给球友修改简历时，碰到一个东南大学本硕博 3 985 的球友，这也是我已知信息中学历最高的球友了。 星球来了一个东南大学本硕博的球友 46.有Lua脚本操作Redis的经验吗？Lua 脚本是处理 Redis 复杂操作的首选方案，比如说原子扣减库存、分布式锁、限流等业务场景，都可以通过 Lua 脚本来实现。 scalegrid.io：lua 脚本 在秒杀场景下，可以用 Lua 脚本把所有检查逻辑都写在一起：先看库存够不够，再看用户有没有买过，所有条件都满足才扣减库存。因为整个脚本是原子执行的，Redis 在执行期间不会处理其他命令，所以可以彻底解决超卖问题。 // 这个秒杀脚本救了我的命String luaScript = local stock = redis.call(GET, KEYS[1]) + if not stock or tonumber(stock) tonumber(ARGV[2]) then + return -1 + // 库存不足 end + if redis.call(SISMEMBER, KEYS[2], ARGV[1]) == 1 then + return -2 + // 重复购买 end + redis.call(DECRBY, KEYS[1], ARGV[2]) + redis.call(SADD, KEYS[2], ARGV[1]) + return 1; 在分布式锁场景下，我一开始用的 SETNX 命令来实现，结果发现如果程序异常退出，锁就死掉了。后来加了过期时间，但又发现可能误删其他线程的锁。最后还是用 Lua 脚本彻底解决了这个问题，确保只有锁的持有者才能释放锁。 // 解锁脚本特别重要，必须验证是自己的锁才能删private final String UNLOCK_SCRIPT = if redis.call(GET, KEYS[1]) == ARGV[1] then + return redis.call(DEL, KEYS[1]) + else + return 0 + end; 甚至还可以用 Lua脚本实现滑动窗口限流器，一次性完成过期数据清理、计数检查、新记录添加三个操作，而且完全原子化。 // 滑动窗口限流，逻辑清晰，性能还好String luaScript = local key = KEYS[1] + local now = tonumber(ARGV[1]) + local window = tonumber(ARGV[2]) + local limit = tonumber(ARGV[3]) + // 先清理过期记录 redis.call(ZREMRANGEBYSCORE, key, 0, now - window) + // 检查当前请求数 local current = redis.call(ZCARD, key) + if current limit then + redis.call(ZADD, key, now, now) + return 1 + else + return 0 + end; memo：2025 年 5 月 30 日，今天有球友在星球里发消息说拿到了金山办公的 offer，问我该选 cpp 还是go，我的建议可以看看符合是否合理，不管如何选择，真的恭喜球友！ 球友拿到了金山办公的软件 47.Redis的管道Pipeline了解吗？了解，Pipeline 允许客户端一次性向 Redis 服务器发送多个命令，而不必等待一个命令响应后才能发送下一个。Redis 服务器会按照命令的顺序依次执行，并将所有结果打包返回给客户端。 三分恶面渣逆袭：Pipelining示意图 正常情况下，每执行一个 Redis 命令都需要一次网络往返：发送命令 - 等待响应 - 发送下一个命令。 客户端 Redis服务器 | | |------- SET key1 val1 ----| |------ OK ---------------| |------- SET key2 val2 ----| |------ OK ---------------| |------- GET key1 --------| |------ val1 -------------| 如果大量请求依次发送，网络延迟会显著增加请求的总执行时间，假如一次 RTT 的时间是 1 毫秒，3 个就是 3 毫秒。有了 Pipeline 后，可以一次性发送 3 个命令，总时间就只需要 1 毫秒。 @Servicepublic class RedisBatchService public void batchInsertUsers(ListUser users) // 不用Pipeline的错误做法 - 很慢 // for (User user : users) // redisTemplate.opsForValue().set(user: + user.getId(), user); // // 使用Pipeline的正确做法 redisTemplate.executePipelined(new RedisCallbackObject() @Override public Object doInRedis(RedisConnection connection) throws DataAccessException for (User user : users) String key = user: + user.getId(); byte[] keyBytes = key.getBytes(); byte[] valueBytes = serialize(user); connection.set(keyBytes, valueBytes); return null; // Pipeline不需要返回值 ); 当然了，Pipeline 不是越大越好，太大会占用过多内存，通常建议每个 Pipeline 包含 1000 到 5000 个命令。可以根据实际情况调整。 public void smartBatchInsert(ListString data) int batchSize = 1000; // 经验值，根据数据大小调整 for (int i = 0; i data.size(); i += batchSize) ListString batch = data.subList(i, Math.min(i + batchSize, data.size())); redisTemplate.executePipelined(new RedisCallbackObject() @Override public Object doInRedis(RedisConnection connection) throws DataAccessException for (String item : batch) connection.set(item.getBytes(), item.getBytes()); return null; ); 什么场景下适合使用 Pipeline呢？需要批量插入、更新或删除数据，或者需要执行大量相似的命令时。比如：系统启动时的缓存预热 - 批量加载热点数据；比如统计数据的批量更新；比如大批量数据的导入导出；比如批量删除过期或无效的缓存。 有了解过 Pipeline 的底层原理吗？有，其实就是缓冲的思想。在技术派实战项目中，我就在 RedisClient 类中封装了一个 PipelineAction 内部类，用来缓存命令。 技术派实战源码：PipelineAction add 方法将命令包装成 Runnable 对象，放入 List 中。当执行 execute 方法时，再调用 RedisTemplate 的 executePipelined 方法开启管道模式将多个命令发送到 Redis 服务端。 二哥的 Java 进阶之路：RedisTemplate的executePipelined Redis 服务端从输入缓冲区读到命令后，会按照 RESP 协议进行命令拆解，再依次执行这些命令。执行结果会写入到输出缓冲区，最后再将所有结果一次性返回给客户端。 typedef struct client sds querybuf; // 输入缓冲区 list *reply; // 输出缓冲区链表 unsigned long reply_bytes; // 输出缓冲区大小 client; Java 面试指南（付费）收录的京东面经同学 8 面试原题：对pipeline的理解，什么场景适合使用pipeline？有了解过pipeline的底层？ memo：2025 年 6 月 1 日，今天有球友在星球里发消息说拿到了百得思维的offer，他是民办二本，对这个结果很满意，也很感谢面渣逆袭和星球的实战项目，让他摆脱了浑浑噩噩的日子。恭喜他！ 球友拿到了一家小厂的 offer 48.🌟Redis能实现分布式锁吗？分布式锁是一种用于控制多个不同进程在分布式系统中访问共享资源的锁机制。它能确保在同一时刻，只有一个节点可以对资源进行访问，从而避免分布式场景下的并发问题。 可以使用 Redis 的 SETNX 命令实现简单的分布式锁。比如 SET key value NX PX 3000 就创建了一个锁名为 key 的分布式锁，锁的持有者为 value。NX 保证只有在 key 不存在时才能创建成功，EX 设置过期时间用以防止死锁。 三分恶面渣逆袭：set原子命令 Redis如何保证 SETNX 不会发生冲突？当我们使用 SET key value NX EX 30 这个命令进行加锁时，Redis 会把整个操作当作一个原子指令来执行。因为 Redis 的命令处理是单线程的，所以在同一时刻只能有一个命令在执行。 比如说两个客户端 A 和 B 同时请求同一个锁： 客户端A: SET lock_key uuid_a NX EX 30客户端B: SET lock_key uuid_b NX EX 30 虽然这两个请求可能几乎同时到达 Redis 服务器，但 Redis 会严格按照到达的先后顺序来处理。假设 A 的请求先到，Redis 会先执行 A 的 SET 命令，这时 lock_key 被设置为 uuid_a。 当处理 B 的请求时，因为 lock_key 已经存在了，NX 条件不满足，所以 B 的 SET 命令会失败，返回 NULL。这样就保证了只有 A 能获取到锁。 关键点在于 NX 的语义：NOT EXISTS，只有在 key 不存在的时候才会设置成功。Redis 在执行这个命令时，会先检查 key 是否存在，如果不存在才会设置值，这整个过程是原子的，不会被其他命令打断。 SETNX有什么问题，如何解决？使用 SETNX 创建分布式锁时，虽然可以通过设置过期时间来避免死锁，但会误删锁。比如线程 A 获取锁后，业务执行时间比较长，锁过期了。这时线程 B 获取到锁，但线程 A 执行完业务逻辑后，会尝试删除锁，这时候删掉的其实是线程 B 的锁。 技术派：Redis 锁 可以通过锁的自动续期机制来解决锁过期的问题，比如 Redisson 的看门狗机制，在后台启动一个定时任务，每隔一段时间就检查锁是否还被当前线程持有，如果是就自动延长过期时间。这样既避免了死锁，又防止了锁被提前释放。 技术派：redisson 看门狗 memo：2025 年 6 月 2 日修改至此，今天在帮一个学院本球友分析 offer 选择后，他又回复说多亏了星球才能一路走到现在，很满足这个结果。看多了拿大厂 offer 球友的感谢，看到学院本也能取得满意的成绩，我也很开心。 学院本拿到offer 后对星球的认可 Redisson了解多少？Redisson 是一个基于 Redis 的 Java 客户端，它不只是对 Redis 的操作进行简单地封装，还提供了很多分布式的数据结构和服务，比如最常用的分布式锁。 RLock lock = redisson.getLock(lock);lock.lock();try // do something finally lock.unlock(); Redisson 的分布式锁比 SETNX 完善的得多，它的看门狗机制可以让我们在获取锁的时候省去手动设置过期时间的步骤，它在内部封装了一个定时任务，每隔 10 秒会检查一次，如果当前线程还持有锁就自动续期 30 秒。 private Long tryAcquire(long waitTime, long leaseTime, TimeUnit unit, long threadId) return get(tryAcquireAsync(waitTime, leaseTime, unit, threadId));private T RFutureLong tryAcquireAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId) RFutureLong ttlRemainingFuture; if (leaseTime != -1) // 手动设置过期时间 ttlRemainingFuture = tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); else // 启用看门狗机制，使用默认的30秒过期时间 ttlRemainingFuture = tryLockInnerAsync(waitTime, internalLockLeaseTime, TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); // 处理获取锁成功的情况 ttlRemainingFuture.onComplete((ttlRemaining, e) - if (e != null) return; // 如果获取锁成功且启用看门狗机制 if (ttlRemaining == null) if (leaseTime != -1) internalLockLeaseTime = unit.toMillis(leaseTime); else scheduleExpirationRenewal(threadId); // 启动看门狗 ); return ttlRemainingFuture; 另外，Redisson 还提供了分布式限流器 RRateLimiter，基于令牌桶算法实现，用于控制分布式环境下的访问频率。 // API 接口限流@RestControllerpublic class ApiController @Autowired private RedissonClient redissonClient; @GetMapping(/api/data) public ResponseEntity? getData() RRateLimiter limiter = redissonClient.getRateLimiter(api.data); limiter.trySetRate(RateType.OVERALL, 100, 1, RateIntervalUnit.MINUTES); if (limiter.tryAcquire()) // 处理请求 return ResponseEntity.ok(processData()); else // 限流触发 return ResponseEntity.status(429).body(Rate limit exceeded); 详细说说Redisson的看门狗机制？Redisson 的看门狗机制是一种自动续期机制，用于解决分布式锁的过期问题。 基本原理是这样的：当调用 lock() 方法加锁时，如果没有显式设置过期时间，Redisson 会默认给锁加一个 30 秒的过期时间，同时启用一个名为“看门狗”的定时任务，每隔 10 秒（默认是过期时间的 13），去检查一次锁是否还被当前线程持有，如果是，就自动续期，将过期时间延长到 30 秒。 郭慕荣博客园：看门狗 // 伪代码展示核心逻辑private void renewExpiration() Timeout task = commandExecutor.getConnectionManager() .newTimeout(new TimerTask() @Override public void run(Timeout timeout) // 用 Lua 脚本检查并续期 if (redis.call(get, lockKey) == currentThreadId) redis.call(expire, lockKey, 30); // 递归调用，继续下一次续期 renewExpiration(); , 10, TimeUnit.SECONDS); 续期的 Lua 脚本会检查锁的 value 是否匹配当前线程，如果匹配就延长过期时间。这样就能保证只有锁的真正持有者才能续期。 当调用 unlock() 方法时，看门狗任务会被取消。或者如果业务逻辑执行完但忘记 unlock 了，看门狗也会帮我们自动检查锁，如果锁已经不属于当前线程了，也会自动停止续期。 这样我们就不用担心业务执行时间过长导致锁被提前释放，也避免了手动估算过期时间的麻烦，同时也解决了分布式环境下的死锁问题。 看门狗机制中的检查锁过程是原子操作吗？是的，Redisson 使用了 Lua 脚本来保证锁检查的原子性。 二哥的 Java 进阶之路：看门狗 lua 脚本检查锁 Redis 在执行 Lua 脚本时，会把整个脚本当作一个命令来处理，期间不会执行其他命令。所以 hexists 检查和 expire 续期是原子执行的。 Redlock你了解多少？Redlock 是 Redis 作者 antirez 提出的一种分布式锁算法，用于解决单个 Redis 实例作为分布式锁时存在的单点故障问题。 Redlock 的核心思想是通过在多个完全独立的 Redis 实例上同时获取锁来实现容错。 二哥的 Java 进阶之路：RedissonRedLock minLocksAmount 方法返回的 locks.size()/2 + 1，正是 Redlock 算法要求的少数服从多数原则。failedLocksLimit 方法会计算允许失败的锁数量，确保即使部分实例失败，只要成功的实例数量超过一半就认为获取锁成功。 红锁会尝试依次向所有 Redis 实例获取锁，并记录成功获取的锁数量，当数量达到 minLocksAmount 时就认为获取成功，否则释放已获取的锁并返回失败。 虽然 Redlock 存在一些争议，比如说时钟漂移问题、网络分区导致的脑裂问题，但它仍然是一个相对成熟的分布式锁解决方案。 红锁能不能保证百分百上锁？不能，Redlock 无法保证百分百上锁成功，这是由分布式系统的本质特性决定的。 当有网络分区时，客户端可能无法与足够数量的 Redis 实例通信。比如在 5 个 Redis 实例的部署中，如果网络分区导致客户端只能访问到 2 个实例，那么无论如何都无法满足红锁要求的少数服从多数原则，获取锁的时候必然失败。 public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException // ... for (ListIteratorRLock iterator = locks.listIterator(); iterator.hasNext();) RLock lock = iterator.next(); boolean lockAcquired; try lockAcquired = lock.tryLock(awaitTime, newLeaseTime, TimeUnit.MILLISECONDS); catch (RedisResponseTimeoutException e) lockAcquired = false; // 网络超时导致失败 catch (Exception e) lockAcquired = false; // 其他异常导致失败 // 如果剩余可尝试的实例数量不足以达到多数派，直接退出 if (locks.size() - acquiredLocks.size() == failedLocksLimit()) break; // 检查是否达到多数派要求 if (acquiredLocks.size() = minLocksAmount(locks)) return true; else unlockInner(acquiredLocks); return false; // 未达到多数派，获取失败 时钟漂移也会影响成功率。即使所有实例都可达，如果各个 Redis 实例之间存在明显的时钟漂移，或者客户端在获取锁的过程中耗时过长，比如网络延迟、GC 停顿等，都可能会导致锁在获取完成前就过期，从而获取失败。 在实际应用中，可以通过重试机制来提高锁的成功率。 for (int i = 0; i maxRetries; i++) if (redLock.tryLock(waitTime, leaseTime, TimeUnit.MILLISECONDS)) return true; Thread.sleep(retryDelay);return false; 项目中有用到分布式锁吗？在PmHub项目中，我有使用 Redission 的分布式锁来确保流程状态的更新按顺序执行，且不被其他流程服务干扰。 PmHub：分布式锁保障流程状态更新 底层结构49.🌟Redis都有哪些底层数据结构？Redis 之所以快，除了基于内存读写之外，还有很重要的一点就是它精心设计的底层数据结构。Redis 总共有 8 种核心的底层数据结构，我按照重要程度来说一下。 三分恶面渣逆袭：Redis Object对应的映射 首先是 SDS，这是 Redis 自己实现的动态字符串，它保留了 C 语言原生的字符串长度，所以获取长度的时间复杂度是 O(1)，在此基础上还支持动态扩容，以及存储二进制数据。 三分恶面渣逆袭：SDS 然后是字典，更底层是用数组+链表实现的哈希表。它的设计很巧妙，用了两个哈希表，平时用第一个，rehash 的时候用第二个，这样可以渐进式地进行扩容，不会阻塞太久。 三分恶面渣逆袭：字典 接下来压缩列表 ziplist，这个设计很有意思。Redis 为了节省内存，设计了这种紧凑型的数据结构，把所有元素连续存储在一块内存里。但是它有个致命问题叫”连锁更新”，就是当我们修改一个元素的时候，可能会导致后面所有的元素都要重新编码，性能会急剧下降。 Shubhi Jain：Ziplist 为了解决压缩列表的问题，Redis 后来设计了 quicklist。这个设计思路很聪明，它把 ziplist 拆分成小块，然后用双向链表把这些小块串起来。这样既保持了 ziplist 节省内存的优势，又避免了连锁更新的问题，因为每个小块的 ziplist 都不会太大。 Mr.于博客园：quicklist 再后来，Redis 又设计了 listpack，这个可以说是 ziplist 的完美替代品。它最大的特点是每个元素只记录自己的长度，不记录前一个元素的长度，这样就彻底解决了连锁更新的问题。Redis 5.0 已经用 listpack 替换了 ziplist。 baseoncpp：listpack 跳表skiplist 主要用在 ZSet 中。它的设计很巧妙，通过多层指针来实现快速查找，平均时间复杂度是 O(log N)。相比红黑树，跳表的实现更简单，而且支持范围查询，这对 Redis 的有序集合来说很重要。 三分恶面渣逆袭：跳表 还有整数集合intset，当 Set 中都是整数且元素数量较少时使用，内部是一个有序数组，查找用的二分法。 zhangtielei.com：intset 最后是双向链表LinkedList，早期版本的 Redis 会在 List 中用到，但 Redis 3.2 后就被 quicklist 替代了，因为纯链表的问题是内存不连续，影响 CPU 缓存性能。 pdai：Redis 底层数据结构和数据类型关系 memo：2025 年 6 月 4 日，今天有球友发喜报说拿到了京东零售的实习 offer，并且部门和业务还是挺不错的，恭喜他！6 月份还有机会，冲。 球友拿到了京东的 offer 简单介绍下链表？Redis 的 linkedlist 是⼀个双向⽆环链表结构，和 Java 中的 LinkedList 类似。 节点由 listNode 表示，每个节点都有指向其前置节点和后置节点的指针，头节点的前置和尾节点的后置均指向 null。 三分恶面渣逆袭：链表linkedlist 关于整数集合，能再详细说说吗？整数集合是 Redis 中一个非常精巧的数据结构，当一个 Set 只包含整数元素，并且数量不多时，默认不超过 512 个，Redis 就会用 intset 来存储这些数据。 三分恶面渣逆袭：整数集合intset intset 最有意思的地方是类型升级机制。它有三种编码方式：16位、32位和 64位，会根据存储的整数大小动态调整。比如原来存的都是小整数，用 16 位编码就够了，但突然插入了一个很大的数，超出了 16 位的范围，这时整个数组会升级到 32 位编码。 typedef struct intset uint32_t encoding; // 编码方式：16位、32位或64位 uint32_t length; // 元素数量 int8_t contents[]; // 保存元素的数组 intset; 当然了，这种升级是有代价的，因为需要重新分配内存并复制数据，并且是不可逆的，但它的好处是可以节省内存空间，特别是在存储大量小整数时。 另外，所有元素在数组中按照从小到大的顺序排列，这样就可以使用二分查找来定位元素，时间复杂度为 O(log N)。 说一下zset 的底层原理？ZSet 是 Redis 最复杂的数据类型，它有两种底层实现方式：压缩列表和跳表。 0xcafebabe：zset 的底层实现 当保存的元素数量少于 128 个，且保存的所有元素大小都小于 64 字节时，Redis 会采用压缩列表的编码方式；否则就用跳表。 当然，这两个条件都可以通过参数进行调整。 选择压缩列表作为底层实现时，每个元素会使用两个紧挨在一起的节点来保存：第一个节点保存元素的成员，第二个节点保存元素的分值。 0xcafebabe：zset 使用压缩列表 所有元素按分值从小到大有序排列，小的放在靠近表头的位置，大的放在靠近表尾的位置。 但跳表的缺点是查找只能按顺序进行，时间复杂度为 O(N)，而且在最坏的情况下，插入和删除操作还可能会引起连锁更新。 当元素数量较多或元素较大时，Redis 会使用 skiplist 的编码方式；这个设计非常的巧妙，同时使用了两种数据结构： typedef struct zset zskiplist *zsl; // 跳跃表 dict *dict; // 字典 zset; 跳表按分数有序保存所有元素，且支持范围查询（如 ZRANGE、ZRANGEBYSCORE），平均时间复杂度为 O(log N)。而哈希表则用来存储成员和分值的映射关系，查找时间复杂度为 O(1)。 0xcafebabe：zset 使用跳表 虽然同时使用两种结构，但它们会通过指针来共享相同元素的成员和分值，因此不会浪费额外的内存。 你知道为什么Redis 7.0要用listpack来替代ziplist吗？答：主要是为了解决压缩列表的一个核心问题——连锁更新。在压缩列表中，每个节点都需要记录前一个节点的长度信息。 wenfh2020.com：redis ziplist 当插入或删除一个节点时，如果这个操作导致某个节点的长度发生了变化，那么后续的节点可能都需要更新它们存储的”前一个节点长度”字段。最坏的情况下，一次操作可能触发整个链表的更新，时间复杂度会从 O(1)退化到 O(n²)。 而 listpack 的设计理念完全不同。它让每个节点只记录自己的长度信息，不再依赖前一个节点的长度。这样就从根本上避免了连锁更新的问题。 极客时间：listpack listpack 中的节点不再保存其前一个节点的长度，而是保存当前节点的编码类型、数据和长度。 极客时间：listpack 的元素 连锁更新是怎么发生的？比如说我们有一个压缩列表，其中有几个节点的长度都是 253 个字节。在 ziplist 的编码中，如果前一个节点的长度小于 254 字节，我们只需要 1 个字节来存储这个长度信息。 Hello Jelly：连锁更新 但如果在这些节点前面插入一个长度为 254 字节的节点，那么原来只需要 1 个字节存储长度的节点现在需要 5 个字节来存储长度信息。这就会导致后续所有节点的长度信息都需要更新。 Java 面试指南（付费）收录的字节跳动商业化一面的原题：说说 Redis 的 zset，什么是跳表，插入一个节点要构建几层索引 Java 面试指南（付费）收录的字节跳动面经同学 9 飞书后端技术一面面试原题：Redis 的数据类型，ZSet 的实现 Java 面试指南（付费）收录的小米暑期实习同学 E 一面面试原题：你知道 Redis 的 zset 底层实现吗 Java 面试指南（付费）收录的腾讯面经同学 23 QQ 后台技术一面面试原题：zset 的底层原理 Java 面试指南（付费）收录的快手面经同学 7 Java 后端技术一面面试原题：说一下 ZSet 底层结构 Java 面试指南（付费）收录的美团同学 9 一面面试原题：redis的数据结构底层原理？ Java 面试指南（付费）收录的腾讯面经同学 27 云后台技术一面面试原题：Zset的底层实现？ Java 面试指南（付费）收录的得物面经同学 9 面试题目原题：Zset的底层如何实现？ memo：2025 年 6 月 5 日，今天有球友在VIP群里咨询 offer 的选择，一个拼多多，一个快手，真让人羡慕的要死啊，😄 拼多多和快手的 offer 选择 50.Redis 为什么不用 C 语言的原生字符串？第一，C 语言的字符串其实就是字符数组，以 \\0 结尾，这意味着如果数据本身包含 \\0 字节，就会被误认为字符串结束。但 Redis 需要存储各种类型的数据，包括图片、序列化对象等二进制数据，这些数据中很可能包含 \\0。 三分恶面渣逆袭：C语言的字符串 第二，如果需要获取字符串长度，C 语言只能调用 strlen() 函数，时间复杂度是 O(N)，因为要遍历整个字符串直到遇到 \\0。 第三，C 语言的字符串不会自动检查边界，如果往一个字符数组里写入超过其容量的数据，就会出现缓冲区溢出。 第四，C 语言的字符串不支持动态扩容，如果需要修改内容，就必须重新分配内存并复制数据，开销很大。 三分恶面渣逆袭：Redis sds Redis 设计的 SDS 完美解决了这些问题，获取长度可以直接通过 len 字段，时间复杂度为 O(1)；free 字段会记录剩余空间，因此 Redis 可以根据预分配策略动态扩容，不用在追加数据时重新分配内存；并且不依赖于 \\0 结尾，可以存储任意二进制数据。 struct sds int len; // 字符串长度 int free; // 剩余空间 char buf[]; // 字符数组 51.你研究过 Redis 的字典源码吗？是的，有研究过。Redis 的字典分为三层，最外层是一个 dict 结构，包含两个哈希表 ht[0] 和 ht[1]，用于存储键值对。每个哈希表由一个数组和链表组成，数组用于快速定位，链表用于解决哈希冲突。 三分恶面渣逆袭：Redis字典 // 最外层的字典结构typedef struct dict dictht ht[2]; // 两个哈希表！这是关键 long rehashidx; // rehash索引，-1表示没有进行rehash // ... dict;// 哈希表结构typedef struct dictht dictEntry **table; // 哈希表数组 unsigned long size; // 哈希表大小 unsigned long sizemask; // 哈希表大小掩码，用于计算索引值 unsigned long used; // 该哈希表已有节点的数量 dictht;// 哈希表节点typedef struct dictEntry void *key; // 键 v; // 值 struct dictEntry *next; // 指向下个哈希表节点，形成链表 dictEntry; 字典最核心的特点是渐进式 rehash，这是我觉得最精彩的部分。传统的哈希表扩容都是一次性完成的，但 Redis 不是这样的。 当负载因子触发 rehash 条件时，Redis 会为哈希表1 分配新的空间，通常是哈希表 0 的两倍大小，然后将 rehashidx 设置为 0。 接下来的关键是，Redis 不会一次性把所有数据从哈希表0 迁移到哈希表1，而是每次操作字典时，顺便迁移哈希表0 中 rehashidx 位置上的所有键值对。迁移完一个槽位后，rehashidx 递增，直到整个哈希表0 迁移完毕。 Kousik Nath：Redis rehash 这种设计的巧妙之处在于把 rehash 的开销分摊到了每次操作中。假设有一个几百万键的哈希表，如果一次性 rehash 可能需要几百毫秒，这对单线程的 Redis 来说是灾难性的。但通过渐进式 rehash，每次操作只增加很少的额外开销，用户基本感觉不到延迟。 在 rehash 期间，查找操作会先查 哈希表 0，没找到再查哈希表 1；但是新插入的数据只会放到哈希表 1 中。这样既可以保证数据的完整性，又能避免数据的重复。 遇到哈希冲突怎么办？Redis 是通过链地址法来解决哈希冲突的，每个哈希表的槽位实际上是一个链表的头指针，当多个键的哈希值映射到同一个槽位时，这些键会以链表的形式串联起来。 Kousik Nath：哈希冲突 具体实现上，Redis 会通过哈希表节点的 next 指针，指向下一个具有相同哈希值的节点。当发生冲突时，新的键值对会插入到链表的头部，时间复杂度是 O(1)。查找时需要遍历整个链表，最坏的情况下时间复杂度为 O(n)，但通常链表都比较短。 另外，Redis 设计的哈希函数在分布上也比较均匀，能够有效减少哈希冲突的发生。 /* MurmurHash2, by Austin Appleby * Note - This code makes a few assumptions about how your machine behaves - * 1. We can read a 4-byte value from any address without crashing * 2. sizeof(int) == 4 * * And it has a few limitations - * * 1. It will not work incrementally. * 2. It will not produce the same results on little-endian and big-endian * machines. */unsigned int dictGenHashFunction(const void *key, int len) /* m and r are mixing constants generated offline. Theyre not really magic, they just happen to work well. */ uint32_t seed = dict_hash_function_seed; const uint32_t m = 0x5bd1e995; const int r = 24; /* Initialize the hash to a random value */ uint32_t h = seed ^ len; /* Mix 4 bytes at a time into the hash */ const unsigned char *data = (const unsigned char *)key; while(len = 4) uint32_t k = *(uint32_t*)data; k *= m; k ^= k r; k *= m; h *= m; h ^= k; data += 4; len -= 4; /* Handle the last few bytes of the input array */ switch(len) case 3: h ^= data[2] 16; case 2: h ^= data[1] 8; case 1: h ^= data[0]; h *= m; ; /* Do a few final mixes of the hash to ensure the last few * bytes are well-incorporated. */ h ^= h 13; h *= m; h ^= h 15; return (unsigned int)h; memo：2025 年 6 月 6 日，今天有球友咨询去金山办公暑期实习，要提前学点什么？又一个凭借 Java 这个载体拿到 Go offer 的球友，说明在求职 Go 岗的时候，也不用说非要提前刻意去学习 Go，当然有一些基础是最好的，我之前也整理过 Go 的学习路线在 Java 进阶之路上。 拿下金山 offer 52.🌟你了解跳表吗？跳表是一种非常巧妙的数据结构，它在有序链表的基础上建立了多层索引，最底层包含所有数据，每往上一层，节点数量就减少一半。 metahub follower：skiplist 它的核心思想是”用空间换时间”，通过多层索引来跳过大量节点，从而提高查找效率。 三分恶面渣逆袭：跳表 每个节点有 50% 的概率只在第 1 层出现，25% 的概率在第 2 层出现，依此类推。查找的时候从最高层开始水平移动，当下一个节点值大于目标时，就向下跳一层，直到找到目标节点。 Dylan Wang：Skiplist 怎么往跳表插入节点呢？首先是找到插入位置，从最高层的头节点开始，在每一层都找到应该插入位置的前驱节点，用一个 update 数组把这些前驱节点记录下来。这个查找过程和普通查找一样，在每层向右移动直到下个节点的值大于要插入的值，然后下降到下一层。 // 记录每层的插入位置zskiplistNode *update[ZSKIPLIST_MAXLEVEL];zskiplistNode *x;int i, level;// 从最高层开始查找x = zsl-header;for (i = zsl-level-1; i = 0; i--) // 在当前层水平移动，找到插入位置 while (x-level[i].forward (x-level[i].forward-score score || (x-level[i].forward-score == score sdscmp(x-level[i].forward-ele, ele) 0))) x = x-level[i].forward; update[i] = x; // 记录每层的前驱节点 接下来随机生成新节点的层数。通常用一个循环，每次有 50% 的概率继续往上，直到随机失败或达到最大层数限制。 // Redis 中的随机层数生成int zslRandomLevel(void) int level = 1; while ((random()0xFFFF) (ZSKIPLIST_P * 0xFFFF)) level += 1; return (level ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;// 生成新节点的层数level = zslRandomLevel(); 创建新节点后，从底层开始到新节点的最高层，在每一层都进行标准的链表插入操作。这一步要利用之前记录的 update 数组，将新节点插入到正确位置，然后更新前后指针的连接关系。 // 更新前进指针for (i = 0; i level; i++) x-level[i].forward = update[i]-level[i].forward; update[i]-level[i].forward = x; // 更新跨度信息 x-level[i].span = update[i]-level[i].span - (rank[0] - rank[i]); update[i]-level[i].span = (rank[0] - rank[i]) + 1;// 更新未涉及层的跨度for (i = level; i zsl-level; i++) update[i]-level[i].span++;// 更新后退指针x-backward = (update[0] == zsl-header) ? NULL : update[0];if (x-level[0].forward) x-level[0].forward-backward = x;else zsl-tail = x;// 更新跳表长度zsl-length++; 我们来模拟一个跳表的插入过程，假设插入的数据依次是 22、19、7、3、37、11、26。 zhangtielei.com：跳表插入过程 那假如我们在一个已经分布了 1、14、27、31、44、56、63、70、80、91 的跳表中插入一个 67 的节点，插入过程是这样的： Dylan Wang：插入节点 zset为什么要使用跳表呢？第一，跳表天然就是有序的数据结构，查找、插入和删除都能保持 O(log n) 的时间复杂度。 第二，跳表支持范围查询，找到起始位置后可以直接沿着底层链表顺序遍历，满足 ZRANGE 按排名获取元素，或者 ZRANGEBYSCORE 按分值范围获取元素。 memo：2025 年 6 月 7 日，今天给一个学院本球友修改简历的时候，他提到实习的同事，都拿到了 20k 以上的 offer，甚至还有 25k 携程 offer 的，自己并不比他们差，问在实习、项目和能力上还能怎么提高？ 学院本球友的目标和计划 我想说的是，这就是为什么很多人选择跑来卷互联网开发的原因啊，上线比其他行业高太多了，虽然互联网开发的工作强度也大，但最起码能劳有所获。 跳表是如何定义的呢？跳表本质上是一个多层链表，底层是一个包含所有元素的有序链表，上一层作为索引层，包含了下一层的部分节点；层数通过随机算法确定，理论上可以无限高。 metahub follower：跳表 跳表节点包含分值 score、成员对象 obj、一个后退指针 backward，以及一个层级数组 level。每个层级包含 forward 前进指针和 span 跨度信息。 typedef struct skiplistNode double score; // 分值（用于排序） robj *obj; // 数据对象 struct skiplistNode *backward; // 后退指针 struct skiplistLevel struct skiplistNode *forward; // 前进指针 unsigned int span; // 跨度（到下个节点的距离） level[]; // 层级数组 skiplistNode; 跳表本身包含头尾节点指针、节点总数 length 和当前最大层数 level。 typedef struct skiplist struct skiplistNode *header, *tail; // 头尾节点 unsigned long length; // 节点数量 int level; // 最大层数 skiplist; span 跨度有什么用？span 记录了当前节点到下一节点之间，底层到底跨越了几个节点，它的主要作用是快速找到 ZSet 中某个分值的排名。 Aparajita Pandey：span 比如说我们执行 ZRANK 命令时，如果没有 span，就需要从头节点开始遍历每个节点，直到找到目标分值，这样时间复杂度是 O(n)。 // 没有span的排名查询 - O(n)int getRankWithoutSpan(skiplist *zsl, double score, robj *obj) skiplistNode *x = zsl-header-level[0].forward; int rank = 0; while (x) if (x-score == score equalStringObjects(x-obj, obj)) return rank + 1; // 排名从1开始 rank++; x = x-level[0].forward; return 0; 但有了 span，我们在从高层往低层搜索的时候，可以直接跳过一些节点，快速定位到目标分值所在的范围。这样就能把时间复杂度降到 O(log n)。 long skiplistGetRank(skiplist *zsl, double score, robj *obj) skiplistNode *x = zsl-header; unsigned long rank = 0; // 从最高层开始查找 for (int i = zsl-level - 1; i = 0; i--) while (x-level[i].forward (x-level[i].forward-score score || (x-level[i].forward-score == score compareStringObjects(x-level[i].forward-obj, obj) 0))) rank += x-level[i].span; // 累加跨度 x = x-level[i].forward; // 找到目标节点 if (x-level[i].forward x-level[i].forward-score == score equalStringObjects(x-level[i].forward-obj, obj)) rank += x-level[i].span; return rank; return 0; 为什么跳表的范围查询效率比字典高？字典是通过哈希函数将键值对分散存储的，元素在内存中是无序分布的，没有任何顺序关系。而跳表本身就是有序的数据结构，所有元素按照分值从小到大排列。 WARRIOR：跳表 当需要进行范围查询时，字典必须遍历所有元素，逐个检查每个元素是否在指定范围内，时间复杂度是 O(n)。比如要找分值在 60 到 80 之间的所有元素，字典只能把整个哈希表扫描一遍，因为它无法知道符合条件的元素在哪里。 而跳表的范围查询就高效多了。首先用 O(log n) 时间找到范围的起始位置，然后沿着底层的有序链表顺序遍历，直到超出范围为止。总时间复杂度是 O(log n + k)，其中 k 是结果集的大小。这种效率差异在数据量大的时候非常明显。 晴天哥：zset 底层由字典和跳表组成 这也是为什么 Redis 的 zset 要用跳表而不是纯哈希表的重要原因，因为 zset 经常需要 ZRANGE、ZRANGEBYSCORE 这类范围操作。实际上 Redis 的 zset 是跳表和哈希表的组合：跳表保证有序性支持范围查询，哈希表保证 O(1) 的单点查找效率，两者互补。 Java 面试指南（付费）收录的小米暑期实习同学 E 一面面试原题：为什么 hash 表范围查询效率比跳表低 Java 面试指南（付费）收录的得物面经同学 8 一面面试原题：跳表的结构 Java 面试指南（付费）收录的美团面经同学 4 一面面试原题：Redis 跳表 Java 面试指南（付费）收录的阿里系面经同学 19 饿了么面试原题：跳表了解吗 memo：2025 年 6 月 8 日，今天有球友发信息称赞 Java 进阶之路的内容写得好，说实话，我是有这个自信的，基本上所写的内容也都是我这些年从读到的所有书籍、视频、教程中提炼到的精华，把一些难懂晦涩的知识都用通俗易懂的语言表达出来，配合手绘图，能让人更容易理解。 球友对二哥的 Java 进阶之路的称赞 53.压缩列表了解吗？答：压缩列表是 Redis 为了节省内存而设计的一种紧凑型数据结构，它会把所有数据连续存储在一块内存当中。 整个结构包含头部信息，如总的字节数、尾部偏移量、节点数量，以及连续的节点数据。 三分恶面渣逆袭：压缩列表组成部分 当 list、hash 和 set 的数据量较小且值都不大时，底层会使用压缩列表来实现。 截图来自 Redis 官网 通常情况在，每个节点包含三个部分：前一个节点的长度、编码类型和实际的数据。 happytree001：ziplist entry 前一个节点的长度是为了支持从后往前遍历；当前一个节点的长度小于 254 字节时，使用 1 字节存储；否则用 5 字节存储，第一个字节设置为 254，后四个字节存储实际长度。 happytree001：ziplist prevlen 编码类型会根据数据的实际情况选择最紧凑的存储方式。 三分恶面渣逆袭：压缩列表示例 但压缩列表有个致命问题，就是连锁更新。当插入或删除节点导致某个节点长度发生变化时，可能会影响后续所有节点存储的“前一个节点长度”字段，最坏情况下时间复杂度会退化到 O(n²)。 hjcenry.com：连锁更新 ziplist 的节点数量会超过 65535 吗？不会。 Zllen 字段的类型是 uint16_t，最大值为 65535，也就是 2 的 16次方，所以压缩列表的节点数量不会超过 65535。 当节点数量小于 65535 时，该字段会存储实际的数量；否则该字段就固定为 65535，实际存储的数量需要逐个遍历节点来计算。 ziplist 的编码类型了解多少？ziplist 的编码类型设计得很精巧，主要分为字符串编码和整数编码两大类，目的是用最少的字节存储数据。 比如 0 到 12 这些小整数直接编码在 type 字段中，只需要 1 个字节。 编码 长度 描述 11000000 1字节 int16_t类型整数，2 字节 11010000 1字节 int32_t类型整数，4 字节 11100000 1字节 int64_t类型整数，8 字节 11110000 1字节 24位有符号整数 ，3 字节 1111xxxx 1字节 数据范围在[0-12]，数据包含在编码中 happytree001：ziplist 小整数编码 对于字符串编码，根据字符串长度有三种格式。长度小于 63 字节的用 00 开头的单字节编码，剩余 6 位存储长度。长度在 63 到 16383 之间的用 01 开头的双字节编码，剩余 14 位存储长度。超过 16383 字节的用 10 开头，后面跟 4 字节存储长度。 编码 长度 描述 00pppppp 1字节 0-63 字节的字符串 01pppppp qqqqqqqq 2字节 64-16383字节的字符串 10______ qqqqqqqq rrrrrrrr ssssssss tttttttt 5字节 16384-4294967295字节的字符串 happytree001：ziplist 字符串编码 Java 面试指南（付费）收录的同学 30 腾讯音乐面试原题：什么情况下使用压缩列表 memo：2025 年 6 月 9 日修改至此，今天有球友特意发私信，感谢面渣逆袭对他的帮助。对，这么棒的内容，我依然选择了免费，因为我相信知识是有价值的，只有诚恳的分享出来才能让更多人受益。 球友对面渣逆袭的认可 54.quicklist 了解吗？quicklist 是 Redis 在 3.2 版本时引入的，专门用于 List 的底层实现，它实际上是一个混合型数据结构，结合了压缩列表和双向链表的优点。 三分恶面渣逆袭：quicklist 在早期的版本中，List 会根据元素的数量和大小采用两种不同的底层数据结构，当元素较少或者较小时，会使用压缩列表；否则用双向链表。 但这种设计有个问题，就是当 List 中的元素数量较多时，压缩列表会因为连锁更新导致性能下降，而双向链表又会占用更多内存。 quicklist 通过将 List 拆分为多个小的 ziplist，再通过指针链接成一个双向链表，巧妙的解决了这个问题。 影中人lx：quicklist 默认情况下，每个 ziplist 可以存储 8KB 的数据，假如每个元素的大小恰好是 1KB，那么一个 quicklist 就可以存储 8 个元素。80 个这样的元素就会被分成 10 个 ziplist。 这样既保留了压缩列表的内存紧凑性，又减少了双向链表指针的数量，进一步降低了内存开销。 metahub follower：ziplist 除此之外，quicklist 还有一个重要的特性，就是它的可配置性，可以通过填充因子控制每个 ziplist 节点的大小。当填充因子为正数时，它还可以限制每个 ziplist 最多包含的元素数量。 # 填充因子，默认 -2（8KB）list-max-ziplist-size 10 如果想进一步节省内存，quicklist 还支持对中间节点进行 LZF 压缩，压缩深度为 1 时，表示除了首尾各 1 个节点不压缩外，其他节点都压缩。 # 压缩深度，默认 0（不压缩）list-compress-depth 1 wingsxdu.com：LZF 压缩算法 LZF 压缩算法了解吗？LZF 是一种快速的无损压缩算法，主要用于减少数据存储空间。它的核心思想是通过查找重复数据来实现压缩，通过一个滑动窗口来查找重复的字节序列，并将这些序列替换为更短的引用。 输入数据: hello world hello redis步骤1: 处理 hello world - 建立字典，记录字节序列位置步骤2: 遇到重复的 hello- 在字典中找到之前的 hello 位置- 用 (距离, 长度) 对替换: (12, 5)输出: hello world + (12,5) + redis 嗨嗨嗨，时隔两年，面渣逆袭第二版 PDF 终于可以下载了。我们做了大量的优化： 对于高频题：会标注在《Java 面试指南（付费）中出现的位置，哪家公司，原题是什么；如果你想节省时间的话，可以优先背诵这些题目，尽快做到知彼知己，百战不殆。 结合项目：包括技术派、mydb、pmhub来组织语言，让面试官最大程度感受到你的诚意，而不是机械化的背诵。 修复问题：第一版中出现的问题，包括球友们的私信反馈，网站留言区的评论，以及 GitHub 仓库中的 issue，让这份面试指南更加完善。 优化排版：增加手绘图，重新组织答案，使其更加口语化，从而更贴近面试官的预期。 你可以扫下面的二维码（或者长按自动识别）关注【沉默王二】公众号，发送关键字 222 来获取 PDF 版本，如果面渣逆袭真的对你有帮助，希望能给二哥的公众号加一个星标，满足我那一丁点虚荣心，这将是我更新下去的最强动力。 面渣逆袭的整理工作真的太不容易了，花了我好多好多的时间和精力，内容完全免费，但质量却有口皆碑，就是为了做一点真正有意义的、纯粹的事情。 memo：2025 年 6 月 10 日，今天有球友发信息说找我修改了简历后，又按照星球的学习资料好好学了一下之后，拿到了字节跳动的 offer，并特意发了一个大红包来感谢。这种被认可被需要的感觉，真好！ 球友拿到了字节跳动的 offer 补充55.假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如何将它们全部找出来？我会使用 SCAN 命令配合 MATCH 参数来解决。 比如要找以 user: 开头的 key，可以执行 SCAN 0 MATCH user:* COUNT 1000。 SCAN 的优势在于它是基于游标的增量迭代，每次只返回一小批结果，不会阻塞服务器。可以从游标 0 开始，每次处理返回的 key 列表，然后用返回的下一个游标继续扫描，直到游标回到 0 表示扫描完成。 使用 Spring Data Redis 的代码示例： @Servicepublic class RedisKeyService @Autowired private RedisTemplateString, Object redisTemplate; public ListString scanKeysByPrefix(String prefix, int batchSize) ListString keys = new ArrayList(); ScanOptions options = ScanOptions.scanOptions() .match(prefix + *) .count(batchSize) .build(); try (CursorString cursor = redisTemplate.scan(options)) while (cursor.hasNext()) keys.add(cursor.next()); return keys; 千万不要用 KEYS 命令，因为 KEYS 会阻塞 Redis 服务器直到遍历完所有 key，在生产环境中对 1 亿个 key 执行 KEYS 是非常危险的。 memo：2025 年 6 月 11 日修改至此，今天有读者留言说，找实习的时候背了一个月的面渣逆袭，然后快手和美团都拿到 offer 了。能帮助到，也是我做技术博主最开心的一件事情了，也感谢读者给的口碑。 拿到快手和美团的读者发来感谢 56.Redis在秒杀场景下可以扮演什么角色？秒杀是一种非常特殊的业务场景，它的特点是在极短时间内会有大量用户涌入系统，对系统的并发处理能力、响应速度和数据一致性都提出了极高的要求。在这种场景下，Redis 作为一种高性能的内存数据库，能够发挥多方面的关键作用。 比如说在秒杀开始前，我们可以将商品信息、库存数据等预先加载到 Redis 中，这样大量的用户读请求就可以直接从 Redis 中获取响应，而不必每次都去访问数据库，这样就能大大减轻数据库的访问压力。 许令波-秒杀系统的设计 其次，Redis 在库存控制方面具有得天独厚的优势。秒杀最核心的问题之一就是容易发生超卖。Redis 提供的原子操作如 DECR、DECRBY 等命令，可以确保在高并发环境下库存计数的准确性。 京东云：超卖 更复杂的逻辑，可以通过 Lua 脚本来实现，因为 Lua 脚本在 Redis 中是原子执行的，所以可以包含复杂的判断和操作逻辑，比如先检查库存是否充足，再进行扣减，这整个过程是不会被其他操作打断的。 第三点，Redis 的分布式锁可以确保多个用户同时抢购同一件商品时的操作是互斥的，保证数据一致性的同时，还可以用来防止用户重复下单。 小米信息技术团队：Redis 分布式锁 第四点，限流削峰。秒杀开始的瞬间，可能会有成千上万的请求同时到达，如果不加控制，很容易导致系统崩溃。Redis 可以实现多种限流算法，比如简单的计数器限流、令牌桶或漏桶算法等。 zhuangyongxin.github.io：令牌桶 通过限流算法我们可以控制单位时间内系统能够处理的请求数量，超出部分可以排队或者直接拒绝，从而保护系统的稳定运行。 memo：2025 年 6 月 12 日修改至此，今天有球友发信息说，大二就拿下了美团的实习 offer，特意发来感谢，说我的付出对他有着巨大的帮助，真的很感动，每一个懂得感恩的球友，你们也是我坚持下去的最强动力。 大二那些美团，特意发来感谢，面渣的口碑继续+1 Redis具体如何实现削峰呢？削峰的本质是将瞬时的高流量请求缓冲起来，通过排队、限流等机制，使系统以一个可承受的速度来处理请求。 那第一步就是缓存预热。在秒杀活动开始前，先把商品信息这些热点数据提前加载到 Redis 中。这样用户访问商品页面时，可以直接从 Redis 读取，数据库基本上不会有压力。 天翼云小翼：Redis 缓存预热 第二步是引入消息队列，特别是下单这种写操作，不能让用户等太久，但后端处理订单、扣库存这些操作又比较重。所以可以用 Redis 的 List 做了个队列，或者直接用 RocketMQ 这种标准的消息中间件，用户下单后立即返回”订单提交成功”，然后把订单数据丢到队列里，后台服务慢慢消费。这样既保证了用户体验，又避免了系统被瞬时写请求压垮。 许令波-排队 第三步，可以在秒杀活动中加入答题环节，只有答对题目的用户才能参与秒杀活动，这样可以最大程度减少无效请求。 许令波-答题 一个比较完整的秒杀削峰处理方案： @Servicepublic class SeckillServiceImpl implements SeckillService @Autowired private RedisTemplateString, String redisTemplate; @Autowired private OrderService orderService; @Autowired private CommodityService commodityService; /** * 秒杀请求入口 */ public Result seckill(Long userId, Long commodityId) // 1. 用户请求频率限制 if (!countRateLimit(user: + userId, 5, 60)) return Result.error(请求过于频繁); // 2. 商品是否在秒杀时间内 if (!isInSeckillTime(commodityId)) return Result.error(秒杀未开始或已结束); // 3. 是否还有库存(快速失败) String stockKey = seckill:stock: + commodityId; Integer stock = Integer.valueOf(redisTemplate.opsForValue().get(stockKey)); if (stock != null stock = 0) return Result.error(商品已售罄); // 4. 全局限流 if (!acquireToken(global, 1000, 100)) // 系统负载过高，将请求放入队列延迟处理 enqueueDelayedRequest(userId, commodityId); return Result.success(秒杀请求已受理，排队处理中); // 5. 检查用户是否已购买 if (hasUserBought(userId, commodityId)) return Result.error(您已经购买过该商品); // 6. 将请求放入队列，返回排队状态 String requestId = generateRequestId(userId, commodityId); enqueueRequest(userId, commodityId, requestId); return Result.success(秒杀请求已提交，请等待结果, requestId); /** * 异步处理秒杀请求 */ @Scheduled(fixedRate = 50) // 每50ms处理一批 public void processSeckillQueue() String queueKey = seckill:queue; // 批量处理，控制处理速度 for (int i = 0; i 10; i++) String requestJson = redisTemplate.opsForList().leftPop(queueKey); if (requestJson == null) break; SeckillRequest request = JSON.parseObject(requestJson, SeckillRequest.class); try // 执行秒杀核心逻辑 boolean success = doSeckill(request.getUserId(), request.getCommodityId()); // 更新请求状态，便于用户查询 String statusKey = seckill:status: + request.getRequestId(); redisTemplate.opsForValue().set(statusKey, success ? SUCCESS : FAILED, 1, TimeUnit.HOURS); catch (Exception e) log.error(处理秒杀请求失败, e); // 记录失败状态 String statusKey = seckill:status: + request.getRequestId(); redisTemplate.opsForValue().set(statusKey, ERROR, 1, TimeUnit.HOURS); /** * 秒杀核心逻辑 */ private boolean doSeckill(Long userId, Long commodityId) // 使用Lua脚本保证原子性操作 String script = -- 检查库存 + local stockKey = KEYS[1] + local stock = tonumber(redis.call(get, stockKey)) + if stock == nil or stock = 0 then + return 0 + end + + -- 检查是否重复购买 + local boughtKey = KEYS[2] + local hasBought = redis.call(sismember, boughtKey, ARGV[1]) + if hasBought == 1 then + return -1 + end + + -- 扣减库存并记录购买 + redis.call(decr, stockKey) + redis.call(sadd, boughtKey, ARGV[1]) + + -- 返回成功 + return 1; String stockKey = seckill:stock: + commodityId; String boughtKey = seckill:bought: + commodityId; Long result = (Long) redisTemplate.execute( new DefaultRedisScript(script, Long.class), Arrays.asList(stockKey, boughtKey), userId.toString() ); if (result == 1) // 创建订单(可以进一步异步化) createOrder(userId, commodityId); return true; return false; // 其他辅助方法... Redis如何做限流呢？限流是为了控制系统的请求速率，防止系统被过多的请求压垮。 Redis 实现限流最简单的方法是基于计数器的固定窗口限流。比如限制用户每分钟最多访问 100 次，我们就用 INCR 命令给每个用户设个计数器，key 是 rate_limit:用户ID:分钟时间戳，每次请求就加 1，同时设置 60 秒过期。如果计数超过 100 就拒绝请求。 // 伪代码String key = rate_limit: + userId;// 尝试获取当前计数Long count = redis.get(key);// 如果key不存在，设置为1并设置过期时间if (count == null) redis.setex(key, 60, 1); // 60秒窗口期 return true; // 允许访问// 如果计数未超过限制if (count maxRequests) redis.incr(key); return true; // 允许访问return false; // 拒绝访问 这种方法简单粗暴，但有个问题就是临界时间会有突刺，比如用户在第 59 秒访问了 100 次，第 61 秒又访问 100 次，相当于 2 秒内访问了 200 次。 第二种就是滑动窗口限流，通过 Redis 的 ZSET 来实现，把每次请求的时间戳作为 score 存进去，然后用 ZREMRANGEBYSCORE 删除窗口外的旧数据，再用 ZCARD 统计当前窗口内的请求数。这样限流就比较均匀了。 // 伪代码String key = sliding_window: + userId;long now = System.currentTimeMillis();// 添加当前请求到有序集合，score为当前时间戳redis.zadd(key, now, String.valueOf(now));// 移除时间窗口之前的请求数据redis.zremrangeByScore(key, 0, now - windowSize);// 设置key过期时间，避免冷用户持续占用内存redis.expire(key, windowSize / 1000 + 1);// 获取当前窗口的请求数Long count = redis.zcard(key);return count = maxRequests; 在实际开发中，通常会采用令牌桶算法，它就像在帝都魔都买车，摇到号才有资格，没摇到就只能等下一次（😁）。 可以在 Redis 里存两个值，一个是令牌数量，一个是上次更新时间。每次请求时用 Lua 脚本计算应该补充多少令牌，然后判断是否有足够的令牌。 李子捌：令牌桶 -- Redis Lua脚本实现令牌桶算法local key = KEYS[1] -- 限流的keylocal max_permits = tonumber(ARGV[1]) -- 最大令牌数local permits_per_second = tonumber(ARGV[2]) -- 每秒产生的令牌数local required_permits = tonumber(ARGV[3]) -- 请求的令牌数-- 获取当前时间local time = redis.call(time)local now_micros = tonumber(time[1]) * 1000000 + tonumber(time[2])-- 获取上次更新的时间和当前存储的令牌数local last_micros = tonumber(redis.call(hget, key, last_micros) or 0)local stored_permits = tonumber(redis.call(hget, key, stored_permits) or 0)-- 计算时间间隔内新产生的令牌数local interval_micros = now_micros - last_microslocal new_permits = interval_micros * permits_per_second / 1000000stored_permits = math.min(max_permits, stored_permits + new_permits)-- 判断令牌是否足够local result = 0if stored_permits = required_permits then -- 令牌足够，更新令牌数和时间 stored_permits = stored_permits - required_permits result = 1end-- 更新Redis中的数据redis.call(hset, key, last_micros, now_micros)redis.call(hset, key, stored_permits, stored_permits)redis.call(expire, key, 10) -- 设置过期时间，避免长期占用内存return result Java 面试指南（付费）收录的农业银行面经同学 3 Java 后端面试原题：秒杀问题（错峰、削峰、前端、流量控制） Java 面试指南（付费）收录的滴滴面经同学 3 网约车后端开发一面原题：限流算法 memo：2025 年 6 月 13 日修改至此，今天在修改简历的过程中，碰到一位西安交通大学的球友，他整个校园经历是比较丰富的，先去了新加坡国立大学做暑期交换生，然后又去了加州大学伯克利分校做学期交换生，希望也都能在学校阶段尽量丰富自己的经历，争取多拿一些奖学金、实习经历，这样才能在毕业时有更强的竞争力。 西安交通大学的球友 57.客户端宕机后 Redis 服务端如何感知到？TCP 的 keepalive 是 Redis 用来检测客户端连接状态的主要机制，默认值为 300 秒。 # 针对低延迟场景，设置为60秒，表示每60秒发送一次keepalive探测config set tcp-keepalive 60 当客户端与服务器在指定时间内没有任何数据交互时，Redis 服务器会发送 TCP ACK 探测包，如果连续多次没有收到响应，TCP 协议栈会通知 Redis 服务端连接已断开，之后，Redis 服务端会清理相关的连接资源，释放连接。 二哥的Java进阶之路：默认的tcp-keepalive和 timeout 另外还有一个 timeout 参数，用来控制客户端连接的空闲超时时间。 # 表示600秒内没有任何命令则断开连接config set timeout 600 默认值为 0，表示永不断开连接；当设置为非零值时，如果客户端在指定时间内没有发送任何命令，服务端会主动断开连接。 Redis 服务器会定期检查空闲连接是否超时，检查频率由 hz 参数控制；这将有助于释放那些客户端异常退出但 TCP 连接未正常关闭的资源。 不同的连接池也会有自己的连接检测机制，比如 Jedis 连接池可以通过设置 testOnBorrow 和 testWhileIdle 来启用连接检测。 # 是否启用连接池spring.redis.jedis.pool.enabled=true# 连接池最大连接数（使用负值表示没有限制）spring.redis.jedis.pool.max-active=200# 连接池最大空闲连接数spring.redis.jedis.pool.max-idle=200# 连接池最小空闲连接数spring.redis.jedis.pool.min-idle=50# 连接池最大阻塞等待时间（使用负值表示没有限制）spring.redis.jedis.pool.max-wait=3000# 空闲连接检查间隔（毫秒）spring.redis.jedis.pool.time-between-eviction-runs=60000","tags":["基础","Java"],"categories":["Java问答笔记"]},{"title":"2025.7.22学习日记","path":"/2025/07/22/学习日记25年7月/2025.7.22学习笔记/","content":"今日学习内容3DGS编写测试脚本. 力扣每日一题一道哈希表的中等题目,比较简单的. JavaSE八股1556 项目篇再次梳理了一遍项目的reqinfo过滤器,把整个项目的过滤器,拦截器逻辑梳理明白了. 代码随想录一道dp题,复习了KMP. 生活篇晚上健身练背,核心,肩,三头,强度中等偏上.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.21学习日记","path":"/2025/07/21/学习日记25年7月/2025.7.21学习笔记/","content":"今日学习内容3DGS编写测试脚本. 项目篇要在dev环境下的话,需要关闭devtools,-Dspring.devtools.restart.enabled=false. 关于spring初始化的一些总结: @ConfigurationProperties 配置类最早被加载，如您的MailConfig此时所有配置属性已就绪 Logback 初始化加载logback-spring.xml创建所有Appender（包括您的AlarmUtil）Spring Bean 初始化 常规@Component和@Bean此时AlarmUtil可能已经缓存了未初始化的状态 力扣每日一题一道简单题,遍历即可. 代码随想录个人博客优化了一下个人博客的背景和打卡墙的逻辑. 生活篇晚上健身主要是核心抗旋转训练,强度适中.","tags":["基础","日记","leetcode","项目","博客"],"categories":["学习日记","2025-07"]},{"title":"2025.7.20学习日记","path":"/2025/07/20/学习日记25年7月/2025.7.20学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道困难题. 生活篇上午踢球颠球 短传 长传 停球.","tags":["基础","日记","leetcode","项目","博客"],"categories":["学习日记","2025-07"]},{"title":"2025.7.19学习日记","path":"/2025/07/19/学习日记25年7月/2025.7.19学习笔记/","content":"今日学习内容3DGS力扣每日一题:中等题. 上午学习计算机网络学习计算机网络基础完成 6262. 个人博客优化了一下个人博客的背景和打卡墙的逻辑. 生活篇","tags":["基础","日记","leetcode","项目","博客"],"categories":["学习日记","2025-07"]},{"title":"2025.7.18学习日记","path":"/2025/07/18/学习日记25年7月/2025.7.18学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道困难题,用的前缀和加堆优化. 上午学习计算机网络记了笔记 5262 个人博客添加了心心念念的打卡墙模块,包括侧边栏的打卡日历还有check-in年度打卡界面,数据来源是归档笔记的日期.然后优化的博客的样式和背景. 生活篇早上踢球颠球 短传 长传 停球.","tags":["基础","日记","leetcode","项目","博客"],"categories":["学习日记","2025-07"]},{"title":"2025.7.17学习日记","path":"/2025/07/17/学习日记25年7月/2025.7.17学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道中等的动态规划题,昨天题目的进阶版本. 上午学习计算机网络记了笔记 4362 代码随想录复习了两道完全背包. 项目相关完成了ReqInfo的拦截器.git rm -r –cached .git rm –cached techub-websrcmainresourcesapplication-email.yml下次执行这个,别全部清除了 生活篇晚上健身主要练核心还有背.","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.16学习日记","path":"/2025/07/16/学习日记25年7月/2025.7.16学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道中等的动态规划题. 上午学习计算机网络记了笔记 3562 代码随想录完成了01背包的题,看了完全背包的讲义. 项目相关完成了ReqInfo的拦截器.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.15学习日记","path":"/2025/07/15/学习日记25年7月/2025.7.15学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道简单题. 上午学习计算机网络记了笔记 2662 代码随想录完成了01背包的题,看了完全背包的讲义. 项目相关完成了ReqInfo的拦截器. 生活晚上健身主要练得胸和肩,强度高.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.14学习日记","path":"/2025/07/14/学习日记25年7月/2025.7.14学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道简单的遍历链表的题目. 上午学习计算机网络记了笔记 1262 代码随想录完成了01背包的题,看了完全背包的讲义 项目相关实现了数据后台接口. 生活篇晚上健身今天主要练的核心,核心抗旋训练和核心抗屈训练,强度非常高,顺带练了一组肩和一组引体.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"计算机网络学习笔记","path":"/2025/07/14/Java问答笔记/计算机网络学习笔记/","content":"基础1.计算机网络体系的结构计算机网络体系结构通过将复杂的网络通信分解成不同的层次，来标准化交互的过程。常见的模型包括 OSI 七层模型、TCP/IP 四层模型和五层体系结构。 OSI 是理论上的网络通信模型，TCPIP 是实际应用层面上的网络通信模型，五层结构是为了方便理解和记忆。 说说 OSI 七层模型？OSI（Open System Interconnection）七层参考模型是一个网络架构模型，由国际标准化组织（ISO）提出，用于描述和标准化各种计算机网络的功能和过程。这七层从高到低分别是： 应用层：最靠近用户的层，负责处理特定的应用程序细节。这一层提供了网络服务与用户应用软件之间的接口。 例如，Web 浏览器、FTP 客户端和服务器、电子邮件客户端等。 表示层：确保从一个系统发送的信息可以被另一个系统的应用层读取。它负责数据的转换、压缩和加密。 例如，确保数据从一种编码格式转换为另一种，如 ASCII 到 EBCDIC。 会话层：管理用户的会话，控制网络上两节点间的对话和数据交换的管理。它负责建立、维护和终止会话。 例如，建立一个会话令牌，以便在网络上的两个节点之间传递。 传输层：提供端到端的通信服务，保证数据的完整性和正确顺序。这一层包括 TCP 和 UDP 等。 网络层：负责在多个网络之间进行数据传输，确保数据能够在复杂的网络结构中找到从源到目的地的最佳路径。这层使用的是 IP（Internet Protocol）协议。 数据链路层：在物理连接中提供可靠的传输，负责建立和维护两个相邻节点间的链路。包括帧同步、MAC（媒体访问控制）。 物理层：负责在物理媒介上实现原始的数据传输，比如电缆、光纤和无线信号传输。涉及的内容包括电压、接口、针脚、电缆的规格和传输速率等。 说说 TCPIP 四层模型？TCPIP 四层模型是互联网通信的核心，定义了一系列协议和标准，确保设备间可以可靠地进行数据传输。 ①、应用层（Application Layer）：直接面向用户和应用程序，提供各种网络服务。它包含了用于特定应用的协议和服务，如 HTTP（HyperText Transfer Protocol）、FTP（File Transfer Protocol）、SMTP（Simple Mail Transfer Protocol）等。 示例：当在浏览器中输入一个 URL 并访问一个网页时，浏览器使用 HTTP 协议从 Web 服务器请求页面内容。 ②、传输层（Transport Layer）：提供端到端的通信服务，确保数据可靠传输。它负责分段数据、流量控制、错误检测和纠正。常见的传输层协议有 TCP 和 UDP。 示例：当发送一封电子邮件时，TCP 协议确保邮件从你的客户端可靠地传输到邮件服务器。 ③、网际层：或者叫网络层（Internet Layer），负责在不同网络之间路由数据包，提供逻辑地址（IP 地址）和网络寻址功能。用于处理数据包的分组、转发和路由选择，确保数据可以从源端传输到目标端。 常见协议：IPv4、IPv6、ICMP（Internet Control Message Protocol）。 示例：当访问一个网站时，网络层协议（如 IPv4）将你的请求从你的计算机通过多个路由器传输到目标服务器。 ④、网络接口层（Network Access Layer）：或者叫链路层（Link Layer），负责将数字信号在物理通道（网线）中准确传输，定义了如何在单一网络链路上传输数据，如何处理数据帧的发送和接收，包括物理地址（MAC 地址）的解析。 常见协议：以太网（Ethernet）、Wi-Fi。 示例：在一个局域网（LAN）中，计算机通过以太网连接交换机，链路层协议负责数据帧在网络设备间的传输。 说说五层体系结构？是对 OSI 和 TCPIP 的折衷，它保留了 TCPIP 的实用性，同时提供了比四层模型更细致的分层，便于教学和理解网络的各个方面。 应用层：作为网络服务和最终用户之间的接口。它提供了一系列供应用程序使用的协议，如 HTTP（网页）、FTP（文件传输）、SMTP（邮件传输）等。使用户的应用程序可以访问网络服务。 传输层：提供进程到进程的通信管理，这一层确保数据按顺序、无错误地传输。主要协议包括 TCP 和 UDP。 网络层：负责数据包从源到目的地的传输和路由选择，包括跨越多个网络（即互联网）。它使用逻辑地址（如 IP 地址）来唯一标识设备。路由器是网络层设备。 数据链路层：确保从一个节点到另一个节点的可靠、有效的数据传输。交换机、网桥是数据链路层设备。 物理层：电缆、光纤、无线电频谱、网络适配器等。 TCP三次握手四次挥手工作在哪一层？三次握手和四次挥手都是工作在传输层。传输层（Transport Layer）是 OSI 模型的第四层，负责提供端到端的通信服务，包括数据传输的建立、维护和终止。 TCP 作为一种面向连接的协议，通过三次握手建立连接，通过四次挥手终止连接，确保数据传输的可靠性和完整性。 讲一下计算机网络？计算机网络是指将多台计算机通过通信设备互联起来，实现资源共享和信息传递的系统。 2.说一下每一层对应的网络协议有哪些？一张表格总结常见网络协议： 3.那么数据在各层之间是怎么传输的呢？对于发送方而言，从上层到下层层层包装，对于接收方而言，从下层到上层，层层解开包装。 发送方的应用进程向接收方的应用进程传送数据 AP 先将数据交给本主机的应用层，应用层加上本层的控制信息 H5 就变成了下一层的数据单元 传输层收到这个数据单元后，加上本层的控制信息 H4，再交给网络层，成为网络层的数据单元 到了数据链路层，控制信息被分成两部分，分别加到本层数据单元的首部（H2）和尾部（T2） 最后的物理层，进行比特流的传输 这个过程类似写信，写一封信，每到一层，就加一个信封，写一些地址的信息。到了目的地之后，又一层层解封，传向下一个目的地。 网络综合4.从浏览器地址栏输入 url 到显示网页的过程了解吗？这个过程包括多个步骤，涵盖了 DNS 解析、TCP 连接、发送 HTTP 请求、服务器处理请求并返回 HTTP 响应、浏览器处理响应并渲染页面等多个环节。 DNS 解析：浏览器会发起一个 DNS 请求到 DNS 服务器，将域名解析为服务器的 IP 地址。 TCP 连接：浏览器通过解析得到的 IP 地址与服务器建立 TCP 连接。这一步涉及到 TCP 的三次握手，用于确保双方都已经准备好进行数据传输了。 发送 HTTP 请求：浏览器构建 HTTP 请求，包括请求行、请求头和请求体；然后将请求发送到服务器。 服务器处理请求：服务器接收到 HTTP 请求后，根据请求的资源路径，经过后端处理，生成 HTTP 响应消息；响应消息包括状态行、响应头和响应体。 浏览器接收 HTTP 响应：浏览器接收到服务器返回的 HTTP 响应数据后，开始解析响应体中的 HTML 内容；然后构建 DOM 树、解析 CSS 和 JavaScript 文件等，最终渲染页面。 断开连接：TCP 四次挥手，连接结束。 我们以输入 www.baidu.com 为例： 各个过程都使用了哪些协议？ 5.说说 DNS 的解析过程？DNS 的全称是 Domain Name System，也就是域名解析系统，它可以将域名映射到对应的 IP 地址上，比如说我们访问 www.javabetter.cn，实际上访问的是我在阿里云上一台丐版服务器，它的 IP 地址是 xxx.xxx.xxx.xxx。 当然了，也可以通过 IP 地址直接访问服务器，但不方便记忆，所以就有了域名系统。一个好的域名可以卖好多好多钱，像 javabetter.cn 这个域名，一年需要 39 块钱。 域名到 IP 之间的映射，就需要 DNS 来完成。 我来说说 DNS 的解析过程吧： 假设我们在浏览器地址栏里键入了 paicoding.com： 浏览器会首先检查自己的缓存中是否有这个域名对应的 IP 地址，如果有，直接返回；如果没有，进入下一步。 检查本地 DNS 缓存是否有该域名的记录。如果没有，向根域名服务器发送请求，根域名服务器将请求指向更具体的服务，如 com 顶级域名服务器。 顶级域名服务器再将请求指向权限域名服务器，通常由域名注册机构直接管理，paicoding.com是在阿里云上注册的，所以阿里云会提供对应的 DNS 解析服务，将域名和阿里云服务器绑定起来。 最终，浏览器使用获得的 IP 地址发起一个 HTTP 请求到目标服务器，然后该服务器返回所请求的网页内容。 6.说说 WebSocket 与 Socket 的区别？ Socket 其实就是等于 IP 地址 + 端口 + 协议。 具体来说，Socket 是一套标准，它完成了对 TCPIP 的高度封装，屏蔽网络细节，以方便开发者更好地进行网络编程。 WebSocket 是一个持久化的协议，它是伴随 H5 而出的协议，用来解决 http 不支持持久化连接的问题。 Socket 一个是网编编程的标准接口，而 WebSocket 则是应用层通信协议。 7.说一下你了解的端口及对应的服务？ 8.平常有抓包吗（补充）？我平常使用最多的就是 chrome 浏览器自带的 network 面板了，可以看到请求的时间、请求的信息，以及响应信息。 更专业的还有 fidder、wireshark 等工具。 HTTP8.说说 HTTP 常用的状态码及其含义？HTTP 状态码用于表示服务器对请求的处理结果，可以分为 5 种： 1xx 服务器收到请求，需要进一步操作，例如 100 Continue。 2xx 请求成功处理，例如 200 OK。 3xx 重定向：需要进一步操作以完成请求；例如 304 Not Modified 表示资源未修改，客户端可以使用缓存。 4xx 客户端错误：请求有问题，例如 404 Not Found 表示资源不存在。 5xx 服务器错误，例如500 Internal Server Error 表示服务器内部错误。 说一下 301 和 302 的区别？ 301：永久性移动，请求的资源已被永久移动到新位置。服务器返回此响应时，会返回新的资源地址。 302：临时性性移动，服务器从另外的地址响应资源，但是客户端还应该使用这个地址。 用一个比喻，301 就是嫁人的新垣结衣，302 就是有男朋友的长泽雅美。 9.HTTP 有哪些请求方式？HTTP 协议定义了多种请求方式，用以指示请求的目的。常见的请求方式有 GET、POST、DELETE、PUT。 GET：请求检索指定的资源。应该只用于获取数据，并且是幂等的，即多次执行相同的 GET 请求应该返回相同的结果，并且不会改变资源的状态。 POST：向指定资源提交数据，请求服务器进行处理（如提交表单或上传文件）。数据被包含在请求体中。可能会创建新的资源或修改现有资源。 DELETE：删除指定的资源。 PUT：用于替换指定的资源。如果指定的资源不存在，创建一个新资源。 HEAD：类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头。可以用于检查资源是否存在，验证资源的更新时间等。 OPTIONS：用于获取服务器支持的 HTTP 请求方法。通常用于跨域请求中的预检请求（CORS）。 TRACE：回显服务器收到的请求，主要用于测试或诊断。但由于安全风险（可能暴露敏感信息），很多服务器会禁用 TRACE 请求。 CONNECT：建立一个到目标资源的隧道（通常用于 SSLTLS 代理），用于在客户端和服务器之间进行加密的隧道传输。 HTTP 的 GET 方法可以实现写操作吗?可以是可以，但是不推荐。 使用 GET 执行写操作可能导致严重的安全问题，如跨站请求伪造（CSRF）。实际开发中，也应该杜绝使用 GET 方法执行写操作。通过在项目接口上明确指出需要的请求方式. 客户端一旦使用错误❎，将会收到一个 405 Method Not Allowed 的响应。 什么是幂等？幂等方法了解哪些？幂等（Idempotence）是一个数学概念，用于描述某些操作的特性，即无论操作执行多少次，结果都是相同的。换句话说，幂等操作可以重复执行而不会改变系统状态。 如果一个操作是幂等的，那么对同一资源执行该操作一次和执行多次的效果相同。 在正确实现的条件下，GET、HEAD、PUT和 DELETE 等方法都是幂等的，而 POST 方法不是。 例如，GET /pageX HTTP/1.1 幂等的。连续调用多次，客户端接收到的结果都是一样的： GET /pageX HTTP/1.1GET /pageX HTTP/1.1GET /pageX HTTP/1.1 DELETE /idX/delete HTTP/1.1 是幂等的，即便是不同请求之间接收到的状态码不一样： DELETE /idX/delete HTTP/1.1 - Returns 200 if idX existsDELETE /idX/delete HTTP/1.1 - Returns 404 as it just got deletedDELETE /idX/delete HTTP/1.1 - Returns 404 10.说⼀下 GET 和 POST 的区别？ GET 请求主要用于获取数据，参数附加在 URL 中，存在长度限制，且容易被浏览器缓存，有安全风险；POST 请求用于提交数据，参数放在请求体中，适合提交大量或敏感的数据。 另外，GET 请求是幂等的，多次请求不会改变服务器状态；而 POST 请求不是幂等的，可能对服务器数据有影响。 11.GET 的长度限制是多少？HTTP 中的 GET 方法是通过 URL 传递数据的，但是 URL 本身其实并没有对数据的长度进行限制，真正限制 GET 长度的是浏览器。 例如 IE 浏览器对 URL 的最大限制是 2000 多个字符，大概 2kb 左右，像 Chrome、Firefox 等浏览器支持的 URL 字符数更多，其中 FireFox 中 URL 的最大长度限制是 65536 个字符，Chrome 则是 8182 个字符。 这个长度限制也不是针对数据部分，而是针对整个 URL。 12.HTTP 请求的过程与原理？HTTP 是基于 TCP/IP 协议的应用层协议，它使用 TCP 作为传输层协议，通过建立 TCP 连接来传输数据。HTTP 遵循标准的客户端-服务器模型，客户端打开连接发出请求，然后等待服务器返回的响应。 在浏览器输入 URL 后，浏览器首先会通过 DNS 解析获取到服务器的 IP 地址，然后与服务器建立 TCP 连接。 TCP 连接建立后，浏览器会向服务器发送 HTTP 请求。 服务器收到请求后，会根据请求的信息处理请求。 处理完请求后，服务器会返回一个 HTTP 响应给浏览器。 浏览器收到响应后，会根据响应的信息渲染页面。然后，浏览器和服务器断开 TCP 连接。 客户端发送一个请求到服务器，服务器处理请求并返回一个响应。这个过程是同步的，也就是说，客户端在发送请求后必须等待服务器的响应。在等待响应的过程中，客户端不会发送其他请求。 怎么利用多线程来下载一个数据呢？可以采取分块下载的策略。首先，通过 HEAD 请求获取文件的总大小。然后根据文件大小和线程数，将文件进行切割。每个线程负责下载一个特定范围的数据。 可以通过设置 HTTP 请求头的 Range 字段指定下载的字节区间。例如，Range: bytes0-1023 表示下载文件的前 1024 字节。 最后启动多线程下载。 代码片段 1：获取文件大小 URL url = new URL(https://javabetter.cn/file.zip);HttpURLConnection connection = (HttpURLConnection) url.openConnection();connection.setRequestMethod(HEAD);int fileSize = connection.getContentLength(); // 获取文件大小connection.disconnect(); 代码片段 2：下载文件 public void downloadChunk(String url, int start, int end, String outputPath) try URL fileUrl = new URL(url); HttpURLConnection connection = (HttpURLConnection) fileUrl.openConnection(); connection.setRequestProperty(Range, bytes= + start + - + end); InputStream inputStream = connection.getInputStream(); RandomAccessFile file = new RandomAccessFile(outputPath, rw); file.seek(start); // 定位到文件的相应位置 byte[] buffer = new byte[1024]; int bytesRead; while ((bytesRead = inputStream.read(buffer)) != -1) file.write(buffer, 0, bytesRead); file.close(); inputStream.close(); connection.disconnect(); catch (IOException e) e.printStackTrace(); 代码片段 3：启动多线程下载 int numThreads = 4;int fileSize = 100000000; // 假设文件大小为 100MBint chunkSize = fileSize / numThreads;String url = https://javabetter.cn/file.zip;String outputPath = path/to/local/file.zip;ExecutorService executor = Executors.newFixedThreadPool(numThreads);for (int i = 0; i numThreads; i++) int start = i * chunkSize; int end = (i == numThreads - 1) ? fileSize - 1 : (start + chunkSize - 1); executor.execute(() - downloadChunk(url, start, end, outputPath));executor.shutdown(); 如果只要下载数据的前十个字节呢？只需要设置 Range 字段为 Range: bytes0-9 即可。 13.说一下 HTTP 的报文结构？HTTP 的报文结构分为：请求报文和响应报文。两者在结构上很相似，都包含了起始行、头部和消息正文。 说下 HTTP 的请求报文结构？请求报文由请求行、请求头部、空行和消息正文组成。如下所示： GET /index.html HTTP/1.1Host: www.javabetter.cnAccept: text/htmlUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3 ①、请求行包括请求方法、请求 URL 和 HTTP 协议的版本。例如：GET /index.html HTTP/1.1。 ②、请求头部包含请求的附加信息，如客户端想要接收的内容类型、浏览器类型等。例如： Host: www.javabetter.cn，表示请求的主机名（域名） Accept: text/html，表示客户端可以接收的媒体类型 User-Agent: Mozilla/5.0，表示客户端的浏览器类型 Range：用于指定请求内容的范围，如断点续传时表示请求的字节范围。 ③、请求头部和消息正文之间有一个空行，表示请求头部结束。 ④、消息正文是可选的，如 POST 请求中的表单数据；GET 请求中没有消息正文。 说下 HTTP 响应报文结构？HTTP/1.0 200 OKContent-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84html bodyJakicDong/body/html ①、状态行包括 HTTP 协议的版本、状态码（如 200、404）和状态消息（如 OK、NotFound）。例如：HTTP/1.0 200 OK。 ②、响应头部包含响应的附加信息，如服务器类型、内容类型、内容长度等。也是键值对，例如： Content-Type: text/plain，表示响应的内容类型 Content-Length: 137582，表示响应的内容长度 Expires: Thu, 05 Dec 1997 16:00:00 GMT，表示资源的过期时间 Last-Modified: Wed, 5 August 1996 15:55:28 GMT，表示资源的最后修改时间 Server: Apache 0.84，表示服务器类型 ③、空行表示响应头部结束。 ④、消息正文（可选）响应的具体内容，如 HTML 页面。不是所有的响应都有消息正文，如 204 No Content 状态码的响应。 14.URI 和 URL 有什么区别? URI，统一资源标识符(Uniform Resource Identifier， URI)，标识的是 Web 上每一种可用的资源，如 HTML 文档、图像、视频片段、程序等都是由一个 URI 进行标识的。 URL，统一资源定位符（Uniform Resource Location），它是 URI 的一种子集，主要作用是提供资源的路径。 它们的主要区别在于，URL 除了提供了资源的标识，还提供了资源访问的方式。这么比喻，URI 像是身份证，可以唯一标识一个人，而 URL 更像一个住址，可以通过 URL 找到这个人——人类住址协议:地球中国北京市海淀区xx 职业技术学院14 号宿舍楼525 号寝张三.男。 15.说下 HTTP1.0，1.1，2.0 的区别？HTTP1.0 默认是短连接，HTTP 1.1 默认是长连接，HTTP 2.0 采用的多路复用。 说下 HTTP1.0 无状态协议：HTTP 1.0 是无状态的，每个请求之间相互独立，服务器不保存任何请求的状态信息。 非持久连接：默认情况下，每个 HTTP 请求响应对之后，连接会被关闭，属于短连接。这意味着对于同一个网站的每个资源请求，如 HTML 页面上的图片和脚本，都需要建立一个新的 TCP 连接。可以设置Connection: keep-alive 强制开启长连接。 说下 HTTP1.1 持久连接：HTTP 1.1 引入了持久连接（也称为 HTTP keep-alive），默认情况下不会立即关闭连接，可以在一个连接上发送多个请求和响应。极大减轻了 TCP 连接的开销。 流水线处理：HTTP 1.1 支持客户端在前一个请求的响应到达之前发送下一个请求，以提高传输效率。 说下 HTTP2.0 二进制协议：HTTP 2.0 使用二进制而不是文本格式来传输数据，解析更加高效。 多路复用：一个 TCP 连接上可以同时进行多个 HTTP 请求响应，解决了 HTTP 1.x 的队头阻塞问题。 头部压缩：HTTP 协议不带状态，所以每次请求都必须附上所有信息。HTTP 2.0 引入了头部压缩机制，可以使用 gzip 或 compress 压缩后再发送，减少了冗余头部信息的带宽消耗。 服务端推送：服务器可以主动向客户端推送资源，而不需要客户端明确请求。 16.HTTP3 了解吗？HTTP/2.0 基于 TCP 协议，而 HTTP/3.0 则基于 QUIC 协议，Quick UDP Connections，直译为快速 UDP 网络连接。 基于 TCP 的 HTTP2.0，尽管从逻辑上来说，不同的流之间相互独立，不会相互影响，但在实际传输的过程中，数据还是要一帧一帧的发送和接收，一旦某一个流的数据有丢包，仍然会阻塞在它之后传输的流数据。 而基于 UDP 的 QUIC 协议可以更彻底地解决这样的问题，让不同的流之间真正的实现相互独立传输，互不干扰。 同时，QUIC 协议在传输的过程中就完成了 TLS 加密握手，更直接了。 目前使用最广泛的是哪个HTTP版本？应该是 HTTP2，在 2022 年 1 月达到峰值，占所有网站的 46.9%。 统计网站：w3techs 17.HTTP 长连接了解吗？在 HTTP 中，长连接是指客户端和服务器之间在一次 HTTP 通信完成后，不会立即断开，而是保留连接以供后续请求复用。这种机制可以减少了频繁建立和关闭连接的开销 如何设置长连接？可以通过 Connection: keep-alive 实现。在 HTTP/1.1 中，长连接是默认开启的。 在什么时候会超时呢？ HTTP 一般会有 httpd 守护进程，里面可以设置 keep-alive timeout，当 tcp 连接闲置超过这个时间就会关闭，也可以在 HTTP 的 header 里面设置超时时间 TCP 的 keep-alive 包含三个参数，支持在系统内核的 net.ipv4 里面设置；当 TCP 连接之后，闲置了 tcp_keepalive_time，则会发生侦测包，如果没有收到对方的 ACK，那么会每隔 tcp_keepalive_intvl 再发一次，直到发送了 tcp_keepalive_probes，就会丢弃该连接。 1. tcp_keepalive_intvl = 152. tcp_keepalive_probes = 53. tcp_keepalive_time = 1800 18.说说 HTTP 与 HTTPS 有哪些区别？HTTPS 是 HTTP 的增强版，在 HTTP 的基础上加入了 SSL/TLS 协议，确保数据在传输过程中是加密的。 HTTP 的默认端⼝号是 80，URL 以http://开头；HTTPS 的默认端⼝号是 443，URL 以https://开头。 19.为什么要用 HTTPS？HTTP 是明文传输的，存在数据窃听、数据篡改和身份伪造等问题。而 HTTPS 通过引入 SSL/TLS，解决了这些问题。 SSL/TLS 在加密过程中涉及到了两种类型的加密方法： 非对称加密：服务器向客户端发送公钥，然后客户端用公钥加密自己的随机密钥，也就是会话密钥，发送给服务器，服务器用私钥解密，得到会话密钥。 对称加密：双方用会话密钥加密通信内容。 客户端会通过数字证书来验证服务器的身份，数字证书由 CA 签发，包含了服务器的公钥、证书的颁发机构、证书的有效期等。 20.HTTPS是怎么建立连接的？ HTTPS 的连接建立在 SSL/TLS 握手之上，其过程可以分为两个阶段：握手阶段和数据传输阶段。 ①、客户端向服务器发起请求 ②、服务器接收到请求后，返回自己的数字证书，包含了公钥、颁发机构等信息。 ③、客户端收到服务器的证书后，验证证书的合法性，如果合法，会生成一个随机码，然后用服务器的公钥加密这个随机码，发送给服务器。 ④、服务器收到会话密钥后，用私钥解密，得到会话密钥。 ⑤、客户端和服务器通过会话密码对通信内容进行加密，然后传输。 如果通信内容被截取，但由于没有会话密钥，所以无法解密。当通信结束后，连接会被关闭，会话密钥也会被销毁，下次通信会重新生成一个会话密钥。 HTTPS 会加密 URL 吗？HTTPS 通过 SSL/TLS 协议确保了客户端与服务器之间交换的数据被加密，这包括 HTTP 头部和正文。 而 URL 是 HTTP 头部的一部分，因此这部分信息也是加密的。 但因为涉及到 SSL 握手的过程，所以域名信息会被暴露出来，需要注意。 另外，完整的 URL 可能在 Web 服务器的日志中记录，这些日志可能是明文的。还有，URL 在浏览器历史记录中也是可见的。 因此，敏感信息永远不应该通过 URL 传递，即使是在使用 HTTPS 的情况下。 什么是中间人攻击？中间人攻击（Man-in-the-Middle, MITM）是一种常见的网络安全威胁，攻击者可以在通信的两端插入自己，以窃取通信双方的信息。 在很多电影中，都会存在这样的场景：主角通过某种方式，将自己伪装成中间人，然后窃取通信双方的信息，阿汤哥的碟中谍中就有很多类似的手笔。 中间人攻击是一个缺乏相互认证的攻击，因此大多数加密协议都会专门加入一些特殊的认证方法，以防止中间人攻击。像 SSL 协议，就是通过验证服务器的数字证书，是否由 CA（权威的受信任的数字证书认证机构）签发，来防止中间人攻击的。 HTTPS怎么保证建立的信道是安全的？主要通过 SSLTLS 协议的多层次安全机制，首先在握手阶段，客户端和服务器使用得是非对称加密，生成的会话密钥只有服务器的私钥才能解密，而私钥只有服务器持有。 在数据传输阶段，即使攻击者拦截了通信数据，没有会话密钥也无法解密。 HTTPS 能抓包吗？可以，HTTPS 可以抓包，但因为通信内容是加密的，需要解密后才能查看。 其原理是通过一个中间人，伪造服务器证书，并取得客户端的信任，然后将客户端的请求转发给服务器，将服务器的响应转发给客户端，完成中间人攻击。 常用的抓包工具有 Wireshark、Fiddler、Charles 等。 21.客户端怎么去校验证书的合法性？推荐阅读：HTTPS 握手过程中，客户端如何验证证书的合法性 首先，所有的证书都是由 CA 机构签发的，CA 机构是一个受信任的第三方机构，它会对证书的申请者进行身份验证，然后签发证书。 CA 就像是网络世界的公安局，具有极高的可信度。 CA 签发证书的过程是非常严格的： 首先，CA 会把持有者的公钥、⽤途、颁发者、有效时间等信息打成⼀个包，然后对这些信息进⾏ Hash 计算，得到⼀个 Hash 值； 然后 CA 会使⽤⾃⼰的私钥将该 Hash 值加密，⽣成 Certificate Signature； 最后将 Certificate Signature 添加在⽂件证书上，形成数字证书。 22.如何理解 HTTP 协议是无状态的？HTTP 协议是无状态的，这意味着每个 HTTP 请求都是独立的，服务器不会保留任何关于客户端请求的历史信息。 换句话说，我家大门常打开，是人是神都欢迎，我不在乎，只要给钱，哦不，按规矩，一切好办。 每个 HTTP 请求都包含了所必须的信息，服务器在处理当前请求时，不依赖于之前的任何请求信息。 服务器不会记录任何客户端请求的状态，每次请求都像是第一次与服务器通信。 由于 HTTP 是无状态的，像用户的购物车状态就必须通过其他方式来保持，如在每次请求中传递用户的 ID，或者使用 Cookie 在客户端保存购物车状态。 那有什么办法记录状态呢？ Cookies：服务器通过 Set-Cookie 响应头将状态信息存储在客户端，客户端在后续请求中发送该 Cookie 以维持状态。 Session：服务器生成一个唯一的会话 ID，存储在 Cookie 中，并在服务器端维护与该会话 ID 关联的状态信息。 Token：使用 JWT（JSON Web Token）等机制在客户端存储状态信息，客户端在每次请求中发送该 Token。 23.说说 Session 和 Cookie 有什么联系和区别?先来看看什么是 Session 和 Cookie ： Cookie 是保存在客户端的一小块文本串的数据。客户端向服务器发起请求时，服务端会向客户端发送一个 Cookie，客户端就把 Cookie 保存起来。在客户端下次向同一服务器再发起请求时，Cookie 被携带发送到服务器。服务端可以根据这个 Cookie 判断用户的身份和状态。 Session 指的就是服务器和客户端一次会话的过程。它是另一种记录客户状态的机制。不同的是 cookie 保存在客户端浏览器中，而 session 保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这就是 session。客户端浏览器再次访问时只需要从该 session 中查找用户的状态。 Session 和 Cookie 到底有什么不同呢？ 存储位置不一样，Cookie 保存在客户端，Session 保存在服务器端。 存储数据类型不一样，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般有效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。 Session 和 Cookie 有什么关联呢？ 可以使用 Cookie 记录 Session 的标识。 用户第一次请求服务器时，服务器根据用户提交的信息，创建对应的 Session，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入 Cookie 中，同时 Cookie 记录此 SessionID 是属于哪个域名。 当用户第二次访问服务器时，请求会自动判断此域名下是否存在 Cookie 信息，如果存在，则自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到，说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。 分布式环境下 Session 怎么处理呢？ 分布式环境下，客户端请求经过负载均衡，可能会分配到不同的服务器上，假如一个用户的请求两次没有落到同一台服务器上，那么在新的服务器上就没有记录用户状态的 Session。 这时候怎么办呢？ 可以使用 Redis 等分布式缓存来存储 Session，在多台服务器之间共享。 客户端无法使用 Cookie 怎么办？ 有可能客户端无法使用 Cookie，比如浏览器禁用 Cookie，或者客户端是安卓、IOS 等等。 这时候怎么办？SessionID 怎么存？怎么传给服务端呢？ 首先是 SessionID 的存储，可以使用客户端的本地存储，比如浏览器的 sessionStorage。 接下来怎么传呢？ 拼接到 URL 里：直接把 SessionID 作为 URL 的请求参数 放到请求头里：把 SessionID 放到请求的 Header 里，比较常用。 TCP24.详细说一下 TCP 的三次握手机制TCP（传输控制协议）的三次握手机制是一种用于在两个 TCP 主机之间建立一个可靠连接的过程。这个机制确保了两端的通信是同步的，并且在数据传输开始前，双方都准备好了进行通信。 ①、第一次握手：SYN（最开始都是 CLOSE，之后服务器进入 LISTEN） 发起连接：客户端发送一个 TCP 报文段到服务器。这个报文段的头部中，SYN 位被设置为 1，表明这是一个连接请求。同时，客户端会随机选择一个序列号（Sequence Number），假设为 x，发送给服务器。 目的：客户端通知服务器它希望建立连接，并告知服务器自己的初始序列号。 状态：客户端进入 SYN_SENT 状态。 ②、第二次握手：SYN + ACK 确认并应答：服务器收到客户端的连接请求后，如果同意建立连接，它会发送一个应答 TCP 报文段给客户端。在这个报文段中，SYN 位和 ACK 位都被设置为 1。服务器也会选择自己的一个随机序列号，假设为 y，并将客户端的序列号加 1（即 x+1）作为确认号（Acknowledgment Number），发送给客户端。 目的：服务器告诉客户端，它的连接请求被接受了，并通知客户端自己的初始序列号。 状态：服务器进入 SYN_RCVD 状态。 ③、第三次握手：ACK 最终确认：客户端收到服务器的应答后，还需要向服务器发送一个确认。这个 TCP 报文段的 ACK 位被设置为 1，确认号被设置为服务器序列号加 1（即 y+1），而自己的序列号是 x+1。 目的：客户端确认收到了服务器的同步应答，完成三次握手，建立连接。 状态：客户端进入 ESTABLISHED 状态，当服务器接收到这个包时，也进入 ESTABLISHED 状态 用大白话讲 TCP 三次握手就是： 三十年前的农村，电话还没有普及，所以，通信基本靠吼。 老张和老王是邻居，这天老张下地了，结果家里有事，热心的邻居老王赶紧跑到村口，开始叫唤老王。 老王：老张唉！我是老王，你能听得到吗？ 老张一听，是老王的声音：老王老王，我是老张，我能听得到，你能听得到吗？ 老王一听，嗯，没错，是老张：老张，我听到了，我有事要跟你说。 “你老婆要生了，赶紧回去吧！” 老张风风火火地赶回家，老婆顺利地生了个带把的大胖小子。握手的故事充满了幸福和美满。 可以再举一个例子说明 TCP 三次握手吗？当然可以，你（客户端）在一个拥挤的聚会上遇到了你想交谈的美女（服务器）。因为周围很吵，你们需要确认对方都准备好交流，并清楚地听到对方说的每一句话。 ①、第一次握手：打招呼 你走向那个美女，大声说：“嘿，我是小二，我们可以聊聊吗？”（你发送了一个连接请求，告诉服务器你想深入交流，并提供了你的微信号x，也就是你们交谈的起点） ②、第二次握手：对方回应 美女一看你挺帅挺有气质，回答说：“嗨，我是小青，可以聊聊。”（服务器接受你的请求，同样愿意深入交流，告诉你它的微信号y，并确认了你的微信号x+1，表示它准备好了） ③、第三次握手：确认准备就绪 你听到美女的回答后，对她说：“太好了，我们以后就微信上聊吧。”（你确认了美女的回答，也告诉她你准备好开始了，通过发送确认号y+1） ④、聊天开始 这时候，你们两个就确认彼此都准备好深入交流了，可以开始你们的对话了。 说说 SYN 的概念？SYN 是 TCP 协议中用来建立连接的一个标志位，全称为 Synchronize Sequence Numbers，也就是同步序列编号。 SYN 不仅确保了序列号的同步，使得后续的数据能够有序传输，还能防止旧的报文段被误认为是新连接。 25.TCP 握手为什么是三次，为什么不能是两次？不能是四次？使用三次握手可以建立一个可靠的连接。这一过程的目的是确保双方都知道对方已准备好进行通信，并同步双方的序列号，从而保持数据包的顺序和完整性。 为什么 TCP 握手不能是两次？ 为了防止服务器一直等，等到黄花菜都凉了。 为了防止客户端已经失效的连接请求突然又传送到了服务器。 要知道，网络传输是有延时的（要通过网络光纤、WIFI、卫星信号传输等）。 假如说客户端发起了 SYN1 的第一次握手。服务器也及时回复了 SYN2 和 ACK1 的第二次握手，但是这个 ACK1 的确认报文段因为某些原因在传输过程中丢失了。 如果没有第三次握手告诉服务器，客户端收到了服务器的回应，那服务器是不知道客户端有没有接收到的。 于是服务器就一直干巴巴地开着端口在等着客户端发消息呢，但其实客户端并没有收到服务器的回应，心灰意冷地跑了。 还有一种情况是，一个旧的、延迟的连接请求（SYN1）被服务器接受，导致服务器错误地开启一个不再需要的连接。 举个例子：假设你（客户端）给你的朋友（服务器）发送了一个邮件（连接请求）。因为某些原因，这封邮件迟迟没有到达朋友那里，可能是因为邮局的延误。于是你决定再发一封新的邮件。朋友收到了第二封邮件，你们成功地建立了连接并开始通信。 但是，过了很久，那封延误的旧邮件突然也到了你朋友那里。如果没有一种机制来识别和处理这种延误的邮件，你的朋友可能会以为这是一个新的连接请求，并尝试响应它，但其实你已经重新发了请求，原来的不需要了。这就导致了不必要的混乱和资源浪费。 所以我们需要“三次握手”来确认这个过程： 第一次握手：客户端发送 SYN 包（连接请求）给服务器，如果这个包延迟了，客户端不会一直等待，它可能会重试并发送一个新的连接请求。第二次握手：服务器收到 SYN 包后，发送一个 SYN-ACK 包（确认接收到连接请求）回客户端。第三次握手：客户端收到 SYN-ACK 包后，再发送一个 ACK 包给服务器，确认收到了服务器的响应。 为什么不是四次？三次握手已经足够创建可靠的连接了，没有必要再多一次握手。 什么是泛洪攻击？泛洪攻击（SYN Flood Attack）是一种常见的 DoS（拒绝服务）攻击，攻击者会发送大量的伪造的 TCP 连接请求，导致服务器资源耗尽，无法处理正常的连接请求。 半连接服务拒绝，也称为 SYN 洪泛攻击或 SYN Flood。 所谓的半连接就是指在 TCP 的三次握手过程中，当服务器接收到来自客户端的第一个 SYN 包后，它会回复一个 SYN-ACK 包，此时连接处于“半开”状态，因为连接的建立还需要客户端发送最后一个 ACK 包。 在收到最后的 ACK 包之前，服务器会为这个尚未完成的连接分配一定的资源，并在它的队列中保留这个连接的位置。 如果让你重新设计，怎么设计？如果重新设计 TCP 的连接建立过程，可以考虑引入 SYN cookies，这种技术通过在 SYN-ACK 响应中编码连接信息，从而在不占用大量资源的情况下验证客户端。 26.三次握手中每一次没收到报文会发生什么情况？ 第一次握手服务端未收到 SYN 报文 服务端不会进行任何的动作，而客户端由于一段时间内没有收到服务端发来的确认报文，等待一段时间后会重新发送 SYN 报文，如果仍然没有回应，会重复这个过程，直到发送次数超过最大重传次数限制，就会返回连接建立失败。 第二次握手客户端未收到服务端响应的 ACK 报文 客户端会继续重传，直到次数限制；而服务端此时会阻塞在 accept()处，等待客户端发送 ACK 报文 第三次握手服务端为收到客户端发送过来的 ACK 报文 服务端同样会采用类似客户端的超时重传机制，如果重试次数超过限制，则 accept()调用返回-1，服务端建立连接失败；而此时客户端认为自己已经建立连接成功，因此开始向服务端发送数据，但是服务端的 accept()系统调用已经返回，此时不在监听状态，因此服务端接收到客户端发送来的数据时会发送 RST 报文给客户端，消除客户端单方面建立连接的状态。 27.第二次握手传回了 ACK，为什么还要传回 SYN？ACK 是为了告诉客户端传来的数据已经接收无误。 而传回 SYN 是为了告诉客户端，服务端响应的确实是客户端发送的报文。 28.第 3 次握手可以携带数据吗？第 3 次握手是可以携带数据的。 此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，它已经建立连接成功，并且确认服务端的接收和发送能力是正常的。 第一次握手不能携带数据是出于安全的考虑，因为如果允许携带数据，攻击者每次在 SYN 报文中携带大量数据，就会导致服务端消耗更多的时间和空间去处理这些报文，会造成 CPU 和内存的消耗。 29.了解 TCP 半连接状态吗？TCP 半连接指的是在 TCP 三次握手过程中，服务器接收到了客户端的 SYN 包，但还没有完成第三次握手，此时的连接处于一种未完全建立的状态。 如果服务器回复了 SYN-ACK，但客户端还没有回复 ACK，该连接将一直保留在半连接队列中，直到超时或被拒绝。 说说半连接队列？TCP 进入三次握手前，服务端会从 CLOSED 状态变为 LISTEN 状态, 同时在内部创建了两个队列：半连接队列（SYN 队列）和全连接队列（ACCEPT 队列）。 顾名思义，半连接队列存放的是三次握手未完成的连接，全连接队列存放的是完成三次握手的连接。 TCP 三次握手时，客户端发送 SYN 到服务端，服务端收到之后，便回复 ACK 和 SYN，状态由 LISTEN 变为 SYN_RCVD，此时这个连接就被推入了 SYN 队列，即半连接队列。 当客户端回复 ACK, 服务端接收后，三次握手就完成了。这时连接会等待被具体的应用取走，在被取走之前，它被推入 ACCEPT 队列，即全连接队列。 什么是 SYN Flood ？SYN Flood 是一种典型的 DDos 攻击，它在短时间内，伪造不存在的 IP 地址, 向服务器发送大量 SYN 报文。当服务器回复 SYN+ACK 报文后，不会收到 ACK 回应报文，那么 SYN 队列里的连接旧不会出对队，久⽽久之就会占满服务端的 SYN 接收队列（半连接队列），使得服务器不能为正常⽤户服务。 那有什么应对方案呢？主要有 syn cookie 和 SYN Proxy 防火墙等。 syn cookie：在收到 SYN 包后，服务器根据一定的方法，以数据包的源地址、端口等信息为参数计算出一个 cookie 值作为自己的 SYNACK 包的序列号，回复 SYN+ACK 后，服务器并不立即分配资源进行处理，等收到发送方的 ACK 包后，重新根据数据包的源地址、端口计算该包中的确认序列号是否正确，如果正确则建立连接，否则丢弃该包。 SYN Proxy 防火墙：服务器防火墙会对收到的每一个 SYN 报文进行代理和回应，并保持半连接。等发送方将 ACK 包返回后，再重新构造 SYN 包发到服务器，建立真正的 TCP 连接。 30.说说 TCP 四次挥手的过程？TCP 连接的断开过程被形象地概括为四次挥手。 第一次挥手：客户端向服务器发送一个 FIN 结束报文，表示客户端没有数据要发送了，但仍然可以接收数据。客户端进入 FIN-WAIT-1 状态。 第二次挥手：服务器接收到 FIN 报文后，向客户端发送一个 ACK 报文，确认已接收到客户端的 FIN 请求。服务器进入 CLOSE-WAIT 状态，客户端进入 FIN-WAIT-2 状态。 第三次挥手：服务器向客户端发送一个 FIN 报文，表示服务器也没有数据要发送了。服务器进入 LAST-ACK 状态。 第四次挥手：客户端接收到 FIN 报文后，向服务器发送一个 ACK 报文，确认已接收到服务器的 FIN 请求。客户端进入 TIME-WAIT 状态，等待一段时间以确保服务器接收到 ACK 报文。服务器接收到 ACK 报文后进入 CLOSED 状态。客户端在等待一段时间后也进入 CLOSED 状态。 31.TCP 挥手为什么需要四次呢？因为 TCP 是全双工通信协议，数据的发送和接收需要两次一来一回，也就是四次，来确保双方都能正确关闭连接。 第一次挥手：客户端表示数据发送完成了，准备关闭，你确认一下。 第二次挥手：服务端回话说 ok，我马上处理完数据，稍等。 第三次挥手：服务端表示处理完了，可以关闭了。 第四次挥手：客户端说好，进入 TIME_WAIT 状态，确保服务端关闭连接后，自己再关闭连接。 32.TCP 四次挥手过程中，为什么需要等待 2MSL, 才进入 CLOSED 关闭状态？ 为什么需要等待？ 1. 为了保证客户端发送的最后一个 ACK 报文段能够到达服务端。 这个 ACK 报文段有可能丢失，因而使处在 LAST-ACK 状态的服务端就收不到对已发送的 FIN + ACK 报文段的确认。服务端会超时重传这个 FIN+ACK 报文段，而客户端就能在 2MSL 时间内（超时 + 1MSL 传输）收到这个重传的 FIN+ACK 报文段。接着客户端重传一次确认，重新启动 2MSL 计时器。最后，客户端和服务器都正常进入到 CLOSED 状态。 2. 防止已失效的连接请求报文段出现在本连接中。客户端在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。 为什么等待的时间是 2MSL？ MSL 是 Maximum Segment Lifetime，报⽂最⼤⽣存时间，它是任何报⽂在⽹络上存在的最⻓时间，超过这个时间报⽂将被丢弃。 TIME_WAIT 等待 2 倍的 MSL，⽐较合理的解释是：⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，所以⼀来⼀回需要等待 2 倍的时间。 ⽐如如果被动关闭⽅没有收到断开连接的最后的 ACK 报⽂，就会触发超时重发 Fin 报⽂，另⼀⽅接收到 FIN 后，会重发 ACK 给被动关闭⽅， ⼀来⼀去正好 2 个 MSL。 33.保活计时器有什么用？除时间等待计时器外，TCP 还有一个保活计时器（keepalive timer）。 设想这样的场景：客户已主动与服务器建立了 TCP 连接。但后来客户端的主机突然发生故障。显然，服务器以后就不能再收到客户端发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就需要使用保活计时器了。 服务器每收到一次客户端的数据，就重新设置保活计时器，时间的设置通常是两个小时。若两个小时都没有收到客户端的数据，服务端就发送一个探测报文段，以后则每隔 75 秒钟发送一次。若连续发送 10 个探测报文段后仍然无客户端的响应，服务端就认为客户端出了故障，接着就关闭这个连接。 34.CLOSE-WAIT 和 TIME-WAIT 的状态和意义？CLOSE-WAIT 状态有什么意义？服务端收到客户端关闭连接的请求并确认之后，就会进入 CLOSE-WAIT 状态。此时服务端可能还有一些数据没有传输完成，因此不能立即关闭连接，而 CLOSE-WAIT 状态就是为了保证服务端在关闭连接之前将待发送的数据处理完。 TIME-WAIT 有什么意义？TIME-WAIT 发生在第四次挥手，当客户端在发送 ACK 确认对方的 FIN 报文后，会进入 TIME_WAIT 状态。 它存在的意义主要有两个： 在 TIME_WAIT 状态中，客户端可以重新发送 ACK 确保对方正常关闭连接。 在 TIME_WAIT 持续的 2MSL 时间后，确保旧数据包完全消失，避免它们干扰未来建立的新连接。 补充：MSL（Maximum Segment Lifetime）：TCP 报文段在网络中的最大存活时间，通常为 30 秒到 2 分钟 35.TIME_WAIT 状态过多会导致什么问题？怎么解决？ TIME_WAIT 状态过多会导致什么问题? 如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器⽅主动发起的断开请求。 过多的 TIME-WAIT 状态主要的危害有两种： 第⼀是内存资源占⽤； 第⼆是对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝； 怎么解决 TIME_WAIT 状态过多？ 服务器可以设置 SO_REUSEADDR 套接字来通知内核，如果端口被占用，但是 TCP 连接位于 TIME_WAIT 状态时可以重用端口。 还可以使用长连接的方式来减少 TCP 的连接和断开，在长连接的业务里往往不需要考虑 TIME_WAIT 状态。 36.说说 TCP 报文头部的格式？一个 TCP 报文段主要由报文段头部（Header）和数据两部分组成。头部包含了确保数据可靠传输所需的各种控制信息，比如说序列号、确认号、窗口大小等。 源端口号（Source Port）：16 位（2 个字节），用于标识发送端的应用程序。 目标端口号（Destination Port）：也是 16 位，用于标识接收端的应用程序。 序列号（Sequence Number）：32 位，用于标识从 TCP 发送者发送的数据字节流中的第一个字节的顺序号。确保数据按顺序接收。 确认号（Acknowledgment Number）：32 位，如果 ACK 标志被设置，则该字段包含发送确认的序列号，即接收 TCP 希望收到的下一个序列号。 数据偏移（Data Offset）：4 位，表示 TCP 报文头部的长度，用于指示数据开始的位置。 保留（Reserved）：6 位，为将来使用预留，目前必须置为 0。 控制位（Flags）：共 6 位，包括 URG（紧急指针字段是否有效）、ACK（确认字段是否有效）、PSH（提示接收端应该尽快将这个报文段交给应用层）、RST（重置连接）、SYN（同步序号，用于建立连接）、FIN（结束发送数据）。 窗口大小（Window）：16 位，用于流量控制，表示接收端还能接收的数据的字节数（基于接收缓冲区的大小）。 校验和（Checksum）：16 位，覆盖整个 TCP 报文段（包括 TCP 头部、数据和一个伪头部）的校验和，用于检测数据在传输过程中的任何变化。 紧急指针（Urgent Pointer）：16 位，只有当 URG 控制位被设置时才有效，指出在报文段中有紧急数据的位置。 37.TCP 为什么可靠？TCP 首先通过三次握手和四次挥手来保证连接的可靠性，然后通过校验和、序列号、确认应答、超时重传、滑动窗口等机制来保证数据的可靠传输。 推荐阅读：TCP 校验和计算方法 ①、校验和：TCP 报文段包括一个校验和字段，用于检测报文段在传输过程中的变化。如果接收方检测到校验和错误，就会丢弃这个报文段。 ②、序列号确认机制：TCP 将数据分成多个小段，每段数据都有唯一的序列号，以确保数据包的顺序传输和完整性。同时，发送方如果没有收到接收方的确认应答，会重传数据。 ③、流量控制：接收方会发送窗口大小告诉发送方它的接收能力。发送方会根据窗口大小调整发送速度，避免网络拥塞。 ④、超时重传：如果发送方发送的数据包超过了最大生存时间，接收方还没有收到，发送方会重传数据包以保证丢失数据重新传输。 ⑤、拥塞控制：TCP 会采用慢启动的策略，一开始发的少，然后逐步增加，当检测到网络拥塞时，会降低发送速率。在网络拥塞缓解后，传输速率也会自动恢复。 38.说说 TCP 的流量控制？TCP 提供了一种机制，可以让发送端根据接收端的实际接收能力控制发送的数据量，这就是流量控制。TCP 通过滑动窗口来控制流量，我们看下简要流程： 首先双方三次握手，初始化各自的窗口大小，均为 400 个字节。 假如当前发送方给接收方发送了 200 个字节，那么，发送方的SND.NXT会右移 200 个字节，也就是说当前的可用窗口减少了 200 个字节。 接受方收到后，放到缓冲队列里面，REV.WND 400-200200 字节，所以 win200 字节返回给发送方。接收方会在 ACK 的报文首部带上缩小后的滑动窗口 200 字节 发送方又发送 200 字节过来，200 字节到达，继续放到缓冲队列。不过这时候，由于大量负载的原因，接受方处理不了这么多字节，只能处理 100 字节，剩余的 100 字节继续放到缓冲队列。这时候，REV.WND 400-200-100100 字节，即 win100 返回发送方。 发送方继续发送 100 字节过来，这时候，接收窗口 win 变为 0。 发送方停止发送，开启一个定时任务，每隔一段时间，就去询问接受方，直到 win 大于 0，才继续开始发送。 39.详细说说 TCP 的滑动窗口？TCP 发送一个数据，如果需要收到确认应答，才会发送下一个数据。这样的话就会有个缺点：效率会比较低。 “用一个比喻，我们在微信上聊天，你打完一句话，我回复一句之后，你才能打下一句。假如我没有及时回复呢？你是把话憋着不说吗？然后傻傻等到我回复之后再接着发下一句？” 为了解决这个问题，TCP 引入了窗口，它是操作系统开辟的一个缓存空间。窗口大小值表示无需等待确认应答，而可以继续发送数据的最大值。 TCP 头部有个字段叫 win，也即那个 16 位的窗口大小，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度，从而达到流量控制的目的。 “通俗点讲，就是接受方每次收到数据包，在发送确认报文的时候，同时告诉发送方，自己的缓存区还有多少空余空间，缓冲区的空余空间，我们就称之为接受窗口大小。这就是 win。” TCP 滑动窗口分为两种: 发送窗口和接收窗口。发送端的滑动窗口包含四大部分，如下： 已发送且已收到 ACK 确认 已发送但未收到 ACK 确认 未发送但可以发送 未发送也不可以发送 深蓝色框里就是发送窗口。 SND.WND: 表示发送窗口的大小, 上图虚线框的格子数是 10 个，即发送窗口大小是 10。 SND.NXT：下一个发送的位置，它指向未发送但可以发送的第一个字节的序列号。 SND.UNA: 一个绝对指针，它指向的是已发送但未确认的第一个字节的序列号。 接收方的滑动窗口包含三大部分，如下： 已成功接收并确认 未收到数据但可以接收 未收到数据并不可以接收的数据 蓝色框内，就是接收窗口。 REV.WND: 表示接收窗口的大小, 上图虚线框的格子就是 9 个。 REV.NXT: 下一个接收的位置，它指向未收到但可以接收的第一个字节的序列号。 40.了解 Nagle 算法和延迟确认吗？ Nagle 算法和延迟确认是干什么的？ 当我们 TCP 报⽂的承载的数据⾮常⼩的时候，例如⼏个字节，那么整个⽹络的效率是很低的，因为每个 TCP 报⽂中都会有 20 个字节的 TCP 头部，也会有 20 个字节的 IP 头部，⽽数据只有⼏个字节，所以在整个报⽂中有效数据占有的比例就会⾮常低。 这就好像快递员开着⼤货⻋送⼀个⼩包裹⼀样浪费。 那么就出现了常⻅的两种策略，来减少⼩报⽂的传输，分别是： Nagle 算法 延迟确认 Nagle 算法 Nagle 算法：任意时刻，最多只能有一个未被确认的小段。所谓 “小段”，指的是小于 MSS 尺寸的数据块，所谓 “未被确认”，是指一个数据块发送出去后，没有收到对方发送的 ACK 确认该数据已收到。 Nagle 算法的策略： 没有已发送未确认报⽂时，⽴刻发送数据。 存在未确认报⽂时，直到「没有已发送未确认报⽂」或「数据⻓度达到 MSS ⼤⼩」时，再发送数据。 只要没满⾜上⾯条件中的⼀条，发送⽅⼀直在囤积数据，直到满⾜上⾯的发送条件。 延迟确认 事实上当没有携带数据的 ACK，它的⽹络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报⽂。 为了解决 ACK 传输效率低问题，所以就衍⽣出了 TCP 延迟确认。 TCP 延迟确认的策略： 当有响应数据要发送时，ACK 会随着响应数据⼀起⽴刻发送给对⽅ 当没有响应数据要发送时，ACK 将会延迟⼀段时间，以等待是否有响应数据可以⼀起发送 如果在延迟等待发送 ACK 期间，对⽅的第⼆个数据报⽂⼜到达了，这时就会⽴刻发送 ACK 一般情况下，Nagle 算法和延迟确认不能一起使用，Nagle 算法意味着延迟发，延迟确认意味着延迟接收，两个凑在一起就会造成更大的延迟，会产生性能问题。 41.说说 TCP 的拥塞控制？什么是拥塞控制？流量控制是为了避免发送⽅的数据填满接收⽅的缓存，但并不能控制整个⽹络。 ⼀般来说，计算机⽹络会处在⼀个共享的环境。因此也有可能会因为其他主机之间的通信使得⽹络拥堵。 当⽹络出现拥堵时，如果继续发送⼤量数据包，可能会导致数据包延时、丢失等，这时 TCP 就会重传数据，但重传会增加⽹络负担，于是会导致更⼤的延迟以及更多的丢包，就进⼊了恶性循环…. 所以，TCP 被设计成了⼀个非常⽆私的协议，当⽹络发送拥塞时，TCP 会⾃我牺牲，降低发送的数据流。 拥塞控制的⽬的就是避免发送⽅的数据填满整个⽹络。 就像是一个水管，不能让太多的水（数据流）流入水管，如果超过水管的承受能力，水管会被撑爆（丢包）。 发送方会维护一个拥塞窗口 cwnd 的变量，调节所要发送数据的量。 什么是拥塞窗⼝？和发送窗⼝有什么关系呢？拥塞窗⼝ cwnd是发送⽅维护的⼀个的状态变量，它会根据⽹络的拥塞程度动态变化的。 发送窗⼝ swnd 和接收窗⼝ rwnd 是约等于的关系，那么由于加⼊了拥塞窗⼝的概念后，此时发送窗⼝的值是 swnd = min(cwnd, rwnd)，也就是拥塞窗⼝和接收窗⼝中的最⼩值。 拥塞窗⼝ cwnd 变化的规则： 只要⽹络中没有出现拥塞， cwnd 就会增⼤； 但⽹络中出现了拥塞， cwnd 就减少； 拥塞控制有哪些常用算法？拥塞控制主要有这几种常用算法： 慢启动 拥塞避免 拥塞发生 快速恢复 ①、慢启动算法 慢启动算法，慢慢启动。 它表示 TCP 建立连接完成后，一开始不要发送大量的数据，而是先探测一下网络的拥塞程度。由小到大逐渐增加拥塞窗口的大小，如果没有出现丢包，每收到一个 ACK，就将拥塞窗口 cwnd 大小就加 1（单位是 MSS）。每轮次发送窗口增加一倍，呈指数增长，如果出现丢包，拥塞窗口就减半，进入拥塞避免阶段。 举个例子： 连接建⽴完成后，⼀开始初始化 cwnd 1 ，表示可以传⼀个 MSS ⼤⼩的数据。 当收到⼀个 ACK 确认应答后，cwnd 增加 1，于是⼀次能够发送 2 个 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以⽐之前多发 2 个，所以这⼀次能够发送 4 个 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以⽐之前多发 4 个，所以这⼀次能够发送 8 个。 发包的个数是指数性的增⻓。 为了防止 cwnd 增长过大引起网络拥塞，还需设置一个慢启动阀值 ssthresh（slow start threshold）状态变量。当cwnd到达该阀值后，就好像水管被关小了水龙头一样，减少拥塞状态。即当 cwnd ssthresh 时，进入了拥塞避免算法。 ②、拥塞避免算法一般来说，慢启动阀值 ssthresh 是 65535 字节，cwnd到达慢启动阀值后 每收到一个 ACK 时，cwnd cwnd + 1cwnd 当每过一个 RTT 时，cwnd cwnd + 1 显然这是一个线性上升的算法，避免过快导致网络拥塞问题。接着上面慢启动的例子，假定 ssthresh 为 8 ： 当 8 个 ACK 应答确认到来时，每个确认增加 18，8 个 ACK 确认 cwnd ⼀共增加 1，于是这⼀次能够发送 9 个 MSS ⼤⼩的数据，变成了线性增⻓。 ③、拥塞发生 当网络拥塞发生丢包时，会有两种情况： RTO 超时重传 快速重传 如果是发生了 RTO 超时重传，就会使用拥塞发生算法 慢启动阀值 sshthresh cwnd 2 cwnd 重置为 1 进入新的慢启动过程 这种方式就像是飙车的时候急刹车，还飞速倒车，这。。。 其实还有更好的处理方式，就是快速重传。发送方收到 3 个连续重复的 ACK 时，就会快速地重传，不必等待 RTO 超时再重传。 发⽣快速重传的拥塞发⽣算法： 拥塞窗口大小 cwnd = cwnd/2慢启动阀值 ssthresh = cwnd进入快速恢复算法 ④、快速恢复 快速重传和快速恢复算法一般同时使用。快速恢复算法认为，还有 3 个重复 ACK 收到，说明网络也没那么糟糕，所以没有必要像 RTO 超时那么强烈。 正如前面所说，进入快速恢复之前，cwnd 和 sshthresh 已被更新： cwnd cwnd 2 sshthresh cwnd 然后，进⼊快速恢复算法如下： cwnd sshthresh + 3 重传重复的那几个 ACK（即丢失的那几个数据包） 如果再收到重复的 ACK，那么 cwnd cwnd +1 如果收到新数据的 ACK 后, cwnd sshthresh。因为收到新数据的 ACK，表明恢复过程已经结束，可以再次进入了拥塞避免的算法了。 42.说说 TCP 的重传机制？超时重传机制是 TCP 的核心之一，它能确保在网络传输中如果某些数据包丢失或没有及时到达的话，TCP 能够重新发送这些数据包，以保证数据完整性。 其原理是在发送某个数据后开启一个计时器，如果在一定时间内没有得到发送数据报的 ACK 报文，就重新发送数据，直到发送成功为止。 重传包括超时重传、快速重传、带选择确认的重传（SACK）和重复 SACK 四种。 超时时间应该设置为多少呢？TCP 中的重传超时时间（RTO，Retransmission Timeout）不是一个固定的值，而是动态计算的，目的是为了适应不同的网络条件。 RTO 有个标准方法的计算公式，叫 Jacobson Karels 算法。 ①、计算 SRTT（Smoothed RTT，平滑往返时间），以避免单次测量中的抖动影响重传时间。 SRTT = (1 - α) * SRTT + α * RTT 其中，α 是一个常量，通常取值为 0.125（即18），表示新测量值对平滑RTT的影响比例。 RTT，也就是 Round-Trip Time，往返时间，即数据包从发送到接收到确认的时间。TCP 会对每个数据包的 RTT 进行测量，并不断更新这个值。 ②、计算 RTTVAR (RTT Variation，表示RTT的变化量，用于衡量RTT的波动) RTTVAR = (1 - β) * RTTVAR + β * (|RTT - SRTT|) β 通常取值为 0.25（即14），表示对RTTVAR更新的权重。 ③、最后，得出最终的 RTO RTO = SRTT + max(G, 4 x RTTVAR) G 是一个小的常量偏移量，用来防止RTO过小。一般来说，G 的值通常是1毫秒。 一般来说，RTO 略微大于 RTT，效果是最佳的。 如果 RTO 设置很大，可能等了很久都没有重发。 如果 RTO 设置很小，那很可能数据还没有丢失，就开始重发了。 超时重传不是十分完美的重传方案，它有这些缺点： 当报文丢失时，需要等待一定的超时周期，才开始重传。 当报文丢失时，在等待超时的过程中，可能会出现这种情况：后面的报文已经被接收端接收了但却迟迟得不到确认，发送端会认为也丢失了，从而引起不必要的重传。 并且，对于 TCP 来说，如果发生一次超时重传，下次的时间间隔就会加倍。 什么是快速重传？TCP 还有另外⼀种快速重传（Fast Retransmit）机制，它不以时间为驱动，⽽是以数据驱动重传。 它不以时间驱动，而是以数据驱动。它是基于接收端的反馈信息来引发重传的。 可以用它来解决超时重发的时间等待问题，快速重传流程如下： 在上图，发送⽅发出了 1，2，3，4，5 份数据： 第⼀份 Seq1 先送到了，于是就 Ack 回 2； 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2； 后⾯的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到； 发送端收到了三个 Ack 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 快速重传机制只解决了⼀个问题，就是超时时间的问题，但是它依然⾯临着另外⼀个问题。就是重传的时候，是重传之前的⼀个，还是重传所有的问题。 ⽐如对于上⾯的例⼦，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的三个 Ack 2 是谁传回来的。 根据 TCP 不同的实现，以上两种情况都是有可能的。可⻅，这是⼀把双刃剑。 为了解决不知道该重传哪些 TCP 报⽂，于是就有 SACK ⽅法。 什么是带选择确认的重传(SACK)为了解决应该重传多少个包的问题? TCP 提供了带选择确认的重传（即 SACK，Selective Acknowledgment）。 SACK 机制就是，在快速重传的基础上，接收方返回最近收到报文段的序列号范围，这样发送方就知道接收方哪些数据包是没收到的。这样就很清楚应该重传哪些数据包。 如上图中，发送⽅收到了三次同样的 ACK 确认报⽂，于是就会触发快速重发机制，通过 SACK 信息发现只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进⾏重发。 什么是重复 SACK（D-SACK）D-SACK，英文是 Duplicate SACK，是在 SACK 的基础上做了一些扩展，主要用来告诉发送方，有哪些数据包，自己重复接受了。 DSACK 的目的是帮助发送方判断，是否发生了包失序、ACK 丢失、包重复或伪重传。让 TCP 可以更好的做网络流控。 例如 ACK 丢包导致的数据包重复： 接收⽅发给发送⽅的两个 ACK 确认应答都丢失了，所以发送⽅超时后，重传第⼀个数据包（3000 ~3499） 于是接收⽅发现数据是重复收到的，于是回了⼀个 SACK 30003500，告诉「发送⽅」 30003500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 D-SACK 。这样发送⽅就知道了，数据没有丢，是接收⽅的 ACK 确认报⽂丢了。 43.说说 TCP 的粘包和拆包？TCP 的粘包和拆包更多的是业务上的概念！ 什么是 TCP 粘包和拆包？ TCP 是面向流，没有界限的一串数据。TCP 底层并不了解上层业务数据的具体含义，它会根据 TCP 缓冲区的实际情况进行包的划分，所以在业务上认为，一个完整的包可能会被 TCP 拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的 TCP 粘包和拆包问题。 为什么会产生粘包和拆包呢? 要发送的数据小于 TCP 发送缓冲区的大小，TCP 将多次写入缓冲区的数据一次发送出去，将会发生粘包； 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包； 要发送的数据大于 TCP 发送缓冲区剩余空间大小，将会发生拆包； 待发送数据大于 MSS（最大报文长度），TCP 在传输前将进行拆包。即 TCP 报文长度 - TCP 头部长度 MSS。 那怎么解决呢？ 发送端将每个数据包封装为固定长度 在数据尾部增加特殊字符进行分割 将数据分为两部分，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小。 63.一个TCP连接可以发送多少次HTTP请求?（补充）一个 TCP 连接可以发送多少次 HTTP 请求，取决于 HTTP 协议的版本。 在 HTTP1.0 中，每个 HTTP 请求-响应使用一个单独的 TCP 连接。这意味着每次发送 HTTP 请求都需要建立一个新的 TCP 连接。 HTTP/1.1 引入了持久连接（Persistent Connection），默认情况下允许在一个 TCP 连接上发送多个 HTTP 请求。 通过使用 Connection: keep-alive 头部实现，保持连接打开状态，直到明确关闭为止。这极大地提高了效率，因为无需为每个请求都建立新的连接。 此外，HTTP/1.1 支持请求管道化（Pipelining），允许客户端在收到前一个响应之前发送多个请求。 HTTP2 进一步优化了连接复用，允许在单个 TCP 连接上同时发送多个请求和响应，这些请求和响应被分割成帧并通过流传输。HTTP2 的多路复用（Multiplexing）机制显著提高了并发性能和资源利用效率。 UDP44.说说 TCP 和 UDP 的区别？TCP 是面向连接的，而 UDP 是无连接的。 TCP 就像是打电话一对一私聊，UDP 就像是拿个大喇叭在广播。 在数据传输开始之前，TCP 需要先建立连接，数据传输完成后，再断开连接。这个过程通常被称为“三次握手”、“四次挥手”。 UDP 是无连接的，发送数据之前不需要建立连接，发送完毕也不需要断开，数据以数据报形式发送。 换句话说：TCP 是可靠的，它通过确认机制、重发机制等来保证数据的可靠传输。而 UDP 是不可靠的，数据包可能会丢失、重复、乱序。 说说 TCP 和 UDP 的应用场景？ TCP： 适用于那些对数据准确性要求高于数据传输速度的场合。例如：网页浏览、电子邮件、文件传输（FTP）、远程控制、数据库链接。 UDP： 适用于对速度要求高、可以容忍一定数据丢失的场合。例如：QQ 聊天、在线视频、网络语音电话、广播通信。容忍一定的数据丢失。 你会如何设计 QQ 中的网络协议？首先，我们要实现登录功能，这是使用 QQ 的第一步，为了保证账号和密码的安全性，我们可以选择 TCP + SSL/TLS 协议来进行登录。 因为 TCP 协议是一种可靠的传输协议，能够保证数据的完整性，而 SSL/TLS 能够对通信进行加密，保证数据的安全性。 接下来，我们需要考虑消息传递的实时性，如语音视频通话等，这时候我们可以选择 UDP 协议。UDP 的传输速度更快，对于实时性服务来说，速度是最重要的。 如何保证消息的不丢失？对于 TCP 协议来说，如果数据包在传输过程中丢失，TCP 协议会自动进行重传。 而对于 UDP 协议来说，我们可以通过应用层的重传机制来保证消息的不丢失。当接收方收到消息后，返回一个确认信息给发送方，如果发送方在一定时间内没有收到确认信息，就重新发送消息。 同时，每个消息都附带一个唯一的序列号，接收方根据序列号判断是否有消息丢失，如果发现序列号不连续，就可以要求发送方重新发送。这样还可以防止消息重复。 当然了，消息持久化也很重要，可以将消息保存在服务器或者本地的数据库中，即使在网络中断或者其他异常情况下，也能从数据库中恢复消息。 45.为什么 QQ 采用 UDP 协议？PS：这是多年前的老题了，拉出来怀怀旧。 首先，QQ 并不是完全基于 UDP 实现。比如在使用 QQ 进行文件传输等活动的时候，就会使用 TCP 作为可靠传输的保证。 使用 UDP 进行交互通信的好处在于，延迟较短，对数据丢失的处理比较简单。同时，TCP 是一个全双工协议，需要建立连接，所以网络开销也会相对大。 如果使用 QQ 语音和 QQ 视频的话，UDP 的优势就更为突出了，首先延迟较小。最重要的一点是不可靠传输，这意味着如果数据丢失的话，不会有重传。因为用户一般来说可以接受图像稍微模糊一点，声音稍微不清晰一点，但是如果在几秒钟以后再出现之前丢失的画面和声音，这恐怕是很难接受的。 由于 QQ 的服务器设计容量是海量级的应用，一台服务器要同时容纳十几万的并发连接，因此服务器端只有采用 UDP 协议与客户端进行通讯才能保证这种超大规模的服务 简单总结一下：UDP 协议是无连接方式的协议，它的效率高，速度快，占资源少，对服务器的压力比较小。但是其传输机制为不可靠传送，必须依靠辅助的算法来完成传输控制。QQ 采用的通信协议以 UDP 为主，辅以 TCP 协议。 46.UDP 协议为什么不可靠？UDP 在传输数据之前不需要先建立连接，远地主机的运输层在接收到 UDP 报文后，不需要确认，提供不可靠交付。总结就以下四点： 不保证消息交付：不确认，不重传，无超时 不保证交付顺序：不设置包序号，不重排，不会发生队首阻塞 不跟踪连接状态：不必建立连接或重启状态机 不进行拥塞控制：不内置客户端或网络反馈机制 47.DNS 为什么要用 UDP?更准确地说，DNS 既使用 TCP 又使用 UDP。 当进行区域传送（主域名服务器向辅助域名服务器传送变化的那部分数据）时会使用 TCP，因为数据同步传送的数据量比一个请求和应答的数据量要多，而 TCP 允许的报文长度更长，因此为了保证数据的正确性，会使用基于可靠连接的 TCP。 当客户端想 DNS 服务器查询域名（域名解析）的时候，一般返回的内容不会超过 UDP 报文的最大长度，即 512 字节，用 UDP 传输时，不需要创建连接，从而大大提高了响应速度，但这要求域名解析服务器和域名服务器都必须自己处理超时和重传从而保证可靠性。 IP48.IP 协议的定义和作用？IP 协议（Internet Protocol）用于在计算机网络之间传输数据包，它定义了数据包的格式和处理规则，确保数据能够从一个设备传输到另一个设备，可能跨越多个中间网络设备（如路由器）。 IP 协议有哪些作用？①、寻址：每个连接到网络的设备都有一个唯一的 IP 地址。IP 协议使用这些地址来标识数据包的源地址和目的地址，确保数据包能够准确地传输到目标设备。 ②、路由：IP 协议负责决定数据包在网络传输中的路径。比如说路由器使用路由表和 IP 地址信息来确定数据包的最佳传输路径。 ③、分片和重组：当数据包过大无法在某个网络上传输时，IP 协议会将数据包分成更小的片段进行传输。接收端会根据头部信息将这些片段重新组装成完整的数据包。 举一个实际的例子来说明？假设有两个设备 A 和 B 通过互联网通信，A 的 IP 地址是 192.168.1.1，B 的 IP 地址是 203.0.113.5。数据包的传输过程如下： ①、设备 A 发送数据包： 设备 A 创建一个 IP 数据包，设置源地址为 192.168.1.1，目的地址为 203.0.113.5，将要传输的数据放入数据部分。 数据包封装后，通过本地网络发送到路由器。 ②、路由器转发数据包： 路由器根据路由表查找目的地址 203.0.113.5，确定数据包的传输路径。 数据包可能经过多个中间路由器，每个路由器都根据路由表选择下一跳，最终到达目标设备的网络。 ③、设备 B 接收数据包： 设备 B 接收数据包，读取 IP 头部信息，验证数据包的完整性。 并数据部分取出，交给上层协议处理（如 TCP 或 UDP）。 49.IP 地址有哪些分类？一个 IP 地址在这个互联网范围内是唯一的，一般可以这么认为，IP 地址 = 网络号，主机号。 网络号：它标志主机所连接的网络地址表示属于互联网的哪一个网络。 主机号：它标志主机地址表示其属于该网络中的哪一台主机。 IP 地址分为 A，B，C，D，E 五大类： A 类地址 (1~126)：以 0 开头，网络号占前 8 位，主机号占后面 24 位。 B 类地址 (128~191)：以 10 开头，网络号占前 16 位，主机号占后面 16 位。 C 类地址 (192~223)：以 110 开头，网络号占前 24 位，主机号占后面 8 位。 D 类地址 (224~239)：以 1110 开头，保留为多播地址。 E 类地址 (240~255)：以 1111 开头，保留位为将来使用 50.域名和 IP 的关系？一个 IP 可以对应多个域名吗？ IP 地址在同一个网络中是唯一的，用来标识每一个网络上的设备，其相当于一个人的身份证号 域名在同一个网络中也是唯一的，就像是一个人的名字、绰号 假如你有多个不用的绰号，你的朋友可以用其中任何一个绰号叫你，但你的身份证号码却是唯一的。但同时你的绰号也可能和别人重复，假如你不在，有人叫你的绰号，其它人可能就答应了。 一个域名可以对应多个 IP，但这种情况 DNS 做负载均衡的，在用户访问过程中，一个域名只能对应一个 IP。 而一个 IP 却可以对应多个域名，是一对多的关系。 51.IPV4 地址不够如何解决？我们知道，IP 地址有 32 位，可以标记 2 的 32 次方个地址，听起来很多，但是全球的网络设备数量已经远远超过这个数字，所以 IPV4 地址已经不够用了，那怎么解决呢？ DHCP：动态主机配置协议，动态分配 IP 地址，只给接入网络的设备分配 IP 地址，因此同一个 MAC 地址的设备，每次接入互联网时，得到的 IP 地址不一定是相同的，该协议使得空闲的 IP 地址可以得到充分利用。 CIDR：无类别域间路由。CIDR 消除了传统的 A 类、B 类、C 类地址以及划分子网的概念，因而更加有效地分配 IPv4 的地址空间，但无法从根本上解决地址耗尽的问题。 NAT：网络地址转换协议，我们知道属于不同局域网的主机可以使用相同的 IP 地址，从而一定程度上缓解了 IP 资源枯竭的问题，然而主机在局域网中使用的 IP 地址是不能在公网中使用的，当局域网主机想要与公网主机进行通信时，NAT 方法可以将该主机 IP 地址转换为全球 IP 地址。该协议能够有效解决 IP 地址不足的问题。 IPv6：作为接替 IPv4 的下一代互联网协议，其可以实现 2 的 128 次方个地址，而这个数量级，即使给地球上每一粒沙子都分配一个 IP 地址也够用，该协议能够从根本上解决 IPv4 地址不够用的问题。 52.说下 ARP 协议的工作过程？ARP（Address Resolution Protocol，地址解析协议）是网络通信中的一种协议，主要目的是将网络层的 IP 地址解析为链路层的 MAC 地址。 ①、ARP 请求 当主机 A 要发送数据给主机 B 时，首先会在自己的 ARP 缓存中查找主机 B 的 MAC 地址。 如果没有找到，主机 A 会向网络中广播一个 ARP 请求数据包，请求网络中的所有主机告诉它们的 MAC 地址；这个请求包含了请求设备和目标设备的 IP 和 MAC 地址。 ②、ARP 应答 网络中的所有主机都会收到这个 ARP 请求，但只有主机 B 会回复 ARP 应答，告诉主机 A 自己的 MAC 地址。 并且主机 B 会将主机 A 的 IP 和 MAC 地址映射关系缓存到自己的 ARP 缓存中，以便下次通信时直接使用。 ③、更新 ARP 缓存 主机 A 收到主机 B 的 ARP 应答后，也会将主机 B 的 IP 和 MAC 地址映射关系缓存到自己的 ARP 缓存中。 53.为什么既有 IP 地址，又有 MAC 地址？ MAC 地址和 IP 地址都有什么作用？ MAC 地址是数据链路层和物理层使用的地址，是写在网卡上的物理地址，用来定义网络设备的位置，不可变更。 IP 地址是网络层和以上各层使用的地址，是一种逻辑地址。IP 地址用来区别网络上的计算机。 为什么有了 MAC 地址还需要 IP 地址？ 如果我们只使用 MAC 地址进行寻址的话，我们需要路由器记住每个 MAC 地址属于哪个子网，不然一次路由器收到数据包都要满世界寻找目的 MAC 地址。而我们知道 MAC 地址的长度为 48 位，也就是最多共有 2 的 48 次方个 MAC 地址，这就意味着每个路由器需要 256T 的内存，显然是不现实的。 和 MAC 地址不同，IP 地址是和地域相关的，在一个子网中的设备，我们给其分配的 IP 地址前缀都是一样的，这样路由器就能根据 IP 地址的前缀知道这个设备属于哪个子网，剩下的寻址就交给子网内部实现，从而大大减少了路由器所需要的内存。 为什么有了 IP 地址还需要 MAC 地址？ 只有当设备连入网络时，才能根据他进入了哪个子网来为其分配 IP 地址，在设备还没有 IP 地址的时候，或者在分配 IP 的过程中。我们需要 MAC 地址来区分不同的设备。 IP 地址可以比作为地址，MAC 地址为收件人，在一次通信过程中，两者是缺一不可的。 54.ICMP 协议的功能？ICMP（Internet Control Message Protocol） ，网际控制报文协议。 ICMP 协议是一种面向无连接的协议，用于传输出错报告控制信息。 它是一个非常重要的协议，它对于网络安全具有极其重要的意义。它属于网络层协议，主要用于在主机与路由器之间传递控制信息，包括报告错误、交换受限控制和状态信息等。 当遇到 IP 数据无法访问目标、IP 路由器无法按当前的传输速率转发数据包等情况时，会自动发送 ICMP 消息。 比如我们日常使用得比较多的 ping，就是基于 ICMP 的。 55.说下 ping 的原理？ping，Packet Internet Groper，一个网络工具，主要用来测试网络连接的可达性和延迟。 Ping 的过程主要基于 ICMP（Internet Control Message Protocol，互联网控制消息协议）实现，其基本过程包括： ①、当执行 Ping 命令，如ping javabetter.cn，Ping 首先解析域名获取 IP 地址，然后向目标 IP 发送一个 ICMP Echo Request 消息。 ②、当目标 IP 收到 ICMP Echo Request 消息后，它会生成一个 ICMP Echo Reply 消息并返回，即 Ping 响应消息。 ③、发起 Ping 命令的设备接收到 ICMP Echo Reply 消息后，计算并显示从发送 Echo Request 到接收到 Echo Reply的时间（通常称为往返时间 RTT，Round-Trip Time），以及可能的丢包情况。 Ping 通常会发送多个请求，以便提供平均响应时间和丢包率等信息，以便我们了解网络连接的质量。 网络安全56.说说有哪些安全攻击？网络安全攻击主要分为两种类型，被动攻击和主动攻击： 被动攻击：是指攻击者从网络上窃听他人的通信内容，通常把这类攻击称为截获，被动攻击主要有两种形式：消息内容泄露攻击和流量分析攻击。由于攻击者没有修改数据，使得这种攻击很难被检测到。 主动攻击：直接对现有的数据和服务造成影响，常见的主动攻击类型有： 篡改：攻击者故意篡改网络上送的报文，甚至把完全伪造的报文传送给接收方。 恶意程序：恶意程序种类繁多，包括计算机病毒、计算机蠕虫、特洛伊木马、后门入侵、流氓软件等等。 拒绝服务 Dos：攻击者向服务器不停地发送分组，使服务器无法提供正常服务。 57.DNS 劫持了解吗？DNS 劫持即域名劫持，是通过将原域名对应的 IP 地址进行替换，从而使用户访问到错误的网站，或者使用户无法正常访问网站的一种攻击方式。 域名劫持往往只能在特定的网络范围内进行，范围外的 DNS 服务器能够返回正常的 IP 地址。攻击者可以冒充原域名所属机构，通过电子邮件的方式修改组织机构的域名注册信息，或者将域名转让给其它主持，并将新的域名信息保存在所指定的 DNS 服务器中，从而使用户无法对原域名来进行解析以访问目标地址。 DNS 劫持的步骤是什么样的？ 获取要劫持的域名信息：攻击者会首先访问域名查询要劫持的站点的域名信息。 控制域名响应的 E-Mail 账号：在获取到域名信息后，攻击者通过暴力破解或者专门的方法破解公司注册域名时使用的 E-mail 账号所对应的密码，更高级的攻击者甚至能够直接对 E-Mail 进行信息窃取。 修改注册信息：当攻击者破解了 E-Mail 后，会利用相关的更改功能修改该域名的注册信息，包括域名拥有者信息，DNS 服务器信息等。 使用 E-Mail 收发确认函：在修改完注册信息后，攻击者 E-Mail 在真正拥有者之前收到修改域名注册信息的相关确认信息，并回复确认修改文件，待网络公司恢复已成功修改信件后，攻击者便成功完成 DNS 劫持。 怎么应对 DNS 劫持？ 直接通过 IP 地址访问网站，避开 DNS 劫持 由于域名劫持往往只能在特定的网络范围内进行，因此一些高级用户可以通过网络设置让 DNS 指向正常的域名服务器以实现对目标网址的正常访问，例如计算机首选 DNS 服务器的地址固定为 8.8.8.8。 58.什么是 CSRF 攻击？如何避免？ 什么是 CSRF 攻击？ CSRF，跨站请求伪造（英文全称是 Cross-site request forgery），是一种挟持用户在当前已登录的 Web 应用程序上执行非本意的操作的攻击方法。 CSRF 是如何攻击的呢？ 来看一个例子： 用户登陆银行，没有退出，浏览器包含了 用户 在银行的身份认证信息。 攻击者将伪造的转账请求，包含在在帖子 用户在银行网站保持登陆的情况下，浏览帖子 将伪造的转账请求连同身份认证信息，发送到银行网站 银行网站看到身份认证信息，以为就是 用户的合法操作，最后造成用户资金损失。 怎么应对 CSRF 攻击呢？ 检查 Referer 字段 HTTP 头中的 Referer 字段记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，而如果黑客要对其实施 CSRF 攻击，他一般只能在他自己的网站构造请求。因此，可以通过验证 Referer 值来防御 CSRF 攻击。 添加校验 token 以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。 敏感操作多重校验 对一些敏感的操作，除了需要校验用户的认证信息，还可以通过邮箱确认、验证码确认这样的方式多重校验。 59.什么是 DoS、DDoS、DRDoS 攻击？ DOS: (Denial of Service), 翻译过来就是拒绝服务, 一切能引起拒绝 行为的攻击都被称为 DOS 攻击。最常见的 DoS 攻击就有计算机网络宽带攻击、连通性攻击。 DDoS: (Distributed Denial of Service)，翻译过来是分布式拒绝服务。是指处于不同位置的多个攻击者同时向一个或几个目标发动攻击，或者一个攻击者控制了位于不同位置的多台机器，并利用这些机器对受害者同时实施攻击。 主要形式有流量攻击和资源耗尽攻击，常见的 DDoS 攻击有：SYN Flood、Ping of Death、ACK Flood、UDP Flood 等。 DRDoS: (Distributed Reflection Denial of Service)，中文是分布式反射拒绝服务，该方式靠的是发送大量带有被害者 IP 地址的数据包给攻击主机，然后攻击主机对 IP 地址源做出大量回应，从而形成拒绝服务攻击。 如何防范 DDoS? 针对 DDoS 中的流量攻击，最直接的方法是增加带宽，理论上只要带宽大于攻击流量就可以了，但是这种方法成本非常高。在有充足带宽的前提下，我们应该尽量提升路由器、网卡、交换机等硬件设施的配置。 针对资源耗尽攻击，我们可以升级主机服务器硬件，在网络带宽得到保证的前提下，使得服务器能够有效对抗海量的 SYN 攻击包。我们也可以安装专业的抗 DDoS 防火墙，从而对抗 SYN Flood 等流量型攻击。瓷碗，负载均衡，CDN 等技术都能有效对抗 DDos 攻击。 60.什么是 XSS 攻击，如何避免?XSS 攻击也是比较常见，XSS，叫跨站脚本攻击（Cross-Site Scripting），因为会与层叠样式表 (Cascading Style Sheets, CSS) 的缩写混淆，因此有人将跨站脚本攻击缩写为 XSS。它指的是恶意攻击者往 Web 页面里插入恶意 html 代码，当用户浏览网页的时候，嵌入其中 Web 里面的 html 代码会被执行，从而达到恶意攻击用户的特殊目的。 XSS 攻击一般分三种类型：存储型 、反射型 、DOM 型 XSS XSS 是如何攻击的呢？ 简单说，XSS 的攻击方式就是想办法“教唆”用户的浏览器去执行一些这个网页中原本不存在的前端代码。 拿反射型举个例子吧，流程图如下： 攻击者构造出特殊的 URL，其中包含恶意代码。 用户打开带有恶意代码的 URL 时，访问正常网站服务器 网站服务端将恶意代码从 URL 中取出，拼接在 HTML 中返回给浏览器。 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行，请求恶意服务器，发送用户数据 攻击者就可以窃取用户的数据，以此冒充用户的行为，调用目标网站接口执行攻击者指定的操作。 如何应对 XSS 攻击？ 对输入进行过滤，过滤标签等，只允许合法值。 HTML 转义 对于链接跳转，如a href=xxx 等，要校验内容，禁止以 script 开头的非法链接。 限制输入长度 61.对称加密与非对称加密有什么区别？对称加密：指加密和解密使用同一密钥，优点是运算速度较快，缺点是如何安全将密钥传输给另一方。常见的对称加密算法有：DES、AES 等。 非对称加密：指的是加密和解密使用不同的密钥（即公钥和私钥）。公钥与私钥是成对存在的，如果用公钥对数据进行加密，只有对应的私钥才能解密。常见的非对称加密算法有 RSA。 62.RSA 和 AES 算法有什么区别？ RSA 采用非对称加密的方式，采用公钥进行加密，私钥解密的形式。其私钥长度一般较长，由于需要大数的乘幂求模等运算，其运算速度较慢，不合适大量数据文件加密。 AES 采用对称加密的方式，其秘钥长度最长只有 256 个比特，加密和解密速度较快，易于硬件实现。由于是对称加密，通信双方在进行数据传输前需要获知加密密钥。","tags":["基础","计算机网络"],"categories":["Java问答笔记"]},{"title":"程序员养生指南","path":"/2025/07/14/杂项笔记/程序员养生指南/","content":"程序员养生指南 程序员养生指南 1. 术语 2. 目标 3. 关键结果 4. 分析 5. 行动 6. 证据 6.1. 输入 6.1.1. 固体 6.1.2. 液体 6.1.3. 气体 6.1.4. 光照 6.1.5. 药物 6.2. 输出 6.2.1. 挥拍运动 6.2.2. 剧烈运动 6.2.3. 走路 6.2.4. 刷牙 6.2.5. 泡澡 6.2.6. 做家务（老年男性） 6.2.7. 睡眠 6.2.8. 久坐 6.3. 上下文 6.3.1. 情绪 6.3.2. 贫富 6.3.3. 体重 6.3.4. 新冠 1. 术语 ACM: All-Cause Mortality 全因死亡率 2. 目标 稳健的活得更久 花更少时间工作：见MetaGPT 3. 关键结果 降低66.67%全因死亡率 增加~20年预期寿命 维持多巴胺于中轴 4. 分析 主要参考：对ACM的学术文献相对较多，可以作为主要参考 增加寿命与ACM关系非线性：显然增加寿命与ACM关系是非线性函数，这里假设 ΔLifeSpan=(1/(1+ΔACM)-1)*10（ΔACM为ACM变化值；公式欢迎优化） 变量无法简单叠加：显然各个变量之间并不符合独立同分布假设，变量之间的实际影响也并不明确 存在矛盾观点：所有的证据都有文献研究对应，但注意到：有些文献之间有显著矛盾的观点（如对于碳水摄入比例的矛盾）；有些文献存在较大争议（如认为22点前睡觉会提升43%全因死亡率） 研究仅表达相关：所有文献表明的更多是相关而非因果，在阅读时要考虑文献是否充分证明了因果 —— 如某文献表明了日均7000步的人有显著低的全因死亡率。但步数少的人可能包含更多长期病患，如果没有合理的排除这块数据，那此文献调查失真 5. 行动 输入 固体：吃白肉（-11%~-3% ACM）、蔬果为主（-26%~-17% ACM），多吃辣（-23% ACM），多吃坚果（-27%~-4% ACM），中量碳水、多吃植物蛋白（-10% ACM），少吃超加工食物（-62%~-18%） 液体：喝咖啡（-22%~-12% ACM），喝牛奶（-17%~-10% ACM），喝茶（-15%~-8% ACM），少喝或不喝甜味饮料（否则每天一杯+7% ACM，+多巴胺），戒酒（否则+~50% ACM，无上限） 气体：不吸烟（否则+50% ACM，-12\\-11年寿命） 光照：晒太阳（-~40% ACM） 药物：二甲双胍（糖尿病人相比正常人可以+3年）、复合维生素（-8%癌症风险）、亚精胺（-60%~-30% ACM）、葡萄糖胺（-39% ACM） 输出 运动：每周3次45分钟挥拍运动（-47% ACM） 日常：刷牙（-25% ACM） 睡眠：每天睡7小时全因死亡率最低；且22-24点间最好，早睡+43% ACM，晚睡+15% ACM（存在争议） 上下文 体重：减肥（-54% ACM） 6. 证据6.1. 输入6.1.1. 固体 白肉 JAMA子刊：食用红肉和加工肉类会增加心脏病和死亡风险！鱼肉和家禽肉则不会 出处：Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality 增加红肉摄入与死亡风险相关。八年内平均每天增加至少半份红肉摄入（半份红肉相当于14g加工红肉或40g非加工红肉）的调查对象，在接下来八年内全因死亡风险增加10％（HR, 1.10; 95%CI, 1.04-1.17）；每周吃两份红肉或加工肉类（但不包括家禽或鱼类）会使全因死亡风险增加3% 红肉和白肉最大的区别是什么？为啥要这么分呢？ 蔬果 每年54万人死亡，竟是因为水果吃得少！？这已成十大死亡因素之一！ 出处：Estimated Global, Regional, and National Cardiovascular Disease Burdens Related to Fruit and Vegetable Consumption: An Analysis from the Global Dietary Database (FS01-01-19) 每天摄入200克新鲜水果可使死亡率降低17%，糖尿病大血管并发症（如中风、缺血性心脏病等）风险降低13%，及糖尿病小血管并发症（如糖尿病肾病、糖尿病眼病、糖尿病足病等）风险降低28% 《自然》子刊：每天二两西兰花，健康长寿都有啦！分析近6万人23年的数据发现，吃含黄酮类食物与死亡风险降低20%相关丨临床大发现 出处：Flavonoid intake is associated with lower mortality in the Danish Diet Cancer and Health Cohort 吃含黄酮类食物与死亡风险降低20%相关 Bondonno博士说道“吃不同蔬菜、水果补充，不同种类的黄酮类化合物是很重要的，这很容易通过饮食实现：一杯茶、一个苹果、一个橘子、100克蓝莓，或100克西兰花，就能提供各种黄酮类化合物，并且总含量超过500毫克。 辣椒 辣椒成死亡克星？据调研，常吃辣患病死亡风险可降低61% 出处1：Chili pepper consumption and mortality in Italian adults 出处2：The Association of Hot Red Chili Pepper Consumption and Mortality: A Large Population-Based Cohort Study 2017年Plos One 的另一项来自美国的研究以16179名，年龄在18岁以上的人群为对象，并对其进行了高达19年的随访，发现在4946例死亡患者中，食用辣椒的参与者的全因死亡率为21.6％，而未食用辣椒的参与者的全因死亡率为33.6％。相较于不吃辣或很少吃（少于每周两次）的人群，每周吃辣＞4次的人群总死亡风险降低23%，心血管死亡风险降低34%。 鸡蛋 每天多吃半个蛋，增加7%的全因和心血管死亡风险？ 出处：NIH-AARP工作主页、Egg and cholesterol consumption and mortality from cardiovascular and different causes in the United States: A population-based cohort study 每天多吃半个蛋，增加7%的全因和心血管死亡风险？在假设性替代分析中，研究者发现，用等量的蛋清鸡蛋替代物、家禽、鱼、乳制品、坚果和豆类分别替代半只全蛋（25克天）可以降低6%、8%、9%、7%、13%和10%的全因死亡率。*鸡蛋 坚果 哈佛20年研究：吃核桃的人更长寿，显著减少全因死亡，延长寿命 出处：Association of Walnut Consumption with Total and Cause-Specific Mortality and Life Expectancy in US Adults 通过分析发现，经常食用核桃可以延长寿命，降低心血管疾病死亡风险。比起不吃核桃，每周食用核桃5份以上（1份28克）的健康预期寿命延长1.3岁，全因死亡风险降低14%，心血管疾病死亡率降低25%。 研究：每日食生坚果，死亡率降20% 出处1：Association of nut consumption with total and cause-specific mortality 出处2：APG_Health--Nutrition-Research-Brochure_DEC-19-18 研究人员发现，每周吃树坚果低于1盎司份量的人，死亡率降低7％。而每周吃了1盎司份量的人，减少11％的死亡率；每周吃2份量的人，减低13％；每周5至6份量者，减少了15％；一周7份以上的人，死亡率则减少20％。 另外两篇发表在《公共科学图书馆在线期刊》(Public Library of Science Online Journal)和《生物医学中心》(BioMed Central)上的医学预科研究论文，展示了试验开始时的横断面数据。这两项研究都评估了7,216名对象，以及他们食用坚果的频率和数量之间的关系。那些每周食用三份以上坚果(包括开心果)的研究对象的死亡率降低39%。 钠（存有大量争议） Eur Heart J：钠摄入量与预期寿命、全因死亡率的关系 出处：Messerli F H, Hofstetter L, Syrogiannouli L, et al. Sodium intake, life expectancy, and all-cause mortality[J]. European heart journal, 2021, 42(21): 2103-2112. 在该分析所包含的181个国家中，研究人员发现钠摄入量与出生时的健康预期寿命（β2.6年克每日钠摄入量，R20.66，P0.001）和60岁时的健康预期寿命（β0.3年克每日钠摄入量，R20.60，P0.048）之间存在正相关关系，但与非传染性疾病死亡（β17次事件克每日钠摄入量，R20.43，P0.100）无关。相反，全因死亡率与钠摄入量成负相关（β−131次事件克每日钠摄入量，R20.60，P0.001）。在仅限于46个收入最高国家的敏感性分析中，钠摄入量与出生时的健康预期寿命呈正相关（β3.4年克每日钠摄入量，R20.53，P0.001），而与全因死亡率（β−168次事件克每日钠摄入量，R20.50，P0.001）呈负相关。 该（大范围）研究认为更多的钠摄入与显著更低的全因死亡率有关 针对该论文的延伸解读和讨论：A Fresh Foray in the Salt Wars: Life Expectancy Higher With Greater Sodium Intake NEJMLancet：不要吃太多盐，中国饮食所致心血管病和癌症死亡全球第一，吃低钠盐可降低全因死亡率 但也有多项研究认为用低钠盐可以降低一系列疾病的发生概率，对全因死亡率的减少有积极影响 碳水（存有大量争议） 低碳生酮饮食（四）碳水化合物与长期死亡率 出处：The Lancet Public Health - Dietary carbohydrate intake and mortality: a prospective cohort study and meta-analysis 碳水越低，寿命越短；碳水越高，寿命也轻微缩短；碳水50%左右（其实按照一般的说法，这也算高碳水）是最长寿命区间 最强营养搭配！BMJ：这么吃，心血管疾病和死亡风险更低 槟榔 如何看待槟榔嚼出来的癌症？槟榔致癌风险究竟有多大？ - 丁香医生的回答 - 知乎 出处：Chewing Betel Quid and the Risk of Metabolic Disease, Cardiovascular Disease, and All-Cause Mortality: A Meta-Analysis(https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070679) 嚼槟榔会增加21%的全因死亡率 热量限制 怎么看待BBC《进食、断食与长寿》？ 限制卡路里动物实验：CR（热量限制，即少吃）延迟了恒河猴的多种疾病发病和死亡率，与CR动物相比，正常喂养的猴子的各种疾病患病风险增加2.9倍，死亡风险增加3.0倍。 综合 最强营养搭配！BMJ：这么吃，心血管疾病和死亡风险更低 Associations of fat and carbohydrate intake with cardiovascular disease and mortality: prospective cohort study of UK Biobank participants 通过对这些参与者的数据进行分析，研究人员发现碳水化合物（糖、淀粉和纤维）和蛋白质的摄入与全因死亡率呈非线性关系，而脂肪则与全因死亡率呈线性相关。其中，较高的糖分摄入与全因死亡风险和患心血管疾病的风险较高均有关联，而较高的饱和脂肪酸摄入与全因死亡风险较高有关。 图1：各种营养元素与全因死亡之间的关系 图2：各种营养元素与心血管疾病之间的关系 进一步研究表明，在所有的饮食模式中，全因死亡率风险最低的饮食方式为：10-30g高纤维、14-30%蛋白质、10-25%单不饱和脂肪酸、5%-7%多不饱和脂肪酸以及20%-30%淀粉摄入。 最优能量来源配比：24%淀粉，15%-17%蛋白质，15%单不饱和脂肪酸，15%糖，6%饱和脂肪酸，6%多不饱和脂肪酸，30g+高纤维 BMJ | 常吃薯片汉堡巧克力等食品，平均死亡年龄仅仅为58岁，死亡风险剧增 Rico-Campà A, Martínez-González M A, Alvarez-Alvarez I, et al. Association between consumption of ultra-processed foods and all cause mortality: SUN prospective cohort study[J]. bmj, 2019, 365. Srour B, Fezeu L K, Kesse-Guyot E, et al. Ultra-processed food intake and risk of cardiovascular disease: prospective cohort study (NutriNet-Santé)[J]. bmj, 2019, 365. Lawrence M A, Baker P I. Ultra-processed food and adverse health outcomes[J]. bmj, 2019, 365. 6.1.2. 液体 牛奶 《柳叶刀》调研21个国家13万人：每天1斤牛奶或酸奶，心血管死亡风险下降23% 出处：Association of dairy intake with cardiovascular disease and mortality in 21 countries from five continents (PURE): a prospective cohort study 与不食用乳制品的人相比，每天摄入两份乳制品（一份指244克牛奶酸奶，15克奶酪或5克黄油）的人，**全因死亡风险下降了17%**，心血管死亡风险下降23%，中风风险下降33% 茶 10万中国人随访7年发现，每周喝三次茶与全因死亡风险降低15%，预期寿命增加1.26年相关 出处：Tea consumption and the risk of atherosclerotic cardiovascular disease and all-cause mortality: The China-PAR project 中国成年人饮茶与死亡风险的前瞻性关联研究 纳入分析的438 443例研究对象随访11.1年共发生死亡34 661例。与从不饮茶者相比，当前非每日饮茶者和每日饮茶者全因死亡HR值（95%CI）依次为0.89（0.86-0.91）和0.92（0.88-0.95）。分性别分析显示，饮茶对全因死亡风险的保护作用主要见于男性（交互P0.05） 无糖（甜味）饮料 「无糖饮料使死亡风险增加 26 %」，是真的吗？ 相比于软饮料摄入量＜1杯月的参与者，混合软饮料摄入≥1杯天的参与者死亡风险增加18%，而摄入含糖软饮料或无糖软饮料会令死亡风险分别增加11%和27%。 Association Between Soft Drink Consumption and Mortality in 10 European Countries 有糖饮料 可乐和奶茶，增加全因死亡率高达62%！果汁降低免疫力，影响肝代谢！含糖饮料那些事 每天1杯含糖饮料增加7%全因死亡率，2杯21% 在34年的随访中，研究人员发现，相比那些一个月喝1杯或者更少含糖饮料的人，每天喝2杯的人总体死亡风险升高了21%，心血管疾病死亡风险升高了31%，癌症死亡风险上升了16%。 只要每天多喝一杯含糖饮料，总体死亡风险将增加7%，心血管疾病的风险将增加10%，癌症相关的死亡风险将16%。 发表在国际顶级期刊《BMJ》上的一篇论文就证明了含糖饮料会在增加患癌风险，当然这篇文章验证的不仅仅是果汁，奶茶也有份——和含糖饮料相关的总体患癌风险要高出通常值18%，100%的鲜榨果汁也会使得整体的患癌风险上升12%。 果汁 JAMA子刊：100%纯果汁可能比含糖饮料更危险 每天多摄入一份12盎司的含糖饮料，全因死亡率风险增加11%； 每天多摄入一份12盎司的果汁，全因死亡率风险增加24%。 咖啡 重磅！多篇研究证实喝咖啡与人群全因死亡率降低直接相关 科普 | 喝咖啡又多了一个新理由：降低死亡率！ 地中海成年人咖啡消耗量及全因，心血管疾病和癌症的死亡率 在最近的荟萃分析中，该研究包括来自不同国家的40项研究和3,852,651名受试者。在这项荟萃分析显示，咖啡摄入量与各种原因的死亡率，CVD和癌症死亡率之间存在非线性关系，每天摄入两杯咖啡的癌症死亡率最低(RR 0.96)，CVD最低的死亡率，每天2.5杯(RR 0.83)，全天最低死亡率为每天3.5杯(RR 0.85)，并且随着咖啡消费量的增加，死亡率没有进一步降低或增加 亚精胺 Science：科学背书！从精液中发现的亚精胺，竟然有着抗衰老、抗癌、保护心血管和神经、改善肥胖和2型糖尿病等逆天神效 饮食中亚精胺摄入量高会降低死亡率 6.1.3. 气体 吸烟 即使是低强度吸烟，也增加死亡风险！ 研究发现：在42 416名男性和86 735名女性（年龄在35-89岁之间，以前没有患病）中，18 985名男性（45%）和18 072名女性（21%）目前吸烟，其中33%的男性吸烟者和39%的女性吸烟者并不每天吸烟。8866名男性（21%）和53 912名女性（62%）从不吸烟。在随访期间，与从不吸烟相比，每天10支烟或每天≥10支烟的全因死亡率危险比分别为1.17（95%置信区间1.10-1.25）和1.54（1.42-1.67）。无论年龄或性别，危险比相似。与每日吸烟关系最密切的疾病是呼吸道癌症、慢性阻塞性肺病和胃肠道及血管疾病。在招募时已经戒烟的人的死亡率低于现在每天吸烟者。 吸烟者平均减少寿命11-12年 吸烟让人过瘾是什么原理？有节制的吸烟依旧有害吗？ 6.1.4. 光照 晒太阳 晒太阳和死亡率的关系，如何科学，安全的晒太阳？ 丹麦一项长达26年的研究发现，多晒太阳能显著延长寿命，即使是由于过度暴晒诱发皮肤癌的患者，平均寿命也比普通人长了6岁。 6.1.5. 药物 NMN 二甲双胍 “胍”吹必看 丨我就是神药——二甲双胍 二甲双胍不仅在多种肿瘤、心血管疾病及糖尿病中发挥保护作用，而且在肥胖、肝病、肾病及衰老方面也大放异彩。 二甲双胍2020最值得了解的“吃瓜”大新闻——护胃、健脑、抗衰、防癌还是致癌？ 二甲双胍真的那么神吗？美研究：父亲服用二甲双胍或致子女有缺陷 不良反应 作为一种使用近百年的药物，二甲双胍的不良反应已经非常明确，常见的有：维生素B12缺乏（7%-17.4%），胃肠道不良反应（最高53%），疲倦（9%），头痛（6%）；严重但不常见的不良反应包括乳酸酸中毒、肝损伤；也有研究表明可能对胎儿致畸 复合维生素 服用复合维生素可降低癌症危险8%，其他效果并不显著 葡萄糖胺 神奇！氨糖降低心血管死亡率65%，与定期运动效果相当 美国西弗吉尼亚大学最新研究发现 氨糖（软骨素） 可以降低心血管死亡率65%，降低总体死亡率39%，效果与坚持定期运动相对 该研究使用1999年至2010年，16,686名成年人的国家健康和营养检查(NHANES)数据，参与者的中位追踪时间为107个月，而其中有648位参与者定期且每服用日500-1000毫克的葡萄糖胺软骨素一年以上。 亚精胺 Science：科学背书！从精液中发现的亚精胺，竟然有着抗衰老、抗癌、保护心血管和神经、改善肥胖和2型糖尿病等逆天神效 亚精胺是最容易从人体肠道吸收的多胺。许多的食物中都含有大量的亚精胺，例如新鲜的青椒、小麦胚芽、花椰菜、西兰花、蘑菇和各种奶酪，尤其在纳豆等大豆制品、香菇和榴莲中含量更高。在本实验中，研究人员选择了829位年龄在45-84岁之间的参与者进行了为期20年的随访，分析了饮食中亚精胺摄入量与人类死亡率之间的潜在关联。 研究发现，女性的亚精胺摄入量高于男性，并且摄入量都会随着年龄的增长而下降。亚精胺的主要来源是全谷物（占13.4%）、苹果和梨（占13.3%）、沙拉（占9.8%）、芽菜（占7.3%）和马铃薯（占6.4%）。研究根据亚精胺摄入量将人群分为三组，低摄入量组（62.2 µmol d）、中摄入量组（62.2–79.8 µmol d）和高摄入量组（ 79.8 µmol d）。随访期间共记录了341例死亡，其中血管疾病137例，癌症94例，其他原因110例。经计算低中高三组的粗略死亡率分别为40.5%、23.7%和15.1%，这些数据表明亚精胺摄入量与全因死亡率之间的负相关关系显著。随着逐步对年龄、性别和热量的比例进行调整，这种相关关系依然显著。 综合 《自然》子刊深度综述：如何开发抗衰老药 6.2. 输出6.2.1. 挥拍运动 哪种运动性价比最高？权威医学杂志“柳叶刀”给出答案了 一周三次，每次45-60分钟，挥拍运动，降低~47%全因死亡率 羽毛球、乒乓球、网球等都算挥拍运动，但由于西化研究背景，可能指网球更多。这隐式的表达了全身锻炼更为重要 6.2.2. 剧烈运动 新研究：每天剧烈运动8分钟，可降低全因死亡和心脏病风险 每周15-20分钟的剧烈运动，降低16-40%的全因死亡率，剧烈运动时间达到50-57分钟周，可以进一步降低全因死亡率。这些发现表明，通过在一周的短时间内累积相对少量的剧烈运动可以降低健康风险。 6.2.3. 走路 走路降低全因死亡率超过50%！每天走多少步最合适？《JAMA》子刊超10年研究告诉你答案 注1：这项研究参与者的平均年龄为45.2岁 注2：平均步数的多少与职业有关，此项研究仅表明相关性，还没有更深度的因果分析 6.2.4. 刷牙 50万国人研究证实：不好好刷牙，致癌！血管疾病也会增多！ 经常不刷牙的人：癌症、慢性阻塞性肺病及肝硬化风险分别增加了9%、12%和25%，过早死亡风险增加25%。 6.2.5. 泡澡 定期洗澡降低心血管疾病发作风险 与每周一至两次泡澡或根本不泡澡相比，每天洗热水澡可以降低28%的心血管疾病总风险，降低26%的中风总风险，脑出血风险下降46%。而浴缸浴的频率与心源性猝死的风险增加无关。 6.2.6. 做家务（老年男性） Housework Reduces All-Cause and Cancer Mortality in Chinese Men 72岁之后男性每周做重型家务可以减少29%平均死亡率 重型家务：吸尘、擦地板、拖地、擦洗窗户、洗车、搬动家具、搬煤气罐等等。 轻型家务：掸灰尘、洗碗、手洗衣服、熨烫、晾衣服、做饭、买日用品等等。 6.2.7. 睡眠 超30万亚洲人数据：每天睡几个小时最有益长寿？ 在男性中，与睡眠时长为7小时相比：睡眠持续时间≥10小时与全因死亡风险增加34%相关； 在女性中，与睡眠持续时间7小时相比：睡眠持续时间≥10小时与全因死亡风险增加48%相关； 颠覆认知！加拿大研究发现：早睡比熬夜或许更伤身，几点睡才好？ 其中一个结论为，就寝时间与全因死亡率的关联性强，过早睡觉和过晚睡觉都会影响健康，但是早睡增加的全因死亡率比晚睡增加的死亡率高，早睡增加了43%的死亡风险，而晚睡增加了15%的死亡风险。 这项调查研究，还存在很多局限性，比如没有直接证明就寝时间与死亡的关系，仅仅说明相关性，通过参与人群自我报告统计睡眠时间，数据不够客观 6.2.8. 久坐 中国居民膳食指南科学研究报告（2021年） 久坐和看电视时间与全因死亡、心血管疾病、癌症和2型糖尿病发病高风险相关，是独立风险因素。久坐时间每天每增加1小时，心血管疾病发生风险增加4%，癌症增加1%，全因死亡风险增加3%。全因死亡和CVD死亡风险增加的久坐时间阈值是6~8hd，看电视时间阈值是3~4hd。 世卫组织关于身体活动和久坐行为的指南 6.3. 上下文6.3.1. 情绪 悲观情绪与更高的全因死亡率和心血管疾病死亡率有关，但乐观情绪并不能起到保护作用 Pessimism is associated with greater all-cause and cardiovascular mortality, but optimism is not protective 在1993-1995年间，一项针对50岁以上澳大利亚人健康的双胞胎研究中包括了生活取向测试（LOT），其中包含乐观和悲观的项目。平均20年后，参与者与来自澳大利亚国家死亡指数的死亡信息相匹配。在2,978名具有很多可用分数的参与者中，有1,068人死亡。生存分析测试了各种乐观因素和悲观情绪分数与任何原因，癌症，心血管疾病或其他已知原因的死亡率之间的关联。年龄调整后的悲观量表上的核心与全因和心血管疾病死亡率相关（每1个标准差单位的危险比，95％置信区间和p值1.134、1.065–1.207、8.85×10 –5和1.196、1.045–1.368、0.0093 ），但不会因癌症死亡。乐观得分与悲观得分之间的相关性很弱（年龄调整后的等级相关系数 − 0.176），但与总死亡率或特定原因死亡率没有显着相关性。反向因果关系（引起悲观情绪的疾病）是不可能的，因为在那种情况下，心血管疾病和癌症都会导致悲观情绪。 6.3.2. 贫富 JAMA子刊：贫富差距真能影响寿命？这可能是真的！ 该研究使用1994-1996年第一次收集的数据，并通过生存模型来分析净资产和长寿之间的关联。结果显示，共收纳5414 名参与者，平均年龄为 46.7岁，包括 2766 名女性。较高的净资产与较低的死亡风险相关。特别是在兄弟姐妹和双胞胎中（n 2490），在较高的净资产和较低的死亡率之间观察到类似的关联，表明拥有更多财富的兄弟姐妹或双胞胎比拥有更少财富的兄弟姐妹双胞胎活得更久。 6.3.3. 体重 JAMA子刊：减肥要趁早，才能有效降低死亡率风险 对体重减轻的死亡率风险评估发现，体重从肥胖减轻到超重的成年人与稳定肥胖人群相比，全因死亡率降低了54％（危险比为0.46），然而从成年初期的超重减轻到中年以前的正常体重的人群的死亡率风险并未降低（风险比为1.12）。 6.3.4. 新冠 Magnitude, demographics and dynamics of the effect of the first wave of the COVID-19 pandemic on all-cause mortality in 21 industrialized countries 目前来看，新冠死亡率（美国）在1.5%左右，人均预期寿命减少了2年 如何看待美国CDC宣称新冠死亡人数被高估？ NVSS deaths","tags":["杂项"],"categories":["杂项笔记"]},{"title":"使用Postshot训练3DGS模型流程","path":"/2025/07/12/3DGS/使用Postshot训练3DGS流程/","content":"1使用命令.txt cd d D:sysDesktopWorkplacegsrosbagrosbag_test D:sysDesktopWorkplacegs70123all_2025-07-12-15-25-10.bag 2使用脚本去畸变 3把文件夹拖到Postshot里面训练","tags":["3DGS","公司"],"categories":["3DGS"]},{"title":"2025.7.12学习日记","path":"/2025/07/12/学习日记25年7月/2025.7.12学习笔记/","content":"今日学习内容3DGS力扣每日一题:一道困难dp. 代码随想录项目相关实现跨域方法,记了一篇笔记","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.11学习日记","path":"/2025/07/11/学习日记25年7月/2025.7.11学习笔记/","content":"今日学习内容3DGS今天主要研究了原版3DGS和​​DASH-GS​的代码，重点看了它们的架构设计和优化流程。​​主要学习了高斯参数的管理、投影变换和渲染部分的实现，感觉并行计算设计很巧妙。​​对比了一下DASH-GS​​优化点，主要还是在动态高斯数量分配和稀疏化处理上做了改进。 力扣每日一题:正难则反的题目,求空位,那就求所有的占用的长度. 代码随想录两道01背包 项目相关微信登录helper 生活晚上健身胸,肩,二头,三头,强度中等偏上.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.10学习日记","path":"/2025/07/10/学习日记25年7月/2025.7.10学习笔记/","content":"今日学习内容3DGS力扣每日一题:昨天的题的改版,需要枚举前三大的空位,然后遍历所有空位判断是否能够通过将桌子移到空位来判断最大连续空位. 代码随想录项目相关实现注册接口和登录接口,目前登录时二维码会一直刷新,需要实现一下接口. 生活晚上健身练背和腿,强度低,主要恢复为主.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"MybatisPlus笔记","path":"/2025/07/09/基础笔记/MybatisPlus笔记/","content":"简介MyBatis-Plus （简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 1快速入门1.1 基本步骤引入mybatis-plus-boot-starter依赖: dependency groupIdcom.baomidou/groupId artifactIdmybatis-plus-boot-starter/artifactId version最新版本/version/dependency 定义Mapper接口并继承BaseMapper: public interface UserMapper extends BaseMapperUser 1.2 定义Mapper为了简化单表CRUD，MybatisPlus提供了一个基础的BaseMapper接口，其中已经实现了单表的CRUD：因此我们自定义的Mapper只要继承了这个BaseMapper，就无需自己实现单表CRUD了。 2 常见注解MybatisPlus中比较常用的几个注解如下: @TableName: 用于指定表名 @Tabled: 用于指定表中的主键字段信息 @TableField: 用于指定表中的普通字段信息 @TableName(sys_user)public class User @TableId(id,type=IdType.AUTO) private Long id; @TableField(nickname) private String name; private Integer age; private String email; 2.1 @TableNameTableName注解除了指定表名以外，还可以指定很多其它属性： 2.2 @TableIdTableId注解支持两个属性： IdType支持的类型有： 这里比较常见的有三种： AUTO： 利用数据库的id自增长 INPUT： 手动生成id ASSIGN_ID： 雪花算法生成Long类型的全局唯一id，这是默认的ID策略 2.3 @TableField一般情况下我们并不需要给字段添加@TableField注解，一些特殊情况除外： 成员变量名与数据库字段名不一致 成员变量是以isXXX命名，按照JavaBean的规范，MybatisPlus识别字段时会把is去除，这就导致与数据库不符。 成员变量名与数据库一致，但是与数据库的关键字冲突。使用@TableField注解给字段名添加转义字符：`` 支持的其它属性如下： 3 常见配置大多数的配置都有默认值，因此我们都无需配置。但还有一些是没有默认值的，例如: 实体类的别名扫描包 全局id类型 mybatis-plus: type-aliases-package: com.itheima.mp.domain.po global-config: db-config: id-type: auto # 全局id类型为自增长 需要注意的是，MyBatisPlus也支持手写SQL的，而mapper文件的读取地址可以自己配置： mybatis-plus: mapper-locations: classpath*:/mapper/**/*.xml # Mapper.xml文件地址，当前这个是默认值。 4 核心功能4.1 条件构造器除了新增以外，修改、删除、查询的SQL语句都需要指定where条件。因此BaseMapper中提供的相关方法除了以id作为where条件以外，还支持更加复杂的where条件。 参数中的Wrapper就是条件构造的抽象类，其下有很多默认实现，继承关系如图： Wrapper的子类AbstractWrapper提供了where中包含的所有条件构造方法： 4.1.1 QueryWrapperQueryWrapper在AbstractWrapper的基础上拓展了一个select方法，允许指定查询字段： 无论是修改、删除、查询，都可以使用QueryWrapper来构建查询条件。接下来看一个例子： @Testvoid testQueryWrapper() // 1.构建查询条件 where name like %o% AND balance = 1000 QueryWrapperUser wrapper = new QueryWrapperUser() .select(id, username, info, balance) .like(username, o) .ge(balance, 1000); // 2.查询数据 ListUser users = userMapper.selectList(wrapper); users.forEach(System.out::println); 4.1.2 UpdateWrapperUpdateWrapper在AbstractWrapper的基础上拓展了一个set方法，允许指定SQL中的SET部分： 基于BaseMapper中的update方法更新时只能直接赋值，对于一些复杂的需求就难以实现。例如：更新id为1,2,4的用户的余额，扣200，对应的SQL应该是： UPDATE user SET balance = balance - 200 WHERE id in (1, 2, 4) SET的赋值结果是基于字段现有值的，这个时候就要利用UpdateWrapper中的setSql功能了： @Testvoid testUpdateWrapper() ListLong ids = List.of(1L, 2L, 4L); // 1.生成SQL UpdateWrapperUser wrapper = new UpdateWrapperUser() .setSql(balance = balance - 200) // SET balance = balance - 200 .in(id, ids); // WHERE id in (1, 2, 4) // 2.更新，注意第一个参数可以给null，也就是不填更新字段和数据， // 而是基于UpdateWrapper中的setSQL来更新 userMapper.update(null, wrapper); 4.1.3 4.1.3.LambdaQueryWrapper无论是QueryWrapper还是UpdateWrapper在构造条件的时候都需要写死字段名称，会出现字符串魔法值。这在编程规范中显然是不推荐的。那怎么样才能不写字段名，又能知道字段名呢？ 其中一种办法是基于变量的gettter方法结合反射技术。因此我们只要将条件对应的字段的getter方法传递给MybatisPlus，它就能计算出对应的变量名了。而传递方法可以使用JDK8中的方法引用和Lambda表达式。因此MybatisPlus又提供了一套基于Lambda的Wrapper，包含两个： LambdaQueryWrapper LambdaUpdateWrapper分别对应QueryWrapper和UpdateWrapper其使用方式如下： @Testvoid testLambdaQueryWrapper() // 1.构建条件 WHERE username LIKE %o% AND balance = 1000 QueryWrapperUser wrapper = new QueryWrapper(); wrapper.lambda() .select(User::getId, User::getUsername, User::getInfo, User::getBalance) .like(User::getUsername, o) .ge(User::getBalance, 1000); // 2.查询 ListUser users = userMapper.selectList(wrapper); users.forEach(System.out::println); 4.2 自定义SQLMybatisPlus提供了自定义SQL功能，可以让我们利用Wrapper生成查询条件，再结合Mapper.xml编写SQL @Testvoid testCustomWrapper() // 1.准备自定义查询条件 ListLong ids = List.of(1L, 2L, 4L); QueryWrapperUser wrapper = new QueryWrapperUser().in(id, ids); // 2.调用mapper的自定义方法，直接传递Wrapper userMapper.deductBalanceByIds(200, wrapper); 然后在UserMapper中自定义SQL： package com.itheima.mp.mapper;import com.baomidou.mybatisplus.core.mapper.BaseMapper;import com.itheima.mp.domain.po.User;import org.apache.ibatis.annotations.Param;import org.apache.ibatis.annotations.Update;import org.apache.ibatis.annotations.Param;public interface UserMapper extends BaseMapperUser @Select(UPDATE user SET balance = balance - #money $ew.customSqlSegment) void deductBalanceByIds(@Param(money) int money, @Param(ew) QueryWrapperUser wrapper); 5 Service接口MybatisPlus不仅提供了BaseMapper，还提供了通用的Service接口及默认实现，封装了一些常用的service模板方法。通用接口为IService，默认实现为ServiceImpl，其中封装的方法可以分为以下几类： save：新增 remove：删除 update：更新 get：查询单个结果 list：查询集合结果 count：计数 page：分页查询 5.1 CRUD我们先俩看下基本的CRUD接口。新增： save是新增单个元素 saveBatch是批量新增 saveOrUpdate是根据id判断，如果数据存在就更新，不存在则新增 saveOrUpdateBatch是批量的新增或修改 删除: removeById：根据id删除 removeByIds：根据id批量删除 removeByMap：根据Map中的键值对为条件删除 remove(Wrapper)：根据Wrapper条件删除 Get: List:","tags":["基础","MybatisPlus"],"categories":["基础笔记"]},{"title":"跨域支持笔记","path":"/2025/07/09/基础笔记/跨域支持笔记/","content":"JAVA后端解决跨域问题的几种方法前后端分离大势所趋，跨域问题更是老生常谈，随便用标题去google或百度一下，能搜出一大片解决方案，那么为啥又要写一遍呢，不急往下看。 问题背景Same Origin Policy，译为“同源策略”SOP。它是对于客户端脚本（尤其是JavaScript）的重要安全度量标准，其目的在于防止某个文档或者脚本从多个不同“origin”（源）装载。它认为自任何站点装载的信赖内容是不安全的。 当被浏览器半信半疑的脚本运行在沙箱时，它们应该只被允许访问来自同一站点的资源，而不是那些来自其它站点可能怀有恶意的资源。 注：具有相同的Origin，也即是拥有相同的协议、主机地址以及端口。一旦这三项数据中有一项不同，那么该资源就将被认为是从不同的Origin得来的，进而不被允许访问。 CORS就是为了解决SOP问题而生的，当然CORS不是唯一的解决方案，不过这里不赘述其他解决办法了。 CORS简介CORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing）。它允许浏览器向跨源(协议 + 域名 + 端口)服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。CORS需要浏览器和服务器同时支持。它的通信过程，都是浏览器自动完成，不需要用户参与。 对于开发者来说，CORS通信与同源的AJAXFetch通信没有差别，代码完全一样。浏览器一旦发现请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。 浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。 浏览器发出CORS简单请求，只需要在头信息之中增加一个Origin字段。 浏览器发出CORS非简单请求，会在正式通信之前，增加一次OPTIONS查询请求，称为”预检”请求（preflight）。浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 简单请求就是HEAD、GET、POST请求，并且HTTP的头信息不超出以下几种字段 Accept、Accept-Language、Content-Language、Last-Event-ID、Content-Type注：Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain反之，就是非简单请求。 其实实现CORS很简单，就是在服务端加一些响应头，并且这样做对前端来说是无感知的，很方便。 反之，就是非简单请求。 其实实现CORS很简单，就是在服务端加一些响应头，并且这样做对前端来说是无感知的，很方便。 详解响应头 Access-Control-Allow-Origin 该字段必填。它的值要么是请求时Origin字段的具体值，要么是一个*，表示接受任意域名的请求。 Access-Control-Allow-Methods 该字段必填。它的值是逗号分隔的一个具体的字符串或者*，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。 Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie.默认情况下，不发生Cookie，即：false。对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是applicationjson，这个值只能设为true。如果服务器不要浏览器发送Cookie，删除该字段即可。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。在有效期间，不用发出另一条预检请求。 顺便提一下，如果在开发中，发现每次发起请求都是两条，一次OPTIONS，一次正常请求，注意是每次，那么就需要配置Access-Control-Max-Age，避免每次都发出预检请求。 解决方法第一种办法import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class CorsConfig implements WebMvcConfigurer @Override public void addCorsMappings(CorsRegistry registry) registry.addMapping(/**) .allowedOrigins(*) .allowedMethods(GET, HEAD, POST, PUT, DELETE, OPTIONS) .allowCredentials(true) .maxAge(3600) .allowedHeaders(*); 这种方式是全局配置的，网上也大都是这种解决办法，但是很多都是基于旧的spring版本，比如 WebMvcConfigurerAdapter 在spring5.0已经被标记为Deprecated，点开源码可以看到： /** * An implementation of @link WebMvcConfigurer with empty methods allowing * subclasses to override only the methods theyre interested in. * * @author Rossen Stoyanchev * @since 3.1 * @deprecated as of 5.0 @link WebMvcConfigurer has default methods (made * possible by a Java 8 baseline) and can be implemented directly without the * need for this adapter */@Deprecatedpublic abstract class WebMvcConfigurerAdapter implements WebMvcConfigurer 像这种过时的类或者方法，spring的作者们一定会在注解上面说明原因，并告诉你新的该用哪个，这是非常优秀的编码习惯，点赞！ spring5最低支持到jdk1.8，所以注释中明确表明，你可以直接实现WebMvcConfigurer接口，无需再用这个适配器，因为jdk1.8支持接口中存在default-method。 Spring Boot 基础就不介绍了，看下这个教程太全了： https://github.com/javastacks/spring-boot-best-practice 第二种办法import org.springframework.context.annotation.Configuration;import javax.servlet.*;import javax.servlet.annotation.WebFilter;import javax.servlet.http.HttpServletResponse;import java.io.IOException;@WebFilter(filterName = CorsFilter )@Configurationpublic class CorsFilter implements Filter @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException HttpServletResponse response = (HttpServletResponse) res; response.setHeader(Access-Control-Allow-Origin,*); response.setHeader(Access-Control-Allow-Credentials, true); response.setHeader(Access-Control-Allow-Methods, POST, GET, PATCH, DELETE, PUT); response.setHeader(Access-Control-Max-Age, 3600); response.setHeader(Access-Control-Allow-Headers, Origin, X-Requested-With, Content-Type, Accept); chain.doFilter(req, res); 这种办法，是基于过滤器的方式，方式简单明了，就是在response中写入这些响应头，好多文章都是第一种和第二种方式都叫你配置，其实这是没有必要的，只需要一种即可。 这里也吐槽一下，大家不求甚解的精神。 第三种办法public class GoodsController @CrossOrigin(origins = http://localhost:4000)@GetMapping(goods-url)public Response queryGoodsWithGoodsUrl(@RequestParam String goodsUrl) throws Exception 没错就是@CrossOrigin注解，点开注解 @Target( ElementType.METHOD, ElementType.TYPE )@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CrossOrigin 从元注解@Target可以看出，注解可以放在method、class等上面，类似RequestMapping，也就是说，整个controller下面的方法可以都受控制，也可以单个方法受控制。 也可以得知，这个是最小粒度的cors控制办法了，精确到单个请求级别。 以上三种方法都可以解决问题，最常用的应该是第一种、第二种，控制在自家几个域名范围下足以，一般没必要搞得太细。 这三种配置方式都用了的话，谁生效呢，类似css中样式，就近原则，懂了吧。","tags":["基础","跨域支持"],"categories":["基础笔记"]},{"title":"2025.7.9学习日记","path":"/2025/07/09/学习日记25年7月/2025.7.9学习笔记/","content":"今日学习内容3DGS力扣每日一题:今天的每日是一个最大移动k次会议,使得最大连续空闲时间最长.抽象成功,和灵神的题解做的一样.转换成了对n+1个空闲时间区间滑窗求定长滑窗的最大和. 代码随想录项目相关实现注册接口 生活晚上健身练的胸 三头 核心 ,强度适中.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.8学习日记","path":"/2025/07/08/学习日记25年7月/2025.7.8学习笔记/","content":"今日学习内容3DGS力扣每日一题:一个dp＋二分的困难题. 代码随想录做了一个N皇后. 项目相关看了两篇项目博客,然后准备明天开始写日志过滤器和用户名密码登录接口. 生活早上踢球颠球 长传 短传 晚上健身练的背 二头 核心,强度偏大.拉伸腿部为主.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.7学习日记","path":"/2025/07/07/学习日记25年7月/2025.7.7学习笔记/","content":"今日学习内容3DGS部署CityGaussian,学习了一下CityGaussian论文,这篇文章通过优化原版3DGS密度控制模块,防止训练时高斯数量爆炸性增长,将原版3DGS存储压缩10倍，训练速度提升25%，内存减半,渲染效果和原版接近. 项目再次梳理了全局视图的框架.明天准备实现验权和登录相关的功能. 力扣每日一题:今天的每日是一个贪心加优先队列的方法. 代码随想录三道题 生活健身主要练胸 肩 三头 ,强度偏大","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.5学习日记","path":"/2025/07/05/学习日记25年7月/2025.7.5学习笔记/","content":"今日学习内容3DGSDifix3D方法相当于在3DGS渲染脚本render.py执行获得2D渲染图像后,再对2D渲染图像使用单步扩散模型,对2D渲染图进行处理.所以这种方式无法做到像原版3DGS一样对3D模型的实时渲染.因为其本质是对2D图像进行去噪,而不是对3D空间中离散的高斯球进行剔除.而3DGS实时渲染的基础就是render.py的快速渲染. 项目力扣每日一题:简单题,遍历求出现次数即可.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.4学习日记","path":"/2025/07/04/学习日记25年7月/2025.7.4学习笔记/","content":"今日学习内容3DGS看了difix3D论文,相当于加了一个单步扩散模型. 项目把全局配置显示拦截器实现了,目前项目可以正常显示主页信息.实现主页显示的过程基本上把项目框架给搭完了. 力扣每日一题:昨天每日的困难版本. 项目","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.3学习日记","path":"/2025/07/03/学习日记25年7月/2025.7.3学习笔记/","content":"今日学习内容3DGS看了difix3D论文,相当于加了一个单步扩散模型. 项目做了一天的项目,完善主页的过程基本把大部分的接口都实现了. 力扣每日一题:二进制有关的. 项目","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.7.1学习日记","path":"/2025/07/01/学习日记25年7月/2025.7.1学习笔记/","content":"今日学习内容3DGS部署CF-3DGS项目,阅读论文.这篇论文通过​​局部-全局联合优化​​直接估计相机位姿，省去SfM预处理步骤,具体效果需要实验验证.部署环境过程遇到一些依赖问题. 力扣每日一题:今天的每日一个简单的遍历即可做出. 项目做了半天之后觉得vue3并不适合这个博客项目,所以又换回到了Thymeleaf作为前端的方案.但是对于项目的理解我觉得是有非常大的进步的.目前重新创建了一个Techub项目,然后基本理解了Thymeleaf的前端框架.","tags":["日记","leetcode","项目"],"categories":["学习日记","2025-07"]},{"title":"2025.6.30学习日记","path":"/2025/06/30/学习日记25年6月/2025.6.30学习笔记/","content":"今日学习内容3DGS学习taming-gs论文. 力扣每日一题:每日是一个简单的哈希题. 项目今天开发了一个返回全文分类的接口.虽然看起来只开发一个接口,但是保证开发规范的话,属于牵一发动全身的感觉,写一个内容需要写好对应的DO DTO,然后最好把所有的常量之类的写成更加直观地枚举类. 个人博客晚上把个人博客的分类和专栏功能加进去了,这回看起来作用非常完善. 整理算法笔记把刷题算法笔记重新重构了一下,保留重要的标题. 生活记录晚上健身今天练的胸 肩 核心 ,强度适中.","tags":["3DGS","日记","leetcode","项目","博客"],"categories":["学习日记","2025-06"]},{"title":"2025.6.29学习日记","path":"/2025/06/29/学习日记25年6月/2025.6.29学习笔记/","content":"力扣每日一题:今天的每日一个滑窗,还是比较好想的. 力扣周赛全回来了,这周周赛做了三道题,而且第三道是最近练的回溯,虽然超时了,dfs改写成dp就过了. 生活记录早上踢球md,我的球踢草丛里了.","tags":["日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"2025.6.28学习日记","path":"/2025/06/28/学习日记25年6月/2025.6.28学习笔记/","content":"今日学习内容3DGS对训练的模型进行了汇总。dash方法的训练速度是最快的，同时渲染质量和原版比略有提升；mip方法渲染质量得分最高，但其受视角局限性影响最严重，渲染视角和输入视角差的稍微多一些就会看到空间中很多严重的鬼影。 力扣每日一题:spring篇spring篇学习完一遍,记了一篇笔记. 项目被依赖项搞了一下午,发现配置子模块后并不会自己扫描子模块的包,需要先配置一个AutoConfig类,然后通过配置AutoConfig配置子模块的包路径,才能被扫描到.不过还是感觉不从0开始,这些问题都不会遇到,还是有用的,印象非常深刻. Unity开发对带球的bug进一步优化. 晚上学代码随想录生活记录steam夏促买了一堆游戏","tags":["3DGS","日记","leetcode","项目","spring","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"2025.6.27学习日记","path":"/2025/06/27/学习日记25年6月/2025.6.27学习笔记/","content":"今日学习内容3DGS对训练的模型进行了汇总。dash方法的训练速度是最快的，同时渲染质量和原版比略有提升；mip方法渲染质量得分最高，但其受视角局限性影响最严重，渲染视角和输入视角差的稍微多一些就会看到空间中很多严重的鬼影。 力扣每日一题:项目添加了自动初始化数据库模块,使用liquibase来管理数据库sql的变更,并且在不开启liquibase的情况也做了考虑.感觉还是从项目里面学比较深刻,光看文档记不住. Unity开发把带球的逻辑重构了,通过切换带球人的图层来实现带球,先在看起来非常完美,然后把之前的踢球逻辑也加上了,看起来很不错. 晚上学代码随想录做了三道回溯 生活记录晚上健身练的背 肩膀 二头 三头 强度比较大.","tags":["3DGS","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"2025.6.26学习日记","path":"/2025/06/26/学习日记25年6月/2025.6.26学习笔记/","content":"今日学习内容3DGS简单跑跑代码,新部署了mip-gs 力扣每日一题:今天的每日和二进制有关的一个脑筋急转弯. Mysql学习54 项目整理项目结构,创建子模块. Unity开发早上把球跟随的逻辑重构了一下,打算的实现方案是通过切换图层,使得带球球员和球没有碰撞,但是能保持球的物理. 生活记录晚上回学校聚餐和球队哥们吃火锅玩阿瓦隆.","tags":["3DGS","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"2025.6.25学习日记","path":"/2025/06/25/学习日记25年6月/2025.6.25学习笔记/","content":"今日学习内容3DGS简单跑跑代码,新部署了mip-gs 力扣每日一题:二分加二分的困难题.已经刷了800道题目了. Mysql学习52 - 54 项目今天准备重新写项目TecHub作为我的主力项目. Unity开发早上新建了一个新项目之后,自动补全就神奇的好了,估计是新建项目之后自动下载了依赖包.晚上把人物移动控制逻辑重构了. 晚上学代码随想录晚上刷了两道回溯题 生活记录晚上健身练的胸肩 膀和二头,强度比较高.","tags":["3DGS","日记","leetcode","项目","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"TecHub项目笔记","path":"/2025/06/25/项目笔记/TecHub项目笔记/","content":"TecHub项目笔记大概会以一个时间线的方式,从0开始搭建项目. 贴一个复制 /** * @author JakicDong * @time 2025.6.25 * @description 启动类 * @version 1.0.0 */ 项目介绍以Spring3框架为基础,VUE3作为前端 配置本地Git仓库和GitHub远程仓库IDEA中建立项目根目录本地仓库,文件变红说明配置成功,然后提交一次.配置远程仓库,先在IDEA中登录GitHub账号,然后GitHub建立远程仓库然后在IDEA中配置远程仓库地址.git remote add origin git@github.com:xxx/xxx,git(origin后面替换成你刚复制的东西) git push -u origin master (解释:该脚本将本地的master 推到刚才设置的github远程仓库中)如果没有配过公钥的话可能会报错,配置一个公钥即可.然后就可以愉快的进行远程版本控制了. 子模块创建需要注意子模块和父模块pom文件的格式.子模块: parent groupIdcom.github.JakicDong.TecHub/groupId artifactIdTecHub/artifactId version1.0-SNAPSHOT/version/parent 父模块 parent groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter-parent/artifactId version3.0.9/version relativePath/ !-- lookup parent from repository --/parent 这个项目用的就是spring3,所以父模块继承一个spring3的框架. 入口 @SpringBootApplicationpublic class TecHubApplication { 这是一个入口类 再次测试} 导入了依赖项 进度将项目结构进行完整的梳理自动加载初始化comgithubJakicDongTecHubconfiginitForumDataSourceInitializer.java 这一部分包括很多的知识点,liquibase来管理更改sql,xml解析,jdbc连接数据库等等,然后实现了一个在没有liquibase的情况下的一个初始化器. 实现了分类接口的list功能实现用户名密码登录方式实现Reqinfo全局拦截器HTTP Request ↓Filter#doFilter() ↓──→ Interceptor#preHandle() ↓Controller Method ↓Interceptor#postHandle() ↓Filter#doFilter()剩余代码 全局异常处理ForumExceptionHandler通过实现ExceptionHandler接口,实现异常处理. 但是注意,这个异常处理只会处理接口中抛出的异常,不会处理没有接口处理的请求. STOMP实现全局消息通知2025.9.8实现 总结一下就是,通过STOMP可以实现一个实时响应的全局通知功能.首先通过客户端连接服务端端口,来实现连接,此时进行身份验证和校验.然后客户端对broker进行订阅,订阅的目的是为了接收服务端发送的消息.然后如果客户端向服务端发送消息,会向进行发送app,然后去掉app前缀,把后面的部分作为消息的destination,然后向MessageMapping节点进行转发即刻进行处理.服务端发送给客户端消息,会直接转发到broker端口进行发送,客户端只需要对broker端口进行监听即可. 然后就是选型,为什么要使用STOMP,而不是RabbitMQ,因为首先STOMP的实时性能更好,并且这个项目的使用场景是公司内部,所以长连接数量不会很多,所以选择STOMP.此外STOMP是一个全双工的协议,可以实现实时的双向通信且实现简单,可以满足项目需求. 关于点赞 收藏 评论等操作的通知在本项目中,点赞收藏等操作的通知是通过Spring的发布者订阅者模式来实现的.因为点赞收藏评论等这些操作他们并不是和Controller功能内聚的,为了实现高内聚低耦合的结果,所以可以这些重复的Controller里面的操作提取出来,通过消息发布订阅模型来实现不论在哪个接口,只需要通过发布消息,来实现相同的功能. 高内聚低耦合：通知属于“横切的后置能力”，不该绑死在某个 Controller 上；抽到事件监听后，控制层更纯净。 复用与一致性：任意入口（多个 Controller、服务内部、任务）只要发布同一种事件，就能触发同一套通知逻辑，避免重复代码。 性能与体验：配合异步，主流程更快返回；通知在后台统一处理。 并且监听侧要做幂等与去重（你们的点赞关注已处理）。注意事务发布时机与是否需要同步异步（强一致走同步或事务提交后事件，其他走异步）。","tags":["项目","TecHub"],"categories":["项目笔记"]},{"title":"2025.6.24学习日记","path":"/2025/06/24/学习日记25年6月/2025.6.24学习笔记/","content":"今日学习内容3DGS实验室汇报,做汇报PPT. 下载mip-splatting工程,该项目引入了3D平滑滤波器和二维Mip滤波器,通过限制3D表示的频率来消除高频伪影,二位Mip滤波器通过模拟物理成像过程的盒式滤波器来缓解混叠效应和拓展问题.使得Mip-splatting可以在各种采样频率下实现无伪影的渲染. 力扣每日一题:一个简单滑窗问题. Mysql学习48 - 52索引篇学习完毕 Spring学习22 - 30 今天实验室汇报项目安装VUE3框架sudo npm install webpack -gsudo npm install -g @vue/clivue ui 可以看到VUE的ui界面 装了spring3框架的paicoding. Unity时隔六个月,把我之前做的2D足球游戏demo部署在mac上了,继续开发. 晚上学代码随想录重新整理一下算法笔记. 生活记录晚上健身练的背和三头,强度比较低.","tags":["3DGS","日记","leetcode","项目","spring","游戏开发"],"categories":["学习日记","2025-06"]},{"title":"游戏开发-2D足球Demo","path":"/2025/06/24/游戏开发/游戏开发-2D足球Demo/","content":"游戏介绍2D足球Demo(todo:想个好名字),是一个基于Unity的2.5D横版足球游戏.玩家操纵角色在球场上和AI进行对战. 核心玩法爽感十足的射门,和守门员的博弈.进攻时,选择远射或过人或者传球给队友;防守时,选择保守的防守还是大胆断球. 画风偏向像素风格. TODO LIST优先级 1 到 10 ,越小优先级越高 3 优化射门BUG3 优化守门员逻辑3 设计对手球员状态机3 优化碰撞箱5 添加更多样式的角色美术类 9 实现角色换装功能5 添加界面5 添加UI6 音乐设计一些音乐 完成清单重构角色移动逻辑1 优化带球逻辑现在的带球逻辑很合理","tags":["游戏开发","2D足球Demo"],"categories":["游戏开发"]},{"title":"Unity游戏开发笔记","path":"/2025/06/24/游戏开发/Unitu开发笔记/","content":"核心组件在Unity中，核心组件之间的关系和访问方式可以这样梳理： 基础组件关系图 GameObject (游戏对象)│├── Transform (变换组件) - 控制位置旋转缩放├── Collider2D (碰撞体) - 物理交互基础├── Rigidbody2D (刚体) - 物理模拟└── MonoBehaviour (脚本) - 自定义逻辑 关键组件访问方式 (1) Transform // 获取自身TransformTransform selfTF = GetComponentTransform(); // 访问属性Vector3 position = selfTF.position;Quaternion rotation = selfTF.rotation;Vector3 scale = selfTF.localScale;// 修改属性selfTF.position = new Vector3(1, 0, 0);selfTF.Rotate(0, 90, 0); (2) GameObject // 获取当前对象GameObject selfGO = gameObject; // 通过名称查找子对象Transform childTF = transform.Find(foot);// 激活状态控制gameObject.SetActive(false); (3) Collider2D Apply// 获取碰撞体Collider2D collider = GetComponentCollider2D();// 启用/禁用collider.enabled = false;// 触发检测 (需挂载脚本)void OnTriggerEnter2D(Collider2D other) if(other.CompareTag(Player)) ... 组件间相互调用方式1：直接获取组件 Apply// 获取同一物体上的组件Rigidbody2D rb = GetComponentRigidbody2D();// 获取其他物体的组件（需先获取GameObject）GameObject player = GameObject.Find(Player);charactor_move_v script = player.GetComponentcharactor_move_v(); 方式2：通过公开引用 Apply// 在Inspector面板拖拽赋值public Transform targetTF;void Start() Vector3 pos = targetTF.position; 方式3：父子层级访问 // 获取父对象Transform parent = transform.parent;// 获取子对象Transform child = transform.GetChild(0); 你的代码中的典型调用案例 Apply// 获取Rigidbody2D组件（物理模拟）private Rigidbody2D rb;void Start() rb = GetComponentRigidbody2D();// 通过标签查找对象if(other.CompareTag(FollowBox)) ...// 访问其他脚本属性float power = currentPlayer.GetComponentcharactor_move_v().kick_power;// 修改碰撞体状态playerCollider.enabled = false; 最佳实践建议缓存引用：在Start()中获取常用组件，避免重复调用GetComponent分层查找：使用transform.Find(“路径子对象”)按路径查找空值检查：对GetComponent结果做null检查性能优化：避免在Update中频繁查找对象这种组件系统构成了Unity的”组合优于继承”架构，通过灵活的组合实现复杂游戏逻辑。 多版本.NET切换dotnet –list-sdks 查看已安装的.NET版本","tags":["游戏开发"],"categories":["游戏开发"]},{"title":"技术派项目环境问题","path":"/2025/06/24/项目笔记/技术派环境问题/","content":"重构成spring3 支持jdk17 Mac的jdk多版本切换cd LibraryJavaJavaVirtualMachinesls -al 查看一下都有哪些版本的jdk # 输入cd ~# 打开环境变量配置文件code .bash_profile# 报错：.bash_profile does not exist.# 第一次配置环境变量，先创建文件touch .bash_profile# 再次执行打开环境变量配置文件code .bash_profile # 复制如下内容粘贴到.bash_profile中，# 因为我是安装了三个，所以配置了三个版本# 你自己是安装了几个版本就配置几个export JAVA_8_HOME=$(/usr/libexec/java_home -v1.8)export JAVA_11_HOME=$(/usr/libexec/java_home -v11)export JAVA_17_HOME=$(/usr/libexec/java_home -v17)alias java8=export JAVA_HOME=$JAVA_8_HOMEalias java11=export JAVA_HOME=$JAVA_11_HOMEalias java17=export JAVA_HOME=$JAVA_17_HOMEexport JAVA_HOME=$JAVA_11_HOME# 记得保存，可以用快捷键 cmd + s # 配置文件立即生效source .bash_profile# 查看 JAVA_HOME 目录echo $JAVA_HOME# 查看 JDK 版本信息java -version #切换到JDK8：java8 # 查看 JDK 版本信息java -version#切换到JDK11：java11# 查看 JDK 版本信息java -version#切换到JDK17：java17# 查看 JDK 版本信息java -version 更改Maven版本路径位置: /Users/mac/tools/apache-maven-3.8.9 sudo code ~/.bash_profile export MAVEN_HOME”Usersmactoolsapache-maven-3.8.9”export PATH”$MAVEN_HOMEbin:$PATH” 安装完maven后更改仓库镜像地址即可,更改为了阿里云 admin模块端口问题paicoding-adminvite.config.ts修改代理端口即可","tags":["项目","技术派"],"categories":["项目笔记"]},{"title":"2025.6.23学习日记","path":"/2025/06/23/学习日记25年6月/2025.6.23学习笔记/","content":"今日学习内容杂谈上周末时隔快一年重新做了一次周赛,第一题手速还是可以的,第二题和第三题两道中等有点吃力,第二题是完全背包相关的题目,动态规划题目一直有点苦手,需要狠狠加强一波.第三题图论虽然想到了思路,但是图论具体细节没实现完全,做的太少了.感觉算法遇到瓶颈了,低分题基本可以做出但是复杂的题目差点意思,现在1750分,接下来一个月准备重心放在算法上. 3DGS今天用陈昊师兄的数据跑了原版和DashGaussian进行测试，发现整体渲染质量都比较低，渲染7000次，PSNR都只有20多，使用viewer工具观察后发现主要是草丛处点云数量过少。个人理解可能是在训练过程中，草丛部分纹理过于复杂，导致局部梯度均值相抵消无法触发3DGS的自动增殖模块，导致高斯无法拟合草丛过于复杂的纹理，使得整体渲染质量下降。 力扣每日一题:今天的每日是关于回文串的困难题. Mysql学习41 - 47 Spring学习17 - 21 做明天实验室汇报的PPT准备讲Dash高斯论文. 汇报内容 本周汇报的文章是DashGaussian,主要是对原版3DGS的训练速度进行了大幅度的优化.甚至可以将训练时间压缩到200秒. 这篇文章发现,高斯训练的速度主要就受两个因素的影响,一个是真值图像的分辨率,还有就是高斯球的数量.而DashGaussian通过动态的调节这两个因素,使得训练速度得到大幅度的提高的同时,渲染质量基本保持一致. 这篇文章发现,基本90%的训练时间基本都在前向渲染,反向传播和高斯球参数更新上;并且高分辨率的图像在渲染前期存在浪费的现象.后期的优化会有非常大的边际效应递减.在右上角的表格可以看到,原版3DGS的图像分辨率是一致的,但是DashGaussian的图像分辨率是随时间动态上升的.这样的话可以减少在训练初期使用高分辨率图像的浪费.第二个图标可以看到,原版3DGS的高斯球数量随着迭代,会以一个上凸的形状快速达到数量上限,导致训练进程变慢,而Dash高斯会在前期抑制高斯球的增殖,高斯球数量以一个下凹的形状上升,可以大幅度增加训练速度.这个图是这篇文章的结构图.他的核心就是引入了对图像分辨率和高斯球数量的动态调节机制. 然后就是一个关键的问题就是通过什么机制来决定分辨率的动态变化的.这篇文章引入了频率公式来表示,这个公式是对每张训练的视图做傅里叶变换,得到它的频率表示Fn,然后计算所有频率分量的L2范数平均值,这个值作为频率能量,高频区域也就是细节部分数值更大,低频(也就是平滑部分)数值小.将高斯球训练的过程比成从低频到高频的过程,初期低分辨率时先对物体轮廓和颜色进行拟合,后期高分辨率部分对纹理和细节进行拟合. 然后就是文章定义的分辨率调度器,会预先计算各个分辨率的频率能量,然后计算sr.然后当训练步数达到sr后切换到更高分辨率.这部分是对分辨率的控制 然后是对高斯球数量的控制:这个是设计的增长曲线,它的特点是是一个凹函数,在前期增长慢,在中期随着分辨率提升加速.第二个公式代表高斯球最终数量上限会根据当前新增高斯球数量来动态的预测.这部分就是对高斯球数量的控制. 这是在不同数据集上进行的实验可以看到训练时间部分和其他相比,有了非常大的提升,并且渲染质量在几个指标略有提升. 可以看到在不同数据集和方法下,加入文章的方法后训练速度都有明显的加快,并且训练质量也有一定增强. 左图可以看到随迭代逐渐变大的图像分辨率,中间图实现部分可以看到本文方法下,高斯球数量曲线呈下凹状态,右图可以看到训练耗时最大的三个部分耗时都分别有了不同程度的下降. 这是文章做的消融实验可以两个模块对渲染速度和质量均有一定的提升. 晚上学代码随想录把二叉树的题做完了,把算法笔记二叉树部分收个尾. 生活记录早训踢球颠球 短传 逆足 晚上健身练练胸和三头,强度比较低.","tags":["3DGS","日记","leetcode","spring"],"categories":["学习日记","2025-06"]},{"title":"2025.6.22学习日记","path":"/2025/06/22/学习日记25年6月/2025.6.22学习笔记/","content":"今日学习内容力扣每日一题:简单的字符串操作. 力扣周赛回归周赛,准备再次开始一段时间的比赛,今天2 3 题答题思路都有,就差一点点,图论和动态规划还需要加强. 生活记录晚上健身练背和二头","tags":["日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"2025.6.21学习日记","path":"/2025/06/21/学习日记25年6月/2025.6.21学习笔记/","content":"今日学习内容力扣每日一题:今天的每日是一个哈希计数＋排序＋贪心的中等题,比较简单. Mysql学习37-41 B树学习记了一篇笔记. 晚上学代码随想录生活记录晚上健身练背和二头","tags":["日记","leetcode","spring","mysql","二叉树"],"categories":["学习日记","2025-06"]},{"title":"一次性搞懂B树","path":"/2025/06/21/算法笔记/一次性搞懂B树/","content":"可视化网站算法可视化网站 B-Tree的引入从磁盘查找数据效率低的原因 读写数据越大速度越慢 读写次数越多速度越慢 设计文件查找系统索引可以提供更快的查询. 哈希表优点:等值查询比较快缺点: hash冲突后,数据散列不均匀,产生大量线性查询,效率低 等值查询可以,但是遇到范围查询,需要遍历,hash不合适 树树的种类树 二叉树 BST二叉查找树 AVL平衡二叉树 红黑树 B树 B+树 二叉排序树 BST插入数据的时候得有序,必须保证: 若左子树不为空,则左子树的所有节点的值小于根节点的值 若右子树不为空,则右子树的所有节点的值大于根节点的值 问题:会退化为链表,查询效率降低为O(n). 平衡二叉树 AVL插入数据的时候保持二叉排序树平衡左子树和右子树的高度差不能超过1. 问题:用插入的成本来弥补查询的成本,插入效率降低为O(logn),但是查询效率还是O(logn).一旦出现插入操作比查询操作多的情况就不合适了. 红黑树最长子树不超过最短子树的2倍. 性质1 :根节点是黑色的. 性质2 :每个红色的节点的两个子节点都是黑色.(从每个叶子到根的所有路径上不能有两个连续的红色节点) 性质3 :从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点. 相当于不让AVL做大量的旋转操作.红黑树口诀:左根右 , 根叶黑 . 不红红 , 黑路同. 问题: 当数据特别多的时候,树的深度会很大,就意味着IO的次数会变多,影响读取的效率. B树B树就是一个有序的多路查询树. 满足下列要求的m叉树: 书中每个节点只多有m个孩子节点(至多有m-1个关键字) 每个节节点的结构为:n代表这个节点有几个关键字.P0第一个子树的地址.k1关键字 例子:m4的4阶B树阶数代表单个节点最多有的子节点数量","tags":["基础","算法","B树"],"categories":["算法笔记"]},{"title":"2025.6.20学习日记","path":"/2025/06/20/学习日记25年6月/2025.6.20学习笔记/","content":"今日学习内容3DGS换了内存条,可能会对训练速度有影响.重新训练DashGaussian,修改了一下训练的代码,将预先存入cuda的代码改成先存入内存,使用时再动态存入cuda.虽然会变慢,但是可以让显存占用减少.下午用R8设备外出采集数据.继续学习Dash论文. 力扣每日一题:上下左右移动,能修改k次移动方向,求最大曼哈顿距离.比较简单,随着移动更新答案即可. Mysql学习索引部分学习了几条. Spring学习学了几条,并且记了笔记. 项目文档学习异常处理部分,学习了一下. 晚上学代码随想录做了几个递归和回溯的二叉树题.把Morris遍历二叉树又复习了一遍,后序遍历比较有意思,是在向右的前序遍历结果上,翻转一下. 生活记录晚上健身练肩,胸,三头递减组.","tags":["3DGS","日记","leetcode","项目","spring","mysql","二叉树","Morris遍历"],"categories":["学习日记","2025-06"]},{"title":"2025.6.19学习日记","path":"/2025/06/19/学习日记25年6月/2025.6.19学习笔记/","content":"今日学习内容3DGS学习了DashGaussian论文,核心目的就是在尽可能保持渲染质量的同时,加速高斯训练速度.文章提出在训练初始阶段,高斯球是相对比较稀疏的,此时没有必要使用高分辨率图像进行训练,所以在训练初期使用低分辨率图像训练,然后通过计算低分辨率和高分辨率渲染的分数,来判断是否需要切换更高分辨率.然后在高斯球致密化策略上,文章提出的方法是在训练前期,抑制高斯球的增殖,然后在训练中后期再鼓励高斯球的增殖.因为高斯训练的速度主要就取决于高斯球的数量,而高斯球数量达到一定数目后,训练效果变化很小,但训练速度会变得非常慢.所以使高斯球数量随时间上升曲线呈下凹的形状是最兼顾训练速度和质量的. 力扣每日一题:又是一个排序＋贪心的中等题,比较简单. Mysql学习优化篇学完了,明天开始索引篇学习. Spring学习学了四条,并且记了笔记. 项目文档学习图片上传部分.MapStruct,一个用于转换对象的工具. 晚上学代码随想录做了几道二叉树题.学习了一个Morris遍历方式,这种方式可以在O(1)的空间复杂度下完成二叉树的遍历,并且不需要使用栈来存储节点.贴一个自己画的伪代码: 生活记录晚上健身练肩,飞鸟超级组四组.","tags":["3DGS","日记","leetcode","项目","spring","mysql","二叉树","Morris遍历"],"categories":["学习日记","2025-06"]},{"title":"2025.6.18学习日记","path":"/2025/06/18/学习日记25年6月/2025.6.18学习笔记/","content":"今日学习内容3DGS训练DashGaussian.昨天DashGaussian训练过程会出现报错,排查后发现是子模块版本问题导致,重新安装子模块后消除报错.并且在昨天的训练过程中,在对于复杂的场景会出现爆显存的问题:通过修改scenecameras.py中图片预先加载到显存的逻辑,把图片加载到内存,解决了爆显存的问题,虽然会因IO时间增加,训练变慢,但是可以接受. 力扣每日一题:排序加贪心的中等题. 二叉树通过中序和后序构造二叉树.通过对中序做一个索引,找到左右树的分割位置,然后递归. class Solution MapInteger,Integer in_idx = new HashMap(); public TreeNode buildTree(int[] inorder, int[] postorder) for(int i=0 ; iinorder.length ; ++i) in_idx.put(inorder[i],i); int n = inorder.length; return dfs(inorder,postorder,0,n-1,0,n-1); private TreeNode dfs(int[] inorder,int[] postorder , int beg_in , int end_in,int beg_pos , int end_pos) //全部闭区间 if(beg_in end_in ||beg_pos end_pos )return null; int rootIndex = in_idx.get(postorder[end_pos]); TreeNode root = new TreeNode(inorder[rootIndex]); int lenOfLeft = rootIndex - beg_in;//左子树个数 int n=inorder.length; root.left = dfs(inorder,postorder,beg_in,beg_in+lenOfLeft-1,beg_pos,beg_pos+lenOfLeft-1); root.right = dfs(inorder,postorder,rootIndex+1,end_in,beg_pos+lenOfLeft,end_pos-1); return root; 通过前序和后序构造二叉树.对应二叉树不唯一,找前序根节点的下一个位置在后序中的位置,然后找左右子树,剩下做法差不多了. Mysql学习Spring学习晚上学代码随想录生活记录晚上健身今天练的后侧链 背 手臂","tags":["3DGS","日记","leetcode","spring","mysql","二叉树"],"categories":["学习日记","2025-06"]},{"title":"2025.6.17学习日记","path":"/2025/06/17/学习日记25年6月/2025.6.17学习笔记/","content":"今日学习内容3DGS在Ubuntu系统上部署了DashGaussian项目.学习DashGaussian项目的改进点.用DashGaussian训练中. 力扣每日一题:一道很有意思的数学题.正问题比较复杂,但是反过来想分割成为多少块就很简单了.https://leetcode.cn/problems/count-the-number-of-arrays-with-k-matching-adjacent-elements?envType=daily-questionenvId=2025-06-17这道题做完之后,做了灵神模运算的笔记,复习了一下快速幂.求组合数的话,先求n! 然后从后向前求1n!. Mysql学习开始学习SQL优化篇,慢sql优化. Spring学习简单学了几条bean相关的. 晚上学代码随想录二叉树三四道题. 生活记录早上踢球今早七点训练一会儿.颠球,逆足,短传.右膝内侧还是有点不舒服,需要养一养.","tags":["3DGS","日记","leetcode","spring","mysql","模运算","快速幂"],"categories":["学习日记","2025-06"]},{"title":"2025.6.16学习日记","path":"/2025/06/16/学习日记25年6月/2025.6.16学习笔记/","content":"今日学习内容3DGS:今日工作总结配环境.DashGaussian项目部署在Win系统有问题,所以装了一个ubuntu双系统.配置ubuntu环境,基本环境已配置完成. 明日工作计划在Ubuntu系统上部署DashGaussian项目.尝试运行一下. 力扣每日一题:维护最小值遍历的简单题. Mysql学习学完了日志篇 项目学习看了三四篇文档. 晚上学代码随想录二叉树三四道题. 生活记录晚上健身晚上健身 后侧链","tags":["3DGS","日记","leetcode","项目","mysql"],"categories":["学习日记","2025-06"]},{"title":"2025.6.14学习日记","path":"/2025/06/14/学习日记25年6月/2025.6.14学习笔记/","content":"今日学习内容今天一直在配环境……………. 3DGS:今日工作总结配置Effect3DGS环境，安装子模块时出现安装报错: 配置Effect3DGS环境时,子模块一直无法安装成功.尝试重新配置了多个conda环境来安装子模块,但重新安装其他依赖后,安装子模块diff-gaussian-rasterization仍然报错.Effect3DGS项目没有环境依赖文件,也没在github上贴出,只能先暂时搁置.看了一些其他的3DGS项目 明日工作计划准备部署DashGaussian项目:3DGS训练加速方法. 力扣每日一题:二分答案+贪心+相邻相减计数 Spring学习做了一个工厂 晚上学代码随想录生活记录","tags":["3DGS","日记","leetcode","spring","mysql"],"categories":["学习日记","2025-06"]},{"title":"2025.6.13学习日记","path":"/2025/06/13/学习日记25年6月/2025.6.13学习笔记/","content":"今日学习内容3DGS:今日工作总结在drjohnson数据集与playroom数据集跑完了原版3DGS.发现viewer中的帧率显示并不准确,存在锁帧率上限和波动的情况,调试了一个测量3DGS的FPS的脚本,可以准确测出3DGS渲染帧率.配置Effect3DGS环境，尝试运行出现一些版本依赖问题。 明日工作计划部署EfficientGS环境,用改进方法跑数据集. 力扣每日一题:二分答案+贪心+相邻相减计数 看mysql看到29条,日志篇延伸出去看多了,看到下午四点半. Spring学习晚上学代码随想录生活记录晚上健身今天主要练的背.","tags":["3DGS","日记","leetcode","spring","mysql"],"categories":["学习日记","2025-06"]},{"title":"Spring学习笔记","path":"/2025/06/12/Java问答笔记/Spring学习笔记/","content":"基础1.Spring 是什么？特性？有哪些模块？一句话概括：Spring 是一个轻量级、非入侵式的控制反转 (IoC)和面向切面 (AOP) 的框架。 2003 年，一个音乐家 Rod Johnson 决定发展一个轻量级的 Java 开发框架，Spring作为 Java 战场的龙骑兵渐渐崛起，并淘汰了EJB这个传统的重装骑兵。 到了现在，企业级开发的标配基本就是 Spring5+ Spring Boot 2 + JDK 8 Spring 有哪些特性呢？ 1. IoC 和 DI 的支持Spring 的核心就是一个大的工厂容器，可以维护所有对象的创建和依赖关系，Spring 工厂用于生成 Bean，并且管理 Bean 的生命周期，实现高内聚低耦合的设计理念。 2. AOP 编程的支持Spring 提供了面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等切面功能。 3. 声明式事务的支持支持通过配置就来完成对事务的管理，而不需要通过硬编码的方式，以前重复的一些事务提交、回滚的 JDBC 代码，都可以不用自己写了。 4. 快捷测试的支持Spring 对 Junit 提供支持，可以通过注解快捷地测试 Spring 程序。 5. 快速集成功能方便集成各种优秀框架，Spring 不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz 等）的直接支持。 6. 复杂 API 模板封装Spring 对 JavaEE 开发中非常难用的一些 API（JDBC、JavaMail、远程调用等）都提供了模板化的封装，这些封装 API 的提供使得应用难度大大降低。 简单说一下什么是AOP 和 IoC？AOP：面向切面编程，是一种编程范式，它的主要作用是将那些与核心业务逻辑无关，但是对多个对象产生影响的公共行为封装起来，如日志记录、性能统计、事务等。IoC：控制反转，是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。 Spring源码看过吗？看过一些，主要就是针对 Spring 循环依赖、Bean 声明周期、AOP、事务、IOC 这五部分。详情看笔记,Spring源码笔记. 2.Spring 有哪些模块呢？Spring 框架是分模块存在，除了最核心的Spring Core Container是必要模块之外，其他模块都是可选，大约有 20 多个模块。 最主要的七大模块： Spring Core：Spring 核心，它是框架最基础的部分，提供 IoC 和依赖注入 DI 特性。 Spring Context：Spring 上下文容器，它是 BeanFactory 功能加强的一个子接口。 Spring Web：它提供 Web 应用开发的支持。 Spring MVC：它针对 Web 应用中 MVC 思想的实现。 Spring DAO：提供对 JDBC 抽象层，简化了 JDBC 编码，同时，编码更具有健壮性。 Spring ORM：它支持用于流行的 ORM 框架的整合，比如：Spring + Hibernate、Spring + iBatis、Spring + JDO 的整合等。 Spring AOP：即面向切面编程，它提供了与 AOP 联盟兼容的编程实现。 3.Spring 有哪些常用注解呢？Spring 提供了大量的注解来简化 Java 应用的开发和配置，主要用于 Web 开发、往容器注入 Bean、AOP、事务控制等。 Web 开发方面有哪些注解呢？①、@Controller：用于标注控制层组件。②、@RestController：是@Controller 和 @ResponseBody 的结合体，返回 JSON 数据时使用。③、@RequestMapping：用于映射请求 URL 到具体的方法上，还可以细分为：@GetMapping：只能用于处理 GET 请求@PostMapping：只能用于处理 POST 请求@DeleteMapping：只能用于处理 DELETE 请求④、@ResponseBody：直接将返回的数据放入 HTTP 响应正文中，一般用于返回 JSON 数据。⑤、@RequestBody：表示一个方法参数应该绑定到 Web 请求体。⑥、@PathVariable：用于接收路径参数，比如 @RequestMapping(“hello{name}”)，这里的 name 就是路径参数。⑦、@RequestParam：用于接收请求参数。比如 @RequestParam(name “key”) String key，这里的 key 就是请求参数。 容器类注解有哪些呢？@Component：标识一个类为 Spring 组件，使其能够被 Spring 容器自动扫描和管理。@Service：标识一个业务逻辑组件（服务层）。比如 @Service(“userService”)，这里的 userService 就是 Bean 的名称。@Repository：标识一个数据访问组件（持久层）。@Autowired：按类型自动注入依赖。@Configuration：用于定义配置类，可替换 XML 配置文件。@Value：用于将 Spring Boot 中 application.properties 配置的属性值赋值给变量。 AOP 方面有哪些注解呢？@Aspect 用于声明一个切面，可以配合其他注解一起使用，比如：@After：在方法执行之后执行。@Before：在方法执行之前执行。@Around：方法前后均执行。@PointCut：定义切点，指定需要拦截的方法。事务注解有哪些？主要就是 @Transactional，用于声明一个方法需要事务支持。 4.Spring 中应用了哪些设计模式呢？Spring 框架中用了蛮多设计模式的： ①、比如说工厂模式用于 BeanFactory 和 ApplicationContext，实现 Bean 的创建和管理。 ApplicationContext context = new ClassPathXmlApplicationContext(applicationContext.xml);MyBean myBean = context.getBean(MyBean.class); ②、比如说单例模式，这样可以保证 Bean 的唯一性，减少系统开销。 ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);MyService myService1 = context.getBean(MyService.class);MyService myService2 = context.getBean(MyService.class);// This will print true because both references point to the same instanceSystem.out.println(myService1 == myService2); ③、比如说 AOP 使用了代理模式来实现横切关注点（如事务管理、日志记录、权限控制等）。 @Transactionalpublic void myTransactionalMethod() // 方法实现 Spring如何实现单例模式？Spring 通过 IOC 容器(控制反转)实现单例模式，具体步骤是： 单例 Bean 在容器初始化时创建并使用 DefaultSingletonBeanRegistry 提供的 singletonObjects进行缓存。 // 单例缓存private final MapString, Object singletonObjects = new ConcurrentHashMap();public Object getSingleton(String beanName) return this.singletonObjects.get(beanName);protected void addSingleton(String beanName, Object singletonObject) this.singletonObjects.put(beanName, singletonObject); 在请求 Bean 时，Spring 会先从缓存中获取。 39.Spring 容器、Web 容器之间的区别？（补充）Spring 容器是 Spring 框架的核心部分，负责管理应用程序中的对象生命周期和依赖注入。Web 容器（也称 Servlet 容器），是用于运行 Java Web 应用程序的服务器环境，支持 Servlet、JSP 等 Web 组件。常见的 Web 容器包括 Apache Tomcat、Jetty等。Spring MVC 是 Spring 框架的一部分，专门用于处理 Web 请求，基于 MVC（Model-View-Controller）设计模式。 IoC5.说一说什么是 IoC、DI？所谓的IoC，就是由容器来控制对象的生命周期和对象之间的关系。控制对象生命周期的不再是引用它的对象，而是容器，这就叫控制反转（Inversion of Control）。 没有 IoC 之前： 我需要一个女朋友，刚好大街上突然看到了一个小姐姐，人很好看，于是我就自己主动上去搭讪，要她的微信号，找机会聊天关心她，然后约她出来吃饭，打听她的爱好，三观。。。 有了 IoC 之后： 我需要一个女朋友，于是我就去找婚介所，告诉婚介所，我需要一个长的像赵露思的，会打 Dota2 的，于是婚介所在它的人才库里开始找，找不到它就直接说没有，找到它就直接介绍给我。 婚介所就相当于一个 IoC 容器，我就是一个对象，我需要的女朋友就是另一个对象，我不用关心女朋友是怎么来的，我只需要告诉婚介所我需要什么样的女朋友，婚介所就帮我去找。 Spring 倡导的开发方式就是这样，所有类的创建和销毁都通过 Spring 容器来，不再是开发者去 new，去 null，这样就实现了对象的解耦。 于是，对于某个对象来说，以前是它控制它依赖的对象，现在是所有对象都被 Spring 控制。 说说什么是DI?IOC 是一种思想，DI 是实现 IOC 的具体方式，比如说利用注入机制（如构造器注入、Setter 注入）将依赖传递给目标对象。 2004 年，Martin Fowler 在他的文章《控制反转容器依赖注入模式》首次提出了 DI（依赖注入，Dependency Injection） 这个名词。 打个比方，你现在想吃韭菜馅的饺子，这时候就有人用针管往你吃的饺子里注入韭菜鸡蛋馅。就好像 A 类需要 B 类，以前是 A 类自己 new 一个 B 类，现在是有人把 B 类注入到 A 类里。 为什么要使用 IoC 呢？在平时的 Java 开发中，如果我们要实现某一个功能，可能至少需要两个以上的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new 一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系。 有了 Spring 之后，就不一样了，创建 B 的工作交给了 Spring 来完成，Spring 创建好了 B 对象后就放到容器中，A 告诉 Spring 我需要 B，Spring 就从容器中取出 B 交给 A 来使用。 至于 B 是怎么来的，A 就不再关心了，Spring 容器想通过 newnew 创建 B 还是 new 创建 B，无所谓。 这就是 IoC 的好处，它降低了对象之间的耦合度，使得程序更加灵活，更加易于维护。 6.能简单说一下 Spring IoC 的实现机制吗？Spring 的 IoC 本质就是一个大工厂，我们想想一个工厂是怎么运行的呢？ 生产产品：一个工厂最核心的功能就是生产产品。在 Spring 里，不用 Bean 自己来实例化，而是交给 Spring，应该怎么实现呢？——答案毫无疑问，反射。那么这个厂子的生产管理是怎么做的？你应该也知道——工厂模式。 库存产品：工厂一般都是有库房的，用来库存产品，毕竟生产的产品不能立马就拉走。Spring 我们都知道是一个容器，这个容器里存的就是对象，不能每次来取对象，都得现场来反射创建对象，得把创建出的对象存起来。 订单处理：还有最重要的一点，工厂根据什么来提供产品呢？订单。这些订单可能五花八门，有线上签签的、有到工厂签的、还有工厂销售上门签的……最后经过处理，指导工厂的出货。在 Spring 里，也有这样的订单，它就是我们 bean 的定义和依赖关系，可以是 xml 形式，也可以是我们最熟悉的注解形式。 我们简单地实现一个 mini 版的 Spring IoC： 7.说说 BeanFactory 和 ApplicantContext?可以这么比喻，BeanFactory 是 Spring 的“心脏”，而 ApplicantContext 是 Spring 的完整“身躯”。 BeanFactory 主要负责配置、创建和管理 bean，为 Spring 提供了基本的依赖注入（DI）支持。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上添加了企业级的功能支持。 详细说说 BeanFactoryBeanFactory 位于整个 Spring IoC 容器的顶端，ApplicationContext 算是 BeanFactory 的子接口。 它最主要的方法就是 getBean()，这个方法负责从容器中返回特定名称或者类型的 Bean 实例。 来看一个 XMLBeanFactory（已过时） 获取 bean 的例子： class HelloWorldApp public static void main(String[] args) BeanFactory factory = new XmlBeanFactory (new ClassPathResource(beans.xml)); HelloWorld obj = (HelloWorld) factory.getBean(itwanger); obj.getMessage(); 请详细说说 ApplicationContextApplicationContext 继承了 HierachicalBeanFactory 和 ListableBeanFactory 接口，算是 BeanFactory 的自动挡版本，是 Spring 应用的默认方式。 ApplicationContext 会在启动时预先创建和配置所有的单例 bean，并支持如 JDBC、ORM 框架的集成，内置面向切面编程（AOP）的支持，可以配置声明式事务管理等。 这是 ApplicationContext 的使用例子： class MainApp public static void main(String[] args) // 使用 AppConfig 配置类初始化 ApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); // 从 ApplicationContext 获取 messageService 的 bean MessageService service = context.getBean(MessageService.class); // 使用 bean service.printMessage(); 通过 AnnotationConfigApplicationContext 类，我们可以使用 Java 配置类来初始化 ApplicationContext，这样就可以使用 Java 代码来配置 Spring 容器。 @Configuration@ComponentScan(basePackages = com.github.paicoding.forum.test.javabetter.spring1) // 替换为你的包名public class AppConfig 8.你知道 Spring 容器启动阶段会干什么吗？Spring 的 IoC 容器工作的过程，可以划分为两个阶段：容器启动阶段和Bean 实例化阶段。其中容器启动阶段主要做的工作是加载和解析配置文件，保存到对应的 Bean 定义中。 容器启动开始，首先会通过某种途径加载 Configuration MetaData，在大部分情况下，容器需要依赖某些工具类（BeanDefinitionReader）对加载的 Configuration MetaData 进行解析和分析，并将分析后的信息组为相应的 BeanDefinition。 最后把这些保存了 Bean 定义必要信息的 BeanDefinition，注册到相应的 BeanDefinitionRegistry，这样容器启动就完成了。 说说 Spring 的 Bean 实例化方式Spring 提供了 4 种不同的方式来实例化 Bean，以满足不同场景下的需求。 说说构造方法的方式在类上使用@Component（或@Service、@Repository 等特定于场景的注解）标注类，然后通过构造方法注入依赖。 @Componentpublic class ExampleBean private DependencyBean dependency; @Autowired public ExampleBean(DependencyBean dependency) this.dependency = dependency; 说说静态工厂的方式在这种方式中，Bean 是由一个静态方法创建的，而不是直接通过构造方法。 public class ClientService private static ClientService clientService = new ClientService(); private ClientService() public static ClientService createInstance() return clientService; 说说实例工厂方法实例化的方式与静态工厂方法相比，实例工厂方法依赖于某个类的实例来创建 Bean。这通常用在需要通过工厂对象的非静态方法来创建 Bean 的场景。 public class ServiceLocator public ClientService createClientServiceInstance() return new ClientService(); 说说 FactoryBean 接口实例化方式FactoryBean 是一个特殊的 Bean 类型，可以在 Spring 容器中返回其他对象的实例。通过实现 FactoryBean 接口，可以自定义实例化逻辑，这对于构建复杂的初始化逻辑非常有用。 public class ToolFactoryBean implements FactoryBeanTool private int factoryId; private int toolId; @Override public Tool getObject() throws Exception return new Tool(toolId); @Override public Class? getObjectType() return Tool.class; @Override public boolean isSingleton() return true; // setter and getter methods for factoryId and toolId 9.你是怎么理解 Bean 的？Bean 是指由 Spring 容器管理的对象，它的生命周期由容器控制，包括创建、初始化、使用和销毁。以通过三种方式声明：注解方式、XML 配置、Java 配置。 ①、使用 @Component、@Service、@Repository、@Controller 等注解定义，主流。 ②、基于 XML 配置，Spring Boot 项目已经不怎么用了。 ③、使用 Java 配置类创建 Bean： @Configurationpublic class AppConfig @Bean public UserService userService() return new UserService(); @Component 和 @Bean 的区别@Component 是 Spring 提供的一个类级别注解，由 Spring 自动扫描并注册到 Spring 容器中。@Bean 是一个方法级别的注解，用于显式地声明一个 Bean，当我们需要第三方库或者无法使用 @Component 注解类时，可以使用 @Bean 来将其实例注册到容器中。 10.能说一下 Bean 的生命周期吗？推荐阅读:https://mp.weixin.qq.com/s/zb6eA3Se0gQoqL8PylCPLw Bean的生命周期大致分为五个阶段： 实例化：Spring 首先使用构造方法或者工厂方法创建一个 Bean 的实例。在这个阶段，Bean 只是一个空的 Java 对象，还未设置任何属性。 属性赋值：Spring 将配置文件中的属性值或依赖的 Bean 注入到该 Bean 中。这个过程称为依赖注入，确保 Bean 所需的所有依赖都被注入。 初始化：Spring 调用 afterPropertiesSet 方法，或通过配置文件指定的 init-method 方法，完成初始化。 使用中：Bean 准备好可以使用了。 销毁：在容器关闭时，Spring 会调用 destroy 方法，完成 Bean 的清理工作。 可以从源码角度讲一下吗？ 实例化：Spring 容器根据 Bean 的定义创建 Bean 的实例，相当于执行构造方法，也就是 new 一个对象。 属性赋值：相当于执行 setter 方法为字段赋值。 初始化：初始化阶段允许执行自定义的逻辑，比如设置某些必要的属性值、开启资源、执行预加载操作等，以确保 Bean 在使用之前是完全配置好的。 销毁：相当于执行 null，释放资源。可以在源码 AbstractAutowireCapableBeanFactory 中的 doCreateBean 方法中，看到 Bean 的前三个生命周期：protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) instanceWrapper = (BeanWrapper)this.factoryBeanInstanceCache.remove(beanName); if (instanceWrapper == null) // 实例化阶段 instanceWrapper = this.createBeanInstance(beanName, mbd, args); ... Object exposedObject = bean; try // 属性赋值阶段 this.populateBean(beanName, mbd, instanceWrapper); // 初始化阶段 exposedObject = this.initializeBean(beanName, exposedObject, mbd); catch (Throwable var18) ... ... 源码位置,如下图: 至于销毁，是在容器关闭的时候调用的，详见 ConfigurableApplicationContext 的 close 方法。 请在一个已有的 Spring Boot 项目中通过单元测试的形式来展示 Spring Bean 的生命周期？第一步，创建一个 LifecycleDemoBean 类： public class LifecycleDemoBean implements InitializingBean, DisposableBean // 使用@Value注解注入属性值，这里演示了如何从配置文件中读取值 // 如果配置文件中没有定义lifecycle.demo.bean.name，则使用默认值default name @Value($lifecycle.demo.bean.name:default name) private String name; // 构造方法：在Bean实例化时调用 public LifecycleDemoBean() System.out.println(LifecycleDemoBean: 实例化); // 属性赋值：Spring通过反射调用setter方法为Bean的属性注入值 public void setName(String name) System.out.println(LifecycleDemoBean: 属性赋值); this.name = name; // 使用@PostConstruct注解的方法：在Bean的属性赋值完成后调用，用于执行初始化逻辑 @PostConstruct public void postConstruct() System.out.println(LifecycleDemoBean: @PostConstruct（初始化）); // 实现InitializingBean接口：afterPropertiesSet方法在@PostConstruct注解的方法之后调用 // 用于执行更多的初始化逻辑 @Override public void afterPropertiesSet() throws Exception System.out.println(LifecycleDemoBean: afterPropertiesSet（InitializingBean）); // 自定义初始化方法：在XML配置或Java配置中指定，执行特定的初始化逻辑 public void customInit() System.out.println(LifecycleDemoBean: customInit（自定义初始化方法）); // 使用@PreDestroy注解的方法：在容器销毁Bean之前调用，用于执行清理工作 @PreDestroy public void preDestroy() System.out.println(LifecycleDemoBean: @PreDestroy（销毁前）); // 实现DisposableBean接口：destroy方法在@PreDestroy注解的方法之后调用 // 用于执行清理资源等销毁逻辑 @Override public void destroy() throws Exception System.out.println(LifecycleDemoBean: destroy（DisposableBean）); // 自定义销毁方法：在XML配置或Java配置中指定，执行特定的清理逻辑 public void customDestroy() System.out.println(LifecycleDemoBean: customDestroy（自定义销毁方法）); ①、实例化 实例化是创建 Bean 实例的过程，即在内存中为 Bean 对象分配空间。这一步是通过调用 Bean 的构造方法完成的。 public LifecycleDemoBean() System.out.println(LifecycleDemoBean: 实例化); 在这里，当 Spring 创建 LifecycleDemoBean 的实例时，会调用其无参数的构造方法，这个过程就是实例化。 ②、属性赋值 在实例化之后，Spring 将根据 Bean 定义中的配置信息，通过反射机制为 Bean 的属性赋值。 @Value($lifecycle.demo.bean.name:default name)private String name;public void setName(String name) System.out.println(LifecycleDemoBean: 属性赋值); this.name = name; @Value注解和 setter 方法体现了属性赋值的过程。@Value注解让 Spring 注入配置值（或默认值），setter 方法则是属性赋值的具体操作。 ③、初始化 初始化阶段允许执行自定义的初始化逻辑，比如检查必要的属性是否已经设置、开启资源等。Spring 提供了多种方式来配置初始化逻辑。 1、使用 @PostConstruct 注解的方法 @PostConstructpublic void postConstruct() System.out.println(LifecycleDemoBean: @PostConstruct（初始化）);@PostConstruct注解的方法在 Bean 的所有属性都被赋值后，且用户自定义的初始化方法之前调用。 2、实现 InitializingBean 接口的 afterPropertiesSet 方法 @Overridepublic void afterPropertiesSet() throws Exception System.out.println(LifecycleDemoBean: afterPropertiesSet（InitializingBean）); afterPropertiesSet 方法提供了另一种初始化 Bean 的方式，也是在所有属性赋值后调用。 3、自定义初始化方法 public void customInit() System.out.println(LifecycleDemoBean: customInit（自定义初始化方法）); 需要在配置类中指定初始化方法： @Bean(initMethod = customInit)public LifecycleDemoBean lifecycleDemoBean() return new LifecycleDemoBean(); ④、销毁 销毁阶段允许执行自定义的销毁逻辑，比如释放资源。类似于初始化阶段，Spring 也提供了多种方式来配置销毁逻辑。 1、使用 @PreDestroy 注解的方法 @PreDestroypublic void preDestroy() System.out.println(LifecycleDemoBean: @PreDestroy（销毁前）); @PreDestroy注解的方法在 Bean 被销毁前调用。 2、实现 DisposableBean 接口的 destroy 方法 @Overridepublic void destroy() throws Exception System.out.println(LifecycleDemoBean: destroy（DisposableBean）); destroy 方法提供了另一种销毁 Bean 的方式，也是在 Bean 被销毁前调用。 3、自定义销毁方法 public void customDestroy() System.out.println(LifecycleDemoBean: customDestroy（自定义销毁方法）); 需要在配置类中指定销毁方法： @Bean(destroyMethod = customDestroy)public LifecycleDemoBean lifecycleDemoBean() return new LifecycleDemoBean(); 第二步，注册 Bean 并指定自定义初始化方法和销毁方法： @Configurationpublic class LifecycleDemoConfig @Bean(initMethod = customInit, destroyMethod = customDestroy) public LifecycleDemoBean lifecycleDemoBean() return new LifecycleDemoBean(); 第三步，编写单元测试： @SpringBootTestpublic class LifecycleDemoTest @Autowired private ApplicationContext context; @Test public void testBeanLifecycle() System.out.println(获取LifecycleDemoBean实例...); LifecycleDemoBean bean = context.getBean(LifecycleDemoBean.class); 运行单元测试，查看控制台输出： LifecycleDemoBean: 实例化LifecycleDemoBean: @PostConstruct（初始化）LifecycleDemoBean: afterPropertiesSet（InitializingBean）LifecycleDemoBean: customInit（自定义初始化方法）获取LifecycleDemoBean实例...LifecycleDemoBean: @PreDestroy（销毁前）LifecycleDemoBean: destroy（DisposableBean）LifecycleDemoBean: customDestroy（自定义销毁方法） Aware 类型的接口有什么作用？通过实现 Aware 接口，Bean 可以获取 Spring 容器的相关信息，如 BeanFactory、ApplicationContext 等。常见 Aware 接口有： 如果配置了 init-method 和 destroy-method，Spring 会在什么时候调用其配置的方法？init-method 在 Bean 初始化阶段调用，依赖注入完成后且 postProcessBeforeInitialization 调用之后执行。destroy-method 在 Bean 销毁阶段调用，容器关闭时调用。 11.为什么 IDEA 不推荐使用 @Autowired 注解注入 Bean？当使用 @Autowired 注解注入 Bean 时，IDEA 会提示“Field injection is not recommended”。 这是因为字段注入的方式： 不能像构造方法那样使用 final 注入不可变对象 隐藏了依赖关系，调用者可以看到构造方法注入或者 setter 注入，但无法看到私有字段的注入 在 Spring 4.3 及更高版本中，如果一个类只有一个构造方法，Spring 会自动使用该构造方法进行依赖注入，无需使用 @Autowired 注解。 @Autowired 和 @Resource 注解的区别？ @Autowired 是 Spring 提供的注解，按类型（byType）注入。 @Resource 是 Java EE 提供的注解，按名称（byName）注入。 虽然 IDEA 不推荐使用 @Autowired，但对 @Resource 注解却没有任何提示。这是因为 @Resource 属于 Java EE 标准的注解，如果使用其他 IOC 容器而不是 Spring 也是可以兼容的。 提到了byType，如果两个类型一致的发生了冲突，应该怎么处理当容器中存在多个相同类型的 bean，编译器会提示 Could not autowire. There is more than one bean of UserRepository2 type. @Componentpublic class UserRepository21 implements UserRepository2 @Componentpublic class UserRepository22 implements UserRepository2 @Componentpublic class UserService2 @Autowired private UserRepository2 userRepository; // 冲突 这时候，就可以配合 @Qualifier 注解来指定具体的 bean 名称： @Component(userRepository21)public class UserRepository21 implements UserRepository2 @Component(userRepository22)public class UserRepository22 implements UserRepository2 @Autowired@Qualifier(userRepository22)private UserRepository2 userRepository22; 或者使用 @Resource 注解按名称进行注入，指定 name 属性。 @Resource(name = userRepository21)private UserRepository2 userRepository21; 12.Spring 有哪些自动装配的方式？什么是自动装配？ Spring IoC 容器知道所有 Bean 的配置信息，此外，通过 Java 反射机制还可以获知实现类的结构信息，如构造方法的结构、属性等信息。掌握所有 Bean 的这些信息后，Spring IoC 容器就可以按照某种规则对容器中的 Bean 进行自动装配，而无须通过显式的方式进行依赖配置。 Spring 提供的这种方式，可以按照某些规则进行 Bean 的自动装配，bean元素提供了一个指定自动装配类型的属性：autowire=自动装配类型 Spring 提供了哪几种自动装配类型？ Spring 提供了 4 种自动装配类型： byName：根据名称进行自动匹配，假设 Boss 有一个名为 car 的属性，如果容器中刚好有一个名为 car 的 bean，Spring 就会自动将其装配给 Boss 的 car 属性 byType：根据类型进行自动匹配，假设 Boss 有一个 Car 类型的属性，如果容器中刚好有一个 Car 类型的 Bean，Spring 就会自动将其装配给 Boss 这个属性 constructor：与 byType 类似， 只不过它是针对构造函数注入而言的。如果 Boss 有一个构造函数，构造函数包含一个 Car 类型的入参，如果容器中有一个 Car 类型的 Bean，则 Spring 将自动把这个 Bean 作为 Boss 构造函数的入参；如果容器中没有找到和构造函数入参匹配类型的 Bean，则 Spring 将抛出异常。 autodetect：根据 Bean 的自省机制决定采用 byType 还是 constructor 进行自动装配，如果 Bean 提供了默认的构造函数，则采用 byType，否则采用 constructor。 13.Bean 的作用域有哪些?在 Spring 中，Bean 默认是单例的，即在整个 Spring 容器中，每个 Bean 只有一个实例。可以通过在配置中指定 scope 属性，将 Bean 改为多例（Prototype）模式，这样每次获取的都是新的实例。 @Bean@Scope(prototype) // 每次获取都是新的实例public MyBean myBean() return new MyBean(); 除了单例和多例，Spring 还支持其他作用域，如请求作用域（Request）、会话作用域（Session）等，适合 Web 应用中特定的使用场景。 request：每一次 HTTP 请求都会产生一个新的 Bean，该 Bean 仅在当前 HTTP Request 内有效。 session：同一个 Session 共享一个 Bean，不同的 Session 使用不同的 Bean。 globalSession：同一个全局 Session 共享一个 Bean，只用于基于 Protlet 的 Web 应用，Spring5 中已经移除。 14.Spring 中的单例 Bean 会存在线程安全问题吗？Spring Bean 的默认作用域是单例（Singleton），这意味着 Spring 容器中只会存在一个 Bean 实例，并且该实例会被多个线程共享。 如果单例 Bean 是无状态的，也就是没有成员变量，那么这个单例 Bean 是线程安全的。比如 Spring MVC 中的 Controller、Service、Dao 等，基本上都是无状态的。 但如果 Bean 的内部状态是可变的，且没有进行适当的同步处理，就可能出现线程安全问题。 单例 Bean 线程安全问题怎么解决呢？第一，使用局部变量。局部变量是线程安全的，因为每个线程都有自己的局部变量副本。尽量使用局部变量而不是共享的成员变量。 public class MyService public void process() int localVar = 0; // 使用局部变量进行操作 第二，尽量使用无状态的 Bean，即不在 Bean 中保存任何可变的状态信息。 public class MyStatelessService public void process() // 无状态处理 第三，同步访问。如果 Bean 中确实需要保存可变状态，可以通过 synchronized 关键字或者 Lock 接口来保证线程安全。 public class MyService private int sharedVar; public synchronized void increment() sharedVar++; 或者将 Bean 中的成员变量保存到 ThreadLocal 中，ThreadLocal 可以保证多线程环境下变量的隔离。 public class MyService private ThreadLocalInteger localVar = ThreadLocal.withInitial(() - 0); public void process() localVar.set(localVar.get() + 1); 再或者使用线程安全的工具类，比如说 AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。 public class MyService private ConcurrentHashMapString, String map = new ConcurrentHashMap(); public void putValue(String key, String value) map.put(key, value); 第四，将 Bean 定义为原型作用域（Prototype）。原型作用域的 Bean 每次请求都会创建一个新的实例，因此不存在线程安全问题。 @Component@Scope(prototype)public class MyService // 实例变量 15.说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。 循环依赖只发生在 Singleton 作用域的 Bean 之间，因为如果是 Prototype 作用域的 Bean，Spring 会直接抛出异常。 原因很简单，AB 循环依赖，A 实例化的时候，发现依赖 B，创建 B 实例，创建 B 的时候发现需要 A，创建 A1 实例……无限套娃。。。。 我们来看一个实例，先是 PrototypeBeanA： @Component@Scope(prototype)public class PrototypeBeanA private final PrototypeBeanB prototypeBeanB; @Autowired public PrototypeBeanA(PrototypeBeanB prototypeBeanB) this.prototypeBeanB = prototypeBeanB; 然后是 PrototypeBeanB： @Component@Scope(prototype)public class PrototypeBeanB private final PrototypeBeanA prototypeBeanA; @Autowired public PrototypeBeanB(PrototypeBeanA prototypeBeanA) this.prototypeBeanA = prototypeBeanA; 再然后是测试： @SpringBootApplicationpublic class DemoApplication public static void main(String[] args) SpringApplication.run(DemoApplication.class, args); @Bean CommandLineRunner commandLineRunner(ApplicationContext ctx) return args - // 尝试获取PrototypeBeanA的实例 PrototypeBeanA beanA = ctx.getBean(PrototypeBeanA.class); ; Spring 可以解决哪些情况的循环依赖？看看这几种情形（AB 循环依赖）： 也就是说： AB 均采用构造器注入，不支持 AB 均采用 setter 注入，支持 AB 均采用属性自动注入，支持 A 中注入的 B 为 setter 注入，B 中注入的 A 为构造器注入，支持 B 中注入的 A 为 setter 注入，A 中注入的 B 为构造器注入，不支持第四种可以，第五种不可以的原因是 Spring 在创建 Bean 时默认会根据自然排序进行创建，所以 A 会先于 B 进行创建。 简单总结下，当循环依赖的实例都采用 setter 方法注入时，Spring 支持，都采用构造器注入的时候，不支持；构造器注入和 setter 注入同时存在的时候，看天（😂）。 16.Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖： 一级缓存：存放完全初始化好的单例 Bean。 二级缓存：存放正在创建但未完全初始化的 Bean 实例。 三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。 三级缓存解决循环依赖的过程是什么样的？ 实例化 Bean 时，将其早期引用放入三级缓存。 其他依赖该 Bean 的对象，可以从缓存中获取其引用。 初始化完成后，将 Bean 移入一级缓存。 假如 A、B 两个类发生循环依赖. A 实例的初始化过程：①、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道。②、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。 ③、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。 发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 已经实例化并且初始化完成了，把 B 放入⼀级缓存。 ④、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存 ⑤、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。 17.为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。 因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。 假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean 初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。 如果缺少第二级缓存会有什么问题？如果没有二级缓存，Spring 无法在未完成初始化的情况下暴露 Bean。会导致代理 Bean 的循环依赖问题，因为某些代理逻辑无法在三级缓存中提前暴露。最终可能抛出 BeanCurrentlyInCreationException。 18.@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor 在 Bean 的初始化阶段，会通过 Bean 后置处理器来进行一些前置和后置的处理。 实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是 AutowiredAnnotationBeanPostProcessor。 Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在 doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。 在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的 postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。 /*** 属性赋值**/protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) //………… if (hasInstAwareBpps) if (pvs == null) pvs = mbd.getPropertyValues(); PropertyValues pvsToUse; for(Iterator var9 = this.getBeanPostProcessorCache().instantiationAware.iterator(); var9.hasNext(); pvs = pvsToUse) InstantiationAwareBeanPostProcessor bp = (InstantiationAwareBeanPostProcessor)var9.next(); pvsToUse = bp.postProcessProperties((PropertyValues)pvs, bw.getWrappedInstance(), beanName); if (pvsToUse == null) if (filteredPds == null) filteredPds = this.filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); //执行后处理器，填充属性，完成自动装配 //调用InstantiationAwareBeanPostProcessor的postProcessPropertyValues()方法 pvsToUse = bp.postProcessPropertyValues((PropertyValues)pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvsToUse == null) return; //………… postProcessorPropertyValues()方法的源码如下，在该方法中，会先调用 findAutowiringMetadata()方法解析出 bean 中带有@Autowired 注解、@Inject 和@Value 注解的属性和方法。然后调用 metadata.inject()方法，进行属性填充。 public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) //@Autowired注解、@Inject和@Value注解的属性和方法 InjectionMetadata metadata = this.findAutowiringMetadata(beanName, bean.getClass(), pvs); try //属性填充 metadata.inject(bean, beanName, pvs); return pvs; catch (BeanCreationException var6) throw var6; catch (Throwable var7) throw new BeanCreationException(beanName, Injection of autowired dependencies failed, var7); AOP19.说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。 举个例子，假如我们现在需要在业务代码开始前进行参数校验，在结束后打印日志，该怎么办呢？ 我们可以把日志记录和数据校验这两个功能抽取出来，形成一个切面，然后在业务代码中引入这个切面，这样就可以实现业务逻辑和通用逻辑的分离。 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。 AOP 有哪些核心概念？ 切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象 连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法 切点（Pointcut）：对连接点进行拦截的定位 通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强 目标对象 （Target）：代理的目标对象 引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法 织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。 织入有哪几种方式？①、编译期织入：切面在目标类编译时被织入。 ②、类加载期织入：切面在目标类加载到 JVM 时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。 ③、运行期织入：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP 容器会为目标对象动态地创建一个代理对象。 Spring AOP 采用运行期织入，而 AspectJ 可以在编译期织入和类加载时织入。 AspectJ 是什么？AspectJ 是一个 AOP 框架，它可以做很多 Spring AOP 干不了的事情，比如说支持编译时、编译后和类加载时织入切面。并且提供更复杂的切点表达式和通知类型。 下面是一个简单的 AspectJ 示例： // 定义一个切面@Aspectpublic class LoggingAspect // 定义一个切点，匹配 com.example 包下的所有方法 @Pointcut(execution(* com.example..*(..))) private void selectAll() // 定义一个前置通知，在匹配的方法执行之前执行 @Before(selectAll()) public void beforeAdvice() System.out.println(A method is about to be executed.); AOP 有哪些环绕方式？AOP 一般有 5 种环绕方式： 前置通知 (@Before) 返回通知 (@AfterReturning) 异常通知 (@AfterThrowing) 后置通知 (@After) 环绕通知 (@Around) 多个切面的情况下，可以通过 @Order 指定先后顺序，数字越小，优先级越高。代码示例如下： @Aspect@Componentpublic class WebLogAspect private final static Logger logger = LoggerFactory.getLogger(WebLogAspect.class); @Pointcut(@annotation(cn.fighter3.spring.aop_demo.WebLog)) public void webLog() @Before(webLog()) public void doBefore(JoinPoint joinPoint) throws Throwable // 开始打印请求日志 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 打印请求相关参数 logger.info(========================================== Start ==========================================); // 打印请求 url logger.info(URL : , request.getRequestURL().toString()); // 打印 Http method logger.info(HTTP Method : , request.getMethod()); // 打印调用 controller 的全路径以及执行方法 logger.info(Class Method : ., joinPoint.getSignature().getDeclaringTypeName(), joinPoint.getSignature().getName()); // 打印请求的 IP logger.info(IP : , request.getRemoteAddr()); // 打印请求入参 logger.info(Request Args : ,new ObjectMapper().writeValueAsString(joinPoint.getArgs())); @After(webLog()) public void doAfter() throws Throwable // 结束后打个分隔线，方便查看 logger.info(=========================================== End ===========================================); @Around(webLog()) public Object doAround(ProceedingJoinPoint proceedingJoinPoint) throws Throwable //开始时间 long startTime = System.currentTimeMillis(); Object result = proceedingJoinPoint.proceed(); // 打印出参 logger.info(Response Args : , new ObjectMapper().writeValueAsString(result)); // 执行耗时 logger.info(Time-Consuming : ms, System.currentTimeMillis() - startTime); return result; Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。 在 Spring 容器初始化 Bean 的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代理对象中织入切面逻辑。这一过程发生在 Spring 容器的后处理器（BeanPostProcessor）阶段。 简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。 AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。 ① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像 @Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。 ② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现 AOP。 ③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。 AOP和 OOP 的关系？AOP 和 OOP 是互补的编程思想： OOP 通过类和对象封装数据和行为，专注于核心业务逻辑。 AOP 提供了解决横切关注点（如日志、权限、事务等）的机制，将这些逻辑集中管理。 20.AOP的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。我们在技术派实战项目中主要利用 AOP 来打印接口的入参和出参日志、执行时间，方便后期 bug 溯源和性能调优。 第一步，自定义注解作为切点 @Target(ElementType.METHOD, ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface MdcDot String bizCode() default ; 第二步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间 第三步，在使用的地方加上自定义注解 第四步，当接口被调用时，就可以看到对应的执行日志。 21.说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。 ①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。 使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。 优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身。 示例代码： public interface Service void perform();public class ServiceImpl implements Service public void perform() System.out.println(Performing service...); public class ServiceInvocationHandler implements InvocationHandler private Object target; public ServiceInvocationHandler(Object target) this.target = target; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable System.out.println(Before method); Object result = method.invoke(target, args); System.out.println(After method); return result; public class Main public static void main(String[] args) Service service = new ServiceImpl(); Service proxy = (Service) Proxy.newProxyInstance( service.getClass().getClassLoader(), service.getClass().getInterfaces(), new ServiceInvocationHandler(service) ); proxy.perform(); ②、CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。 使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑。 优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。 public class Service public void perform() System.out.println(Performing service...); public class ServiceInterceptor implements MethodInterceptor @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable System.out.println(Before method); Object result = proxy.invokeSuper(obj, args); System.out.println(After method); return result; public class Main public static void main(String[] args) Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(Service.class); enhancer.setCallback(new ServiceInterceptor()); Service proxy = (Service) enhancer.create(); proxy.perform(); 选择 CGLIB 还是 JDK 动态代理？如果目标对象没有实现任何接口，则只能使用 CGLIB 代理。如果目标对象实现了接口，通常首选 JDK 动态代理。虽然 CGLIB 在代理类的生成过程中可能消耗更多资源，但在运行时具有较高的性能。对于性能敏感且代理对象创建频率不高的场景，可以考虑使用 CGLIB。JDK 动态代理是 Java 原生支持的，不需要额外引入库。而 CGLIB 需要将 CGLIB 库作为依赖加入项目中。 你会用 JDK 动态代理和 CGLIB 吗？假设我们有这样一个小场景，客服中转，解决用户问题： ①、JDK 动态代理实现： 第一步，创建接口public interface ISolver void solve(); 第二步，实现对应接口 public class Solver implements ISolver @Override public void solve() System.out.println(疯狂掉头发解决问题……); 第三步，动态代理工厂:ProxyFactory，直接用反射方式生成一个目标对象的代理，这里用了一个匿名内部类方式重写 InvocationHandler 方法。public class ProxyFactory // 维护一个目标对象 private Object target; public ProxyFactory(Object target) this.target = target; // 为目标对象生成代理对象 public Object getProxyInstance() return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable System.out.println(请问有什么可以帮到您？); // 调用目标对象方法 Object returnValue = method.invoke(target, args); System.out.println(问题已经解决啦！); return null; ); 第四步，客户端：Client，生成一个代理对象实例，通过代理对象调用目标对象方法public class Client public static void main(String[] args) //目标对象:程序员 ISolver developer = new Solver(); //代理：客服小姐姐 ISolver csProxy = (ISolver) new ProxyFactory(developer).getProxyInstance(); //目标方法：解决问题 csProxy.solve(); ②、CGLIB 动态代理实现： 第一步：定义目标类（Solver），目标类 Solver 定义了一个 solve 方法，模拟了解决问题的行为。目标类不需要实现任何接口，这与 JDK 动态代理的要求不同。 public class Solver public void solve() System.out.println(疯狂掉头发解决问题……); 第二步：动态代理工厂（ProxyFactory），ProxyFactory 类实现了 MethodInterceptor 接口，这是 CGLIB 提供的一个方法拦截接口，用于定义方法的拦截逻辑。public class ProxyFactory implements MethodInterceptor //维护一个目标对象 private Object target; public ProxyFactory(Object target) this.target = target; //为目标对象生成代理对象 public Object getProxyInstance() //工具类 Enhancer en = new Enhancer(); //设置父类 en.setSuperclass(target.getClass()); //设置回调函数 en.setCallback(this); //创建子类对象代理 return en.create(); @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable System.out.println(请问有什么可以帮到您？); // 执行目标对象的方法 Object returnValue = method.invoke(target, args); System.out.println(问题已经解决啦！); return null; ProxyFactory 接收一个 Object 类型的 target，即目标对象的实例。 使用 CGLIB 的 Enhancer 类来生成目标类的子类（代理对象）。通过 setSuperclass 设置代理对象的父类为目标对象的类，setCallback 设置方法拦截器为当前对象（this），最后调用 create 方法生成并返回代理对象。 重写 MethodInterceptor 接口的 intercept 方法以提供方法拦截逻辑。在目标方法执行前后添加自定义逻辑，然后通过 method.invoke 调用目标对象的方法。 第三步：客户端使用代理，首先创建目标对象（Solver 的实例），然后使用 ProxyFactory 创建该目标对象的代理。通过代理对象调用 solve 方法时，会先执行 intercept 方法中定义的逻辑，然后执行目标方法，最后再执行 intercept 方法中的后续逻辑。public class Client public static void main(String[] args) //目标对象:程序员 Solver developer = new Solver(); //代理：客服小姐姐 Solver csProxy = (Solver) new ProxyFactory(developer).getProxyInstance(); //目标方法：解决问题 csProxy.solve(); 22.说说 Spring AOP 和 AspectJ AOP 区别?Spring AOP 属于运行时增强，主要具有如下特点： 基于动态代理来实现，默认如果使用接口的，用 JDK 提供的动态代理实现，如果是方法则使用 CGLIB 实现 Spring AOP 需要依赖 IoC 容器来管理，并且只能作用于 Spring 容器，使用纯 Java 代码实现 在性能上，由于 Spring AOP 是基于动态代理来实现的，在容器启动时需要生成代理实例，在方法调用上也会增加栈的深度，使得 Spring AOP 的性能不如 AspectJ 的那么好。 Spring AOP 致力于解决企业级开发中最普遍的 AOP(方法织入)。 AspectJ 是一个易用的功能强大的 AOP 框架，属于编译时增强， 可以单独使用，也可以整合到其它框架中，是 AOP 编程的完全解决方案。AspectJ 需要用到单独的编译器 ajc。 AspectJ 属于静态织入，通过修改代码来实现，在实际运行之前就完成了织入，所以说它生成的类是没有额外运行时开销的，一般有如下几个织入的时机： 编译期织入（Compile-time weaving）：如类 A 使用 AspectJ 添加了一个属性，类 B 引用了它，这个场景就需要编译期的时候就进行织入，否则没法编译类 B。 编译后织入（Post-compile weaving）：也就是已经生成了 .class 文件，或已经打成 jar 包了，这种情况我们需要增强处理的话，就要用到编译后织入。 类加载后织入（Load-time weaving）：指的是在加载类的时候进行织入，要实现这个时期的织入，有几种常见的方法 整体对比如下： 40.说说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java 提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。 事务Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现。 23.Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。 介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager 来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚。 public class AccountService private TransactionTemplate transactionTemplate; public void setTransactionTemplate(TransactionTemplate transactionTemplate) this.transactionTemplate = transactionTemplate; public void transfer(final String out, final String in, final Double money) transactionTemplate.execute(new TransactionCallbackWithoutResult() @Override protected void doInTransactionWithoutResult(TransactionStatus status) // 转出 accountDao.outMoney(out, money); // 转入 accountDao.inMoney(in, money); ); 在上面的代码中，我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法内部实现事务的控制。 介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。 相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。 不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。 说说两者的区别？ 编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。 声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务。 24.说说 Spring 的事务隔离级别？事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 TransactionDefinition 接口中。 ①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 😁），MySQL 默认的是可重复读，Oracle 默认的读已提交。 ②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。 ③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。 ④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。 ⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。 25.Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上下文如何在方法调用链中传播。 Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 事务传播机制是使用 ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。 @Transactionalpublic void parentMethod() new Thread(() - childMethod()).start();public void childMethod() // 这里的操作将不会在 parentMethod 的事务范围内执行 Spring 默认的事务传播行为是 PROPAFATION_REQUIRED，即如果多个 ServiceX#methodX() 都工作在事务环境下，且程序中存在这样的调用链 Service1#method1()-Service2#method2()-Service3#method3()，那么这 3 个服务类的 3 个方法都通过 Spring 的事务传播机制工作在同一个事务中。 protected 和 private 加事务会生效吗在 Spring 中，只有通过 Spring 容器的 AOP 代理调用的公开方法（public method）上的@Transactional注解才会生效。 如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于 CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。 26.声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。 第一步，在 Bean 初始化阶段创建代理对象： Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。 在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属性切面，也就是 @Transactional 注解及其属性值。 然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。 第二步，在执行目标方法时进行事务增强操作： 当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为MethodInterceptor，事务增强对该接口的实现为TransactionInterceptor，类图如下： 事务拦截器TransactionInterceptor在invoke方法中，通过调用父类TransactionAspectSupport的invokeWithinTransaction方法进行事务处理，包括开启事务、事务提交、异常回滚等。 27.声明式事务在哪些情况下会失效？ 1、@Transactional 应用在非 public 修饰的方法上 如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。 是因为在 Spring AOP 代理时，TransactionInterceptor （事务拦截器）在目标方法执行前后进行拦截，DynamicAdvisedInterceptor（CglibAopProxy 的内部类）的 intercept 方法或 JdkDynamicAopProxy 的 invoke 方法会间接调用 AbstractFallbackTransactionAttributeSource 的 computeTransactionAttribute方法，获取 Transactional 注解的事务配置信息。 protected TransactionAttribute computeTransactionAttribute(Method method, Class? targetClass) // Dont allow no-public methods as required. if (allowPublicMethodsOnly() !Modifier.isPublic(method.getModifiers())) return null; 此方法会检查目标方法的修饰符是否为 public，不是 public 则不会获取 @Transactional 的属性配置信息。 2、@Transactional 注解属性 propagation 设置错误 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务方式执行；错误使用场景：在业务逻辑必须运行在事务环境下以确保数据一致性的情况下使用 SUPPORTS。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：总是以非事务方式执行，如果当前存在事务，则挂起该事务。错误使用场景：在需要事务支持的操作中使用 NOT_SUPPORTED。 TransactionDefinition.PROPAGATION_NEVER：总是以非事务方式执行，如果当前存在事务，则抛出异常。错误使用场景：在应该在事务环境下执行的操作中使用 NEVER。 3、@Transactional 注解属性 rollbackFor 设置错误 rollbackFor 用来指定能够触发事务回滚的异常类型。Spring 默认抛出未检查 unchecked 异常（继承自 RuntimeException 的异常）或者 Error 才回滚事务，其他异常不会触发回滚事务。 // 希望自定义的异常可以进行回滚@Transactional(propagation= Propagation.REQUIRED,rollbackFor= MyException.class) 若在目标方法中抛出的异常是 rollbackFor 指定的异常的子类，事务同样会回滚。 4、同一个类中方法调用，导致@Transactional 失效 开发中避免不了会对同一个类里面的方法调用，比如有一个类 Test，它的一个方法 A，A 调用本类的方法 B（不论方法 B 是用 public 还是 private 修饰），但方法 A 没有声明注解事务，而 B 方法有。 则外部调用方法 A 之后，方法 B 的事务是不会起作用的。这也是经常犯错误的一个地方。 那为啥会出现这种情况呢？其实还是由 Spring AOP 代理造成的，因为只有事务方法被当前类以外的代码调用时，才会由 Spring 生成的代理对象来管理。 //@Transactional@GetMapping(/test)private Integer A() throws Exception CityInfoDict cityInfoDict = new CityInfoDict(); cityInfoDict.setCityName(2); /** * B 插入字段为 3的数据 */ this.insertB(); /** * A 插入字段为 2的数据 */ int insert = cityInfoDictMapper.insert(cityInfoDict); return insert;@Transactional()public Integer insertB() throws Exception CityInfoDict cityInfoDict = new CityInfoDict(); cityInfoDict.setCityName(3); cityInfoDict.setParentCityId(3); return cityInfoDictMapper.insert(cityInfoDict); 这种情况是最常见的一种@Transactional 注解失效场景。 @Transactionalprivate Integer A() throws Exception int insert = 0; try CityInfoDict cityInfoDict = new CityInfoDict(); cityInfoDict.setCityName(2); cityInfoDict.setParentCityId(2); /** * A 插入字段为 2的数据 */ insert = cityInfoDictMapper.insert(cityInfoDict); /** * B 插入字段为 3的数据 */ b.insertB(); catch (Exception e) e.printStackTrace(); 如果 B 方法内部抛了异常，而 A 方法此时 try catch 了 B 方法的异常，那这个事务就不能正常回滚了，会抛出异常： org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only MVC28.Spring MVC 的核心组件？ DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。 Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。 HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping 将不同的请求映射到不同的 Handler。 HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。 HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和 HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。 HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。 ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。 ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。 29.Spring MVC 的工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。 DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。 然后 DispatcherServlet 通过 ViewResolver 解析视图。 最后，DispatcherServlet 渲染视图并将响应返回给客户端。 ①、发起请求：客户端通过 HTTP 协议向服务器发起请求。 ②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。 ③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个 Controller 应该处理这个请求。通常会根据请求的 URL 来确定。 ④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用 HandlerAdapter 来调用 Controller 的处理方法。 ⑤、执行处理器：Controller 处理请求，处理完后返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。 ⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用 ViewResolver 来解析视图名称，找到具体的视图页面。 ⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。 ⑧、响应结果：DispatcherServlet 将视图结果返回给客户端。 Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。 在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。 这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。 Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller注解的类，还可以是实现了特定接口的其他类（如 HttpRequestHandler 或 SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。 HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。 30.SpringMVC Restful 风格的接口的流程是什么样的呢？PS:这是一道全新的八股，毕竟 ModelAndView 这种方式应该没人用了吧？现在都是前后端分离接口，八股也该更新换代了。我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody @GetMapping(/user)@ResponseBodypublic User user() return new User(1,张三); 加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同： 1. 客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServlet 2. DispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理 3. DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪个 Controller 4. Controller 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理 5. HandlerAdapter 执行完对 Controller 的请求，会调用 HandlerMethodReturnValueHandler 去处理返回值，主要的过程： 5.1. 调用 RequestResponseBodyMethodProcessor，创建 ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例 5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入 ServletServerHttpResponse 的 OutputStream 输出流中 5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化 6. 执行完请求后，返回的 ModealAndView 为 null，ServletServerHttpResponse 里也已经写入了响应，所以不用关心 View 的处理 SpringBoot31.介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架。 以前的 Spring 开发需要配置大量的 xml 文件，并且需要引入大量的第三方 jar 包，还需要手动放到 classpath 下。现在只需要引入一个 Starter，或者一个注解，就可以轻松搞定。 Spring Boot 的优点非常多，比如说： Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。 Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和 Spring MVC。 Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控。 Spring Boot常用注解有哪些？ @SpringBootApplication：Spring Boot 应用的入口，用在启动类上。 还有一些 Spring 框架本身的注解，比如 @Component、**@RestController、@Service、@ConfigurationProperties、@Transactional** 等。 32.SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。 在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。 Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。 main 类启动的时候，Spring Boot 会通过底层的AutoConfigurationImportSelector 类加载自动装配类。 @AutoConfigurationPackage //将main同级的包下的所有组件注册到容器中@Import(AutoConfigurationImportSelector.class) //加载自动装配类 xxxAutoconfigurationpublic @interface EnableAutoConfiguration String ENABLED_OVERRIDE_PROPERTY = spring.boot.enableautoconfiguration; Class?[] exclude() default ; String[] excludeName() default ; AutoConfigurationImportSelector实现了ImportSelector接口，该接口的作用是收集需要导入的配置类，配合 @Import() 将相应的类导入到 Spring 容器中。 获取注入类的方法是 selectImports()，它实际调用的是getAutoConfigurationEntry()，这个方法是获取自动装配类的关键。 protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) // 检查自动配置是否启用。如果@ConditionalOnClass等条件注解使得自动配置不适用于当前环境，则返回一个空的配置条目。 if (!isEnabled(annotationMetadata)) return EMPTY_ENTRY; // 获取启动类上的@EnableAutoConfiguration注解的属性，这可能包括对特定自动配置类的排除。 AnnotationAttributes attributes = getAttributes(annotationMetadata); // 从spring.factories中获取所有候选的自动配置类。这是通过加载META-INF/spring.factories文件中对应的条目来实现的。 ListString configurations = getCandidateConfigurations(annotationMetadata, attributes); // 移除配置列表中的重复项，确保每个自动配置类只被考虑一次。 configurations = removeDuplicates(configurations); // 根据注解属性解析出需要排除的自动配置类。 SetString exclusions = getExclusions(annotationMetadata, attributes); // 检查排除的类是否存在于候选配置中，如果存在，则抛出异常。 checkExcludedClasses(configurations, exclusions); // 从候选配置中移除排除的类。 configurations.removeAll(exclusions); // 应用过滤器进一步筛选自动配置类。过滤器可能基于条件注解如@ConditionalOnBean等来排除特定的配置类。 configurations = getConfigurationClassFilter().filter(configurations); // 触发自动配置导入事件，允许监听器对自动配置过程进行干预。 fireAutoConfigurationImportEvents(configurations, exclusions); // 创建并返回一个包含最终确定的自动配置类和排除的配置类的AutoConfigurationEntry对象。 return new AutoConfigurationEntry(configurations, exclusions); 总结：Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。 33.如何自定义一个 SpringBoot Starter?创建一个自定义的 Spring Boot Starter，需要这几步： 第一步，创建一个新的 Maven 项目，例如命名为 my-spring-boot-starter。在 pom.xml 文件中添加必要的依赖和配置： properties spring.boot.version2.3.1.RELEASE/spring.boot.version/propertiesdependencies dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-autoconfigure/artifactId version$spring.boot.version/version /dependency dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter/artifactId version$spring.boot.version/version /dependency/dependencies 第二步，在 srcmainjava 下创建一个自动配置类，比如 MyServiceAutoConfiguration.java：（通常是 autoconfigure 包下）。 @Configuration@EnableConfigurationProperties(MyStarterProperties.class)public class MyServiceAutoConfiguration @Bean @ConditionalOnMissingBean public MyService myService(MyStarterProperties properties) return new MyService(properties.getMessage()); 第三步，创建一个配置属性类 MyStarterProperties.java： @ConfigurationProperties(prefix = mystarter)public class MyStarterProperties private String message = 二哥的 Java 进阶之路不错啊!; public String getMessage() return message; public void setMessage(String message) this.message = message; 第四步，创建一个简单的服务类 MyService.java： public class MyService private final String message; public MyService(String message) this.message = message; public String getMessage() return message; 第五步，配置 spring.factories，在 srcmainresourcesMETA-INF 目录下创建 spring.factories 文件，并添加：org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.itwanger.mystarter.autoconfigure.MyServiceAutoConfiguration 第六步，使用 Maven 打包这个项目：mvn clean install 第七步，在其他的 Spring Boot 项目中，通过 Maven 来添加这个自定义的 Starter 依赖，并通过 application.properties 配置欢迎消息：mystarter.message=javabetter.cn 然后就可以在 Spring Boot 项目中注入 MyStarterProperties 来使用它。 -启动项目，然后在浏览器中输入 localhost:8081hello，就可以看到欢迎消息了。 Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。 起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat 和 Jackson 等依赖。 自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。 比如有了 spring-boot-starter-web，我们开发者就不需要再手动配置 Tomcat、Spring MVC 等，Spring Boot 会自动帮我们完成这些工作。 34.Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责： 第一步，创建 SpringApplication 实例，负责应用的启动和初始化； 第二步，从 application.yml 中加载配置文件和环境变量； 第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入； 第四步，启动内嵌的 Web 容器。 第五步，发布启动完成事件 ApplicationReadyEvent，并调用 ApplicationRunner 的 run 方法完成启动后的逻辑。 关键的代码逻辑如下： public ConfigurableApplicationContext run(String... args) // 1. 创建启动时的监听器并触发启动事件 SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); // 2. 准备运行环境 ConfigurableEnvironment environment = prepareEnvironment(listeners); configureIgnoreBeanInfo(environment); // 3. 创建上下文 ConfigurableApplicationContext context = createApplicationContext(); try // 4. 准备上下文 prepareContext(context, environment, listeners, args); // 5. 刷新上下文，完成 Bean 初始化和装配 refreshContext(context); // 6. 调用运行器 afterRefresh(context, args); // 7. 触发启动完成事件 listeners.started(context); catch (Exception ex) handleRunFailure(context, ex, listeners); return context; 以TecHub实战项目为例。在启动类 QuickForumApplication 中，main 方法会调用 SpringApplication.run() 启动项目。 该方法负责 Spring 应用的上下文环境（ApplicationContext）准备，包括： 扫描配置文件，添加依赖项 初始化和加载 Bean 定义 启动内嵌的 Web 容器等 发布启动完成事件 了解@SpringBootApplication 注解吗？@SpringBootApplication是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解： @SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。 @EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。 @ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean。@SpringBootApplicationpublic class Application public static void main(String[] args) SpringApplication.run(Application.class, args); 为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架内部的一系列处理流程。 当运行一个 Spring Boot 程序时，通常会调用主类中的main方法，这个方法会执行SpringApplication.run()，比如： @SpringBootApplicationpublic class MyApplication public static void main(String[] args) SpringApplication.run(MyApplication.class, args); SpringApplication.run(Class? primarySource, String... args)方法接收两个参数：第一个是主应用类（即包含main方法的类），第二个是命令行参数。primarySource参数提供了一个起点，Spring Boot 通过它来加载应用上下文。 Spring Boot 利用 Java 反射机制来读取传递给run方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication。 Spring Boot 默认的包扫描路径是什么？Spring Boot 的默认包扫描路径是以启动类 @SpringBootApplication 注解所在的包为根目录的，即默认情况下，Spring Boot 会扫描启动类所在包及其子包下的所有组件。 比如说在技术派实战项目中，启动类QuickForumApplication所在的包是com.github.paicoding.forum.web，那么 Spring Boot 默认会扫描com.github.paicoding.forum.web包及其子包下的所有组件。 @SpringBootApplication 是一个组合注解，它里面的@ComponentScan注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。 @ComponentScan(excludeFilters = @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) )public @interface SpringBootApplication 比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。 如果需要自定义包扫描路径，可以在@SpringBootApplication注解上添加@ComponentScan注解，指定要扫描的包路径。 @SpringBootApplication@ComponentScan(basePackages = com.github.paicoding.forum)public class QuickForumApplication public static void main(String[] args) SpringApplication.run(QuickForumApplication.class, args); 这种方式会覆盖默认的包扫描路径，只扫描com.github.paicoding.forum包及其子包下的所有组件。 36.SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种 Model-View-Controller（模型-视图-控制器）的开发模式。 Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个 SpringMVC 的 Web 项目。 38.Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。 特性: Spring Cloud35.对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。 什么是微服务？ 2014 年 Martin Fowler 提出的一种新的架构形式。微服务架构是一种架构模式，提倡将单一应用程序划分成一组小的服务，服务之间相互协调，互相配合，为用户提供最终价值。每个服务运行在其独立的进程中，服务与服务之间采用轻量级的通信机制(如 HTTP 或 Dubbo)互相协作，每个服务都围绕着具体的业务进行构建，并且能够被独立的部署到生产环境中，另外，应尽量避免统一的，集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具(如 Maven)对其进行构建。 微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。 微服务架构主要要解决哪些问题？ 服务很多，客户端怎么访问，如何提供对外网关? 这么多服务，服务之间如何通信? HTTP 还是 RPC? 这么多服务，如何治理? 服务的注册和发现。 服务挂了怎么办？熔断机制。 有哪些主流微服务框架？ Spring Cloud Netflix Spring Cloud Alibaba SpringBoot + Dubbo + ZooKeeper SpringCloud 有哪些核心组件？ 补充37.SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务。 ①、@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml： @Scheduled(cron = 0 15 5 * * ?)public void autoRefreshCache() log.info(开始刷新sitemap.xml的url地址，避免出现数据不一致问题!); refreshSitemap(); log.info(刷新完成！); @Scheduled 注解支持多种调度选项，如 fixedRate、fixedDelay 和 cron 表达式。 ②、@EnableScheduling：用于开启定时任务的支持。 用SpringTask资源占用太高，有什么其他的方式解决？（补充） 第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。 ①、在订单创建时，将订单超时检查任务放入消息队列，并设置延迟时间（即订单超时时间）。 @Servicepublic class OrderService @Autowired private RabbitTemplate rabbitTemplate; public void createOrder(Order order) // 创建订单逻辑 // ... // 发送延迟消息 rabbitTemplate.convertAndSend(orderExchange, orderTimeoutQueue, order, message - message.getMessageProperties().setExpiration(600000); // 设置延迟时间（10分钟） return message; ); ②、使用消费者从队列中消费消息，当消费到超时任务时，执行订单超时处理逻辑。 @Servicepublic class OrderTimeoutConsumer @RabbitListener(queues = orderTimeoutQueue) public void handleOrderTimeout(Order order) // 处理订单超时逻辑 // ... 第二，使用数据库调度器（如 Quartz）。 ①、创建一个 Quartz 任务类，处理订单超时逻辑。 public class OrderTimeoutJob implements Job @Override public void execute(JobExecutionContext context) throws JobExecutionException // 获取订单信息 Order order = (Order) context.getJobDetail().getJobDataMap().get(order); // 处理订单超时逻辑 // ... ②、在订单创建时，调度一个 Quartz 任务，设置任务的触发时间为订单超时时间。 @Servicepublic class OrderService @Autowired private Scheduler scheduler; public void createOrder(Order order) // 创建订单逻辑 // ... // 调度 Quartz 任务 JobDetail jobDetail = JobBuilder.newJob(OrderTimeoutJob.class) .usingJobData(order, order) .build(); Trigger trigger = TriggerBuilder.newTrigger() .startAt(new Date(System.currentTimeMillis() + 600000)) // 设置触发时间（10分钟后） .build(); try scheduler.scheduleJob(jobDetail, trigger); catch (SchedulerException e) e.printStackTrace(); 41.Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。 它通过注解（如 @Cacheable、@CachePut、@CacheEvict）来实现缓存管理，极大简化了代码实现。 @Cacheable：缓存方法的返回值。 @CachePut：用于更新缓存，每次调用方法都会将结果重新写入缓存。 @CacheEvict：用于删除缓存。使用示例： Spring Cache 和 Redis 有什么区别？ Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。 Redis 是一个分布式的缓存中间件，支持多种数据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。 Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。 Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。 在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。 有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。 Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。 说说Spring Cache 的底层原理？Spring Cache 是基于 AOP 和缓存抽象层实现的。它通过 AOP 拦截被 @Cacheable、@CachePut 和 @CacheEvict 注解的方法，在方法调用前后自动执行缓存逻辑。 其提供的 CacheManager 和 Cache 等接口，不依赖具体的缓存实现，因此可以灵活地集成 Redis、Caffeine 等多种缓存。 ConcurrentMapCacheManager：基于 Java ConcurrentMap 的本地缓存实现。 RedisCacheManager：基于 Redis 的分布式缓存实现。 CaffeineCacheManager：基于 Caffeine 的缓存实现。","tags":["基础","spring"],"categories":["Java问答笔记"]},{"title":"2025.6.12学习日记","path":"/2025/06/12/学习日记25年6月/2025.6.12学习笔记/","content":"今日学习内容3DGS:今日工作总结今天对开源数据集train,truck,drjohnson进行训练和测试.结果主要用于之后跑不同改进版本3DGS的参考。结果在飞书文档中在训练drjohnson数据集过程中,训练速度明显变慢.分析原因：可能是drjohnson数据集中，图像分辨率相比train和truck数据集更高,点云数量更大，并且在train.py训练过程中，高斯球克隆更加激进，导致显存交换更加频繁，IO时间变长或者缓存碎片化，导致训练时间成倍增加。 明日工作计划继续将drjohnson数据集与playroom数据集进行训练和测试作为参考.部署EfficientGS方法进行测试,该方法主要针对室外大场景,对原版3DGS进行性能优化. 力扣每日一题:循环数组简单题. 上午看mysql看到24题. 下午看项目看了一篇Servlet的文章. Spring学习写了四条,开个头. 晚上学代码随想录继续二叉树. 做了一道数组模拟环形链表找入口的题,挺有意思的. https://leetcode.cn/problems/find-the-duplicate-number 生活记录晚上健身今天主要练的背.","tags":["3DGS","日记","leetcode","spring","mysql"],"categories":["学习日记","2025-06"]},{"title":"3DGS-Ubuntu环境.md","path":"/2025/06/11/3DGS/3DGS-Ubuntu环境/","content":"环境:Ubuntu20.04和Win11双系统CUDA11.8Anacoda3 安装clash for linux 快捷命令 clashtun on off anaconda:source conda activate 虚拟环境名","tags":["3DGS","公司"],"categories":["3DGS"]},{"title":"3DGS学习笔记","path":"/2025/06/11/3DGS/3DGS学习笔记/","content":"环境配置https://www.youtube.com/watch?v=UXtuigy_wYc youtube复现视频https://github.com/graphdeco-inria/gaussian-splatting 3DGS的github源码地址https://dl.acm.org/doi/10.1145/3592433 论文地址https://arxiv.org/abs/2308.04079 下载依赖和环境视频老哥的github地址https://github.com/jonstephens85/gaussian-splatting-Windows 下载git(跳过)测试是否下载git –version 下载anaconda(跳过)下载CUDA :nvcc –version测试版本nvidia-smi确定电脑最高支持的CUDA版本 ,我的最高支持12.8 准备下载CUDA 11.8版本安装vs2019官方下载地址：https://visualstudio.microsoft.com/zh-hans/vs/older-downloads/ 下载colmap准备编译打开anaconda prompt D: 切换D盘D:\\user\\desktop\\workplace\\3DGS\\userdesktopworkplace3DGS 要将 Anaconda 创建的虚拟环境设置在 D 盘，可以按照以下步骤操作： 修改 Anaconda 配置首先，需要修改 Anaconda 的配置，使其将环境创建在指定路径。在 Anaconda Prompt 中执行以下命令：conda config –add envs_dirs D:\\Anaconda\\envs（如果需要，将 D:\\Anaconda\\envs 替换为你想要的路径） 确保路径存在确保你指定的路径已经存在。如果不存在，请手动创建该文件夹。 创建虚拟环境现在你就可以创建虚拟环境，新的环境将被创建在 D 盘的指定路径下：conda create –name env_name python3.x 检查环境位置可以使用以下命令查看虚拟环境的位置：conda info –envs经过以上步骤后，你的虚拟环境将会在 D 盘创建。conda create -n gaussian_splatting python3.7conda activate gaussian_splattingconda install -c conda-forge vs2019_win-64pip install torch1.13.1+cu117 torchvision0.14.1+cu117 torchaudio0.13.1 –extra-index-url https://download.pytorch.org/whl/cu117pip install submodulesdiff-gaussian-rasterizationpip install submodulessimple-knnpip install submodulesfused-ssim 训练3dgs的运行训练下述所有命令都是在终端里运行的，运行时保持是从gaussian-splatting目录下开始输入的。省流版本:conda activate gaussian_splattingd:cd D:\\sys\\Desktop\\Workplace\\3DGS新的路径训练:python train.py -s data_train -m data_train/output (参数为输出地址)python render.py -m data_train/output把train文件夹的method复制到test文件夹python metrics.py -m data_train/output 1.视频截取帧这里可以用自己手机拍摄的一段视频，一两分钟即可，可以参考一下作者的训练时间，作者用自己的笔记本（4060 8G），大概训练了两个小时左右，跑完了所有的迭代。 在gaussian-splatting目录下新建一个data文件夹，将你拍摄的视频移动到该data文件夹下，并将你的视频改名为input，后缀.mp4不用改。然后在data文件夹里再建一个与视频同名的文件夹，名字也是input。然后就可以输入命令啦（终端里从gaussian-splattingdata目录下开始输入） cd data ffmpeg -i input.mp4 -vf setpts=0.2*PTS input\\input_%04d.jpg #推荐运行这个指令 ffmpeg -i input.mp4 -vf setpts=0.2*PTS,fps=1 input\\input_%04d.jpg #如果需要调整抽帧频率可以参考这个指令。选择一个运行即可 这里简单的说一下各个参数的含义。setpts0.2*PTS 将视频播放速度加快到原来的 5 倍。这意味着原视频的每秒帧数增加到 5 倍。如果原始视频是 30 FPS，加速后的视频将以 150 FPS 播放。尽管视频播放速度加快了，ffmpeg fps1 会以每秒一帧的频率提取图片。 这样就可以把你的视频截取为帧并保存在input文件夹里，在input文件夹里应该可以看到许多张照片。 2.产生点云在终端gaussian-splatting目录下输入cd ..python convert.py -s data 这个就是利用安装的colmap产生点云，会花费一些时间，等待完成即可。 3.查看点云终端里输入 colmap 调出来colmap后，选择file-import model然后选择gaussian-splattingdatasparse0文件夹，选择确定，即可打开生成的点云，遇到弹窗×掉即可。可以看到生成的点云还有相机路径。 4.开始训练同样，在终端里gaussian-splatting目录下，输入 python train.py -s data -m dataoutput (参数为输出地址)python train.py -s data -m dataimages 成功开始会出现如下图所示然后耐心等待训练完成以后即可。 5.查看结果同样，在终端里gaussian-splatting目录下，输入 .\\viewers\\bin\\SIBR_gaussianViewer_app -m dataoutput 即可打开viewer窗口，可以把你的场景拖大，下面是一些快捷按键 w uio asd jkl 就是可以控制视角的变化，大家自己按一下就知道是干啥的了，这里就不一一列举对应的功能了（作者已经累了），注意切换输入法为英文输入。 至此，就全部结束啦，完结撒花！ 1 . convert.py将input数据集转换成为点云通过sfm算法将输入的图片集转换成点云,这种方式的具体流程如下: 特征提取：从输入的图像集中，对每一张图像提取特征点及其描述子，常用的特征提取算法有 SIFT、SURF、ORB 等。 全局特征匹配：在所有图像的特征点之间进行匹配，找出不同图像中表示同一物理点的特征点对。由于是全局匹配，可能会处理大量的特征点对，计算量较大。 相机位姿估计：根据匹配的特征点对，使用诸如对极几何、PnP 等算法来估计相机的相对位姿。 三角测量：利用已知的相机位姿和匹配的特征点，通过三角测量计算出三维点的坐标，从而生成点云。 全局优化：使用束调整（Bundle Adjustment）等方法对相机位姿和三维点的坐标进行全局优化，提高点云的精度。SfM 侧重于对静态图像集进行全局处理，通过全局优化来生成高精度的点云,SfM相比ORB方法 更侧重于离线的高精度三维重建. 渲染辐射场的几种方法建立了最近的数据集。革命性地合成了用多张照片或视频捕获的场景。然而，实现高视觉质量仍然需要训练和渲染成本高昂的神经网络，而最近更快的方法不可避免地要牺牲速度来换取质量。对于无界和完整的场景(而不是孤立的对象)和1080p分辨率的渲染，目前没有一种方法可以实现实时显示速率。我们介绍了三个关键要素，使我们能够在保持有竞争力的训练时间的同时实现最先进的视觉质量，并且重要的是允许在1080p分辨率下实现高质量的实时(≥30 fps)新视图合成。首先，从相机校准过程中产生的稀疏点开始，我们用3D高斯分布表示场景，该分布保留了用于场景优化的连续体辐射场的理想属性，同时避免了在空白空间中不必要的计算;其次，我们对3D高斯分布进行交错优化密度控制，特别是优化各向异性协方差以实现场景的准确表示;第三，我们开发了一种支持各向异性喷溅的快速可视性感知渲染算法，既加速了训练，又允许实时渲染。我们展示了最先进的视觉质量和实时性 3dgs流程3DGS流程：（1）通过colmap等工具从多视角图像获取SfM点云（SfM是一种三维重建算法，通过两个或多个场景图片恢复相机位姿，并重建三维坐标点），对 SfM 点云进行了初始化。 （2）点云中的每一个点代表着一个三维的高斯分布，除了点的位置（均值）外，还有协方差、不透明度、颜色（球谐函数）–3D 高斯球云。 （3）将这些椭球体沿着特定角度投影到对应位姿所在平面（Splatting）。一个椭球体投影到平面会得到一个椭圆；然后通过计算待求解像素和椭圆中心的距离，我们得到不透明度（离的近，说明越不透明）；每个椭球又代表各自的颜色，进行alpha composting来合成颜色，然后快速的对所有像素做“可微光栅化”，渲染得到图像。 （4）得到渲染图像Image后，再与gt图像比较，得到损失loss，并沿蓝色箭头反向传播，随机梯度下降；向下送入自适应密度控制中（增密或修剪），更新点云优化。 代码运行流程1.Runningpython train.py -s 示例： python train.py -s data360_extra_scenestreehill 运行完在output下得到相应的文件夹outputtreehill， 将得到的结果路径添加至SIBR_viewer.py（model_path r’D:\\gaussian-splatting\\output\\treehill’），运行即可获得可视化。 densify_and_prune操作会改变高斯数量。结合在一起，允许模型根据当前两个的训练状态动态地调整高斯的数量，从而实现更好的表示能力和计算效率。因此，在这个过程中，高斯的数量会变化，所以需要在执行后打印出当前的高斯数量。在训练的时候添加高斯数量打印： print(f”Iteration {iteration}: Number of Gaussians after densification and pruning: {gaussians.get_xyz.shape[0]}”) 2. Evaluationpython train.py -s –eval # Train with traintest splitpython render.py -m # Generate renderingspython metrics.py -m # Compute error metrics on renderings 训练模型-渲染图像-计算指标 示例：Evaluation运行，输入命令行(python train.py -s + 数据集的路径) python train.py -s data360_extra_scenestreehill –evalpython render.py -m outputtreehillpython metrics.py -m outputtreehill 2. Processing your own Scenes按照README.me进行，选择的mill19building-pixsfm进行简单测试 图像目录结构： location|---input |---image 0 |---image 1 |---...python convert.py -s location [--resize] #If not resizing, ImageMagick is not needed示例：data/mill19/building-pixsfm|---input |---image 0 |---image 1 |---... 然后运行： python convert.py -s datamill19building-pixsfm 两句代码优化3DGS显存问题做实验的时候发现，对于同一个场景，图片越多显存占用越大。但是训练的时候是一张图片做一次梯度下降，说明batch_size一直是1。那数据集变大，batch_size不变的情况下，显存占用却变大了，这明显很不合理。当训练的图片比较多的时候，显存就不够了。 图片越多，显存占用越大，这就暗示了3DGS源码里一定是训练前预先将所有的图片都加载显存中的。事实也确实如此，在训练前的预处理中，为每张图片都构造了一个对应的Camera对象，Camera的初始化函数中就将图片加载到了显存中，如下。 源码scenecameras.py中39行，修改前self.original_image image.clamp(0.0, 1.0).to(self.data_device) 修改后self.original_image image.clamp(0.0, 1.0)这优化很简单，不要提前载入显存就好了，直接把“.to(self.data_device)”删掉。训练的时候，具体训练哪张图片，载入哪张图片就好了。这个在3DGS源码中，已经做了，如下。经过所述的代码修改后，确实显存减少了，但是训练速度变慢了。因为相比修改前，多出来的耗时在每次训练迭代中都有一次图片加载到显存中的操作。在cuda中将数据从内存载入显存是可以异步进行的，pytorch也提供了接口，所以对代码做出以下修改。 源码train.py中90行，修改前gt_image viewpoint_cam.original_image.cuda() 修改后，需移动到render_pkg render(…)之前gt_image viewpoint_cam.original_image.cuda(non_blockingTrue)不过仅仅只是将载入方式设置为non_blocking还不够，因为可能在要使用gt_image之前其还没有完成载入显存的操作，此时也会发生阻塞。所以需要将这句代码往前放，可以放到“render_pkg render(viewpoint_cam, gaussians, pipe, bg)”这句之前。这样就几乎不会影响训练速度。","tags":["3DGS","公司"],"categories":["3DGS"]},{"title":"2025.6.11学习日记","path":"/2025/06/11/学习日记25年6月/2025.6.11学习笔记/","content":"今日学习内容3DGS:今日工作总结:​一. 代码运行与结构梳理​​:学习了原版3DGS的项目结构和训练流程: train.py : 训练脚本,主要负责对高斯球参数进行训练. render.py : 渲染脚本,用于将训练得到的高斯球参数渲染成图像. metrics.py : 评估脚本,对比真值图像和渲染图像,用于评估渲染结果的质量. 二.训练结果:火车数据集301张图像. train.py :ITER 7000:L1损失 : 0.06603520661592484PSNR :20.096060180664065训练时间 : 13分30秒ITER 30000:L1损失 : 0.038734884932637215PSNR : 24.450721740722656s训练时间:77分25秒 render.py : metrics.py :SSIM: 0.87444342PSNR: 25.8431702LPIPS: 0.1703709 ​​明日工作计划:​​今天对火车的数据集进行了实验,明天准备开始对其他数据集进行实验,得出一组原版3DGS的训练结果作为参考,方便对后续3DGS改进算法进行实验对比. 下午回学校开会来着晚上继续做力扣二叉树篇明日计划3dgs明天尽量让时间一直在跑代码,然后我可以学java. mysql学习力扣项目文档生活记录1. 早上足球训练早上7点,颠球短传训练.","tags":["3DGS","日记"],"categories":["学习日记","2025-06"]},{"title":"nvm更改node版本","path":"/2025/06/11/杂项笔记/nvm更改node版本/","content":"安装brew首先安装 Brew。这个就不详细说了 没有的话自己去搜一下 第一步：进行nvm 安装操作brew install nvm 执行后：== Pouring nvm-0.39.1_1.all.bottle.tar.gz== CaveatsPlease note that upstream has asked us to make explicit managingnvm via Homebrew is unsupported by them and you should check anyproblems against the standard nvm install method prior to reporting.You should create NVMs working directory if it doesnt exist: // 这里就是提示你创建一个 nvm文件 mkdir ~/.nvmAdd the following to ~/.zshrc or your desired shellconfiguration file: // 这里就是想让你进行一些配置 export NVM_DIR=$HOME/.nvm [ -s /opt/homebrew/opt/nvm/nvm.sh ] \\. /opt/homebrew/opt/nvm/nvm.sh # This loads nvm [ -s /opt/homebrew/opt/nvm/etc/bash_completion.d/nvm ] \\. /opt/homebrew/opt/nvm/etc/bash_completion.d/nvm # This loads nvm bash_completionYou can set $NVM_DIR to any location, but leaving it unchanged from/opt/homebrew/opt/nvm will destroy any nvm-installed Node installationsupon upgrade/reinstall.Type `nvm help` for further information.== Summary🍺 /opt/homebrew/Cellar/nvm/0.39.1_1: 9 files, 184KB 执行 nvm –versionnvm --version //出现问题。去进行配置 zsh: command not found: nvm 第二步：nvm配置1.vim ~/.bash_profile点击 i 进行插入操作 插入下面配置 export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh 插入完成后 点击Esc 然后 使用 :wq. 保存并退出 执行：source ~/.bash_profile 如果出问题先不管接着往下走 2.vim ~/.zshrc点击 i 进行插入操作 插入下面配置 export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh 插入完成后 点击Esc 然后 使用 :wq. 保存并退出 执行：source ~/.zshrc 3.vim ~/.profile点击 i 进行插入操作 插入下面配置 export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh 插入完成后 点击Esc 然后 使用 :wq. 保存并退出 执行：source ~/.profile 最后开始进行测试执行：nvm --version 显示版本号就说明配置成功：0.39.1 通过nvm 进行node 版本控制 版本号根据自己的需求定义 nvm install 12.6.0 查看版本：node -v nvm 常用命令：以下用8.9.2版本为例nvm ls ：打印出所有的版本 install stable：安装最稳定的版本nvm install v8.9.2 ： 安装node的8.9.2的版本（删除用uninstall）nvm current ：当前使用的node版本nvm use v8.9.2 ：将node改为8.9.2版本nvm alias default 0.12.7：设置默认 node 版本为 0.12.7nvm alias default ：设置系统默认的node版本nvm alias ：给不同的版本号添加别名nvm unalias ： 删除已定义的别名nvm reinstall-packages ：在当前版本node环境下，重新全局安装指定版本号的npm包npm install -g mz-fis：安装 mz-fis 模块至全局目录，安装的路径：/Users/你的用户名/.nvm/versions/node/v0.12.7/lib/mz-fisnvm use 4：切换至 4.2.2 版本（支持模糊查询）npm install -g react-native-cli：安装 react-native-cli 模块至全局目录，安装的路径：/Users/你的用户名/.nvm/versions/node/v4.2.2/lib/react-native-cli node npm 版本对照链接: 版本对照 末尾 npm 降级执行：sudo npm install npm@6.9.0 -g 问题是不可控的 如果未能解决你的问题 就祝你顺利","tags":["nvm","node"],"categories":["杂项笔记"]},{"title":"Java集合框架笔记","path":"/2025/06/10/基础笔记/Java集合框架笔记/","content":"框架图先贴一个Java集合框架图 可以看出,集合主要分成两大部分: Collection:主要由 List、Set、Queue 组成，List 代表有序、可复的集合，典型代表就是封装了动态数组的 ArrayList 和封装了链表的 LinkedList；Set 代表⽆序、不可复的集合，典型代表就是 HashSet 和 TreeSet；Queue 代表队列，典型代表就是双端队列ArrayDeque，以及优先级队列 PriorityQueue。 Map:代表键值对的集合，典型代表就是 HashMap。 CollectionListList 的特点是存取有序，可以存放复的元素，可以⽤下标对元素进⾏操作。 ArrayListArrayList的增删改查: // 创建⼀个集合ArrayListString list = new ArrayListString();// 添加元素list.add(王⼆);list.add(沉默);list.add(陈清扬);// 遍历集合 for 循环for (int i = 0; i list.size(); i++) String s = list.get(i); System.out.println(s);// 遍历集合 for eachfor (String s : list) stem.out.println(s);// 删除元素list.remove(1);// 遍历集合for (String s : list) System.out.println(s);// 修改元素list.set(1,王⼆狗);// 遍历集合for (String s : list) System.out.println(s);","tags":["基础","Java","集合"],"categories":["基础笔记"]},{"title":"Mysql学习笔记","path":"/2025/06/10/Java问答笔记/Mysql学习笔记/","content":"Mysql基础🌟0.什么是MYSQLMySQL 是⼀个开源的关系型数据库，现在⾪属于 Oracle 公司。 删除创建一张表DROP TABLE 删除表CREATE TABLE 创建表创建表的时候，可以通过 PRIMARY KEY 设定主键。 CREATE TABLE users ( id INT AUTO_INCREMENT, name VARCHAR(100) NOT NULL, email VARCHAR(100), PRIMARY KEY (id)); 写一个升序降序的SQL语句可以使用ORDER BY字句对查询结果进行排序.默认情况下是升序排序.如需要降序,使用关键字DESC例子: SELECT id, name, salaryFROM employeesORDER BY salary DESC; 如若对多个字段进行排序: SELECT id, name, salaryFROM employeesORDER BY salary DESC, name ASC; 优先级从左到右,相当于先按工资降序,工资相同再按照姓名升序. MYSQL出现性能差的原因可能是 SQL 查询使⽤了全表扫描，也可能是查询语句过于复杂，如多表JOIN或嵌套⼦查询。也有可能是单表数据量过⼤。 通常情况下,增加索引就可以解决大部分的性能问题.对于热点数据,增加redis缓存,减轻对数据库的压力. 1.两张表怎么进行连接可以通过内连接inner join、外连接 outer join 、交叉连接 cross join 等方式来进行连接. 什么是内连接内连接⽤于返回两个表中有匹配关系的⾏。假设有两张表，⽤户表和订单表，想查询有订单的⽤户，就可以使⽤内连接 users INNER JOIN orders，按照⽤户 ID 关联就⾏了。 SELECT users.name, orders.order_idFROM usersINNER JOIN orders ON users.id = orders.user_id; 两表匹配的行才会输出. 什么是外连接和内连接不同，外连接不仅返回两个表中匹配的⾏，还返回没有匹配的⾏，⽤ null 来填充。外连接⼜分为左外连接 left join 和右外连接 right join。 left join 会保留左表中符合条件的所有记录，如果右表中有匹配的记录，就返回匹配的记录，否则就⽤null 填充，常⽤于某表中有，但另外⼀张表中可能没有的数据的查询场景。假设要查询所有⽤户及他们的订单，即使⽤户没有下单，就可以使⽤左连接： SELECT users.id, users.name, orders.order_idFROM usersLEFT JOIN orders ON users.id = orders.user_id; (这里面左表就是users,users所有行都会输出) 右连接就是左连接的镜像，right join 会保留右表中符合条件的所有记录，如果左表中有匹配的记录，就返回匹配的记录，否则就⽤ null 填充。 什么是交叉连接交叉连接会返回两张表的笛卡尔积，也就是将左表的每⼀⾏与右表的每⼀⾏进⾏组合，返回的⾏数是两张表⾏数的乘积。假设有 A 表和 B 表，A 表有 2 ⾏数据，B 表有 3 ⾏数据，那么交叉连接的结果就是 2 * 3 6 ⾏。笛卡尔积是数学中的⼀个概念，例如集合 A{a,b}，集合 B{0,1,2} ，那么 A x B {a,0,a,1,a,2,b,0,b,1,b,2,}。 SELECT A.id, B.idFROM ACROSS JOIN B; 2.内连接 左连接 右连接有什么区别左连接 FROM 表a join 表b 相当于 a在左 b在右MySQL 的连接主要分为内连接和外连接，外连接⼜可以分为左连接和右连接。 内连接相当于找两表的交集.左连接和右连接可以⽤来找出两个表中不同的记录，相当于两个数据集的并集。两者的区别是，左连接会保留左表中符合条件的所有记录，右连接则刚好相反。 例子:有三张表，⼀张⽂章表 article，主要存⽂章标题 title.⼀张⽂章详情表 article_detail，主要存⽂章的内容 content.⼀张⽂章评论表 comment，主要存评论 content.三个表通过⽂章 id关联。内连接:返回至少有评论的文章标题和评论内容 SELECT LEFT(a.title, 20) AS ArticleTitle, LEFT(c.content, 20) AS CommentContent //返回LEFT左数前20字符 AS为列或表起临时别名FROM article a //article起别名 aINNER JOIN comment c ON a.id = c.article_id //内连接 id相同 起别名cLIMIT 2; //只返回两条 左连接:返回所有⽂章的标题和⽂章评论，即使某些⽂章没有评论（填充为 NULL）。 SELECT LEFT(a.title, 20) AS ArticleTitle, LEFT(c.content, 20) ASCommentContentFROM article aLEFT JOIN comment c ON a.id = c.article_idLIMIT 2; 右连接:调换了位置 SELECT LEFT(a.title, 20) AS ArticleTitle, LEFT(c.content, 20) ASCommentContentFROM comment cRIGHT JOIN article a ON a.id = c.article_idLIMIT 2; 3.数据库的三大范式 - 第⼀范式:确保表的每⼀列都是不可分割的基本数据单元.⽐如说⽤户地址，应该拆分成省、市、区、详细地址等 4 个字段。相当于保持列的原子性. - 第⼆范式:要求表中的每⼀列都和主键直接相关。⽐如在订单表中，商品名称、单位、商品价格等字段应该拆分到商品表中。然后再创建一个订单商品关联表.不能出现部分依赖的情况.相当于让解决复合主键的部分依赖问题.订单明细(订单ID, 产品ID, 产品名称, 数量)↑ 复合主键是(订单ID, 产品ID)，但”产品名称”只依赖”产品ID”（部分依赖）修改为:订单明细(订单ID, 产品ID, 数量)产品(产品ID, 产品名称) - 第三范式:⾮主键列应该只依赖于主键列。⽐如说在设计订单信息表的时候，可以把客户名称、所属公司、联系⽅式等信息拆分到客户信息表中，然后在订单信息表中⽤客户编号进⾏关联。**相当于消除传递依赖.**将​​传递依赖​​的字段（A→B→C，其中A是主键）拆分到新表中。学生(学号, 姓名, 宿舍号, 宿舍费用)↑ “宿舍费用”依赖”宿舍号”，而”宿舍号”依赖”学号”（传递依赖）修改为:学生(学号, 姓名, 宿舍号)宿舍(宿舍号, 宿舍费用) 建表时需要考虑哪些问题⾸先需要考虑表是否符合数据库的三⼤范式，确保字段不可再分，消除⾮主键依赖，确保字段仅依赖于主键等。然后在选择字段类型时，应该尽量选择合适的数据类型。在字符集上，尽量选择 utf8mb4，这样不仅可以⽀持中⽂和英⽂，还可以⽀持表情符号等。当数据量较⼤时，⽐如上千万⾏数据，需要考虑分表。⽐如订单表，可以采⽤⽔平分表的⽅式来分散单表存储压⼒。 水平分表(按行拆分):水平分表是将​​同一张表中的数据按行拆分到多个结构相同的表中 按照id范围分表 按照时间范围分表 按照hash分表 按照地域分表 垂直分表(按列拆分):不同分表包含不同字段,表字段过多或冷热数据分离时使用. 4.varchar 与 char 的区别latin1 字符集，且列属性定义为 NOT NULL。 varchar 是可变⻓度的字符类型，原则上最多可以容纳 65535 个字符，但考虑字符集，以及MySQL 需要 1 到 2 个字节来表示字符串⻓度，所以实际上最⼤可以设置到 65533。char 是固定⻓度的字符类型，当定义⼀个 CHAR(10) 字段时，不管实际存储的字符⻓度是多少，都只会占⽤ 10 个字符的空间。如果插⼊的数据⼩于 10 个字符，剩余的部分会⽤空格填充。 varchar在输入过长时也会截断. 5.blob 和 text 有什么区别blob ⽤于存储⼆进制数据，⽐如图⽚、⾳频、视频、⽂件等；但实际开发中，我们都会把这些⽂件存储到 OSS 或者⽂件服务器上，然后在数据库中存储⽂件的 URL。 text ⽤于存储⽂本数据，⽐如⽂章、评论、⽇志等。 6.DATETIME 和 TIMESTAMP 有什么区别DATETIME 直接存储⽇期和时间的完整值，与时区⽆关。TIMESTAMP 存储的是 Unix 时间戳，1970-01-01 00:00:01 UTC 以来的秒数，受时区影响。 另外，DATETIME 的默认值为 null，占⽤ 8 个字节；TIMESTAMP 的默认值为当前时间——CURRENT_TIMESTAMP，占 4 个字节，实际开发中更常⽤，因为可以⾃动更新。 7. in 和 exists 的区别当使⽤ IN 时，MySQL 会⾸先执⾏⼦查询，然后将⼦查询的结果集⽤于外部查询的条件。这意味着⼦查询的结果集需要全部加载到内存中。 ⽽ EXISTS 会对外部查询的每⼀⾏，执⾏⼀次⼦查询。如果⼦查询返回任何⾏，则EXISTS条件为真。EXISTS 关注的是⼦查询是否返回⾏，⽽不是返回的具体值。 -- IN 的临时表可能成为性能瓶颈SELECT * FROM usersWHERE id IN (SELECT user_id FROM orders WHERE amount 100);//查询所有下过单笔订单金额超过100的用户（完整信息）-- EXISTS 可以利⽤关联索引SELECT * FROM users uWHERE EXISTS (SELECT 1 FROM orders oWHERE o.user_id = u.id AND o.amount 100); IN 适⽤于⼦查询结果集较⼩的情况。如果⼦查询返回⼤量数据,IN的查询效率会下降,因为他会把整个结果存到内存当中. ⽽ EXISTS 适⽤于⼦查询结果集可能很⼤的情况。由于 EXISTS 只需要判断⼦查询是否返回⾏，⽽不需要加载整个结果集，因此在某些情况下性能更好，特别是当⼦查询可以使⽤索引时。 NULL值IN的返回结果中如果有NULL值,可能会出现意料外的情况:比如WHERE column IN ((subquery))，如果subquery为NULL,这个条件永远不会为真,除非column也为NULL. EXISTS如果有NULL值的话,因为EXISTS只关心是否有⾏,所以不会出现NULL值的影响. 8.记录货币⽤什么类型⽐较好?如果是电商、交易、账单等涉及货币的场景，建议使⽤ DECIMAL 类型，因为 DECIMAL 类型是精确数值类型，不会出现浮点数计算误差。 如果是银⾏,涉及到⽀付的场景，建议使⽤ BIGINT 类型。可以将货币⾦额乘以⼀个固定因⼦，⽐如 100，表示以“分”为单位，然后存储为 BIGINT 。这种⽅式既避免了浮点数问题，同时也提供了不错的性能。但在展示的时候需要除以相应的因⼦。 为什么不推荐使⽤ FLOAT 或 DOUBLE？因为 FLOAT 和 DOUBLE 都是浮点数类型，会存在精度问题。在许多编程语⾔中， 0.1 + 0.2 的结果会是类似 0.30000000000000004 的值，⽽不是预期的 0.3 。 9.如何存储emoji?因为 emoji是 4 个字节的 UTF-8 字符，⽽ MySQL 的 utf8 字符集只⽀持最多 3 个字节的 UTF-8 字符，所以在 MySQL 中存储 emoji 时，需要使⽤ utf8mb4 字符集。 MySQL 8.0 已经默认⽀持 utf8mb4 字符集，可以通过 SHOW VARIABLES WHERE Variable_name LIKE character\\_set\\_% OR Variable_name LIKE collation%; 查看。 10.drop、delete 与 truncate 的区别？ DROP 是物理删除，⽤来删除整张表，包括表结构，且不能回滚。 DELETE ⽀持⾏级删除，可以带 WHERE 条件，可以回滚。 TRUNCATE ⽤于清空表中的所有数据，但会保留表结构，不能回滚。 11.UNION 与 UNION ALL 的区别？UNION 会⾃动去除合并后结果集中的重复⾏。UNION ALL 不会去重，会将所有结果集合并起来。 12.count(1)、count(*) 与 count(列名) 的区别？在InnoDB引擎里面,count(1)和count(*)没有任何区别,都用来统计所有行,包括NULL.如果有索引,那么count(1)和count(*)都会走索引,而count(列名)会走主键索引. COUNT(列名) 只统计列名不为 NULL 的⾏数。 13.SQL 查询语句的执⾏顺序了解吗？了解,先执行FROM确定主表,，再执⾏JOIN连接，然后 WHERE 进⾏过滤，接着 GROUP BY 进⾏分组，HAVING 过滤聚合结果，SELECT 选择最终列，ORDER BY 排序，最后 LIMIT 限制返回⾏数。 WHERE 先执⾏是为了减少数据量，HAVING 只能过滤聚合数据，ORDER BY 必须在SELECT 之后排序最终结果，LIMIT 最后执⾏以减少数据传输。 这个执⾏顺序与编写 SQL 语句的顺序不同，这也是为什么有时候在 SELECT ⼦句中定义的别名不能在 WHERE ⼦句中使⽤得原因，因为 WHERE 是在 SELECT 之前执⾏的。 LIMIT 为什么在最后执⾏？因为 LIMIT 是在最终结果集上执⾏的，如果在 WHERE 之前执⾏ LIMIT，那么就会先返回所有⾏，然后再进⾏ LIMIT 限制，这样会增加数据传输的开销。 ORDER BY 为什么在 SELECT 之后执⾏？因为排序需要基于最终返回的列，如果 ORDER BY 早于 SELECT 执⾏，计算 类的聚合函数就会出问题。比如说如果要按照所选的平均值排序,order by先执行,还没有计算平均值. 14.介绍⼀下 MySQL 的常⽤命令MySQL 的常⽤命令主要包括数据库操作命令、表操作命令、⾏数据 CRUD 命令、索引和约束的创建修改命令、⽤户和权限管理的命令、事务控制的命令等。 -- 数据库操作CREATE DATABASE db_name; -- 创建数据库DROP DATABASE db_name; -- 删除数据库SHOW DATABASES; -- 查看所有数据库USE db_name; -- 切换数据库-- 表操作CREATE TABLE table_name ( -- 创建表 id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(50) NOT NULL);DROP TABLE table_name; -- 删除表ALTER TABLE table_name ADD COLUMN col_name INT; -- 添加列ALTER TABLE table_name DROP COLUMN col_name; -- 删除列SHOW TABLES; -- 查看所有表DESC table_name; -- 查看表结构-- 数据CRUDINSERT INTO table_name VALUES (1, test); -- 插入数据SELECT * FROM table_name; -- 查询数据UPDATE table_name SET name=new WHERE id=1;-- 更新数据DELETE FROM table_name WHERE id=1; -- 删除数据-- 索引和约束CREATE INDEX idx_name ON table_name(col); -- 创建索引ALTER TABLE table_name ADD PRIMARY KEY(id); -- 添加主键ALTER TABLE table_name ADD UNIQUE(col_name); -- 添加唯一约束ALTER TABLE table_name ADD FOREIGN KEY(col) REFERENCES other_table(col); -- 外键-- 用户权限CREATE USER user@host IDENTIFIED BY pwd; -- 创建用户GRANT ALL ON db_name.* TO user@host; -- 授权REVOKE ALL ON db_name.* FROM user@host; -- 撤销权限DROP USER user@host; -- 删除用户-- 事务控制START TRANSACTION; -- 开始事务COMMIT; -- 提交事务ROLLBACK; -- 回滚事务SET autocommit=0; -- 关闭自动提交 说说数据库操作命令?CREATE DATABASE database_name; ⽤于创建数据库;USE database_name;⽤于显示所有数据库;DROP DATABASE database_name;⽤于删除数据库;SHOW DATABASES; 换数据库。 说说表操作命令？CREATE TABLE table_name (列名1 数据类型1, 列名2 数据类型2,...); 用于创建表；DROP TABLE table_name; 用于删除表；SHOW TABLES; 用于显示所有表；DESCRIBE table_name; 用于查看表结构；ALTER TABLE table_name ADD column_name datatype; 用于修改表。 说说行数据的 CRUD 命令？INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...); 用于插入数据；SELECT column_names FROM table_name WHERE condition; 用于查询数据；UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; 用于更新数据；DELETE FROM table_name WHERE condition; 用于删除数据。 说说索引和约束的创建修改命令？CREATE INDEX index_name ON table_name (column_name); 用于创建索引；ALTER TABLE table_name ADD PRIMARY KEY (column_name); 用于添加主键；ALTER TABLE table_name ADD CONSTRAINT fk_name FOREIGN KEY (column_name) REFERENCES parent_table (parent_column_name); 用于添加外键。 说说用户和权限管理的命令？CREATE USER username@host IDENTIFIED BY password; 用于创建用户；GRANT ALL PRIVILEGES ON database_name.table_name TO username@host; 用于授予权限；REVOKE ALL PRIVILEGES ON database_name.table_name FROM username@host; 用于撤销权限；DROP USER username@host; 用于删除用户。 说说事务控制的命令？START TRANSACTION; 用于开始事务；COMMIT; 用于提交事务；ROLLBACK; 用于回滚事务。 15.MySQL bin 目录下的可执行文件了解吗MySQL 的 bin 目录下有很多可执行文件，主要用于管理 MySQL 服务器、数据库、表、数据等。比如说：mysql：用于连接 MySQL 服务器mysqldump：用于数据库备份，对数据备份、迁移或恢复时非常有用mysqladmin：用来执行一些管理操作，比如说创建数据库、删除数据库、查看 MySQL 服务器的状态等。mysqlcheck：用于检查、修复、分析和优化数据库表，对数据库的维护和性能优化非常有用。mysqlimport：用于从文本文件中导入数据到数据库表中，适合批量数据导入。mysqlshow：用于显示 MySQL 数据库服务器中的数据库、表、列等信息。mysqlbinlog：用于查看 MySQL 二进制日志文件的内容，可以用于恢复数据、查看数据变更等。 16.MySQL 第 3-10 条记录怎么查？可以使用 limit 语句，结合偏移量和行数来实现。 SELECT * FROM table_name LIMIT 2, 8; limit 语句用于限制查询结果的数量，偏移量表示从哪条记录开始，行数表示返回的记录数量。2：偏移量，表示跳过前两条记录，从第三条记录开始。8：行数，表示从偏移量开始，返回 8 条记录。偏移量是从 0 开始的，即第一条记录的偏移量是 0；如果想从第 3 条记录开始，偏移量就应该是 2。 17.用过哪些 MySQL 函数？用过挺多的，比如说处理字符串的函数：CONCAT(): 用于连接两个或多个字符串。LENGTH(): 用于返回字符串的长度。SUBSTRING(): 从字符串中提取子字符串。REPLACE(): 替换字符串中的某部分。TRIM(): 去除字符串两侧的空格或其他指定字符。 处理数字的函数：ABS(): 返回一个数的绝对值。ROUND(): 四舍五入到指定的小数位数。MOD(): 返回除法操作的余数。 日期和时间处理函数：NOW(): 返回当前的日期和时间。CURDATE(): 返回当前的日期。 汇总函数：SUM(): 计算数值列的总和。AVG(): 计算数值列的平均值。COUNT(): 计算某列的行数。 逻辑函数：IF(): 如果条件为真，则返回一个值；否则返回另一个值。CASE: 根据一系列条件返回值。 18.说说 SQL 的隐式数据类型转换？当一个整数和一个浮点数相加时，整数会被转换为浮点数。SELECT 1 + 1.0; – 结果为 2.0当一个字符串和一个整数相加时，字符串会被转换为整数。SELECT 1 + 1; – 结果为 2隐式转换会导致意想不到的结果，最好通过显式转换来规避。SELECT CAST(1 AS SIGNED INTEGER) + 1; – 结果为 2 19. 说说 SQL 的语法树解析？SQL 语法树解析是将 SQL 查询语句转换成抽象语法树 —— AST 的过程，是数据库引擎处理查询的第一步，也是防止 SQL 注入的重要手段。通常分为 3 个阶段。 第一个阶段，词法分析：拆解 SQL 语句，识别关键字、表名、列名等。 —start—比如说：SELECT id, name FROM users WHERE age 18;将会被拆解为：[SELECT] [id] [,] [name] [FROM] [users] [WHERE] [age] [] [18] [;]—end— 第二个阶段，语法分析：检查 SQL 是否符合语法规则，并构建抽象语法树。 —start—比如说上面的语句会被构建成如下的语法树： SELECT / \\ Columns FROM / \\ | id name users | WHERE | age 18或者这样表示：SELECT ├── COLUMNS: id, name ├── FROM: users ├── WHERE │ ├── CONDITION: age 18—end— 第三个阶段，语义分析：检查表、列是否存在，进行权限验证等。 —start—比如说执行：SELECT id, name FROM users WHERE age ‘eighteen’;会报错：ERROR: Column ‘age’ is INT, but ‘eighteen’ is STRING.—end— 数据库架构20.说说 MySQL 的基础架构？MySQL 采用分层架构，主要包括连接层、服务层、和存储引擎层。①、连接层主要负责客户端连接的管理，包括验证用户身份、权限校验、连接管理等。可以通过数据库连接池来提升连接的处理效率。②、服务层是 MySQL 的核心，主要负责查询解析、优化、执行等操作。在这一层，SQL 语句会经过解析、优化器优化，然后转发到存储引擎执行，并返回结果。这一层包含查询解析器、优化器、执行计划生成器、日志模块等。③、存储引擎层负责数据的实际存储和提取。MySQL 支持多种存储引擎，如 InnoDB、MyISAM、Memory 等。binlog写入在哪一层？binlog 在服务层，负责记录 SQL 语句的变化。它记录了所有对数据库进行更改的操作，用于数据恢复、主从复制等。 21.一条查询语句SELECT是如何执行的？当我们执行一条 SELECT 语句时，MySQL 并不会直接去磁盘读取数据，而是经过 6 个步骤来解析、优化、执行，然后再返回结果。第一步，客户端发送 SQL 查询语句到 MySQL 服务器。 第二步，MySQL 服务器的连接器开始处理这个请求，跟客户端建立连接、获取权限、管理连接。 第三步，解析器对 SQL 语句进行解析，检查语句是否符合 SQL 语法规则，确保数据库、表和列都是存在的，并处理 SQL 语句中的名称解析和权限验证。 第四步，优化器负责确定 SQL 语句的执行计划，这包括选择使用哪些索引，以及决定表之间的连接顺序等。 第五步，执行器会调用存储引擎的 API来进行数据的读写。 第六步，存储引擎负责查询数据，并将执行结果返回给客户端。客户端接收到查询结果，完成这次查询请求。 22.一条更新语句UPDATE是如何执行的？undo log 回滚日志: 用于事务的回滚操作.redo log 重做日志: 用于实现事务的持久性,保持数据一致性.总的来说，一条 UPDATE 语句的执行过程包括读取数据页、加锁解锁、事务提交、日志记录等多个步骤。拿 update test set a=1 where id=2 举例来说：在事务开始前，MySQL 需要记录undo log，用于事务回滚。 操作 id 旧值 新值 update 2 N 1 除了记录 undo log，存储引擎还会将更新操作写入 redo log，状态标记为 prepare，并确保 redo log 持久化到磁盘。这一步可以保证即使系统崩溃，数据也能通过 redo log 恢复到一致状态。 写完redo log 后，MySQL 会获取行锁，将 a 的值修改为 1，标记为脏页，此时数据仍然在内存的 buffer pool 中，不会立即写入磁盘。后台线程会在适当的时候将脏页刷盘，以提高性能。 最后提交事务，redo log 中的记录被标记为 committed，行锁释放。 如果 MySQL 开启了 binlog，还会将更新操作记录到 binlog 中，主要用于主从复制。 以及数据恢复，可以结合 redo log 进行点对点的恢复。binlog 的写入通常发生在事务提交时，与 redo log 共同构成“两阶段提交”，确保两者的一致性。 注意，redo log 的写入有两个阶段的提交，一是 binlog 写入之前prepare 状态的写入，二是binlog写入之后 commit 状态的写入。 23.说说 MySQL 的段区页行MySQL 是以表的形式存储数据的，而表空间的结构则由段、区、页、行组成。 ①、段：表空间由多个段组成，常见的段有数据段、索引段、回滚段等。创建索引时会创建两个段，数据段和索引段，数据段用来存储叶子节点中的数据；索引段用来存储非叶子节点的数据。回滚段包含了事务执行过程中用于数据回滚的旧数据。 ②、区：段由一个或多个区组成，区是一组连续的页，通常包含 64 个连续的页，也就是 1M 的数据。使用区而非单独的页进行数据分配可以优化磁盘操作，减少磁盘寻道时间，特别是在大量数据进行读写时。 ③、页：页是 InnoDB 存储数据的基本单元，标准大小为 16 KB，索引树上的一个节点就是一个页。也就意味着数据库每次读写都是以 16 KB 为单位的，一次最少从磁盘中读取 16KB 的数据到内存，一次最少写入 16KB 的数据到磁盘。 ④、行：InnoDB 采用行存储方式，意味着数据按照行进行组织和管理，行数据可能有多个格式，比如说 COMPACT、REDUNDANT、DYNAMIC 等。MySQL 8.0 默认的行格式是 DYNAMIC，由COMPACT 演变而来，意味着这些数据如果超过了页内联存储的限制，则会被存储在溢出页中。可以通过 show table status like ‘%article%’ 查看行格式。 了解 MySQL的数据行、行溢出机制吗？InnoDB从磁盘中读取数据的最小单位是数据页。 一 行有哪些格式Mysql的数据行有两种格式:Compact格式和Redundant格式。Compact是一种紧凑的行格式，设计的初衷就是为了让一个数据页中可以存放更多的数据行。 你品一品，让一个数据页中可以存放更多的数据行是一个多么激动人心的事，MySQL以数据页为单位从磁盘中读数据，如果能做到让一个数据页中有更多的行，那岂不是使用的空间变少了，且整体的效率直线飙升？ 官网介绍：Compact能比Redundant格式节约20%的存储。 Compact从MySQL5.0引入，MySQL5.1之后，行格式默认设置成 Compact 。所以本文描述的也是Compact格式。 二、紧凑的行格式长啥样？表中有的列允许为null，有的列是变长的varchar类型。那Compact行格式是如何组织描述这些信息的呢？如下图： 每部分包含的数据可能要比我上面标注的1、2、3还要多。 为了给大家更直观的感受和理解我只是挑了一部分展示给大家看。 三、MySQL单行能存多大体量的数据？在MySQL的设定中，单行数据最大能存储65535byte的数据（注意是byte，而不是字符）MySQL不允许创建一个长度为65535byte的列，因为数据页中每一行中都有我们上图提到的隐藏列。所以将varchar的长度降低到65532byte即可成功创建该表. 所以如果你将charset换成utf8这种编码格式，那varchar(N)中的N其实指的N个字符，而不是N个byte。假如encodeutf8时三个byte表示一个字符。那么65535 3 21845个字符。 四、Compact格式是如何做到紧凑的？MySQL每次进行随机的IO读默认情况下，数据页的大小为16KB。数据页中存储着数行。那就意味着一个数据页中能存储越多的数据行，MySQL整体的进行的IO次数就越少？性能就越快？Compact格式的实现思路是：当列的类型为VARCHAR、 VARBINARY、 BLOB、TEXT时，该列超过768byte的数据放到其他数据页中去。如下图：MySQL这样做，有效的防止了单个varchar列或者Text列太大导致单个数据页中存放的行记录过少而让IO飙升的窘境且占内存的。 五、什么是行溢出？如果数据页默认大小为16KB，换算成byte： 16*1024 16384 byte那你有没有发现，单页能存储的16384byte和单行最大能存储的 65535byte 差了好几倍呢 也就是说，假如你要存储的数据行很大超过了65532byte那么你是写入不进去的。假如你要存储的单行数据小于65535byte但是大于16384byte，这时你可以成功insert，但是一个数据页又存储不了你插入的数据。这时肯定会行溢出！ 其实在MySQL的设定中，发生行溢出并不是达到16384byte边缘才会发生。对于varchar、text等类型的行。当这种列存储的长度达到几百byte时就会发生行溢。 六、行 如何溢出？还是看这张图：在MySQL设定中，当varchar列长度达到768byte后，会将该列的前768byte当作当作prefix存放在行中，多出来的数据溢出存放到溢出页中，然后通过一个偏移量指针将两者关联起来，这就是行溢出机制。 七、思考一个问题不知道你有没有想过这样一个问题：首先你肯定知道，MySQL使用的是B+Tree的聚簇索引，在这棵B+Tree中非叶子节点是只存索引不存数据，叶子节点中存储着真实的数据。同时叶子结点指向数据页。那当单行存不下的时候，为啥不存储在两个数据页中呢？就像下图这样～。单个节点存储下，我用多个节点存总行吧！说不定这样我的B+Tee还能变大长高（这其实是错误的想法）这个错误的描述对应的脑图如下： 那MySQL不这样做的原因如下：MySQL想让一个数据页中能存放更多的数据行，至少也得要存放两行数据。否则就失去了B+Tree的意义。B+Tree也退化成一个低效的链表。你可以品一下这句蓝色的话，他说的每个数据页至少要存放两行数据的意思不是说 数据页不能只存一行。你确确实实可以只往里面写一行数据，然后去吃个饭，干点别的。一直让这个数据页中只有一行数据。 这句话的意思是，当你往这个数据页中写入一行数据时，即使它很大将达到了数据页的极限，但是通过行溢出机制。依然能保证你的下一条数据还能写入到这个数据页中。 存储引擎24.MySQL 有哪些常见存储引擎？MySQL 支持多种存储引擎，常见的有 MyISAM、InnoDB、MEMORY 等。—这部分是帮助理解 start，面试中可不背—我来做一个表格对比：—这部分是帮助理解 end，面试中可不背—除此之外，我还了解到：①、MySQL 5.5 之前，默认存储引擎是 MyISAM，5.5 之后是 InnoDB。②、InnoDB 支持的哈希索引是自适应的，不能人为干预。③、InnoDB 从 MySQL 5.6 开始，支持全文索引。④、InnoDB 的最小表空间略小于 10M，最大表空间取决于页面大小。如何切换 MySQL 的数据引擎？可以通过 alter table 语句来切换 MySQL 的数据引擎。ALTER TABLE your_table_name ENGINE=InnoDB;不过不建议，应该提前设计好到底用哪一种存储引擎。 25.存储引擎应该怎么选择？大多数情况下，使用默认的 InnoDB 就可以了，InnoDB 可以提供事务、行级锁、外键、B+ 树索引等能力。MyISAM 适合读多写少的场景。MEMORY 适合临时表，数据量不大的情况。因为数据都存放在内存，所以速度非常快。 26.InnoDB 和 MyISAM 主要有什么区别？InnoDB 和 MyISAM 的最大区别在于事务支持和锁机制。InnoDB 支持事务、行级锁，适合大多数业务系统；而 MyISAM 不支持事务，用的是表锁，查询快但写入性能差，适合读多写少的场景。 另外，从存储结构上来说，MyISAM 用三种格式的文件来存储，.frm 文件存储表的定义；.MYD 存储数据；.MYI 存储索引；而 InnoDB 用两种格式的文件来存储，.frm 文件存储表的定义；.ibd 存储数据和索引。 从索引类型上来说，MyISAM 为非聚簇索引，索引和数据分开存储，索引保存的是数据文件的指针。 InnoDB 为聚簇索引，索引和数据不分开。 更细微的层面上来讲，MyISAM 不支持外键，可以没有主键，表的具体行数存储在表的属性中，查询时可以直接返回；InnoDB 支持外键，必须有主键，具体行数需要扫描整个表才能返回，有索引的情况下会扫描索引。 InnoDB的内存结构了解吗？InnoDB 的内存区域主要有两块，buffer pool 和 log buffer。 buffer pool 用于缓存数据页和索引页，提升读写性能；log buffer 用于缓存 redo log，提升写入性能。InnoDB引擎框架图如下: 数据页的结构了解过吗InnoDB 的数据页由 7 部分组成，其中文件头、页头和文件尾的大小是固定的，分别为 38、56 和 8 个字节，用来标记该页的一些信息。行记录、空闲空间和页目录的大小是动态的，为实际的行记录存储空间。 真实的记录会按照指定的行格式存储到 User Records 中。 每个数据页的 File Header 都有一个上一页和下一页的编号，所有的数据页会形成一个双向链表。 在 InnoDB 中，默认的页大小是 16KB。可以通过 show variables like innodb_page_size; 查看。 27. InnoDB 的 Buffer Pool了解吗？Buffer Pool 是 InnoDB 存储引擎中的一个内存缓冲区，它会将经常使用的数据页、索引页加载进内存，读的时候先查询 Buffer Pool，如果命中就不用访问磁盘了。 如果没有命中，就从磁盘读取，并加载到 Buffer Pool，此时可能会触发页淘汰，将不常用的页移出 Buffer Pool。 写操作时不会直接写入磁盘，而是先修改内存中的页，此时页被标记为脏页，后台线程会定期将脏页刷新到磁盘。 Buffer Pool 可以显著减少磁盘的读写次数，从而提升 MySQL 的读写性能。 Buffer Pool 的默认大小是多少？本机上 InnoDB 的 Buffer Pool 默认大小是 128MB。 SHOW VARIABLES LIKE innodb_buffer_pool_size; 另外，在具有 1GB-4GB RAM 的系统上，默认值为系统 RAM 的 25%；在具有超过 4GB RAM 的系统上，默认值为系统 RAM 的 50%，但不超过 4GB。 InnoDB 对 LRU 算法的优化了解吗？LRU (least resently used)：近期最少使用 LFU (least freqently used)：频数最少使用 了解，InnoDB 对 LRU 算法进行了改良，最近访问的数据并不直接放到 LRU 链表的头部，而是放在一个叫 midpoiont 的位置。默认情况下，midpoint 位于 LRU 列表的 58 处。 比如 Buffer Pool 有 100 页，新页插入的位置大概是在第 80 页；当页数据被频繁访问后，再将其移动到 young 区，这样做的好处是热点页能长时间保留在内存中，不容易被挤出去。 —-这部分是帮助理解 start，面试中可不背—- 可以通过 innodb_old_blocks_pct 参数来调整 Buffer Pool 中 old 和 young 区的比例；通过 innodb_old_blocks_time 参数来调整页在 young 区的停留时间。 默认情况下，LRU 链表中 old 区占 37%；同一页再次访问提升的最小时间间隔是 1000 毫秒。也就是说，如果某页在 1 秒内被多次访问，只会计算一次，不会立刻升级为热点页，防止短时间批量访问导致缓存污染。 —-这部分是帮助理解 end，面试中可不背—- 日志28.MySQL 日志文件有哪些？有 6 大类，其中错误日志用于问题诊断，慢查询日志用于 SQL 性能分析，general log 用于记录所有的 SQL 语句，binlog 用于主从复制和数据恢复，redo log 用于保证事务持久性，undo log 用于事务回滚和 MVCC。 —-这部分是帮助理解 start，面试中可不背—- ①、错误日志（Error Log）：记录 MySQL 服务器启动、运行或停止时出现的问题。②、慢查询日志（Slow Query Log）：记录执行时间超过 long_query_time 值的所有 SQL 语句。这个时间值是可配置的，默认情况下，慢查询日志功能是关闭的。③、一般查询日志（General Query Log）：记录 MySQL 服务器的启动关闭信息，客户端的连接信息，以及更新、查询的 SQL 语句等。④、二进制日志（Binary Log）：记录所有修改数据库状态的 SQL 语句，以及每个语句的执行时间，如 INSERT、UPDATE、DELETE 等，但不包括 SELECT 和 SHOW 这类的操作。⑤、重做日志（Redo Log）：记录对于 InnoDB 表的每个写操作，不是 SQL 级别的，而是物理级别的，主要用于崩溃恢复。⑥、回滚日志（Undo Log，或者叫事务日志）：记录数据被修改前的值，用于事务的回滚。 —-这部分是帮助理解 end，面试中可不背—- 请重点说说 binlog？binlog 是一种物理日志，会在磁盘上记录数据库的所有修改操作。如果误删了数据，就可以使用 binlog 进行回退到误删之前的状态。 # 步骤1：恢复全量备份mysql -u root -p full_backup.sql# 步骤2：应用Binlog到指定时间点mysqlbinlog --start-datetime=2025-03-13 14:00:00 --stop-datetime=2025-03-13 15:00:00 binlog.000001 | mysql -u root -p 如果要搭建主从复制，就可以让从库定时读取主库的 binlog。MySQL 提供了三种格式的 binlog：Statement、Row 和 Mixed，分别对应 SQL 语句级别、行级别和混合级别，默认为行级别。从后缀名上来看，binlog 文件分为两类：以 .index 结尾的索引文件，以 .00000* 结尾的二进制日志文件。binlog 默认是没有启用的。 生产环境中是一定要启用的，可以通过在 my.cnf 文件中配置 log_bin 参数，以启用 binlog。 log_bin = mysql-bin #开启binlog#mysql-bin.*日志文件最大字节（单位：字节）#设置最大100MBmax_binlog_size=104857600#设置了只保留7天BINLOG（单位：天）expire_logs_days = 7#binlog日志只记录指定库的更新#binlog-do-db=db_name#binlog日志不记录指定库的更新#binlog-ignore-db=db_name#写缓冲多少次，刷一次磁盘，默认0sync_binlog=0 binlog 的配置参数都了解哪些？log_bin = mysql-bin 用于启用 binlog，这样就可以在 MySQL 的数据目录中找到 db-bin.000001、db-bin.000002 等日志文件。max_binlog_size=104857600 用于设置每个 binlog 文件的大小，不建议设置太大，网络传送起来比较麻烦。当 binlog 文件达到 max_binlog_size 时，MySQL 会关闭当前文件并创建一个新的 binlog 文件。expire_logs_days = 7 用于设置 binlog 文件的自动过期时间为 7 天。过期的 binlog 文件会被自动删除。防止长时间累积的 binlog 文件占用过多存储空间，所以这个配置很重要。binlog-do-db=db_name，指定哪些数据库表的更新应该被记录。binlog-ignore-db=db_name，指定忽略哪些数据库表的更新。sync_binlog=0，设置每多少次 binlog 写操作会触发一次磁盘同步操作。默认值为 0，表示 MySQL 不会主动触发同步操作，而是依赖操作系统的磁盘缓存策略。即当执行写操作时，数据会先写入缓存，当缓存区满了再由操作系统将数据一次性刷入磁盘。如果设置为 1，表示每次 binlog 写操作后都会同步到磁盘，虽然可以保证数据能够及时写入磁盘，但会降低性能。可以通过 show variables like %log_bin%; 查看 binlog 是否开启。 有了binlog为什么还要undolog redolog？binlog 属于 Server 层，与存储引擎无关，无法直接操作物理数据页。而 redo log 和 undo log 是 InnoDB 存储引擎实现 ACID的基石。————–ps————-ACID: **原子性(Atomicity)**：事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败回滚通过undo log实现，记录事务开始前的状态，用于回滚 **一致性(Consistency)**：事务执行前后，数据库从一个一致状态转变为另一个一致状态通过其他三个特性(AID)共同保证 **隔离性(Isolation)**：多个并发事务执行时，一个事务的执行不应影响其他事务通过锁机制和MVCC(多版本并发控制)实现 **持久性(Durability)**：事务一旦提交，其结果就是永久性的通过redo log实现，即使系统崩溃也能恢复数据 ————–ps————- binlog 关注的是逻辑变更的全局记录；redo log 用于确保物理变更的持久性，确保事务最终能够刷盘成功；undo log 是逻辑逆向操作日志，记录的是旧值，方便恢复到事务开始前的状态。 另外一种回答方式。 binlog 会记录整个 SQL 或行变化；redo log 是为了恢复已提交但未刷盘的数据，undo log 是为了撤销未提交的事务。 以一次事务更新为例： # 开启事务BEGIN;# 更新数据UPDATE users SET age = age + 1 WHERE id = 1;# 提交事务COMMIT; 事务开始的时候会生成 undo log，记录更新前的数据，比如原值是 18： undo log: id=1, age=18 修改数据的时候，会将数据写入到 redo log。 比如数据页 page_id=123 上，id1 的用户被更新为 age26： redo log (prepare): page_id=123, offset=0x40, before=18, after=26 等事务提交的时候，redo log 刷盘，binlog 刷盘。 binlog 写完之后，redo log 的状态会变为 commit： redo log (commit): page_id=123, offset=0x40, before=18, after=26 binlog 如果是 Statement 格式，会记录一条 SQL 语句： UPDATE users SET age age + 1 WHERE id 1;binlog 如果是 Row 格式，会记录： 表：usersbefore: id=1, age=18after: id=1, age=26 随后，后台线程会将 redo log 中的变更异步刷新到磁盘。 详细探究一下binlog(长文警告⚠️):MySQL 的 Binlog 日志是一种二进制格式的日志，Binlog 记录所有的 DDL 和 DML 语句(除了数据查询语句SELECT、SHOW等)，以 Event 的形式记录，同时记录语句执行时间。 Binlog 的主要作用有两个：1. 数据恢复:因为 Binlog 详细记录了所有修改数据的 SQL，当某一时刻的数据误操作而导致出问题，或者数据库宕机数据丢失，那么可以根据 Binlog 来回放历史数据。2. 主从复制:想要做多机备份的业务，可以去监听当前写库的 Binlog 日志，同步写库的所有更改。 Binlog 包括两类文件：二进制日志索引文件(.index)：记录所有的二进制文件。二进制日志文件(.00000*)：记录所有 DDL 和 DML 语句事件。 Binlog 日志功能默认是开启的，线上情况下 Binlog 日志的增长速度是很快的，在 MySQL 的配置文件 my.cnf 中提供一些参数来对 Binlog 进行设置。 #设置此参数表示启用binlog功能，并制定二进制日志的存储目录log-bin=/home/mysql/binlog/#mysql-bin.*日志文件最大字节（单位：字节）#设置最大100MBmax_binlog_size=104857600#设置了只保留7天BINLOG（单位：天）expire_logs_days = 7#binlog日志只记录指定库的更新#binlog-do-db=db_name#binlog日志不记录指定库的更新#binlog-ignore-db=db_name#写缓冲多少次，刷一次磁盘，默认0sync_binlog=0 需要注意的是：max_binlog_size ：Binlog 最大和默认值是 1G，该设置并不能严格控制 Binlog 的大小，尤其是 Binlog 比较靠近最大值而又遇到一个比较大事务时，为了保证事务的完整性不可能做切换日志的动作，只能将该事务的所有 SQL 都记录进当前日志直到事务结束。所以真实文件有时候会大于 max_binlog_size 设定值。expire_logs_days ：Binlog 过期删除不是服务定时执行，是需要借助事件触发才执行，事件包括： 服务器重启 服务器被更新 日志达到了最大日志长度 max_binlog_size 日志被刷新 二进制日志由配置文件的 log-bin 选项负责启用，MySQL 服务器将在数据根目录创建两个新文件mysql-bin.000001 和 mysql-bin.index，若配置选项没有给出文件名，MySQL 将使用主机名称命名这两个文件，其中 .index 文件包含一份全体日志文件的清单。 sync_binlog：这个参数决定了 Binlog 日志的更新频率。默认 0 ，表示该操作由操作系统根据自身负载自行决定多久写一次磁盘。 sync_binlog = 1 表示每一条事务提交都会立刻写盘。sync_binlog=n 表示 n 个事务提交才会写盘。 根据 MySQL 文档，写 Binlog 的时机是：SQL transaction 执行完，但任何相关的 Locks 还未释放或事务还未最终 commit 前。这样保证了 Binlog 记录的操作时序与数据库实际的数据变更顺序一致。 检查 Binlog 文件是否已开启： mysql show variables like %log_bin%;+---------------------------------+------------------------------------+| Variable_name | Value |+---------------------------------+------------------------------------+| log_bin | ON || log_bin_basename | /usr/local/mysql/data/binlog || log_bin_index | /usr/local/mysql/data/binlog.index || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF || sql_log_bin | ON |+---------------------------------+------------------------------------+6 rows in set (0.00 sec) MySQL 会把用户对所有数据库的内容和结构的修改情况记入 mysql-bin.n 文件，而不会记录 SELECT 和没有实际更新的 UPDATE 语句。 如果你不知道现在有哪些 Binlog 文件，可以使用如下命令： show binary logs; #查看binlog列表show master status; #查看最新的binlogmysql show binary logs;+------------------+-----------+-----------+| Log_name | File_size | Encrypted |+------------------+-----------+-----------+| mysql-bin.000001 | 179 | No || mysql-bin.000002 | 156 | No |+------------------+-----------+-----------+2 rows in set (0.00 sec) Binlog 文件是二进制文件，强行打开看到的必然是乱码，MySQL 提供了命令行的方式来展示 Binlog 日志： mysqlbinlog mysql-bin.000002 | more mysqlbinlog 命令即可查看。虽然看起来凌乱其实也有迹可循。Binlog 通过事件的方式来管理日志信息，可以通过 show binlog events in 的语法来查看当前 Binlog 文件对应的详细事件信息。 mysql show binlog events in mysql-bin.000001;+------------------+-----+----------------+-----------+-------------+-----------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-----------------------------------+| mysql-bin.000001 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000001 | 125 | Previous_gtids | 1 | 156 | || mysql-bin.000001 | 156 | Stop | 1 | 179 | |+------------------+-----+----------------+-----------+-------------+-----------------------------------+3 rows in set (0.01 sec) 这是一份没有任何写入数据的 Binlog 日志文件。Binlog 的版本是V4，可以看到日志的结束时间为 Stop。出现 Stop event 有两种情况： 是 master shut down 的时候会在 Binlog 文件结尾出现 是备机在关闭的时候会写入 relay log 结尾，或者执行 RESET SLAVE 命令执行 本文出现的原因是我有手动停止过 MySQL 服务。一般来说一份正常的 Binlog 日志文件会以 Rotate event 结束。当 Binlog 文件超过指定大小，Rotate event 会写在文件最后，指向下一个 Binlog 文件。我们来看看有过数据操作的 Binlog 日志文件是什么样子的。 mysql show binlog events in mysql-bin.000002;+------------------+-----+----------------+-----------+-------------+-----------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-----------------------------------+| mysql-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000002 | 125 | Previous_gtids | 1 | 156 | |+------------------+-----+----------------+-----------+-------------+-----------------------------------+2 rows in set (0.00 sec) 上面是没有任何数据操作且没有被截断的 Binlog。接下来我们插入一条数据，再看看 Binlog 事件。 mysql show binlog events in mysql-bin.000002;+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| mysql-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000002 | 125 | Previous_gtids | 1 | 156 | || mysql-bin.000002 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= ANONYMOUS || mysql-bin.000002 | 235 | Query | 1 | 323 | BEGIN || mysql-bin.000002 | 323 | Intvar | 1 | 355 | INSERT_ID=13 || mysql-bin.000002 | 355 | Query | 1 | 494 | use `test_db`; INSERT INTO `test_db`.`test_db`(`name`) VALUES (xdfdf) || mysql-bin.000002 | 494 | Xid | 1 | 525 | COMMIT /* xid=192 */ |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+7 rows in set (0.00 sec) 这是加入一条数据之后的 Binlog 事件。 我们对 event 查询的数据行关键字段来解释一下： Pos：当前事件的开始位置，每个事件都占用固定的字节大小，结束位置(End_log_position)减去Pos，就是这个事件占用的字节数。上面的日志中我们能看到，第一个事件位置并不是从 0 开始，而是从 4。MySQL 通过文件中的前 4 个字节，来判断这是不是一个 Binlog 文件。这种方式很常见，很多格式的文件，如 pdf、doc、jpg等，都会通常前几个特定字符判断是否是合法文件。 Event_type：表示事件的类型 Server_id：表示产生这个事件的 MySQL server_id，通过设置 my.cnf 中的 server-id 选项进行配置 End_log_position：下一个事件的开始位置 Info：包含事件的具体信息 Binlog 日志格式:针对不同的使用场景，Binlog 也提供了可定制化的服务，提供了三种模式来提供不同详细程度的日志内容。 Statement 模式：基于 SQL 语句的复制(statement-based replication-SBR) Row 模式：基于行的复制(row-based replication-RBR) Mixed 模式：混合模式复制(mixed-based replication-MBR) Statement 模式保存每一条修改数据的SQL。该模式只保存一条普通的SQL语句，不涉及到执行的上下文信息。因为每台 MySQL 数据库的本地环境可能不一样，那么对于依赖到本地环境的函数或者上下文处理的逻辑 SQL 去处理的时候可能同样的语句在不同的机器上执行出来的效果不一致。比如像 sleep()函数，last_insert_id()函数，等等，这些都跟特定时间的本地环境有关。 Row 模式MySQL V5.1.5 版本开始支持Row模式的 Binlog，它与 Statement 模式的区别在于它不保存具体的 SQL 语句，而是记录具体被修改的信息。比如一条 update 语句更新10条数据，如果是 Statement 模式那就保存一条 SQL 就够，但是 Row 模式会保存每一行分别更新了什么，有10条数据。Row 模式的优缺点就很明显了。保存每一个更改的详细信息必然会带来存储空间的快速膨胀，换来的是事件操作的详细记录。所以要求越高代价越高。 Mixed 模式Mixed 模式即以上两种模式的综合体。既然上面两种模式分别走了极简和一丝不苟的极端，那是否可以区分使用场景的情况下将这两种模式综合起来呢？在 Mixed 模式中，一般的更新语句使用 Statement 模式来保存 Binlog，但是遇到一些函数操作，可能会影响数据准确性的操作则使用 Row 模式来保存。这种方式需要根据每一条具体的 SQL 语句来区分选择哪种模式。MySQL 从 V5.1.8 开始提供 Mixed 模式，V5.7.7 之前的版本默认是Statement 模式，之后默认使用Row模式， 但是在 8.0 以上版本已经默认使用 Mixed 模式了。 查询当前 Binlog 日志使用格式： mysql show global variables like %binlog_format%;+---------------------------------+---------+| Variable_name | Value |+---------------------------------+---------+| binlog_format | MIXED || default_week_format | 0 || information_schema_stats_expiry | 86400 || innodb_default_row_format | dynamic || require_row_format | OFF |+---------------------------------+---------+5 rows in set (0.01 sec) 如何通过 mysqlbinlog 命令手动恢复数据上面说过每一条 event 都有位点信息，如果我们当前的 MySQL 库被无操作或者误删除了，那么该如何通过 Binlog 来恢复到删除之前的数据状态呢？首先发现误操作之后，先停止 MySQL 服务，防止继续更新。接着通过 mysqlbinlog命令对二进制文件进行分析，查看误操作之前的位点信息在哪里。接下来肯定就是恢复数据，当前数据库的数据已经是错的，那么就从开始位置到误操作之前位点的数据肯定的都是正确的；如果误操作之后也有正常的数据进来，这一段时间的位点数据也要备份。比如说：误操作的位点开始值为 501，误操作结束的位置为705，之后到800的位点都是正确数据。那么从 0 - 500 ，706 - 800 都是有效数据，接着我们就可以进行数据恢复了。先将数据库备份并清空。接着使用 mysqlbinlog 来恢复数据：0 - 500 的数据： mysqlbinlog --start-position=0 --stop-position=500 bin-log.000003 /root/back.sql; 上面命令的作用就是将 0 -500 位点的数据恢复到自定义的 SQL 文件中。同理 706 - 800 的数据也是一样操作。之后我们执行这两个 SQL 文件就行了。 Binlog 事件类型上面我们说到了 Binlog 日志中的事件，不同的操作会对应着不同的事件类型，且不同的 Binlog 日志模式同一个操作的事件类型也不同，下面我们一起看看常见的事件类型。首先我们看看源码中的事件类型定义：源码位置：libbinlogeventsincludebinlog_event.h enum Log_event_type /** Every time you update this enum (when you add a type), you have to fix Format_description_event::Format_description_event(). */ UNKNOWN_EVENT= 0, START_EVENT_V3= 1, QUERY_EVENT= 2, STOP_EVENT= 3, ROTATE_EVENT= 4, INTVAR_EVENT= 5, LOAD_EVENT= 6, SLAVE_EVENT= 7, CREATE_FILE_EVENT= 8, APPEND_BLOCK_EVENT= 9, EXEC_LOAD_EVENT= 10, DELETE_FILE_EVENT= 11, /** NEW_LOAD_EVENT is like LOAD_EVENT except that it has a longer sql_ex, allowing multibyte TERMINATED BY etc; both types share the same class (Load_event) */ NEW_LOAD_EVENT= 12, RAND_EVENT= 13, USER_VAR_EVENT= 14, FORMAT_DESCRIPTION_EVENT= 15, XID_EVENT= 16, BEGIN_LOAD_QUERY_EVENT= 17, EXECUTE_LOAD_QUERY_EVENT= 18, TABLE_MAP_EVENT = 19, /** The PRE_GA event numbers were used for 5.1.0 to 5.1.15 and are therefore obsolete. */ PRE_GA_WRITE_ROWS_EVENT = 20, PRE_GA_UPDATE_ROWS_EVENT = 21, PRE_GA_DELETE_ROWS_EVENT = 22, /** The V1 event numbers are used from 5.1.16 until mysql-trunk-xx */ WRITE_ROWS_EVENT_V1 = 23, UPDATE_ROWS_EVENT_V1 = 24, DELETE_ROWS_EVENT_V1 = 25, /** Something out of the ordinary happened on the master */ INCIDENT_EVENT= 26, /** Heartbeat event to be send by master at its idle time to ensure masters online status to slave */ HEARTBEAT_LOG_EVENT= 27, /** In some situations, it is necessary to send over ignorable data to the slave: data that a slave can handle in case there is code for handling it, but which can be ignored if it is not recognized. */ IGNORABLE_LOG_EVENT= 28, ROWS_QUERY_LOG_EVENT= 29, /** Version 2 of the Row events */ WRITE_ROWS_EVENT = 30, UPDATE_ROWS_EVENT = 31, DELETE_ROWS_EVENT = 32, GTID_LOG_EVENT= 33, ANONYMOUS_GTID_LOG_EVENT= 34, PREVIOUS_GTIDS_LOG_EVENT= 35, TRANSACTION_CONTEXT_EVENT= 36, VIEW_CHANGE_EVENT= 37, /* Prepared XA transaction terminal event similar to Xid */ XA_PREPARE_LOG_EVENT= 38, /** Add new events here - right above this comment! Existing events (except ENUM_END_EVENT) should never change their numbers */ ENUM_END_EVENT /* end marker */; 这么多的事件类型我们就不一一介绍，挑出来一些常用的来看看。FORMAT_DESCRIPTION_EVENTFORMAT_DESCRIPTION_EVENT 是 Binlog V4 中为了取代之前版本中的 START_EVENT_V3 事件而引入的。它是 Binlog 文件中的第一个事件，而且，该事件只会在 Binlog 中出现一次。MySQL 根据 FORMAT_DESCRIPTION_EVENT 的定义来解析其它事件。它通常指定了 MySQL 的版本，Binlog 的版本，该 Binlog 文件的创建时间。 QUERY_EVENT QUERY_EVENT 类型的事件通常在以下几种情况下使用： 事务开始时，执行的 BEGIN 操作STATEMENT 格式中的 DML 操作ROW 格式中的 DDL 操作比如上文我们插入一条数据之后的 Binlog 日志： mysql show binlog events in mysql-bin.000002;+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+| mysql-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || mysql-bin.000002 | 125 | Previous_gtids | 1 | 156 | || mysql-bin.000002 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= ANONYMOUS || mysql-bin.000002 | 235 | Query | 1 | 323 | BEGIN || mysql-bin.000002 | 323 | Intvar | 1 | 355 | INSERT_ID=13 || mysql-bin.000002 | 355 | Query | 1 | 494 | use `test_db`; INSERT INTO `test_db`.`test_db`(`name`) VALUES (xdfdf) || mysql-bin.000002 | 494 | Xid | 1 | 525 | COMMIT /* xid=192 */ |+------------------+-----+----------------+-----------+-------------+-------------------------------------------------------------------------+7 rows in set (0.00 sec) XID_EVENT在事务提交时，不管是 STATEMENT 还 是ROW 格式的 Binlog，都会在末尾添加一个 XID_EVENT 事件代表事务的结束。该事件记录了该事务的 ID，在 MySQL 进行崩溃恢复时，根据事务在 Binlog 中的提交情况来决定是否提交存储引擎中状态为 prepared 的事务。ROWS_EVENT对于 ROW 格式的 Binlog，所有的 DML 语句都是记录在 ROWS_EVENT 中。ROWS_EVENT分为三种：WRITE_ROWS_EVENTUPDATE_ROWS_EVENTDELETE_ROWS_EVENT分别对应 insert，update 和 delete 操作。对于 insert 操作，WRITE_ROWS_EVENT 包含了要插入的数据。对于 update 操作，UPDATE_ROWS_EVENT 不仅包含了修改后的数据，还包含了修改前的值。对于 delete 操作，仅仅需要指定删除的主键（在没有主键的情况下，会给定所有列）。 对比 QUERY_EVENT 事件，是以文本形式记录 DML 操作的。而对于 ROWS_EVENT 事件，并不是文本形式，所以在通过 mysqlbinlog 查看基于 ROW 格式的 Binlog 时，需要指定 -vv –base64-outputdecode-rows。 我们来测试一下，首先将日志格式改为 Rows： mysql set binlog_format=row;Query OK, 0 rows affected (0.00 sec)mysqlmysql flush logs;Query OK, 0 rows affected (0.01 sec) 然后刷新一下日志文件，重新开始一个 Binlog 日志。我们插入一条数据之后看一下日志： mysql show binlog events in binlog.000008;+---------------+-----+----------------+-----------+-------------+--------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+---------------+-----+----------------+-----------+-------------+--------------------------------------+| binlog.000008 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.21, Binlog ver: 4 || binlog.000008 | 125 | Previous_gtids | 1 | 156 | || binlog.000008 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= ANONYMOUS || binlog.000008 | 235 | Query | 1 | 313 | BEGIN || binlog.000008 | 313 | Table_map | 1 | 377 | table_id: 85 (test_db.test_db) || binlog.000008 | 377 | Write_rows | 1 | 423 | table_id: 85 flags: STMT_END_F || binlog.000008 | 423 | Xid | 1 | 454 | COMMIT /* xid=44 */ |+---------------+-----+----------------+-----------+-------------+--------------------------------------+7 rows in set (0.01 sec) 说说 redo log 的工作机制？当事务启动时，MySQL 会为该事务分配一个唯一标识符。在事务执行过程中，每次对数据进行修改，MySQL 都会生成一条 Redo Log，记录修改前后的数据状态。这些 Redo Log 首先会被写入内存中的 Redo Log Buffer。当事务提交时，MySQL 再将 Redo Log Buffer 中的记录刷新到磁盘上的 Redo Log 文件中。只有当 Redo Log 成功写入磁盘，事务才算真正提交成功。当 MySQL 崩溃重启时，会先检查 Redo Log。对于已提交的事务，MySQL 会重放 Redo Log 中的记录。对于未提交的事务，MySQL 会通过 Undo Log 回滚这些修改，确保数据恢复到崩溃前的一致性状态。Redo Log 是循环使用的，当文件写满后会覆盖最早的记录。为避免覆盖未持久化的记录，MySQL 会定期执行 CheckPoint 操作，将内存中的数据页刷新到磁盘，并记录 CheckPoint 点。 重启时，MySQL 只会重放 CheckPoint 之后的 Redo Log，从而提高恢复效率。 省流版: 事务开始 记录undo log（旧数据） 修改Buffer Pool中的数据 写入redo log（prepare状态） 写入binlog 提交事务（redo log标记为commit） 后台异步刷脏页到磁盘 redo log 文件的大小是固定的吗？redo log 文件是固定大小的，通常配置为一组文件，使用环形方式写入，旧的日志会在空间需要时被覆盖。 命名方式为 ib_logfile0、iblogfile1、、、iblogfilen。默认 2 个文件，每个文件大小为 48MB。可以通过 show variables like innodb_log_file_size; 查看 redo log 文件的大小；通过 show variables like innodb_log_files_in_group; 查看 redo log 文件的数量。 说一说WAL?WAL——Write-Ahead Logging。 预写日志是 InnoDB 实现事务持久化的核心机制，它的思想是：先写日志再刷磁盘。即在修改数据页之前，先将修改记录写入 Redo Log。这样的话，即使数据页尚未写入磁盘，系统崩溃时也能通过 Redo Log 恢复数据。—-这部分是帮助理解 start，面试中可不背—-解释一下为什么需要 WAL：数据最终是要写入磁盘的，但磁盘 IO 很慢；如果每次更新都立刻把数据页刷盘，性能很差；如果还没写入磁盘就宕机，事务会丢失。WAL 的好处是更新时不直接写数据页，而是先写一份变更记录到 redo log，后台再慢慢把真正的数据页刷盘，一举多得。—-这部分是帮助理解 end，面试中可不背—- 29.binlog 和 redo log 有什么区别？binlog 由 MySQL 的 Server 层实现，与存储引擎无关；redo log 由 InnoDB 存储引擎实现。 binlog 记录的是逻辑日志，包括原始的 SQL 语句或者行数据变化，例如“将 id2 这行数据的 age 字段+1”。redo log 记录物理日志，即数据页的具体修改，例如“将 page_id123 上 offset0x40 的数据从 18 修改为 26”。binlog 是追加写入的，文件写满后会新建文件继续写入，不会覆盖历史日志，保存的是全量操作记录；redo log 是循环写入的，空间是固定的，写满后会覆盖旧的日志，仅保存未刷盘的脏页日志，已持久化的数据会被清除。另外，为保证两种日志的一致性，innodb 采用了两阶段提交策略，redo log 在事务执行过程中持续写入，并在事务提交前进入 prepare 状态；binlog 在事务提交的最后阶段写入，之后 redo log 会被标记为 commit 状态。可以通过回放 binlog 实现数据同步或者恢复到指定时间点；redo log 用来确保事务提交后即使系统宕机，数据仍然可以通过重放 redo log 恢复。 30.为什么要两阶段提交?为了保证 redo log 和 binlog 中的数据一致性，防止主从复制和事务状态不一致。 为什么 2PC 能保证 redo log 和 binlog 的强⼀致性？假如 MySQL 在预写 redo log 之后、写入 binlog 之前崩溃。那么 MySQL 重启后 InnoDB 会回滚该事务，因为 redo log 不是提交状态。并且由于 binlog 中没有写入数据，所以从库也不会有该事务的数据。 假如 MySQL 在写入 binlog 之后、redo log 提交之前崩溃。那么 MySQL 重启后 InnoDB 会提交该事务，因为 redo log 是提交状态。并且由于 binlog 中有写入数据，所以从库也会同步到该事务的数据。伪代码如下: // 事务开始begin;// try // 执行 SQL execute SQL; // 写入 redo log 并标记为 prepare write redo log prepare xid; // 写入 binlog write binlog xid sql; // 提交 redo log commit redo log xid;// catch // 回滚 redo log innodb rollback redo log xid;// 事务结束end; XID 了解吗？XID 是 binlog 中用来标识事务提交的唯一标识符。 在事务提交时，会写入一个 XID_EVENT 到 binlog，表示这个事务真正完成了。 Log_name | Pos | Event_type | Server_id | End_log_pos | Info | mysql-bin.000003 | 2005 | Gtid | 1013307 | 2070 | SET @@SESSION.GTID_NEXT= f971d5f1-d450-11ec-9e7b-5254000a56df:11 || mysql-bin.000003 | 2070 | Query | 1013307 | 2142 | BEGIN || mysql-bin.000003 | 2142 | Table_map | 1013307 | 2187 | table_id: 109 (test.t1) || mysql-bin.000003 | 2187 | Write_rows | 1013307 | 2227 | table_id: 109 flags: STMT_END_F || mysql-bin.000003 | 2227 | Xid | 1013307 | 2258 | COMMIT /* xid=121 */ 它不仅用于主从复制中事务完整性的判断，也在崩溃恢复中对 redo log 和 binlog 的一致性校验起到关键作用。 XID 可以帮助 MySQL 判断哪些 redo log 是已提交的，哪些是未提交需要回滚的，是两阶段提交机制中非常关键的一环。 31.redo log 的写入过程了解吗？InnoDB 会先将 Redo Log 写入内存中的 Redo Log Buffer，之后再以一定的频率刷入到磁盘的 Redo Log File 中。 哪些场景会触发 redo log 的刷盘动作？比如说 Redo Log Buffer 的空间不足时，事务提交时，触发 Checkpoint 时，后台线程定期刷盘时。不过，Redo Log Buffer 刷盘到 Redo Log File 还会涉及到操作系统的磁盘缓存策略，可能不会立即刷盘，而是等待一定时间后才刷盘。 innodb_flush_log_at_trx_commit 参数你了解多少？innodb_flush_log_at_trx_commit 参数是用来控制事务提交时，Redo Log 的刷盘策略，一共有三种。 0 表示事务提交时不刷盘，而是交给后台线程每隔 1 秒执行一次。这种方式性能最好，但是在 MySQL 宕机时可能会丢失一秒内的事务。 1 表示事务提交时会立即刷盘，确保事务提交后数据就持久化到磁盘。这种方式是最安全的，也是 InnoDB 的默认值。 2 表示事务提交时只把 Redo Log Buffer 写入到 Page Cache，由操作系统决定什么时候刷盘。操作系统宕机时，可能会丢失一部分数据。 一个没有提交事务的 redo log，会不会刷盘？InnoDB 有一个后台线程，每隔 1 秒会把Redo Log Buffer中的日志写入到文件系统的缓存中，然后调用刷盘操作。 因此，一个没有提交事务的 Redo Log 也可能会被刷新到磁盘中。另外，如果当 Redo Log Buffer 占用的空间即将达到 innodb_log_buffer_size 的一半时，也会触发刷盘操作。 Redo Log Buffer 是顺序写还是随机写？MySQL 在启动后会向操作系统申请一块连续的内存空间作为 Redo Log Buffer，并将其分为若干个连续的 Redo Log Block。那为了提高写入效率，Redo Log Buffer 采用了顺序写入的方式，会先往前面的 Redo Log Block 中写入，当写满后再往后面的 Block 中写入。于此同时，InnoDB 还提供了一个全局变量 buf_free，来控制后续的 redo log 记录应该写入到 block 中的哪个位置。 buf_next_to_write 了解吗？buf_next_to_write 指向 Redo Log Buffer 中下一次需要写入硬盘的起始位置。 而 buf_free 指向的是 Redo Log Buffer 中空闲区域的起始位置。 了解 MTR 吗？Mini Transaction 是 InnoDB 内部用于操作数据页的原子操作单元。 mtr_t mtr;mtr_start(mtr);// 1. 加锁// 对待访问的index加锁mtr_s_lock(rw_lock_t, mtr);mtr_x_lock(rw_lock_t, mtr);// 对待读写的page加锁mtr_memo_push(mtr, buf_block_t, MTR_MEMO_PAGE_S_FIX);mtr_memo_push(mtr, buf_block_t, MTR_MEMO_PAGE_X_FIX);// 2. 访问或修改pagebtr_cur_search_to_nth_levelbtr_cur_optimistic_insert// 3. 为修改操作生成redomlog_openmlog_write_initial_log_record_fastmlog_close// 4. 持久化redo，解锁mtr_commit(mtr); 多个事务的 Redo Log 会以 MTR 为单位交替写入到 Redo Log Buffer 中，假如事务 1 和事务 2 均有两个 MTR，一旦某个 MTR 结束，就会将其生成的若干条 Redo Log 记录顺序写入到 Redo Log Buffer 中。 也就是说，一个 MTR 会包含一组 Redo Log 记录，是 MySQL 崩溃后恢复事务的最小执行单元。 Redo Log Block 的结构了解吗？Redo Log Block 由日志头、日志体和日志尾组成，一共占用 512 个字节，其中日志头占用 12 个字节，日志尾占用 4 个字节，剩余的 496 个字节用于存储日志体。日志头包含了当前 Block 的序列号、第一条日志的序列号、类型等信息。日志尾主要存储的是 LOG_BLOCK_CHECKSUM，也就是 Block 的校验和，主要用于判断 Block 是否完整。 Redo Log Block 为什么设计成 512 字节？因为机械硬盘的物理扇区大小通常为 512 字节，Redo Log Block 也设计为同样的大小，就可以确保每次写入都是整数个扇区，减少对齐开销。 比如说操作系统的页缓存默认为 4KB，8 个 Redo Log Block 就可以组合成一个页缓存单元，从而提升 Redo Log Buffer 的写入效率。 LSN 了解吗？Log Sequence Number 是一个 8 字节的单调递增整数，用来标识事务写入 redo log 的字节总量，存在于 redo log、数据页头部和 checkpoint 中。 —-这部分是帮助理解 start，面试中可不背—-MySQL 在第一次启动时，LSN 的初始值并不为 0，而是 8704；当 MySQL 再次启动时，会继续使用上一次服务停止时的 LSN。 在计算 LSN 的增量时，不仅需要考虑 log block body 的大小，还需要考虑 log block header 和 log block tail 中部分字节数。 比如说在上图中，事务 3 的 MTR 总量为 300 字节，那么写入到 Redo Log Buffer 中的 LSN 会增长为 8704 + 300 + 12 9016。 假如事务 4 的 MTR 总量为 900 字节，那么再次写入到 Redo Log Buffer 中的 LSN 会增长为 9016 + 900 + 122 + 42 9948。 2 个 12 字节的 log block header + 2 个 4 字节的 log block tail。 —-这部分是帮助理解 end，面试中可不背—- 核心作用有三个： 第一，redo log 按照 LSN 递增顺序记录所有数据的修改操作。LSN 的递增量等于每次写入日志的字节数。 第二，InnoDB 的每个数据页头部中，都会记录该页最后一次刷新到磁盘时的 LSN。如果数据页的 LSN 小于 redo log 的 LSN，说明该页需要从日志中恢复；否则说明该页已更新。 第三，checkpoint 通过 LSN 记录已刷新到磁盘的数据页位置，减少恢复时需要处理的日志。 —-这部分是帮助理解 start，面试中可不背—- 可以通过 show engine innodb status; 查看当前的 LSN 信息。 Log sequence number：当前系统最大 LSN（已生成的日志总量）。 Log flushed up to：已写入磁盘的 redo log LSN。 Pages flushed up to：已刷新到数据页的 LSN。 Last checkpoint at：最后一次检查点的 LSN，表示已持久化的数据状态。 —-这部分是帮助理解 end，面试中可不背—- Checkpoint 了解多少？Checkpoint 是 InnoDB 为了保证事务持久性和回收 redo log 空间的一种机制。 它的作用是在合适的时机将部分脏页刷入磁盘，比如说 buffer pool 的容量不足时。并记录当前 LSN 为 Checkpoint LSN，表示这个位置之前的 redo log file 已经安全，可以被覆盖了。 MySQL 崩溃恢复时只需要从 Checkpoint 之后开始恢复 redo log 就可以了，这样可以最大程度减少恢复所花费的时间。 redo log file 的写入是循环的，其中有两个标记位置非常重要，也就是 Checkpoint 和 write pos。 write pos 是 redo log 当前写入的位置，Checkpoint 是可以被覆盖的位置。 当 write pos 追上 Checkpoint 时，表示 redo log 日志已经写满。这时候就要暂停写入并强制刷盘，释放可覆写的日志空间。 关于redo log 的调优参数了解多少？如果是高并发写入的电商系统，可以最大化写入吞吐量，容忍秒级数据丢失的风险。 innodb_flush_log_at_trx_commit = 2sync_binlog = 1000innodb_redo_log_capacity = 64Ginnodb_io_capacity = 5000innodb_lru_scan_depth = 512innodb_log_buffer_size = 256M 如果是金融交易系统，需要保证数据零丢失，接受较低的吞吐量。 innodb_flush_log_at_trx_commit = 1sync_binlog = 1innodb_redo_log_capacity = 32Ginnodb_io_capacity = 2000innodb_lru_scan_depth = 1024 核心参数一览表: 总结 对数据一致性要求高的场景，如金融交易使用innodb_flush_log_at_trx_commit1，对写入吞吐量敏感的场景，如日志采集可以使用 2 或 0，需要结合 sync_binlog 参数 sync_binlog 参数控制 binlog 的刷盘策略，可以设置为 0、1、N，0 表示依赖系统刷盘，1 表示每次事务提交都刷盘（推荐与 innodb_flush_log_at_trx_commit1 搭配），N1000 表示累计 1000 次事务后刷盘 innodb_redo_log_capacity 动态调整 Redo Log 总容量，可以根据业务负载情况调整，建议设置为 1 小时写入量的峰值（如每秒 10MB 写入则设为 36GB） innodb_io_capacity 定义 InnoDB 后台线程的每秒 IO 操作上限，直接影响脏页刷新速率；机械硬盘建议 200-500，SSD 建议 1000-2000，NVMe SSD 可设为 5000+ innodb_lru_scan_depth 控制每个缓冲池实例中 LRU 列表的扫描深度，决定每秒可刷新的脏页数量，默认值 1024 适用于多数场景，IO 密集型负载可适当降低（如 512），减少 CPU 开销。 SQL优化🌟32.什么是慢 SQL？拓展阅读: https://juejin.cn/post/7048974570228809741MySQL 中有一个叫long_query_time的参数，原则上执行时间超过该参数值的 SQL 就是慢 SQL，会被记录到慢查询日志中。 —-这部分是帮助理解 start，面试中可不背—- 可通过 show variables like ‘long_query_time’; 查看当前的 long_query_time 的参数值。—-这部分是帮助理解 end，面试中可不背—- SQL 的执行过程了解吗？SQL 的执行过程大致可以分为六个阶段：连接管理、语法解析、语义分析、查询优化、执行器调度、存储引擎读写等。Server 层负责理解和规划 SQL 怎么执行，存储引擎层负责数据的真正读写。 —-这部分是帮助理解 start，面试中可不背—- 来详细拆解一下： 客户端发送 SQL 语句给 MySQL 服务器。 如果查询缓存打开则会优先查询缓存，缓存中有对应的结果就直接返回。不过，MySQL 8.0 已经移除了查询缓存。这部分的功能正在被 Redis 等缓存中间件取代。 分析器对 SQL 语句进行语法分析，判断是否有语法错误。 搞清楚 SQL 语句要干嘛后，MySQL 会通过优化器生成执行计划。 执行器调用存储引擎的接口，执行 SQL 语句。 SQL 执行过程中，优化器通过成本计算预估出执行效率最高的方式，基本的预估维度为： IO 成本：从磁盘读取数据到内存的开销。 CPU 成本：CPU 处理内存中数据的开销。 基于这两个维度，可以得出影响 SQL 执行效率的因素有： ①、IO 成本，数据量越大，IO 成本越高。所以要尽量查询必要的字段；尽量分页查询；尽量通过索引加快查询。 ②、CPU 成本，尽量避免复杂的查询条件，如有必要，考虑对子查询结果进行过滤。 —-这部分是帮助理解 end，面试中可不背—- 如何优化慢SQL?首先，需要找到那些比较慢的 SQL，可以通过启用慢查询日志，记录那些超过指定执行时间的 SQL 查询。 也可以使用 show processlist; 命令查看当前正在执行的 SQL 语句，找出执行时间较长的 SQL。 或者在业务基建中加入对慢 SQL 的监控，常见的方案有字节码插桩、连接池扩展、ORM 框架扩展等。 然后，使用 EXPLAIN 查看慢 SQL 的执行计划，看看有没有用索引，大部分情况下，慢 SQL 的原因都是因为没有用到索引。 EXPLAIN SELECT * FROM your_table WHERE conditions;最后，根据分析结果，通过添加索引、优化查询条件、减少返回字段等方式进行优化。 慢sql日志怎么开启？编辑 MySQL 的配置文件 my.cnf，设置 slow_query_log 参数为 1。 slow_query_log = 1slow_query_log_file = /var/log/mysql/slow.loglong_query_time = 2 # 记录执行时间超过2秒的查询 然后重启 MySQL 就好了。 也可以通过 set global 命令动态设置。 SET GLOBAL slow_query_log = ON;SET GLOBAL slow_query_log_file = /var/log/mysql/slow.log;SET GLOBAL long_query_time = 2; 🌟33.你知道哪些方法来优化 SQL？SQL 优化的方法非常多，但本质上就一句话：尽可能少地扫描、尽快地返回结果。最常见的做法就是加索引、改写 SQL 让它用上索引，比如说使用覆盖索引、让联合索引遵守最左前缀原则等。 如何利用覆盖索引？覆盖索引的核心是“查询所需的字段都在同一个索引里”，这样 MySQL 就不需要回表，直接从索引中返回结果。 实际使用中，我会优先考虑把 WHERE 和 SELECT 涉及的字段一起建联合索引，并通过 EXPLAIN 观察结果是否有 Using index，确认命中索引。 —-这部分是帮助理解 start，面试中可不背—- 举个例子，现在要从 test 表中查询 city 为上海的 name 字段。 select name from test where city=上海 如果仅在 city 字段上添加索引，那么这条查询语句会先通过索引找到 city 为上海的行，然后再回表查询 name 字段。 为了避免回表查询，可以在 city 和 name 字段上建立联合索引，这样查询结果就可以直接从索引中获取。 alter table test add index index1(city,name); 相当于利用空间换时间,把查询结果都放到了索引里,不需要回表查询。—-这部分是帮助理解 end，面试中可不背—- 如何正确使用联合索引？使用联合索引最重要的一条是遵守最左前缀原则，也就是查询条件需要从索引的左侧字段开始。 —-这部分是帮助理解 start，面试中可不背—-比如说我们创建了一个三列的联合索引。 CREATE INDEX idx_name_age_sex ON user(name, age, sex); 我们来看一下什么样的查询条件可以用到这个索引：—-这部分是帮助理解 end，面试中可不背—- 如何进行分页优化？分页优化的核心是避免深度偏移(Deep Offset)带来的全表扫描，可以通过两种方式来优化：延迟关联和添加书签。 延迟关联适用于需要从多个表中获取数据且主表行数较多的情况。它首先从索引表中检索出需要的行 ID，然后再根据这些 ID 去关联其他的表获取详细信息。 SELECT e.id, e.name, d.detailsFROM employees eJOIN department d ON e.department_id = d.idORDER BY e.idLIMIT 1000, 20; 延迟关联后，第一步只查主键，速度快，第二步只处理 20 条数据，效率高。 SELECT e.id, e.name, d.detailsFROM ( SELECT id FROM employees ORDER BY id LIMIT 1000, 20) AS subJOIN employees e ON sub.id = e.idJOIN department d ON e.department_id = d.id; 添加书签的方式是通过记住上一次查询返回的最后一行主键值，然后在下一次查询的时候从这个值开始，从而跳过偏移量计算，仅扫描目标数据，适合翻页、资讯流等场景。 假设需要对用户表进行分页。 SELECT id, nameFROM usersORDER BY idLIMIT 1000, 20; 通过添加书签来优化后，查询不再使用OFFSET，而是从上一页最后一个用户的 ID 开始查询。这种方法可以有效避免不必要的数据扫描，提高了分页查询的效率。 SELECT id, nameFROM usersWHERE id last_max_id -- 假设last_max_id是上一页最后一行的IDORDER BY idLIMIT 20; 为什么分页会变慢？分页查询的效率问题主要是由于 OFFSET 的存在，OFFSET 会导致 MySQL 必须扫描和跳过 offset + limit 条数据，这个过程是非常耗时的。 比如说，我们要查询第 100000 条数据，那么 MySQL 就必须扫描 100000 条数据，然后再返回 10 条数据。 SELECT * FROM user ORDER BY id LIMIT 100000, 10; 数据越多、偏移越大，就越慢！ JOIN 代替子查询有什么好处？第一，JOIN 的 ON 条件能更直接地触发索引，而子查询可能因嵌套导致索引失效。第二，JOIN 的一次连接操作替代了子查询的多次重复执行，尤其在大数据量的情况下性能差异明显。 —-这部分是帮助理解 start，面试中可不背—- 比如说我们有两个表 orders 和 customers。 CREATE TABLE orders ( order_id INT PRIMARY KEY, customer_id INT, amount DECIMAL(10,2), INDEX idx_customer_id (customer_id) -- customer_id字段有索引);CREATE TABLE customers ( customer_id INT PRIMARY KEY, name VARCHAR(100)); 子查询的写法： SELECT o.order_id, o.amount, (SELECT c.name FROM customers c WHERE c.customer_id = o.customer_id) AS customer_nameFROM orders o; JOIN 的写法： SELECT o.order_id, o.amount, c.name AS customer_nameFROM orders oJOIN customers c ON o.customer_id = c.customer_id; 对于子查询，执行流程是这样的： 外层 orders 表的每一行都会触发一次子查询。 如果 orders 表有 1000 条记录，则子查询会执行 1000 次。 每次子查询都需要单独查询 customers 表（即使 customer_id 相同）。 而 JOIN 的执行流程是这样的： 数据库优化器会将两张表的连接操作合并为一次执行。 通过索引（如 orders.customer_id 和 customers.customer_id）快速关联数据。 仅执行一次关联操作，而非多次子查询。来看一下子查询的执行计划：EXPLAIN SELECT o.order_id, (SELECT c.name FROM customers c WHERE c.customer_id = o.customer_id) FROM orders o; 子查询（DEPENDENT SUBQUERY）类型表明其依赖外层查询的每一行，导致重复执行。 再对比看一下 JOIN 的执行计划： EXPLAIN SELECT o.order_id, (SELECT c.name FROM customers c WHERE c.customer_id = o.customer_id) FROM orders o; JOIN 通过 eq_ref 类型直接利用主键（customers.customer_id）快速关联，减少扫描次数。 JOIN操作为什么要小表驱动大表？ 第一，如果大表的 JOIN 字段有索引，那么小表的每一行都可以通过索引快速匹配大表。时间复杂度为**小表行数 N 乘以大表索引查找复杂度 log(大表行数 M)**，总复杂度为 N*log(M)。显然小表做驱动表比大表做驱动表的时间复杂度 M*log(N) 更低。 第二，如果大表没有索引，需要将小表的数据加载到内存，再全表扫描大表进行匹配。时间复杂度为小表分段数 K 乘以大表行数 M，其中 K 小表行数 N 内存大小 join_buffer_size。显然小表做驱动表的时候 K 的值更小，大表做驱动表的时候需要多次分段。 -- 小表驱动（高效）SELECT * FROM small_table sJOIN large_table l ON s.id = l.id; -- l.id有索引-- 大表驱动（低效）SELECT * FROM large_table lJOIN small_table s ON l.id = s.id; -- s.id无索引 当使用 left join 时，左表是驱动表，右表是被驱动表。 当使用 right join 时，刚好相反。 当使用 join 时，MySQL 会选择数据量比较小的表作为驱动表，大表作为被驱动表。 这里的小表指实际参与 JOIN 的数据量，而不是表的总行数。大表经过 where 条件过滤后也可能成为逻辑小表。– 实际参与JOIN的数据量决定小表 SELECT * FROM large_table lJOIN small_table s ON l.id = s.idWHERE l.created_at 2025-01-01; -- l经过过滤后可能成为小表 也可以强制通过 STRAIGHT_JOIN 提示 MySQL 使用指定的驱动表。 explain select table_1.col1, table_2.col2, table_3.col2from table_1straight_join table_2 on table_1.col1=table_2.col1straight_join table_3 on table_1.col1 = table_3.col1;explain select straight_join table_1.col1, table_2.col2, table_3.col2from table_1join table_2 on table_1.col1=table_2.col1join table_3 on table_1.col1 = table_3.col1; 为什么要避免使用 JOIN 关联太多的表？第一，多表 JOIN 的执行路径会随着表的数量呈现指数级增长，优化器需要估算所有路径的成本，有可能会导致出现大表驱动小表的情况。 SELECT * FROM AJOIN B ON A.id = B.a_idJOIN C ON B.id = C.b_idJOIN D ON C.id = D.c_idJOIN E ON D.id = E.d_id; -- 5 个表，优化器需评估 5! = 120 种顺序 第二，多表 JOIN 需要缓存中间结果集，可能超出 join_buffer_size，这种情况下内存临时表就会转为磁盘临时表，性能也会急剧下降。《阿里巴巴 Java 开发手册》上就规定，不要使用 join 关联太多的表，最多不要超过 3 张表。 如何进行排序优化？第⼀，对 ORDER BY 涉及的字段创建索引，避免 filesort。 -- 优化前（可能触发 filesort）SELECT * FROM users ORDER BY age DESC;-- 优化后（添加索引）ALTER TABLE users ADD INDEX idx_age (age); 如果是多个字段，联合索引需要保证ORDER BY 的列是索引的最左前缀。 -- 联合索引需与 ORDER BY 顺序⼀致（age 在前，name 在后）ALTER TABLE users ADD INDEX idx_age_name (age, name);-- 有效利⽤索引的查询SELECT * FROM users ORDER BY age, name;-- ⽆效案例（索引失效，因 name 在索引中排在 age 之后）SELECT * FROM users ORDER BY name, age; 第⼆，可以适当调整排序参数，如增⼤ sort_buffer_size、max_length_for_sort_data 等，让排序在内存中完成。—-这部分是帮助理解 start，⾯试中可不背—- sort_buffer_size：用于控制排序缓冲区的大小，默认为 256KB。也就是说，如果排序的数据量小于 256KB，MySQL 会在内存中直接排序；否则就要在磁盘上进行 filesort。 max_length_for_sort_data：单行数据的最大长度，会影响排序算法选择。如果单行数据超过该值，MySQL 会使用双路排序，否则使用单路排序。 max_sort_length：限制字符串排序时比较的前缀长度。当 MySQL 不得不对 text、blob 字段进行排序时，会截取前 max_sort_length 个字符进行比较。—-这部分是帮助理解 end，面试中可不背—-第三，可以通过 where 和 limit 限制待排序的数据量，减少排序的开销。-- 优化前SELECT * FROM users ORDER BY age LIMIT 100;-- 优化后（减少数据传输和排序开销）SELECT id, name, age FROM users ORDER BY age LIMIT 100;-- 深度分页优化（避免 OFFSET 扫描全表）SELECT * FROM users ORDER BY age LIMIT 10000, 20; -- 低效SELECT * FROM users WHERE age last_age ORDER BY age LIMIT 20; -- 高效（记录上一页最后一条的 age 值） 什么是 filesort？Mysql如何执行ORDER BY 当不能使用索引生成排序结果的时候，MySQL 需要自己进行排序，如果数据量比较小，会在内存中进行；如果数据量比较大就需要写临时文件到磁盘再排序，我们将这个过程称为文件排序。 —-这部分是帮助理解 start，面试中可不背—-让我们来验证一下 filesort 的情况 能够看得出来，当 order by id 也就是主键的时候，没有触发 filesort；当 order by age 的时候，由于没有索引，就触发了 filesort。—-这部分是帮助理解 end，面试中可不背—- 全字段排序和 rowid 排序了解多少？当排序字段是索引字段且满足最左前缀原则时，MySQL 可以直接利用索引的有序性完成排序。 当无法使用索引排序时，MySQL 需要在内存或磁盘中进行排序操作，分为全字段排序和 rowid 排序两种算法。 全字段排序会一次性取出满足条件行的所有字段，然后在 sort buffer 中进行排序，排序后直接返回结果，无需回表。 以 SELECT * FROM user WHERE name = 王二 ORDER BY age 为例： 从 name 索引中找到第一个满足 name’张三’ 的主键 id；根据主键 id 取出整行所有的字段，存入 sort buffer；重复上述过程直到处理完所有满足条件的行对 sort buffer 中的数据按 age 排序，返回结果。 优点是仅需要一次磁盘 IO 缺点是内存占用大，如果数量超过 sort buffer 的话，需要分片读取并借助临时文件合并排序，IO 次数反而会增加。也无法处理包含 text 和 blob 类型的字段。 rowid 排序分为两个阶段： 第一阶段：根据查询条件取出排序字段和主键 ID，存入sort buffer进行排序； 第二阶段：根据排序后的主键 ID 回表取出其他需要的字段。 同样以 SELECT * FROM user WHERE name = 王二 ORDER BY age 为例： 从 name 索引中找到第一个满足 name’张三’ 的主键 id； 根据主键 id 取出排序字段 age，连同主键 id 一起存入 sort buffer； 重复上述过程直到处理完所有满足条件的行 对 sort buffer 中的数据按 age 排序； 遍历排序后的主键 id，回表取出其他所需字段，返回结果。 优点是内存占用较少，适合字段多或者数据量大的场景，缺点是需要两次磁盘 IO。 MySQL 会根据系统变量 max_length_for_sort_data 和查询字段的总大小来决定使用全字段排序还是 rowid 排序。 如果查询字段总长度 max_length_for_sort_data，MySQL 会使用全字段排序；否则会使用 rowid 排序。 你对 Sort_merge_passes 参数了解吗？深入了解 MySQL Order By 文件排序Sort_merge_passes 是一个状态变量，用于统计 MySQL 在执行排序操作时进行归并排序的次数。当 MySQL 需要进行排序但排序数据无法完全放入 sort_buffer_size 定义的内存缓冲区时，就会使用临时文件进行外部排序，这时就会产生 Sort_merge_passes。 如果 Sort_merge_passes 在短时间内快速激增，说明排序操作的数据量较大，需要调整 sort_buffer_size 或者优化查询语句。 MySQL 在执行排序操作时，会经历两个过程： 内存排序阶段，MySQL 首先尝试在 sort buffer 中进行排序。如果数据量小于 sort_buffer_size 缓冲区大小，会完全在内存中完成快速排序。 外部排序阶段，如果数据量超过 sort_buffer_size，MySQL 会将数据分成多个块，每块单独排序后写入临时文件，然后对这些已排序的块进行归并排序。每次归并操作都会增加 Sort_merge_passes 的计数。 条件下推你了解多少？条件下推的核心思想是将外层的过滤条件，比如说 where、join 等，尽可能地下推到查询计划的更底层，比如说子查询、连接操作之前，从而减少中间结果的数据量。 比如说原始查询是： SELECT * FROM ( SELECT * FROM orders WHERE total 100) AS subqueryWHERE subquery.status = shipped; 就可以将条件下推到子查询： SELECT * FROM ( SELECT * FROM orders WHERE total 100 AND status = shipped) AS subquery; 这样就可以减少查询返回的数据量，避免外层再过滤。 再比如说 union 中的原始查询是： (SELECT * FROM t1) UNION ALL (SELECT * FROM t2)ORDER BY col LIMIT 10; 就可以将条件下推到每个子查询： (SELECT * FROM t1 ORDER BY col LIMIT 10)UNION ALL (SELECT * FROM t2 ORDER BY col LIMIT 10); 每个子查询仅返回前 10 条数据，减少临时表的数据量。 再比如说连接查询 join 中的原始查询是： SELECT * FROM ordersJOIN customers ON orders.customer_id = customers.idWHERE customers.country = china; 就可以将条件下推到表扫描的时候： SELECT * FROM ordersJOIN ( SELECT * FROM customers WHERE country = china) AS filtered_customersON orders.customer_id = filtered_customers.id; 先过滤 customers 表，减少 join 时的数据量。 为什么要尽量避免使用 select *？SELECT * 会强制 MySQL 读取表中所有字段的数据，包括应用程序可能并不需要的，比如 text、blob 类型的大字段。加载冗余数据会占用更多的缓存空间，从而挤占其他重要数据的缓存资源，降低整体系统的吞吐量。也会增加网络传输的开销，尤其是在大字段的情况下。最重要的是，SELECT * 可能会导致覆盖索引失效，本来可以走索引的查询最后变成了全表扫描。 -- 使用覆盖索引（假设索引为 idx_country）SELECT id, country FROM users WHERE country = china; -- 可能仅扫描索引-- 使用 SELECT *SELECT * FROM users WHERE country = china; -- 需回表读取所有列 你还知道哪些 SQL 优化方法？①、避免使用 != 或者 操作符 ! 或者 操作符会导致 MySQL 无法使用索引，从而导致全表扫描。 可以把column’aaa’，改成column’aaa’ or column’aaa’。 ②、使用前缀索引 比如，邮箱的后缀一般都是固定的@xxx.com，那么类似这种后面几位为固定值的字段就非常适合定义为前缀索引：alter table test add index index2(email(6));需要注意的是，MySQL 无法利用前缀索引做 order by 和 group by 操作。 ③、避免在列上使用函数 在 where 子句中直接对列使用函数会导致索引失效，因为 MySQL 需要对每行的列应用函数后再进行比较。select name from test where date_format(create_time,%Y-%m-%d)=2021-01-01;可以改成： select name from test where create_time=2021-01-01 00:00:00 and create_time2021-01-02 00:00:00; 通过日期的范围查询，而不是在列上使用函数，可以利用 create_time 上的索引。 34.🌟explain平常有用过吗？经常用，explain 是 MySQL 提供的一个用于查看 SQL 执行计划的工具，可以帮助我们分析查询语句的性能问题。 一共有 10 来个输出参数。 比如说 typeALL,keyNULL 表示 SQL 正在全表扫描，可以考虑为 where 字段添加索引进行优化；ExtraUsing filesort 表示 SQL 正在文件排序，可以考虑为 order by 字段添加索引。 使用方式也非常简单，直接在 select 前加上 explain 关键字就可以了。 explain select * from students where name=王二;更高级的用法可以配合 formatjson 参数，将 explain 的输出结果以 JSON 格式返回。explain format=json select * from students where name=王二; explain 输出结果中常见的字段含义理解吗？在 EXPLAIN 输出结果中我最关注的字段是 type、key、rows 和 Extra。 我会通过它们判断 SQL 有没有走索引、是否全表扫描、预估扫描行数是否太大，以及是否触发了 filesort 或临时表。一旦发现问题，比如 typeALL 或者 ExtraUsing filesort，我会考虑建索引、改写 SQL 或控制查询结果集来做优化。 —-这部分是帮助理解 start，面试中可不背—-以 EXPLAIN SELECT * FROM orders WHERE user_id = 100 的输出为例： 非表格版本：①、id 列：查询的执行顺序编号。id 相同：同一执行层级，按 table 列从上到下顺序执行（如多表 JOIN）；id 递增：嵌套子查询，数值越大优先级越高，越先执行。 EXPLAIN SELECT * FROM t1 JOIN (SELECT * FROM t2 WHERE id = 1) AS sub; t2 子查询的 id2，优先执行。 ②、select_type 列：查询的类型。常见的类型有： SIMPLE：简单查询，不包含子查询或者 UNION。 PRIMARY：查询中如果包含子查询，则最外层查询被标记为 PRIMARY。需要关注子查询或派生表性能。 SUBQUERY：子查询；需要避免多层嵌套，尽量改写为 JOIN。 DERIVED：派生表（FROM 子句中的子查询）。需要减少派生表数据量，或物化为临时表。 ③、table 列：查的哪个表。 derivedN：表示派生表（N 对应 id）。 unionNM,N：表示 UNION 合并的结果（M、N 为参与 UNION 的 id）。 ④、type 列：表示 MySQL 在表中找到所需行的方式。 system，表仅有一行（系统表或衍生表），无需优化。 const：通过主键或唯一索引找到一行（如 WHERE id 1）。理想情况。 eq_ref：对主键唯一索引 JOIN 匹配（如 A JOIN B ON A.id B.id）。确保 JOIN 字段有索引。 ref：非唯一索引匹配（如 WHERE name ‘王二’，name 有普通索引）。 range：只检索给定范围的行，使用索引来检索。在where语句中使用 bettween…and、、、、in 等条件查询 type 都是 range。 index：全索引扫描，如果不需要回表，可接受；否则考虑覆盖索引。 ALL：全表扫描，效率最低。⑤、possible_keys 列：可能会用到的索引，但并不一定实际被使用。 ⑥、key 列：实际使用的索引。如果为 NULL，则没有使用索引。如果为 PRIMARY，则使用了主键索引。 ⑦、key_len 列：使用的索引字节数，反映索引列的利用率。使用联合索引 (a, b)，key_len 是 a 和 b 的字节总和（仅当查询条件用到 a 或 a+b 时有效）。 – 表结构：CREATE TABLE t (a INT, b VARCHAR(20), INDEX idx_a_b (a, b));EXPLAIN SELECT * FROM t WHERE a 1 AND b ‘test’;key_len 4（INT） + 20*3（utf8） + 2 66 字节。 ⑧、ref 列：与索引列比较的值或列。 const：常量。例如 WHERE column ‘value’。 func：函数。例如 WHERE column func(column)。 ⑨、rows 列：优化器估算的需要扫描的行数。数值越小越好，若与实际差距大，可能统计信息过期（需 ANALYZE TABLE）。结合 filtered 字段可以计算最终返回行数（rows × filtered）。 ⑩、Extra 列：附加信息。 Using index：覆盖索引，无需回表。 Using where：存储引擎返回结果后，Server 层需要再次过滤（条件未完全下推）。 Using temporary ：使用临时表（常见于 GROUP BY、DISTINCT）。 Using filesort：文件排序（常见于 ORDER BY）。考虑为 ORDER BY 字段添加索引。 Select tables optimized away：优化器已优化（如 COUNT(*) 通过索引直接统计）。 Using join buffer：使用连接缓冲区（Block Nested Loop 或 Hash Join）。考虑增大 join_buffer_size。 —-这部分是帮助理解 end，面试中可不背—- type的执行效率等级，达到什么级别比较合适？从高到低的效率排序是 system、const、eq_ref、ref、range、index 和 ALL。 一般情况下，建议 type 值达到 const、eq_ref 或 ref，因为这些类型表明查询使用了索引，效率较高。如果是范围查询，range 类型也是可以接受的。ALL 类型表示全表扫描，性能最差，往往不可接受，需要优化。 索引35.🌟索引为什么能提高MySQL查询效率？索引就像一本书的目录，能让 MySQL 快速定位数据，避免全表扫描。 它一般是 B+ 树结构，查找效率是 O(log n)，比从头到尾扫一遍数据要快得多。 除了查得快，索引还能加速排序、分组、连接等操作。可以通过 create index 创建索引，比如：create index idx_name on students(name); 36.🌟能简单说一下索引的分类吗？从功能上分类的话，有主键索引、唯一索引、全文索引；从数据结构上分类的话，有 B+ 树索引、哈希索引；从存储内容上分类的话，有聚簇索引、非聚簇索引。 你对主键索引了解多少？主键索引用于唯一标识表中的每条记录，其列值必须唯一且非空。创建主键时，MySQL 会自动生成对应的唯一索引。 每个表只能有一个主键索引，一般是表中的自增 id 字段。 CREATE TABLE emp6 (emp_id INT PRIMARY KEY, name VARCHAR(50)); -- 单列主键CREATE TABLE CountryLanguage ( CountryCode CHAR(3), Language VARCHAR(30), PRIMARY KEY (CountryCode, Language) -- 复合主键); —- 这部分是帮助理解 start，面试中可不背 —- 如果创建表的时候没有指定主键，MySQL 的 InnoDB 存储引擎会优先选择一个非空的唯一索引作为主键；如果没有符合条件的索引，MySQL 会自动生成一个隐藏的 _rowid 列作为主键。 可以通过 show index from table_name 查看索引信息： Table 当前索引所属的表名。 Non_unique 是否唯一索引，0 表示唯一索引（如主键），1 表示非唯一。 Key_name 主键索引默认叫 PRIMARY；普通索引为自定义名。 Seq_in_index 索引中的列顺序，在联合索引中这个字段表示第几列（第 1 个）。 Column_name 当前索引中包含的字段名。 Collation A 表示升序（Ascend）；D 表示降序。 Cardinality 索引的基数，即不重复的索引值的数量。越高说明区分度越好（影响优化器是否用此索引）。 Sub_part 前缀索引的长度。 Packed 是否压缩存储索引；一般不用，默认为 NULL。 Null 字段是否允许为 NULL；主键字段不允许为 NULL。 Index_type 索引底层结构，InnoDB 默认是 B+ 树（BTREE）。 Comment 索引的注释。 Visible 是否可见；MySQL 8.0+ 可隐藏索引。—- 这部分是帮助理解 end，面试中可不背 —- 唯一索引和主键索引有什么区别？主键索引=唯一索引+非空。每个表只能有一个主键索引，但可以有多个唯一索引。 -- 在 email 列上添加唯一索引CREATE TABLE users ( id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(50) NOT NULL, email VARCHAR(100) NOT NULL, UNIQUE KEY uk_email (email) -- 唯一索引);-- 复合唯一索引（保证 user_id 和 role 组合唯一）CREATE TABLE user_roles ( user_id INT NOT NULL, role VARCHAR(20) NOT NULL, UNIQUE KEY uk_user_role (user_id, role)); 主键索引不允许插入 NULL 值，尝试插入 NULL 会报错；唯一索引允许插入多个 NULL 值。 unique key 和 unique index 有什么区别？创建唯一键时，MySQL 会自动生成一个同名的唯一索引；反之，创建唯一索引也会隐式添加唯一性约束。 可通过 UNIQUE KEY uk_name 定义或者 CONSTRAINT uk_name UNIQUE 定义唯一键。 CREATE TABLE users ( id INT PRIMARY KEY, email VARCHAR(100), -- 显式命名唯一键 CONSTRAINT uk_email UNIQUE (email));CREATE TABLE users3 ( id INT PRIMARY KEY, email VARCHAR(100), UNIQUE KEY uk_email (email) -- 唯一索引); 可通过 CREATE UNIQUE INDEX 创建唯一索引。 CREATE TABLE users ( id INT PRIMARY KEY, email VARCHAR(100));-- 手动创建唯一索引CREATE UNIQUE INDEX uk_email ON users(email); 通过 SHOW CREATE TABLE table_name 查看表结构时，结果都是一样的。 普通索引和唯一索引有什么区别？普通索引仅用于加速查询，不限制字段值的唯一性；适用于高频写入的字段、范围查询的字段。 -- 日志时间戳允许重复，无需唯一性检查CREATE INDEX idx_log_time ON access_logs(access_time);-- 订单状态允许重复，但需频繁按状态过滤数据CREATE INDEX idx_order_status ON orders(status); 唯一索引强制字段值的唯一性，插入或更新时会触发唯一性检查；适用于业务唯一性约束的字段、防止数据重复插入的字段。 -- 用户邮箱必须唯一CREATE UNIQUE INDEX uk_email ON users(email);-- 确保同一用户对同一商品只能有一条未支付订单CREATE UNIQUE INDEX uk_user_product ON orders(user_id, product_id) WHERE status = unpaid; 你对全文索引了解多少？全文索引是 MySQL 一种优化文本数据检索的特殊类型索引，适用于 CHAR、VARCHAR 和 TEXT 等字段。 MySQL 5.7 及以上版本内置了 ngram 解析器，可处理中文、日文和韩文等分词。 建表时通过 FULLTEXT (title, body) 来定义。通过 MATCH(col1, col2) AGAINST(keyword) 进行检索，默认按照降序返回结果，支持布尔模式查询。 + 表示必须包含； - 表示排除； * 表示通配符；-- 建表时创建全文索引（支持中文）CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY, title VARCHAR(200), content TEXT, FULLTEXT(title, content) WITH PARSER ngram) ENGINE=InnoDB;-- 使用布尔模式查询SELECT * FROM articles WHERE MATCH(title, content) AGAINST(+MySQL -Oracle IN BOOLEAN MODE); 底层使用倒排索引将字段中的文本内容进行分词，然后建立一个倒排表。性能比 LIKE ‘%keyword%’ 高很多。 —- 这部分是帮助理解 start，面试中可不背 —- 倒排索引通过一个辅助表存储单词与单词自身在一个或多个文档中所在位置之间的映射，通常采用关联数组实现。 有两种表现形式：inverted file index（{单词，单词所在文档的ID}）和full inverted index（{单词，(单词所在文档的ID，在具体文档中的位置)}） 比如有这样一个文档： DocumentId Text 1 Pease porridge hot, pease porridge cold 2 Pease porridge in the pot 3 Nine days old 4 Some like it hot, some like it cold 5 Some like it in the pot 6 Nine days old inverted file index 的关联数组存储形式为： days → 3,6 old → 3,6 pease → 1,2 porridge → 1,2 ... full inverted index 更加详细： days → (3:5),(6:5) old → (3:11),(6:11) pease → (1:1),(1:7),(2:1) porridge → (1:7),(2:7) ... full inverted index 不仅存储了文档 ID，还存储了单词在文档中的具体位置。 InnoDB 采用的是 full inverted index 的方式实现全文索引。 如果需要处理中文分词的话，一定要记得加上 WITH PARSER ngram，否则可能查不出来数据。 不过，对于复杂的中文场景，建议使用 Elasticsearch 等专业搜索引擎替代，技术派项目中就用了这种方案。 —- 这部分是帮助理解 end，面试中可不背 —- 37.🌟创建索引有哪些注意点？第一，选择合适的字段 比如说频繁出现在 WHERE、JOIN、ORDER BY、GROUP BY 中的字段。 优先选择区分度高的字段，比如用户 ID、手机号等唯一值多的，而不是性别、状态等区分度极低的字段，如果真的需要，可以考虑联合索引。 第二，要控制索引的数量，避免过度索引，每个索引都要占用存储空间，单表的索引数量不建议超过 5 个。 要定期通过 SHOW INDEX FROM table_name 查看索引的使用情况，删除不必要的索引。比如说已经有联合索引 (a, b)，单索引（a）就是冗余的。 第三，联合索引的时候要遵循最左前缀原则，即在查询条件中使用联合索引的第一个字段，才能充分利用索引。 比如说联合索引 (A, B, C) 可支持 A、A+B、A+B+C 的查询，但无法支持 B 或 C 的单独查询。 区分度高的字段放在左侧，等值查询的字段优先于范围查询的字段。例如 WHERE A1 AND B10 AND C2，优先 (A, C, B)。 如果联合索引包含查询的所需字段，还可以避免回表，提高查询效率。 38.🌟索引哪些情况下会失效呢？简版：比如索引列使用了函数、使用了通配符开头的模糊查询、联合索引不满足最左前缀原则，或者使用 or 的时候部分字段无索引等。 第一，对索引列使用函数或表达式会导致索引失效。 -- 索引失效SELECT * FROM users WHERE YEAR(create_time) = 2023;SELECT * FROM products WHERE price*2 100;-- 优化方案（使用范围查询）SELECT * FROM users WHERE create_time BETWEEN 2023-01-01 AND 2023-12-31;SELECT * FROM products WHERE price 50; 第二，LIKE 模糊查询以通配符开头会导致索引失效。 -- 索引失效SELECT * FROM articles WHERE title LIKE %数据库%;-- 可以使用索引（但范围有限）SELECT * FROM articles WHERE title LIKE 数据库%;-- 解决方案：考虑全文索引或搜索引擎SELECT * FROM articles WHERE MATCH(title) AGAINST(数据库); 第三，联合索引违反了最左前缀原则，索引会失效。 -- 假设有联合索引 (a, b, c)SELECT * FROM table WHERE b = 2 AND c = 3; -- 索引失效SELECT * FROM table WHERE a = 1 AND c = 3; -- 只使用a列索引-- 正确使用联合索引SELECT * FROM table WHERE a = 1 AND b = 2 AND c = 3; 联合索引，但 WHERE 不满足最左前缀原则，索引无法起效。例如：SELECT * FROM table WHERE column2 = 2，联合索引为 (column1, column2)。 —- 这部分是帮助理解 start，面试中可不背 —- 第四，使用 OR 连接非索引列条件，会导致索引失效。 -- 假设name有索引但age没有SELECT * FROM users WHERE name = 张三 OR age = 25; -- 全表扫描-- 优化方案1：使用UNION ALLSELECT * FROM users WHERE name = 张三UNION ALLSELECT * FROM users WHERE age = 25 AND name != 张三;-- 优化方案2：考虑为age添加索引 第五，使用 ! 或 不等值查询会导致索引失效。 SELECT * FROM user WHERE status != 1; -- 若大部分行 `status=1`，可能全表扫描-- 优化方案：使用范围查询SELECT * FROM user WHERE status 1 OR status 1; —- 这部分是帮助理解 end，面试中可不背 —- 什么情况下模糊查询不走索引？模糊查询主要使用 LIKE 语句，结合通配符来实现。%（代表任意多个字符）和 _（代表单个字符） SELECT * FROM table WHERE column LIKE %xxx%; 这个查询会返回所有 column 列中包含 xxx 的记录。但是，如果模糊查询的通配符** % 出现在搜索字符串的开始位置，如 LIKE ‘%xxx’，MySQL 将无法使用索引，因为数据库必须扫描全表以匹配任意位置的字符串**。 39.索引不适合哪些场景呢？第一，区分度低的列，可以和其他高区分度的列组成联合索引。 第二，频繁更新的列，索引会增加更新的成本。 第三，TEXT、BLOB 等大对象类型的字段，可以使用前缀索引、全文索引替代。 第四，当表的数据量很小的时候，不超过 1000 行，全表扫描可能比使用索引更快。 —- 这部分是帮助理解 start，面试中可不背 —- 原因时当数据量很小时，全表扫描的成本很低，因为所有的数据可能都加载到内存中了，使用索引反而需要先查找索引，再通过索引去找到实际的数据行，增加了额外的 IO 寻址时间。 —- 这部分是帮助理解 end，面试中可不背 —- 性别字段要建立索引吗？性别字段不适合建立单独索引。因为性别字段的区分度很低。如果性别字段确实经常用于查询条件，数据规模也比较大，可以将性别字段作为联合索引的一部分，与区分度高的字段一起，效果会好很多。 什么是区分度？区分度是衡量一个字段在 MySQL 表中唯一值的比例。 区分度 = 字段的唯一值数量 / 字段的总记录数；越接近 1，就越适合作为索引。因为索引可以更有效地缩小查询范围。 例如，一个表中有 1000 条记录，其中性别字段只有两个值（男、女），那么性别字段的区分度只有 0.002，就不适合建立索引。 可以通过COUNT(DISTINCT column_name)和COUNT(*)的比值来计算字段的区分度。例如： SELECT COUNT(DISTINCT gender) / COUNT(*) AS gender_selectivityFROM users; 什么样的字段适合加索引？一句话回答：一般来说，主键、唯一键、以及经常作为查询条件的字段最适合加索引。除此之外，字段的区分度要高，这样索引才能起到过滤作用；如果字段经常用于表连接、排序或分组，也建议加索引。同时如果多个字段经常一起出现在查询条件中，也可以建立联合索引来提升性能。 —- 这部分是帮助理解 start，面试中可不背 —- 查询条件中的高频字段，比如说WHERE子句中频繁用于等值查询、范围查询或者 IN 列表的字段。 SELECT * FROM orders WHERE status = PAID AND create_time 2023-01-01;-- 若`status`和`create_time`常组合查询，建联合索引`(status, create_time)` 多表连接时的关联字段，比如说 user.id 和 order.user_id。 SELECT * FROM user u JOIN order o ON u.id = o.user_id; -- `user_id`需索引 参与排序或者分组的字段，可以直接利用索引的有序性，避免文件排序。 SELECT * FROM product ORDER BY price DESC; -- 单字段排序SELECT category, COUNT(*) FROM product GROUP BY category; -- 分组统计 需要利用覆盖索引的字段，可以避免回表操作。 -- 创建联合索引`(user_id, create_time)`SELECT user_id, create_time FROM orders WHERE user_id = 100; -- 覆盖索引生效 —- 这部分是帮助理解 end，面试中可不背 —- 40.索引是不是建的越多越好？索引不是越多越好。虽然索引可以加快查询，但也会带来写入变慢、占用更多存储空间、甚至让优化器选错索引的风险。 —- 这部分是帮助理解 start，面试中可不背 —- 每次数据写入（INSERTUPDATEDELETE）时，MySQL 都需同步更新所有相关索引，索引越多，维护成本越高。 假如某表有 10 个索引，插入一行数据需更新 10 个 B+树结构，导致写入延迟增加 5~10 倍。 假如某表数据量 100GB，若建 5 个索引，总存储可能达到 200GB+。 索引过多时，优化器需评估更多可能的执行路径，可能导致选择困难症，优化器也会选错索引。 再比如说，已有联合索引 (A, B, C)，再单独建 (A) 或 (A, B) 索引即为冗余。 单表索引数量建议不超过 5 个，MySQL 官方建议单表索引总字段数 ≤ 表字段数的 30%。 —- 这部分是帮助理解 end，面试中可不背 —- 说说索引优化的思路？一句话回答：先通过慢查询日志找出性能瓶颈，然后用 EXPLAIN 分析执行计划，判断是否走了索引、是否回表、是否排序。接着根据字段特性设计合适的索引，如选择区分度高的字段，使用联合索引和覆盖索引，避免索引失效的写法，最后通过实测来验证优化效果。 41.🌟为什么 InnoDB 要使用 B+树作为索引？一句话总结：因为 B+ 树是一种高度平衡的多路查找树，能有效降低磁盘的 IO 次数，并且支持有序遍历和范围查询。 查询性能非常高，其结构也适合 MySQL 按照页为单位在磁盘上存储。 像其他选项，比如说哈希表不支持范围查询，二叉树层级太深，B 树又不方便范围扫描，所以最终选择了 B+ 树。 再换一种回答： 相比哈希表：B+ 树支持范围查询和排序相比二叉树和红黑树：B+ 树更“矮胖”，层级更少，磁盘 IO 次数更少相比 B 树：B+ 树的非叶子节点只存储键值，叶子节点存储数据并通过链表连接，支持范围查询另外一种回答版本： B+树是一种自平衡的多路查找树，和红黑树、二叉平衡树不同，B+树的每个节点可以有 m 个子节点，而红黑树和二叉平衡树都只有 2 个。 另外，和 B 树不同，B+树的非叶子节点只存储键值，不存储数据，而叶子节点存储了所有的数据，并且构成了一个有序链表。 这样做的好处是，非叶子节点上由于没有存储数据，就可以存储更多的键值对，再加上叶子节点构成了一个有序链表，范围查询时就可以直接通过叶子节点间的指针顺序访问整个查询范围内的所有记录，而无需对树进行多次遍历。查询的效率比 B 树更高。 先说说 B 树。B 树是一种自平衡的多路查找树，和红黑树、二叉平衡树不同，B 树的每个节点可以有 m 个子节点，而红黑树和二叉平衡树都只有 2 个。换句话说，红黑树、二叉平衡树是细高个，而 B 树是矮胖子。 再来说说内存和磁盘的 IO 读写。 为了提高读写效率，从磁盘往内存中读数据的时候，一次会读取至少一页的数据，如果不满一页，会再多读点。 比如说查询只需要读取 2KB 的数据，但 MySQL 实际上会读取 4KB 的数据，以装满整页。页是 MySQL 进行内存和磁盘交互的最小逻辑单元。 再比如说需要读取 5KB 的数据，实际上 MySQL 会读取 8KB 的数据，刚好两页。 因为读的次数越多，效率就越低。就好比我们在工地上搬砖，一次搬 10 块砖肯定比一次搬 1 块砖的效率要高，反正我每次都搬 10 块（😁）。 对于红黑树、二叉平衡树这种细高个来说，每次搬的砖少，因为力气不够嘛，那来回跑的次数就越多。 通常 B+ 树高度为 3-4 层即可支持 TB 级数据，而每次查询只需 2-4 次磁盘 IO，远低于二叉树或红黑树的 O(log2N) 复杂度 树越高，意味着查找数据时就需要更多的磁盘 IO，因为每一层都可能需要从磁盘加载新的节点。 B 树的节点通常与页的大小对齐，这样每次从磁盘加载一个节点时，正好就是一页的大小。 B 树的一个节点通常包括三个部分： 键值：即表中的主键 指针：存储子节点的信息 数据：除主键外的行数据正所谓“祸兮福所倚，福兮祸所伏”，因为 B 树的每个节点上都存储了数据，就导致每个节点能存储的键值和指针变少了，因为每一个节点的大小是固定的，对吧？于是 B+树就来了，B+树的非叶子节点只存储键值，不存储数据，而叶子节点会存储所有的行数据，并且构成一个有序链表。 这样做的好处是，非叶子节点由于没有存储数据，就可以存储更多的键值对，树就变得更加矮胖了，于是就更有劲了，每次搬的砖也就更多了（😂）。 相比 B 树，B+ 树的非叶子节点可容纳的键值更多，一个 16KB 的节点可存储约 1200 个键值，大幅降低树的高度。 由此一来，查找数据进行的磁盘 IO 就更少了，查询的效率也就更高了。 再加上叶子节点构成了一个有序链表，范围查询时就可以直接通过叶子节点间的指针顺序访问整个查询范围内的所有记录，而无需对树进行多次遍历。 B 树就做不到这一点。 —- 这部分是帮助理解 end，面试中可不背 —- B+树的叶子节点是单向链表还是双向链表？如果从大值向小值检索，如何操作？B+树的叶子节点是通过双向链表连接的，这样可以方便范围查询和反向遍历。 当执行范围查询时，可以从范围的开始点或结束点开始，向前或向后遍历。在需要对数据进行逆序处理时，双向链表非常有用。如果需要在 B+树中从大值向小值进行检索，可以先定位到最右侧节点，找到包含最大值的叶子节点。从根节点开始向右遍历树的方式实现。 定位到最右侧的叶子节点后，再利用叶节点间的双向链表向左遍历就好了。 为什么 MongoDB 的索引用 B树，而 MySQL 用 B+ 树？MongoDB 通常以 JSON 格式存储文档，查询以单键查询（如 find({_id: 123})）为主。B 树的“节点既存键又存数据”的特性允许查询在非叶子节点提前终止，从而减少 IO 次数。 MySQL 的查询通常涉及范围（WHERE id 100）、排序（ORDER BY）、连接（JOIN）等操作。B+ 树的叶子节点是链表结构，天然支持顺序遍历，无需回溯至根节点或中序遍历，效率远高于 B 树。 42.🌟一棵B+树能存储多少条数据呢？一句话回复：一棵 B+ 树能存多少数据，取决于它的分支因子和高度。在 InnoDB 中，页的默认大小为 16KB，当主键为 bigint 时，3 层 B+ 树通常可以存储约 2000 万条数据。 —- 这部分是帮助理解 start，面试中可不背 —- 先来看一下计算公式： 最大记录数 = (分支因子)^(树高度-1) × 叶子节点容量 再来看一下关键参数：①、页大小，默认 16KB②、主键大小，假设是 bigint 类型，那么它的大小就是 8 个字节。③、页指针大小，InnoDB 源码中设置为 6 字节，4 字节页号 + 2 字节页内偏移。 所以非叶子节点可以存储 1638414(键值+指针)1170 个这样的单元。 当层高为 2 时，根节点可以存储 1170 个指针，指向 1170 个叶子节点，所以总数据量为 1170×16 18720 条。 当层高为 3 时，根节点指向 1170 个非叶子节点，每个非叶子节点再指向 1170 个叶子节点，所以总数据量为 1170×1170×16≈21,902,400 条（约2,190万条）记录。 推荐阅读：清幽之地：InnoDB 一棵 B+树可以存放多少行数据？ —- 这部分是帮助理解 end，面试中可不背 —- 现在有一张表 2kw 数据，我这个 b+树的高度有几层？对于 2KW 条数据来说，B+树的高度为 3 层就够了。 每个叶子节点能存放多少条数据？如果单行数据大小为 1KB，那么每页可存储约 16 行（16KB1KB）数据。 —- 这部分是帮助理解 start，面试中可不背 —- 假设有这样一个表结构： CREATE TABLE `user` ( `id` BIGINT PRIMARY KEY, -- 8字节 `name` VARCHAR(255) NOT NULL, -- 实际长度50字节（UTF8MB4，每个字符最多4字节） `age` TINYINT, -- 1字节 `email` VARCHAR(255) -- 实际长度30字节，可为NULL) ROW_FORMAT=COMPACT; 那么一行数据的大小为：8 + 50 + 1 + 30 89 字节。 行格式的开销为：行头 5 字节+指针 6 字节+可变长度字段开销 2 字节（name 和 email 各占 1 字节）+ NULL 位图 1 字节 14 字节。 所以每行数据的实际大小为：89 + 14 103 字节。 每页大小默认为 16KB，那么每页最多可以存储 16384 103 ≈ 158 行数据。 —- 这部分是帮助理解 end，面试中可不背 —- 43.索引为什么用 B+树不用普通二叉树？普通二叉树的每个节点最多有两个子节点。当数据按顺序递增插入时，二叉树会退化成链表，导致树的高度等于数据量。 此时查找 id7 就需要 7 次 IO 操作，相当于全表扫描。而 B+ 树作为多叉平衡树，能将数亿级的数据量控制在 3-4 层的树高，能极大减少磁盘的 IO 次数。 为什么不用平衡二叉树呢？平衡二叉树虽然解决了普通二叉树的退化问题，但每个节点最多只有两个子节点的问题依然存在。并且平衡二叉树的插入和删除操作也会导致频繁的旋转操作，影响性能。 44.🌟为什么用 B+ 树而不用 B 树呢？B+ 树相比 B 树有 3 个显著优势： 第一，B 树的每个节点既存储键值，又存储数据和指针，导致单节点存储的键值数量较少。 一个 16KB 的 InnoDB 页，如果数据较大，B 树的非叶子节点只能容纳几十个键值，而 B+ 树的非叶子节点可以容纳上千个键值。 第二，B 树的范围查询需要通过中序遍历逐层回溯；而 B+ 树的叶子节点通过双向链表顺序连接，范围查询只需定位起始点后顺序遍历链表即可，没有回溯开销。 第三，B 树的数据可能存储在任意节点，假如目标数据恰好位于根节点或上层节点，查询仅需 1-2 次 IO；但如果数据位于底层节点，则需多次 IO，导致查询时间波动较大。 而 B+ 树的所有数据都存储在叶子节点，查询路径的长度是固定的，**时间稳定为 O(logN)**，对 MySQL 在高并发场景下的稳定性至关重要。 B+树的时间复杂度是多少？O(logN)。 为什么用 B+树不用跳表呢？跳表本质上还是链表结构，只不过把某些节点抽到上层做了索引。 一条数据一个节点，如果需要存放 2000 万条数据，且每次查询都要能达到二分查找的效果，那么跳表的高度大约为 24 层（2 的 24 次方）。 在最坏的情况下，这 24 层数据分散在不同的数据页，查找一次数据就需要 24 次磁盘 IO。 而 2000 万条数据在 B+树中只需要 3 层就可以了。 B+树的范围查找怎么做的？一句话回答： 先通过索引路径定位到第一个满足条件的叶子节点，然后顺着叶子节点之间的链表向右向左扫描，直到超过范围。 详细版： B+ 树索引的范围查找主要依赖叶子节点之间的双向链表来完成。 第一步，从 B+ 树的根节点开始，通过索引键值逐层向下，找到第一个满足条件的叶子节点。 第二步，利用叶子节点之间的双向链表，从起始节点开始，依次向后遍历每个节点。当索引值超过查询范围，或者遍历到链表末尾时，终止查询。 了解快排吗快速排序使用分治法将一个序列分为较小和较大的 2 个子序列，然后递归排序两个子序列，由东尼·霍尔在 1960 年提出。 其核心思想是： 选择一个基准值。 将数组分为两部分，左边小于基准值，右边大于或等于基准值。 对左右两部分递归排序，最终合并。public static void quickSort(int[] arr, int low, int high) if (low high) int pivotIndex = partition(arr, low, high); quickSort(arr, low, pivotIndex - 1); quickSort(arr, pivotIndex + 1, high); private static int partition(int[] arr, int low, int high) int pivot = arr[high]; int i = low - 1; for (int j = low; j high; j++) if (arr[j] = pivot) i++; swap(arr, i, j); swap(arr, i + 1, high); return i + 1;private static void swap(int[] arr, int i, int j) int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; 45.B+树索引和 Hash 索引有什么区别？简版回答：B+ 树索引支持范围查询、有序扫描，是 InnoDB 的默认索引结构。 Hash 索引只支持等值查找，速度快但功能弱，常见于 Memory 引擎。 稍微详细一点的回答： B+ 树索引是一种平衡多路搜索树，所有数据存储在叶子节点上，非叶子节点仅存储索引键。叶子节点通过指针连接形成有序链表，天然支持排序。并且支持范围查询、模糊查询，是 InnoDB 默认的索引结构。 Hash 索引基于哈希函数将键值映射到固定长度的哈希值，通过哈希值定位数据存储的位置。完全无序，只支持等值查询，常见于 Memory 引擎。 —- 这部分是帮助理解 start，面试中可不背 —- 因为 B+ 树是 InnoDB 的默认索引类型，所以创建 B+ 树的时候不需要指定索引类型。 CREATE TABLE example_btree ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255), INDEX name_index (name)) ENGINE=InnoDB; 可以通过UNIQUE HASH创建哈希索引： CREATE TABLE example_hash ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255), UNIQUE HASH (name)) ENGINE=MEMORY; InnoDB 并不提供直接创建哈希索引的选项，因为 B+ 树索引能够很好地支持范围查询和等值查询，满足了大多数数据库操作的需要。 不过，InnoDB 内部使用了一种名为“自适应哈希索引”（Adaptive Hash Index, AHI）的技术，当某些索引值频繁访问时，InnoDB 会在 B+ 树基础上自动创建哈希索引，兼具两者的优点。 可通过 SHOW VARIABLES LIKE innodb_adaptive_hash_index; 查看自适应哈希索引的状态。 如果返回的值是 ON，说明自适应哈希索引是开启的。—- 这部分是帮助理解 end，面试中可不背 —- 46.🌟聚族索引和非聚族索引有什么区别？聚簇索引的叶子节点存储了完整的数据行，数据和索引是在一起的。InnoDB 的主键索引就是聚簇索引，叶子节点不仅存储了主键值，还存储了其他列的值，因此按照主键进行查询的速度会非常快。 每个表只能有一个聚簇索引，通常由主键定义。如果没有显式指定主键，InnoDB 会隐式创建一个隐藏的主键索引 row_id。非聚簇索引的叶子节点只包含了主键值，需要通过回表按照主键去聚簇索引查找其他列的值，唯一索引、普通索引等非主键索引都是非聚簇索引。 每个表都可以创建多个非聚簇索引，如果不想回表的话，可以通过覆盖索引把要查询的字段也放到索引中。 —- 这部分是帮助大家理解 start，面试中可不背 —- 一张表只能有一个聚簇索引。 CREATE TABLE user ( id INT PRIMARY KEY, name VARCHAR(100), age INT);主键 id 是聚簇索引，B+ 树的叶子节点直接存储了 (id, name, age)。 一张表可以有多个非聚簇索引。 CREATE INDEX idx_name ON user(name);CREATE INDEX idx_age ON user(age);idx_name 是非聚簇索引，叶子节点存的是 name - id，查整行数据要回表。 idx_age 也是非聚簇索引，叶子节点存的是 age - id，查整行数据也要回表。 想要了解更多聚簇索引和非聚簇索引，推荐阅读：https://www.cnblogs.com/vipstone/p/16370305.htmlhttps://learnku.com/articles/50096https://blog.csdn.net/m0_52226803/article/details/135494499https://mp.weixin.qq.com/s/F0cEzIqecF4sWg7ZRmHKRQ—- 这部分是帮助理解 end，面试中可不背 —- 47.🌟回表了解吗？当使用非聚簇索引进行查询时，MySQL 需要先通过非聚簇索引找到主键值，然后再根据主键值回到聚簇索引中查找完整数据行，这个过程称为回表。 假设现在有一张用户表 users： CREATE TABLE users ( id INT PRIMARY KEY, name VARCHAR(50), age INT, email VARCHAR(50), INDEX (name)); 执行查询： SELECT * FROM users WHERE name = 王二; 查询过程如下： 第一步，MySQL 使用 name 列上的非聚簇索引查找所有 name ‘王二’ 的主键 id。 第二步，使用主键 id 到聚簇索引中查找完整记录。 回表的代价是什么？回表通常需要访问额外的数据页，如果数据不在内存中，还需要从磁盘读取，增加 IO 开销。 可通过覆盖索引或者联合索引来避免回表。 -- 原表结构CREATE TABLE users ( id INT PRIMARY KEY, name VARCHAR(50), age INT, INDEX idx_name (name));-- 需要查询name和ageSELECT name, age FROM users WHERE name = 张三;-- 这会回表，因为age不在idx_name索引中-- 优化方案1：创建包含age的联合索引ALTER TABLE users ADD INDEX idx_name_age (name, age);-- 现在同样的查询不需要回表 什么情况下会触发回表？ 第一，当查询字段不在非聚簇索引中时，必须回表到主键索引获取数据。第二，查询字段包含非索引列（如 SELECT *），必然触发回表。 回表记录越多好吗？回表记录越多，通常代表性能越差，因为每条记录都需要通过主键再查询一次完整数据。这个过程涉及内存访问或磁盘 IO，尤其当缓存命中率不高时，回表会严重影响查询效率。 了解 MRR 吗？MRR 是 InnoDB 为了解决回表带来的大量随机 IO 问题而引入的一种优化策略。 它会先把非聚簇索引查到的主键值列表进行排序，再按顺序去主键索引中批量回表，将随机 IO 转换为顺序 IO，以减少磁盘寻道时间。 —- 这部分是帮助理解 start，面试中可不背 —- 可通过 SHOW VARIABLES LIKE optimizer_switch; 查看 MRR 是否启用。 其中 mrr=on 表示启用 MRR，mrr_cost_based=on 表示基于成本决定使用 MRR。 另外可以通过 show variables like read_rnd_buffer_size; 查看 MRR 的缓冲区大小，默认是 256KB。 我们来创建一个表，插入一些数据，然后执行一个查询来演示 MRR 的效果。 CREATE DATABASE IF NOT EXISTS mrr_test; USE mrr_test; CREATE TABLE IF NOT EXISTS orders (id INT AUTO_INCREMENT PRIMARY KEY, user_id INT, order_date DATE, amount DECIMAL(10,2), status VARCHAR(20), INDEX idx_user_date(user_id, order_date));DELIMITER //CREATE PROCEDURE generate_test_data()BEGIN DECLARE i INT DEFAULT 1; WHILE i = 100000 DO INSERT INTO orders (user_id, order_date, amount, status) VALUES ( FLOOR(1 + RAND() * 1000), -- Random user_id between 1 and 1000 DATE_ADD(2023-01-01, INTERVAL FLOOR(RAND() * 365) DAY), -- Random date in 2023 ROUND(10 + RAND() * 990, 2), -- Random amount between 10 and 1000 ELT(1 + FLOOR(RAND() * 3), completed, pending, cancelled) -- Random status ); SET i = i + 1; END WHILE;END //DELIMITER ;CALL generate_test_data();DROP PROCEDURE generate_test_data; 查看 MRR 开启和关闭时的性能数据： -- 确保MRR开启并设置足够大的缓冲区SET SESSION optimizer_switch=mrr=on,mrr_cost_based=off;SET SESSION read_rnd_buffer_size = 16*1024*1024;-- 清理缓存和状态FLUSH STATUS;FLUSH TABLES;-- 强制使用二级索引并回表查询（通过选择未被索引的列）SELECT Raw data access pattern with MRR ON as test_case;SELECT /*+ MRR(orders_mrr_test) */ id, shipping_address, customer_nameFROM orders_mrr_test FORCE INDEX(idx_user_date)WHERE user_id IN (100,200,300,400,500,600,700,800,900,1000)AND order_date BETWEEN 2023-03-01 AND 2023-04-01LIMIT 15;-- 显示处理器状态SHOW STATUS LIKE Handler_%;SHOW STATUS LIKE %mrr%;-- 对比：关闭MRRSET SESSION optimizer_switch=mrr=off,mrr_cost_based=off;FLUSH STATUS;FLUSH TABLES;SELECT Raw data access pattern with MRR OFF as test_case;SELECT id, shipping_address, customer_nameFROM orders_mrr_test FORCE INDEX(idx_user_date)WHERE user_id IN (100,200,300,400,500,600,700,800,900,1000)AND order_date BETWEEN 2023-03-01 AND 2023-04-01LIMIT 15;-- 显示处理器状态SHOW STATUS LIKE Handler_%;SHOW STATUS LIKE %mrr%;-- 显示详细的执行计划EXPLAIN FORMAT=TREESELECT /*+ MRR(orders_mrr_test) */ id, shipping_address, customer_nameFROM orders_mrr_test FORCE INDEX(idx_user_date)WHERE user_id IN (100,200,300,400,500,600,700,800,900,1000)AND order_date BETWEEN 2023-03-01 AND 2023-04-01; 可以看到 MRR 开启时的结果对比： Wrap 也给出了对应的结果说明：也可以在 explain 中确认 MRR 的使用情况。—- 这部分是帮助理解 end，面试中可不背 —- 48.🌟联合索引了解吗？（补充）联合索引就是把多个字段放在一个索引里，但必须遵守“最左前缀”原则，只有从第一个字段开始连续使用，索引才会生效。 联合索引会按字段顺序构建B+树。例如（age, name）索引会先按照 age 排序，age 相同则按照 name 排序，若两者都相同则按主键排序，确保叶子节点无重复索引项。 创建(A,B,C)联合索引相当于同时创建了(A)、(A,B)和(A,B,C)三个索引。 -- 创建联合索引CREATE INDEX idx_order_user_product ON orders(user_id, product_id, create_time)-- 高效查询SELECT * FROM orders WHERE user_id=1001 AND product_id=2002ORDER BY create_time DESC 联合索引底层的存储结构是怎样的？联合索引在底层采用 B+ 树结构进行存储，这一点与单列索引相同。 与单列索引不同的是，联合索引的每个节点会存储所有索引列的值，而不仅仅是第一列的值。例如，对于联合索引(a,b,c)，每个节点都包含 a、b、c 三列的值。 非叶子节点示例： [(a=1, b=2, c=3) → 子节点1, (a=5, b=3, c=1) → 子节点2]叶子节点示例（InnoDB）： (a=1, b=2, c=3) → PK=100 | (a=1, b=2, c=4) → PK=101 （通过指针连接形成双向链表） 联合索引的叶子节点存的什么内容?联合索引属于非聚簇索引，叶子节点存储的是联合索引各列的值和对应行的主键值，而不是完整的数据行。查询非索引字段时，需要通过主键值回表到聚簇索引获取完整数据。 例如索引(a, b)的叶子节点会完整存储(a, b)的值，并按字段顺序排序（如 a 优先，a 相同则按 b 排序）。如果主键是 id，叶子节点会存储 (a, b, id) 的组合。 49.🌟覆盖索引了解吗？覆盖索引指的是：查询所需的字段全部都在索引中，不需要回表，从索引页就能直接返回结果。 empname 和 job 两个字段是一个联合索引，而查询也恰好是这两个字段，这时候单次查询就可以达到目的，不需要回表。 可以将高频查询的字段（如 WHERE 条件和 SELECT 列）组合为联合索引，实现覆盖索引。 例如： CREATE INDEX idx_empname_job ON employee(empname, job); 这样查询的时候就可以走索引： SELECT empname, job FROM employee WHERE empname = 王二 AND job = 程序员; 普通索引只用于加速查询条件的匹配，而覆盖索引还能直接提供查询结果。 一个表（name, sex,age,id），select age,id,name from tblname where name’paicoding’;怎么建索引由于查询条件有 name 字段，所以最少应该为 name 字段添加一个索引。、 CREATE INDEX idx_name ON tblname(name); 查询结果中还需要 age、id 字段，可以为这三个字段创建一个联合索引，利用覆盖索引，直接从索引中获取数据，减少回表。 CREATE INDEX idx_name_age_id ON tblname (name, age, id); 50.🌟什么是最左前缀原则？最左前缀原则指的是：MySQL 使用联合索引时，必须从最左边的字段开始匹配，才能命中索引。 假设有一个联合索引 (A, B, C)，其生效条件如下： 如果排序或分组的列是最左前缀的一部分，索引还可以加速操作。 -- 索引(a,b)SELECT * FROM table WHERE a = 1 ORDER BY b; -- 可以利用索引排序 范围查询后的列还能用索引吗？范围查询只能应用于最左前缀的最后一列。范围查询之后的列无法使用索引。 -- 索引(a,b,c)SELECT * FROM table WHERE a = 1 AND b 2 AND c = 3; -- 只能使用a和b，c无法使用索引 为什么不从最左开始查，就无法匹配呢？一句话回答： 因为联合索引在 B+ 树中是按照最左字段优先排序构建的，如果跳过最左字段，MySQL 无法判断查找范围从哪里开始，自然也就无法使用索引。 比如有一个 user 表，我们给 name 和 age 建立了一个联合索引 (name, age)。 ALTER TABLE user add INDEX comidx_name_phone (name,age); 联合索引在 B+ 树中按照从左到右的顺序依次建立搜索树，name 在左，age 在右。 当我们使用 where name ‘王二’ and age ‘20’ 去查询的时候， B+ 树会优先比较 name 来确定下一步应该搜索的方向，往左还是往右。 如果 name 相同的时候再比较 age。 但如果查询条件没有 name，就不知道应该怎么查了，因为 name 是 B+树中的前置条件，没有 name，索引就派不上用场了。 联合索引 (a, b)，where a 1 和 where b 1，效果是一样的吗不一样。 WHERE a = 1 能命中联合索引，因为 a 是联合索引的第一个字段，符合最左前缀匹配原则。而 WHERE b = 1 无法命中联合索引，因为缺少 a 的匹配条件，MySQL 会全表扫描。 —- 这部分是帮助理解 start，面试中可不背 —- 我们来验证一下，假设有一个 ab 表，建立了联合索引 (a, b)： CREATE TABLE ab ( a INT, b INT, INDEX ab_index (a, b)); 插入数据： INSERT INTO ab (a, b) VALUES (1, 2), (1, 3), (2, 1), (3, 3), (2, 2); 执行查询： 通过 explain 可以看到，WHERE a 1 使用了联合索引，而 WHERE b 1 需要全表扫描，依次检查每一行。 —- 这部分是帮助理解 end，面试中可不背 —- 假如有联合索引 abc，下面的 sql 怎么走的联合索引？select * from t where a = 2 and b = 2;select * from t where b = 2 and c = 2;select * from t where a 2 and b = 2; 第一条 SQL 语句包含条件 a 2 和 b 2，刚好符合联合索引的前两列。 第二条 SQL 语句由于未使用最左前缀中的 a，会触发全表扫描。 第三条 SQL 语句在范围条件 a 2 之后，索引后会停止匹配，b 2 的条件需要额外过滤。 (A,B,C) 联合索引 select * from tbn where a? and b in (?,?) and c? 会走索引吗？这个查询会命中联合索引，因为 a 是等值匹配，b 是 IN 等值多匹配，c 是 b 之后的范围条件，符合最左前缀原则。 对于 a?：这是一个精确匹配，并且是联合索引的第一个字段，所以一定会命中索引。 对于 b IN (?, ?)：等价于 b? OR b?，属于多值匹配，并且是联合索引的第二个字段，所以也会命中索引。 对于 c?：这是一个范围条件，属于联合索引的第三个字段，也会命中索引。 —- 这部分是帮助理解 start，面试中可不背 —- 来验证一下。 第一步，建表。 CREATE TABLE tbn (A INT, B INT, C INT, D TEXT); 第二步，创建索引。 CREATE INDEX idx_abc ON tbn (A, B, C); 第三步，插入数据。 INSERT INTO tbn VALUES (1, 2, 3, First);INSERT INTO tbn VALUES (1, 2, 4, Second);INSERT INTO tbn VALUES (1, 3, 5, Third);INSERT INTO tbn VALUES (2, 2, 3, Fourth);INSERT INTO tbn VALUES (2, 3, 4, Fifth); 第四步，执行查询。 EXPLAIN SELECT * FROM tbn WHERE A=1 AND B IN (2, 3) AND C3\\G 从 EXPLAIN 输出结果来看，我们可以得到 MySQL 是如何执行查询的一些关键信息： type: 查询类型，这里是 range，表示 MySQL 使用了范围查找，这是因为查询条件包含了 操作符。 possible_keys: 可能被用来执行查询的索引，这里是 idx_abc，表示 MySQL 认为 idx_abc 索引会用于查询优化。 key: 实际用来执行查询的索引，也是 idx_abc，这确定这条查询命中了联合索引。 Extra: 提供了关于查询执行的额外信息。Using index condition 表示 MySQL 使用了索引下推（Index Condition Pushdown，ICP），这是 MySQL 的一个优化方式，它允许在索引层面过滤数据。 —- 这部分是帮助理解 end，面试中可不背 —- 联合索引的一个场景题：(a,b,c)联合索引，(b,c)是否会走索引吗？根据最左前缀原则，(b,c) 查询不会走索引。 因为联合索引 (a,b,c) 中，a 是最左边的列，联合索引在创建索引树的时候需要先有 a，然后才会有 b 和 c。而查询条件中没有包含 a，所以 MySQL 无法利用这个索引。 EXPLAIN SELECT * FROM tbn WHERE B=1 AND C=1\\G 建立联合索引(a,b,c)，where c 5 是否会用到索引？为什么？不会。只有索引的第三列 c 被用作查询条件，而前两列 a 和 b 都没有被使用。这不符合最左前缀原则。 EXPLAIN SELECT * FROM tbn WHERE C=5\\G sql中使用like，如果遵循最左前缀匹配，查询是不是一定会用到索引？如果查询模式是后缀通配符 LIKE prefix%，且该字段有索引，优化器通常会使用索引。否则即便是遵循最左前缀匹配，LIKE 字段也无法命中索引。 如 age 18 and name LIKE ‘%xxx’，MySQL 会先使用联合索引 age_name 找到 age 符合条件的所有行，然后再全表扫描进行 name 字段的过滤。 type: ref 表示使用索引查找匹配某个值的所有行。 如果是后缀通配符，如 age = 18 and name LIKE xxx%，MySQL 会直接使用联合索引 age_name 找到所有符合条件的行。 type 为 range，表示 MySQL 使用了索引范围扫描，filtered 为 100.00%，表示在扫描的行中，所有的行都满足 WHERE 条件。 51.🌟什么是索引下推？索引下推是指：MySQL 把 WHERE 条件尽可能“下推”到索引扫描阶段，在存储引擎层提前过滤掉不符合条件的记录。 当查询条件包含索引列但未完全匹配时，ICP 会在存储引擎层过滤非索引列条件，以减少回表次数。 传统的查询流程是，存储引擎通过联合索引定位到符合最左前缀条件的主键 ID；回表读取完整数据行并返回给 Server 层；Server 层对所有返回的行进行 WHERE 条件过滤。 有了 ICP 后，存储引擎在索引层直接过滤可下推的条件，仅对符合索引条件的记录回表读取数据，再返回给 Server 层进行剩余条件过滤。 —- 这部分是帮助理解 start，面试中可不背 —- 例如有一张 user 表，建了一个联合索引（name, age），查询语句：select * from user where name like 张% and age=10;，没有索引下推优化的情况下： MySQL 会使用索引 name 找到所有 name like 张% 的主键，根据这些主键，一条条回表查询整行数据，并在 Server 层过滤掉不符合 age=10 的数据行。 启用 ICP 后，InnoDB 会通过联合索引直接筛选出符合条件的主键 ID（name like 张% and age=10），然后再回表查询整行数据。 换句话说，假设 name like ‘张%’ 找到 10000 行数据，age10 只有其中 10 行，没有索引下推的情况下，MySQL 会回表 10000 次，读取 10000 行数据，然后在 Server 层过滤掉 9990 行。 而有了索引下推后，MySQL 只会回表 10 次，读取 10 行数据。 我们来验证一下。 从结果中我们可以清楚地看到 ICP 的效果。ICP 开启时，Extra 列显示”Using index condition”，表明过滤条件被下推到存储引擎层。 ICP关闭时，Extra 列仅显示”Using where”，表明过滤条件在服务器层执行。 -- 开启ICPSET optimizer_switch=index_condition_pushdown=on;-- 清理状态FLUSH STATUS;SELECT Performance test with ICP ON as test_case;-- 执行查询并分析性能EXPLAIN ANALYZESELECT /*+ ICP_ON */ *FROM orders_mrr_testWHERE user_id BETWEEN 100 AND 200 AND order_date = 2023-01-01 AND order_date 2023-02-01 AND order_date NOT LIKE 2023-01-15%;-- 显示处理器状态SHOW STATUS LIKE Handler_read%;-- 关闭ICPSET optimizer_switch=index_condition_pushdown=off;-- 清理状态FLUSH STATUS;SELECT Performance test with ICP OFF as test_case;-- 执行相同的查询EXPLAIN ANALYZESELECT *FROM orders_mrr_testWHERE user_id BETWEEN 100 AND 200 AND order_date = 2023-01-01 AND order_date 2023-02-01 AND order_date NOT LIKE 2023-01-15%;-- 显示处理器状态SHOW STATUS LIKE Handler_read%; 实际的性能差距也很大。ICP 开启时，实际扫描行数：1,649 行，执行时间：约12.3 毫秒。关闭时，实际扫描行数：19,959 行，执行时间：约 32.1 毫秒。Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现。 52.如何查看是否用到了索引？（补充）可以通过 EXPLAIN 关键字来查看是否使用了索引。 EXPLAIN SELECT * FROM table WHERE column = value; 如果使用了索引，结果中的 key 值会显示索引的名称。 联合索引 abc，a1,c1b1,c1a1,c1,b1 走不走索引？ac 能用上索引，条件 a1 符合最左前缀原则，触发索引的第一列 a；由于跳过了中间列 b，c1 无法直接利用索引的有序性优化，但可通过索引下推在存储引擎层过滤 c 的条件，减少回表次数。 bc 无法使用索引，只能全表扫描，因为不符合最左前缀原则；acb 虽然顺序是乱的，但 MySQL 优化器会自动重排为 abc，所以能命中索引。 —- 这部分是帮助理解 start，面试中可不背 —- 我们通过实际的 SQL 来验证一下。 示例 1（a1,c1）： EXPLAIN SELECT * FROM tbn WHERE A=1 AND C=1\\G key 是 idx_abc，表明 a1,c1 会使用联合索引。Extra: Using index condition 表示 ICP 生效。 示例 2（b1,c1）： EXPLAIN SELECT * FROM tbn WHERE B=1 AND C=1\\G key 是 NULL，表明 b1,c1 不会使用联合索引。这是因为查询条件没有遵循最左前缀原则。 示例 3（a1,c1,b1）： EXPLAIN SELECT * FROM tbn WHERE A=1 AND C=1 AND B=1\\G 优化器会自动调整条件顺序为 a1 AND b1 AND c1。 key 是 idx_abc，表明 a1,c1,b1 会使用联合索引。 并且 rows1，因为 MySQL 优化器会自动重排查询条件，以满足最左前缀原则，直接使用联合索引找出 a1 AND b1 AND c1 的行。 锁53.🌟MySQL 中有哪几种锁？MySQL 中有多种类型的锁，可以从不同维度来分类，按锁粒度划分的话，有表锁、行锁。 按照加锁机制划分的话，有乐观锁和悲观锁。按照兼容性划分的话，有共享锁和排他锁。 —- 这部分是帮助理解 start，面试中可不背 —- 表锁：锁定整个表，资源开销小，加锁快，但并发度低，不会出现死锁；适合查询为主、少量更新的场景（如 MyISAM 引擎）。 再细分的话，有表共享读锁（S锁）：允许多个事务同时读，但阻塞写操作；表独占写锁（X锁）：独占表，阻塞其他事务的读写。 行锁：锁定单行或多行，开销大、加锁慢，可能出现死锁，但并发度高（InnoDB 默认支持）。 再细分的话，有记录锁（Record Lock）：锁定索引中的具体记录；间隙锁（Gap Lock）：锁定索引记录之间的间隙，防止幻读；临键锁（Next-Key Lock）：结合记录锁和间隙锁，锁定一个左开右闭的区间（如 (5, 10]）。 共享锁（S锁读锁），允许多个事务同时读取数据，但阻塞写操作。语法：SELECT ... LOCK IN SHARE MODE 排他锁（X锁写锁），独占数据，阻塞其他事务的读写。语法：SELECT ... FOR UPDATE。 乐观锁假设冲突少，通过版本号或 CAS 机制检测冲突（如 UPDATE SET version=version+1 WHERE version=old_version）。 悲观锁假设并发冲突频繁，先加锁再操作SELECT FOR UPDATE。—- 这部分是帮助理解 end，面试中可不背 —- 54.全局锁了解吗？（补充）全局锁就是对整个数据库实例进行加锁，当执行全局锁定操作时，整个数据库将会处于只读状态，所有写操作都会被阻塞，直到全局锁被释放。 在进行全库备份，或者数据迁移时，可以使用全局锁来保证数据的一致性。 在 MySQL 中，可以使用 FLUSH TABLES WITH READ LOCK 命令来获取全局锁。 执行该命令后，所有表将被锁定为只读状态。记得在完成备份或迁移后，使用 UNLOCK TABLES 命令释放全局锁。 -- 锁定整个数据库FLUSH TABLES WITH READ LOCK;-- 执行备份操作-- 例如使用 mysqldump 进行备份! mysqldump -u username -p database_name backup.sql-- 释放全局锁定UNLOCK TABLES; 表锁了解吗？了解。表锁常见于 MyISAM 引擎，InnoDB 也可以手动通过 LOCK TABLES 加锁。 适合读多写少、全表扫描或者表结构变更的场景用。 表锁又可以细分为共享锁和排他锁。共享锁允许多个事务同时读表，但不允许写操作。 LOCK TABLES table_name READ; -- 显式加读锁SELECT * FROM table_name; -- 其他会话可读，不可写UNLOCK TABLES; -- 释放锁 排他锁只允许一个事务进行写操作，其他事务不能读也不能写。 LOCK TABLES table_name WRITE; -- 显式加写锁INSERT/UPDATE/DELETE table_name; -- 其他会话读写均阻塞UNLOCK TABLES; MyISAM 在执行 SELECT 时会自动加读锁，执行 INSERTUPDATEDELETE 时会加写锁。 对于 InnoDB 引擎，无索引的 UPDATE/DELETE 可能会导致锁升级为表锁。 UPDATE innodb_table SET name=new WHERE name=old; -- 全表扫描，退化为表锁 执行 ALTER TABLE 时会自动加表锁，阻塞所有读写操作。 55.🌟说说 MySQL 的行锁？行锁是 InnoDB 存储引擎中最细粒度的锁，它锁定表中的一行记录，允许其他事务访问表中的其他行。 底层是通过给索引加锁实现的，这就意味着只有通过索引条件检索数据时，InnoDB 才能使用行级锁，否则会退化为表锁。 行锁又可以细分为记录锁、间隙锁和临键锁三种形式。通过 SELECT ... FOR UPDATE 可以加排他锁。 START TRANSACTION;-- 加排他锁，锁定某一行SELECT * FROM your_table WHERE id = 1 FOR UPDATE;-- 对该行进行操作UPDATE your_table SET column1 = new_value WHERE id = 1;COMMIT; 通过 SELECT ...LOCK IN SHARE MODE 可以加共享锁。 START TRANSACTION;-- 加共享锁，锁定某一行SELECT * FROM your_table WHERE id = 1 LOCK IN SHARE MODE;-- 只能读取该行，不能修改COMMIT; select for update 有什么需要注意的？第一，必须在事务中使用，否则锁会立即释放。 START TRANSACTION;SELECT * FROM your_table WHERE id = 1 FOR UPDATE;-- 对该行进行操作COMMIT; 第二，使用时必须注意是否命中索引，否则可能锁全表。 -- name 没有索引，会退化为表锁SELECT * FROM user WHERE name = 王二 FOR UPDATE; —- 这部分是帮助理解 start，面试中可不背 —- 假设有一张名为 orders 的表，包含以下数据： CREATE TABLE orders ( id INT PRIMARY KEY, order_no VARCHAR(255), amount DECIMAL(10,2), status VARCHAR(50), INDEX (order_no) -- order_no 上有索引); 表中的数据是这样的： 如果我们通过主键索引执行 SELECT FOR UPDATE，确实只会锁定特定的行： START TRANSACTION;SELECT * FROM orders WHERE id = 1 FOR UPDATE;-- 对 id=1 的行进行操作COMMIT; 由于 id 是主键，所以只会锁定 id1 这行，不会影响其他行的操作。其他事务依然可以对 id 2, 3, 4, 5 等行执行更新操作，因为它们没有被锁定。如果使用 order_no 这个普通索引执行 SELECT FOR UPDATE，也只会锁定特定的行： START TRANSACTION;SELECT * FROM orders WHERE order_no = 10001 FOR UPDATE;-- 对 order_no=10001 的行进行操作COMMIT; 因为 order_no 是唯一索引，所以只会锁定 order_no10001 这行，不会影响其他行的操作。 但如果 WHERE 条件是 status’pending’，而 status 上没有索引： START TRANSACTION;SELECT * FROM orders WHERE status = pending FOR UPDATE;-- 对 status=pending 的行进行操作COMMIT; 就会退化为表锁，因为在这种情况下，MySQL 需要全表扫描检查每一行的 status。 —- 这部分是帮助理解 end，面试中可不背 —- 说说记录锁吧？记录锁是行锁最基本的表现形式，当我们使用唯一索引或者主键索引进行等值查询时，MySQL 会为该记录自动添加排他锁，禁止其他事务读取或者修改锁定记录。 例如： SELECT * FROM table WHERE id = 1 FOR UPDATE; -- 加X锁UPDATE table SET name = 王二 WHERE id = 1; -- 隐式加X锁 间隙锁了解吗？间隙锁用于在范围查询时锁定记录之间的“间隙”，防止其他事务在该范围内插入新记录。仅在可重复读及以上的隔离级别下生效，主要用于防止幻读。 —- 这部分是帮助大家理解 start，面试中可不背 —- 例如事务 A 锁定了 (1000,2000) 区间，会阻止事务 B 在此区间插入新记录： -- 事务ABEGIN;SELECT * FROM orders WHERE amount BETWEEN 1000 AND 2000 FOR UPDATE;-- 事务B尝试插入会被阻塞INSERT INTO orders VALUES(null,1500,pending); -- 阻塞/code 假设表 test_gaplock 有 id、age、name 三个字段，其中 id 是主键，age 上有索引，并插入了 4 条数据。 CREATE TABLE `test_gaplock` ( `id` int(11) NOT NULL, `age` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`), KEY `age` (`age`)) ENGINE=InnoDB;insert into test_gaplock values(1,1,张三),(6,6,吴老二),(8,8,赵四),(12,12,熊大); 间隙锁会锁住： (−∞, 1)：最小记录之前的间隙。 (1, 6)、(6, 8)、(8, 12)：记录之间的间隙。 (12, +∞)：最大记录之后的间隙。 假设有两个事务，T1 执行以下语句： START TRANSACTION;SELECT * FROM test_gaplock WHERE age 5 FOR UPDATE; T2 执行以下语句： START TRANSACTION;INSERT INTO test_gaplock VALUES (7, 7, 王五); T1 会锁住 (6, 8) 的间隙，防止其他事务在这个范围内插入新记录。 T2 在插入 (7, 7, 王五) 时，会被阻塞，可以在另外一个会话中执行 SHOW ENGINE INNODB STATUS 查看到间隙锁的信息。 执行什么命令会加上间隙锁？在可重复读隔离级别下，执行FOR UPDATE / LOCK IN SHARE MODE等加锁语句，且查询条件是范围查询时，就会自动加上间隙锁。 -- SELECT ... FOR UPDATE + 范围查询SELECT * FROM user WHERE score 100 FOR UPDATE;-- SELECT ... LOCK IN SHARE MODE + 范围查询SELECT * FROM user WHERE id BETWEEN 10 AND 20 LOCK IN SHARE MODE;-- UPDATE/DELETE + 范围查询DELETE FROM user WHERE score 50; 56.临键锁了解吗？临键锁是记录锁和间隙锁的结合体，锁住的是索引记录和索引记录之间的间隙。 小徐先生的编程世界：临键锁 和间隙锁不同，临键锁的间隙是一个左开右闭区间。例如 (1,3] 表示锁定大于 1 且小于等于 3 的所有记录。 当 InnoDB 执行一个范围查询时，会使用临键锁来锁定满足条件的行数据以及该范围内的间隙。 IServise：临键锁 比如说下面这条语句会锁定 id 在 5 到 10 之间的所有记录，以及这些记录之间的间隙。 SELECT * FROM table WHERE id BETWEEN 5 AND 10 FOR UPDATE; MySQL 默认的行锁类型就是临键锁。当使用唯一索引的等值查询匹配到一条记录时，临键锁会退化成记录锁；如果没有匹配到任何记录，会退化成间隙锁。 57.意向锁是什么知道吗？意向锁是一种表级锁，表示事务打算对表中的某些行数据加锁，但不会直接锁定数据行本身。 由 InnoDB 自动管理，当事务需要添加行锁时，会先在表上添加意向锁。这样当要添加表锁的时候，可以通过查看表上的意向锁，快速判断是否有冲突，而无需逐行检查，从而提高加锁效率。 三分恶面渣逆袭：意向锁 当执行 SELECT ... LOCK IN SHARE MODE 时，会自动加意向共享锁；当执行 SELECT ... FOR UPDATE 时，会自动加意向排他锁。 意向锁之间互相兼容，也不会与行锁冲突。 兼容关系 意向共享锁 意向排他锁 共享锁(表级) 排他锁(表级) 意向共享锁 兼容 兼容 兼容 冲突 意向排他锁 兼容 兼容 冲突 冲突 S锁 兼容 冲突 兼容 冲突 X锁 冲突 冲突 冲突 冲突 意向锁的意义是什么？在没有意向锁的情况下，当事务 A 持有某表的行锁时，如果事务 B 想添加表锁，InnoDB 必须检查表中每一行数据是否被加锁，这种全表扫描的方式效率极低。 IServise：意向锁 有了意向锁之后，事务在加行锁前，先在表上加对应的意向锁；其他事务加表锁时，只需检查表上的意向锁，无需逐行检查。 -- 事务A获取某行的排他锁BEGIN;SELECT * FROM users WHERE id = 6 FOR UPDATE; -- 自动加IX锁和行X锁-- 事务B尝试加表锁LOCK TABLES users READ; -- 发现表上有IX锁，与S锁冲突，直接阻塞而无需扫描全表 58.🌟MySQL的乐观锁和悲观锁了解吗？悲观锁是一种”先上锁再操作”的保守策略，它假设数据被外界访问时必然会产生冲突，因此在数据处理过程中全程加锁，保证同一时间只有一个线程可以访问数据。 牧小农：悲观锁 MySQL 中的行锁和表锁都是悲观锁。 牧小农：悲观锁的处理思路 乐观锁会假设并发操作不会总发生冲突，属于小概率事件，因此不会在读取数据时加锁，而是在提交更新时才检查数据是否被其他事务修改过。 牧小农：乐观锁 乐观锁并不是 MySQL 内置的锁机制，而是通过程序逻辑实现的，常见的实现方式有版本号机制和时间戳机制。通过在表中增加 version 字段或者 timestamp 字段来实现。 -— 这部分是帮助大家理解 start，面试中可不背 —- 当事务 A 已经上锁后，事务 B 会一直等待事务 A 释放锁；如果事务 A 长时间不释放锁，事务 B 就会报错 Lock wait timeout exceeded; try restarting transaction。 牧小农：的实现方式 事务 A 和事务 B 同时读取同一个主键 ID 的数据，版本号为 0；事务 A 将版本号（version1）作为条件进行数据更新，同时版本号 +1；事务 B 也将 version1 作为更新条件，发现版本号不匹配，更新失败。 牧小农：乐观锁的实现方式 -— 这部分是帮助大家理解 end，面试中可不背 —- 如何通过悲观锁和乐观锁解决库存超卖问题？悲观锁通过 SELECT ... FOR UPDATE 在查询时直接锁定记录，确保其他事务必须等待当前事务完成才能操作该行数据。 BEGIN;-- 对id=1的商品记录加排他锁SELECT stock FROM products WHERE id=1 FOR UPDATE;-- 生成订单INSERT INTO orders (user_id, product_id) VALUES (123, 1);-- 扣减库存UPDATE products SET stock=stock-1 WHERE id=1;COMMIT; 乐观锁通过在表中增加 version 字段作为判断条件。 -- 查询商品信息，获取版本号SELECT stock, version FROM products WHERE id=1;-- 更新库存时检查版本号UPDATE products SET stock=stock-1, version=version+1 WHERE id=1 AND version=旧版本号; -— 这部分是帮助大家理解 start，面试中可不背 —- 库存超卖是一个非常经典的问题： 事务A查询商品库存，得到库存值为1 事务B也查询同一商品库存，同样得到库存值为1 事务A基于查询结果执行库存扣减，将库存更新为0 事务B也执行库存扣减，将库存更新为-1 悲观锁的关键点： 必须在一个事务中执行； 通过 SELECT ... FOR UPDATE 锁定行，确保其他事务必须等待当前事务完成才能操作该行数据； 记得给查询条件加索引，避免全表扫描导致锁升级为表锁。 乐观锁的关键点： 在表中增加 version 字段； 查询时获取当前版本号； 更新时检查版本号是否发生了变化。 Java 程序的完整代码示例： @Servicepublic class ProductService @Autowired private ProductMapper productMapper; @Transactional public boolean purchaseWithOptimisticLock(Long productId, int quantity) int retryCount = 0; while(retryCount 3) // 最大重试次数 Product product = productMapper.selectById(productId); if(product.getStock() quantity) return false; // 库存不足 int updated = productMapper.reduceStockWithVersion( productId, quantity, product.getVersion()); if(updated 0) return true; // 更新成功 retryCount++; return false; // 更新失败 对应的 mapper： @Update(UPDATE products SET stock=stock-#quantity, version=version+1 + WHERE id=#productId AND version=#version)int reduceStockWithVersion(@Param(productId) Long productId, @Param(quantity) int quantity, @Param(version) int version); 时间戳机制实现的乐观锁： UPDATE products SET stock=stock-1, update_time=NOW() WHERE id=1 AND update_time=旧时间戳; 这两种方式都需要保证操作的原子性，需要将多个 SQL 放在同一个事务中执行。 推荐阅读：牧小农：悲观锁和乐观锁 -— 这部分是帮助大家理解 end，面试中可不背 —- 59.遇到过MySQL死锁问题吗，你是如何解决的？遇到过。MySQL 的死锁是由于多个事务持有资源并相互等待引起的。我通过 SHOW ENGINE INNODB STATUS 查看死锁信息，定位到是加锁顺序不一致导致的，最后通过调整加锁顺序解决了这个问题。 draven.co：死锁的发生 比如说技术派项目中，两个事务分别更新两张表，但是更新顺序不一致。 -- 创建表/插入数据CREATE TABLE account ( id INT AUTO_INCREMENT PRIMARY KEY, balance INT NOT NULL);INSERT INTO account (balance) VALUES (100), (200);-- 事务 1START TRANSACTION;-- 锁住 id=1 的行UPDATE account SET balance = balance - 10 WHERE id = 1;-- 等待锁住 id=2 的行（事务 2 已锁住）UPDATE account SET balance = balance + 10 WHERE id = 2;-- 事务 2START TRANSACTION;-- 锁住 id=2 的行UPDATE account SET balance = balance - 10 WHERE id = 2;-- 等待锁住 id=1 的行（事务 1 已锁住）UPDATE account SET balance = balance + 10 WHERE id = 1; 访问相同的资源，但顺序不同，就会导致死锁。 二哥的 Java 进阶之路：死锁 解决办法也很简单，先使用 SHOW ENGINE INNODB STATUS\\G; 确认死锁的具体信息，然后调整资源的访问顺序。 二哥的 Java 进阶之路：查看死锁 事务60.🌟MySQL事务的四大特性说一下？事务是一条或多条 SQL 语句组成的执行单元。四个特性分别是原子性、一致性、隔离性和持久性。原子性保证事务中的操作要么全部执行、要么全部失败；一致性保证数据从事务开始前的一个一致状态转移到结束后的另外一个一致状态；隔离性保证并发事务之间互不干扰；持久性保证事务提交后数据不会丢失。 北野新津：ACID 详细说一下原子性？原子性意味着事务中的所有操作要么全部完成，要么全部不完成，它是不可分割的单位。如果事务中的任何一个操作失败了，整个事务都会回滚到事务开始之前的状态，如同这些操作从未被执行过一样。 START TRANSACTION;UPDATE accounts SET balance = balance - 100 WHERE user_id = 1;UPDATE accounts SET balance = balance + 100 WHERE user_id = 2;-- 如果第二条语句失败，第一条也会回滚COMMIT; 简短回答：原子性要求事务的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务中的操作不能只执行其中一部分。 详细说一下一致性？一致性确保事务从一个一致的状态转换到另一个一致的状态。 比如在银行转账事务中，无论发生什么，转账前后两个账户的总金额应保持不变。假如 A 账户（100 块）给 B 账户（10 块）转了 10 块钱，不管成功与否，A 和 B 的总金额都是 110 块。 -- 假设 A 账户余额为 100，B 账户余额为 10-- 转账前状态SELECT balance FROM accounts WHERE user_id = A; -- 100SELECT balance FROM accounts WHERE user_id = B; -- 10-- 转账操作START TRANSACTION;UPDATE accounts SET balance = balance - 10 WHERE user_id = A;UPDATE accounts SET balance = balance + 10 WHERE user_id = B;COMMIT;-- 转账后状态SELECT balance FROM accounts WHERE user_id = A; -- 90SELECT balance FROM accounts WHERE user_id = B; -- 20`-- 总金额仍然是 110 简短回答：一致性确保数据的状态从一个一致状态转变为另一个一致状态。一致性与业务规则有关，比如银行转账，不论事务成功还是失败，转账双方的总金额应该是不变的。 详细说一下隔离性？隔离性意味着并发执行的事务是彼此隔离的，一个事务的执行不会被其他事务干扰。事务之间是井水不犯河水的。 隔离性主要是为了解决事务并发执行时可能出现的脏读、不可重复读、幻读等问题。 -— 这部分是帮助大家理解 start，面试中可不背 —- 比如说在读未提交的隔离级别下，会出现脏读现象：一个事务C 读取了事务B 尚未提交的修改数据。如果事务B 最终回滚，事务C 读取的数据就是无效的“脏数据”。 -- 会话 A-- 创建模拟并发的测试表DROP TABLE IF EXISTS accounts;CREATE TABLE accounts ( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(50), balance DECIMAL(10,2));-- 插入测试数据INSERT INTO accounts (name, balance) VALUES(王二, 1000.00),(张三, 2000.00),(李四, 3000.00);-- 会话B 中，设置隔离级别为读未提交SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;START TRANSACTION;-- 在会话 B 中更新数据但不提交UPDATE accounts SET balance = balance - 500 WHERE name=王二;-- 会话C 是读为提交级别，读取数据，得到 500SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT * FROM accounts WHERE name=王二;-- 继续别的操作，基于 500-- 会话 B 的事务回滚，导致会话 A 读到的数据其实是脏数据ROLLBACK; 二哥的 Java 进阶之路：读未提交下出现脏读 通过升级隔离级别为读已提交可以解决脏读的问题。 -- 会话 B 修改为读已提交SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;-- 执行第一次查询 1000SELECT * FROM accounts WHERE name=王二;-- 会话 C 中，设置隔离级别为读已提交SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;-- 在会话 C 中更新数据但不提交START TRANSACTION;UPDATE accounts SET balance = balance + 200 WHERE name=王二;-- 会话 B 中再次读取数据，结果仍然为 1000SELECT * FROM accounts WHERE name=王二;-- 会话 C 中回滚事务ROLLBACK;-- 会话 B 中再次读取数据，结果仍然为 1000SELECT * FROM accounts WHERE name=王二; 二哥的 Java 进阶之路：读已提交可以解决脏读问题 但会出现不可重复读的问题：事务B 第一次读取某行数据值为X，期间事务C修改该数据为Y并提交，事务B 再次读取时发现值变为Y，导致两次读取结果不一致。 -- 会话 B 修改为读已提交SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;-- 执行第一次查询 1000START TRANSACTION;SELECT * FROM accounts WHERE name=王二;-- 会话 C 中，设置隔离级别为读已提交SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;-- 在会话 C 中更新数据并提交START TRANSACTION;UPDATE accounts SET balance = balance + 200 WHERE name=王二;-- 会话 C 提交事务COMMIT;-- 会话 B 中再次读取数据，结果仍然为 1200SELECT * FROM accounts WHERE name=王二; 二哥的 Java 进阶之路：读已提交会出现不可重复读的问题 可以通过升级隔离级别为可重复读来解决不可重复读的问题。 -- 会话 B 修改为可重复读SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;-- 开始事务并执行第一次查询 1000START TRANSACTION;SELECT * FROM accounts WHERE name=王二;-- 会话 C 中，设置隔离级别为可重复读SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;-- 在会话 C 中更新数据并提交START TRANSACTION;UPDATE accounts SET balance = balance + 200 WHERE name=王二;-- 会话 C 提交事务COMMIT;-- 会话 B 中再次读取数据，结果仍然为 1000SELECT * FROM accounts WHERE name=王二; 二哥的 Java 进阶之路：可重复读级别解决不可重复读的问题 但可重复读级别下仍然会出现幻读的问题：事务B 第一次查询获得 2条数据，事务C 新增 1条数据并提交后，事务B 再次查询时仍然为 2 条数据，但可以更新新增的数据，再次查询时就发现有 3 条数据了。 -- 会话 B 修改为可重复读SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;-- 执行第一次查询，查到 2 条记录START TRANSACTION;SELECT * FROM accounts WHERE balance 1000;-- 会话 C 中，设置隔离级别为可重复读SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;-- 在会话 C 中新增数据并提交START TRANSACTION;INSERT INTO accounts (name, balance) VALUES (王五, 4000);-- 会话 C 提交事务COMMIT;-- 会话 B 中再次读取数据，结果仍然为 2 条SELECT * FROM accounts WHERE balance 1000;-- 会话 B 中尝试更新王五的余额为 5000，竟然成功了UPDATE accounts SET balance = 5000 WHERE name=王五;-- 会话 B 中再次读取数据，发现 3 条记录SELECT * FROM accounts WHERE balance 1000; 二哥的 Java 进阶之路：可重复读级别下可能出现幻读 可以通过升级隔离级别为串行化来解决幻读的问题。 -- 会话 B 修改为可串行化SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;-- 执行第一次查询，查到 2 条记录START TRANSACTION;SELECT * FROM accounts WHERE balance 1000;-- 会话 C 中，设置隔离级别为可串行化SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;-- 在会话 C 中新增数据，会卡住START TRANSACTION;INSERT INTO accounts (name, balance) VALUES (王五, 4000);-- 只有等会话 B 提交事务后会话 C 才会继续执行并提交事务COMMIT; 二哥的 Java 进阶之路：串行化隔离级别下不会出现幻读问题 隔离级别 是否会脏读 是否会不可重复读 是否会幻读 Read Uncommitted（读未提交） ✅ 可能 ✅ 可能 ✅ 可能 Read Committed（读已提交） ❌ 不会 ✅ 可能 ✅ 可能 Repeatable Read（可重复读） ❌ 不会 ❌ 不会 ✅ 可能（但 InnoDB 已解决） Serializable（可串行化） ❌ 不会 ❌ 不会 ❌ 不会 -— 这部分是帮助大家理解 end，面试中可不背 —- 简短回答：多个并发事务之间需要相互隔离，即一个事务的执行不能被其他事务干扰。 详细说一下持久性？持久性确保事务一旦提交，它对数据所做的更改就是永久性的，即使系统发生崩溃，数据也能恢复到最近一次提交的状态。 MySQL 的持久性是通过 InnoDB 引擎的 redo log 实现的。在事务提交时，InnoDB 会先将修改操作写入 redo log，并刷盘持久化。崩溃后，InnoDB 会通过 redo log 恢复数据，从而保证事务提交成功的数据不会丢失。 Mayank Sharma：可持久化 简短回答：一旦事务提交，则其所做的修改将永久保存到 MySQL 中。即使发生系统崩溃，修改的数据也不会丢失。 61.ACID 靠什么保证的呢？一句话总结： ACID 中的原子性主要通过 Undo Log 来实现，持久性通过 Redo Log 来实现，隔离性由 MVCC 和锁机制来实现，一致性则由其他三大特性共同保证。 ACID 的保证机制 详细说说如何保证原子性？事务对数据进行修改前，会记录一份快照到 Undo Log，如果事务中有任何一步执行失败，系统会读取 Undo Log 将所有操作回滚，恢复到事务开始前的状态，从而保证事务要么全部成功，要么全部失败。 小许 code：undo log保证原子性 1）BEGIN;2）UPDATE user SET balance = balance - 100 WHERE id = 1; = 写入 Undo Log：记录 id=1 的原始余额 5003）UPDATE user SET balance = balance + 100 WHERE id = 2; = 写入 Undo Log：记录 id=2 的原始余额 3004）COMMIT; = 清空 Undo Log，事务成功❗如果失败： = 执行 ROLLBACK：根据 Undo Log 把数据还原！ 推荐阅读：庖丁解InnoDB之UNDO LOG 详细说说如何保证持久性？MySQL 的持久性主要由预写 Redo Log、双写机制、两阶段提交以及 Checkpoint 刷盘机制共同保证。 当事务提交时，MySQL 会先将事务的修改操作写入 Redo Log，并强制刷盘，然后再将内存中的数据页刷入磁盘。这样即使系统崩溃，重启后也能通过 Redo Log 重放恢复数据。 小许 code：redo log 的 WAL，Write-Ahead Logging 在将数据页写入到磁盘时，如果发生崩溃，可能会导致数据页不完整。InnoDB 的数据页大小为16KB，通常大于操作系统的 4KB页大小。 为了解决只写入部分的问题，MySQL 采用了双写机制，脏盘刷页时，先将数据页写入到一个双写缓冲区中，2M 的连续空间，然后再将其写入到磁盘的实际位置。 BookSea：Doublewrite 崩溃恢复时，如果发现数据页不完整，会从双写缓冲区中恢复副本，确保数据页的完整性。如果双写缓存区中的页也不完整的话,会从 Redo Log 中恢复。 在涉及主从复制时，MySQL 通过两阶段提交保证 Redo Log 和 Binlog 的一致性：第一阶段，写入 Redo Log 并标记为 prepare 状态；第二阶段，写入 Binlog 再提交 Redo Log 为 commit 状态。 一树一溪：2PC 崩溃恢复时，如果发现 Redo Log 是 prepare 但 Binlog 完整，则会提交事务；反之会回滚，避免主从不一致。 另外，由于 Redo Log 的容量有限，Checkpoint 机制会定期将内存中的脏页刷到磁盘，这样能减少崩溃恢复时需要处理的 Redo Log 数量。 小许 code：Checkpoint 推荐阅读：深入解析MySQL双写缓冲区、MySQL 事务二阶段提交 详细说说如何保证隔离性？隔离性主要通过锁机制和 MVCC 来实现。 比如说一个事务正在修改某条数据时，MySQL 会通过临键锁来防止其他事务同时进行修改，避免数据冲突。 阿里云社区：临键锁 同时，临键锁可以防止幻读现象的发生。比如事务 A 查询 id 10 的记录，那么临键锁不仅会锁住 id10 的行，还会锁住 10 后面的“间隙”，防止其他事务插入 id15 的数据。 假如表中的主键有 id: 5, 10, 15, 20, 25，那么 InnoDB 会对以下区间和记录加锁： 加锁对象 类型 锁定含义 (10, 15] 临键锁 锁住 id15 和前间隙，防止插入11~14 (15, 20] 临键锁 锁住了 id20 和前间隙 (20, 25] 临键锁 锁住了 id25 和前间隙 (25, +∞) 间隙锁 锁住尾部防止插入30等 MVCC 主要用来优化读操作，通过保存数据的历史版本，让读操作不需要加锁就能直接读取快照，提高读的并发性能。 小余哥：ReadView 不同的隔离级别对应不同的实现策略，比如说在可重复读隔离级别下，事务第一次查询时会生成一个 Read View，之后所有读操作都复用这个视图，保证多次读取的结果一致。 如何保证一致性呢？MySQL 的一致性并不是靠某一个机制单独保证的，而是原子性、隔离性和持久性协同作用的结果。 事务会不会自动提交？是的，MySQL 默认开启了事务自动提交模式。 每条单独的 SQL 语句都会被视为一个独立的事务处理单元；SQL 语句执行成功后会自动执行 COMMIT；执行失败时会自动 ROLLBACK。 可通过 SELECT @@autocommit; 查看当前会话的自动提交状态。 二哥的 Java 进阶之路：@@autocommit 如果需要执行多条 SQL 语句，可以将它们放在一个事务中，使用 START TRANSACTION 开启事务，执行完所有 SQL 语句后手动提交。 START TRANSACTION;UPDATE accounts SET balance = balance - 100 WHERE user_id = 1;UPDATE accounts SET balance = balance + 100 WHERE user_id = 2;COMMIT; 62.🌟事务的隔离级别有哪些？隔离级别定义了一个事务可能受其他事务影响的程度，MySQL 支持四种隔离级别，分别是：读未提交、读已提交、可重复读和串行化。 draven.co：事务的四个隔离级别 读未提交会出现脏读，读已提交会出现不可重复读，可重复读是 InnoDB 默认的隔离级别，可以避免脏读和不可重复读，但会出现幻读。不过通过 MVCC 和临键锁，能够防止大多数并发问题。 串行化最安全，但性能较差，通常不推荐使用。 详细说说读未提交？事务可以读取其他未提交事务修改的数据。也就是说，如果未提交的事务一旦回滚，读取到的数据就会变成了“脏数据”，通常不会使用。 易尘埃：读未提交 什么是读已提交？读已提交避免了脏读，但可能会出现不可重复读，即同一事务内多次读取同一数据结果会不同，因为其他事务提交的修改，对当前事务是可见的。 易尘埃：读已提交 是 Oracle、SQL Server 等数据库的默认隔离级别。 什么是可重复读？可重复读能确保同一事务内多次读取相同数据的结果一致，即使其他事务已提交修改。 易尘埃：可重复读 是 MySQL 默认的隔离级别，避免了“脏读”和“不可重复读”，通过 MVCC 和临键锁也能在一定程度上避免幻读。 -- Session A:START TRANSACTION;SELECT balance FROM accounts WHERE id=1; --返回500-- Session B:UPDATE accounts SET balance = balance +100 WHERE id=1;COMMIT;-- Session A再次查询:SELECT balance FROM accounts WHERE id=1; --仍返回500(可重复读)-- Session A更新后查询:UPDATE accounts SET balance = balance +50 WHERE id=1; --基于最新值550更新为600 SELECT balance FROM accounts WHERE id=1; --返回600 什么是串行化？串行化是最高的隔离级别，通过强制事务串行执行来解决“幻读”问题。 易尘埃：串行化 但会导致大量的锁竞争问题，实际应用中很少用。 A 事务未提交，B 事务上查询到的是旧值还是新值？如果 B 是普通的 SELECT，也就是快照读，它读的是旧值，即事务 A 修改前的快照，并且不会阻塞；如果 B 是当前读，比如 SELECT … FOR UPDATE，它会被阻塞直到事务 A 提交或回滚。 -- 会话 A 中，更新王二的余额START TRANSACTION;UPDATE accounts SET balance = 8000 WHERE name = 王二;-- 此时并没有 COMMIT-- 会话 B 中查询王二的余额SELECT * FROM accounts WHERE name = 王二;-- 会话 B 会读取到 旧值 1000-- 会话 C 中使用当前读查询王二的余额SELECT * FROM accounts WHERE name = 王二 FOR UPDATE;-- 会话 C 会被阻塞，直到会话 A 提交或回滚 二哥的 Java 进阶之路：快照读和当前读的差别 怎么更改事务的隔离级别？MySQL 支持通过 SET 语句修改事务隔离级别，包括全局级别、当前会话，但一般不建议在生产环境中随意修改隔离级别。 测试环境下可以使用 SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; 可以修改当前会话的隔离级别。 使用 SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED; 可以修改全局隔离级别，影响新的连接，但不会改变现有会话。 63.事务的隔离级别是如何实现的？读未提交通过行锁共享锁确保一个事务在更新行数据但没有提交的情况下，其他事务不能更新该行数据，但不会阻止脏读，意味着事务2 可以在事务1 提交之前读取到事务1 修改的数据。 allaroundjava：Read uncommitted 读已提交会在更新数据前加行级排他锁，不允许其他事务写入或者读取未提交的数据，也就意味着事务2 不能在事务 1 提交之前读取到事务1 修改的数据，从而解决脏读的问题。 allaroundjava：Read committed 另外，读已提交会在每次读取数据前都生成一个新的 ReadView，所以会出现不可重复读的问题。 可重复读只在第一次读操作时生成 ReadView，后续读操作都会使用这个 ReadView，从而避免不可重复读的问题。 另外，对于当前读操作，可重复读会通过临键锁来锁住当前行和前间隙，防止其他事务在这个范围内插入数据，从而避免幻读的问题。 allaroundjava：Repeatable read 串行化级别下，事务在读操作时，会先加表级共享锁；在写操作时，会先加表级排他锁。 直到事务结束后才释放锁，这样就能确保事务之间不会相互干扰。 64.🌟请详细说说幻读呢？幻读是指在同一个事务中，多次执行相同的范围查询，结果却不同。这种现象通常发生在其他事务在两次查询之间插入或删除了符合当前查询条件的数据。 Jenny：Phantom read -— 这部分是帮助大家理解 start，面试中可以不背 —- 比如说事务 A 在第一次查询某个条件范围的数据行后，事务 B 插入了一条新数据且符合条件范围，事务 A 再次查询时，发现多了一条数据。 我们来验证一下，先创建测试表，插入测试数据。 CREATE TABLE `user_info` ( `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT 主键id, `name` VARCHAR(32) NOT NULL DEFAULT COMMENT 姓名, `gender` VARCHAR(32) NOT NULL DEFAULT COMMENT 性别, `email` VARCHAR(32) NOT NULL DEFAULT COMMENT 邮箱, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT=用户信息表;-- 插入测试数据INSERT INTO `user_info` (`id`, `name`, `gender`, `email`) VALUES (1, Curry, 男, curry@163.com), (2, Wade, 男, wade@163.com), (3, James, 男, james@163.com);COMMIT; 然后我们在事务 A 中执行查询 SELECT * FROM user_info WHERE id 1;，在事务 B 中插入数据 INSERT INTO user_info (name, gender, email) VALUES (wanger, 女, wanger@163.com);，再在事务 A 中修改刚刚插入的数据 update user_info set gender=男 where id = 4;，最后在事务 A 中再次查询 SELECT * FROM user_info WHERE id 1;。 二哥的 Java 进阶之路：可以发现产生幻读了 -— 这部分是帮助大家理解 end，面试中可以不背 —- 如何避免幻读？MySQL 在可重复读隔离级别下，通过 MVCC 和临键锁可以在一定程度上避免幻读。 比如说在查询时显示加锁，利用临键锁锁定查询范围，防止其他事务插入新的数据。 START TRANSACTION;SELECT * FROM user_info WHERE id 1 FOR UPDATE; -- 加临键锁COMMIT; 其他事务在插入数据时，会被阻塞，直到当前事务提交或回滚。 二哥的 Java 进阶之路：临键锁能防止幻读 -— 这部分是帮助大家理解 start，面试中可以不背 —- 解释一下。 如果查询语句中包含显式加锁（如 FOR UPDATE），InnoDB 会使用当前读，直接读取最新的数据，并加锁。 在范围查询时，InnoDB 不仅会对符合条件的记录加行锁，还会对相邻的索引间隙加间隙锁，从而形成临键锁。 转转技术：临键锁 临键锁可以防止其他事务在间隙中插入新数据，从而避免幻读。 -— 这部分是帮助大家理解 end，面试中可以不背 —- 比如说在执行查询的事务中，不要尝试去更新其他事务插入删除的数据，利用快照读来避免幻读。 二哥的 Java 进阶之路：只用快照读 -— 这部分是帮助大家理解 start，面试中可以不背 —- 使用 SELECT 查询时，如果没有显式加锁，InnoDB 会使用 MVCC 提供一致性视图。 每个事务在启动时都会生成一个 Read View，用来确定哪些数据对当前事务可见。 Keep It Simple：Read View 其他事务在当前事务启动后插入的新数据不会被当前事务看到，因此不会出现幻读。 -— 这部分是帮助大家理解 end，面试中可以不背 —- 什么是当前读呢？当前读是指读取记录的最新已提交版本，并且在读取时对记录加锁，确保其他并发事务不能修改当前记录。 比如 SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE，以及 UPDATE、DELETE，都属于当前读。 为什么 UPDATE 和 DELETE 也属于当前读？因为更新、删除这些操作，本质上不仅是写操作，还需要在写之前读取数据，然后才能修改或删除。为了保证修改的是最新的数据，并防止并发冲突，InnoDB 必须读取最新版本的数据并加锁，因此 UPDATE 和 DELETE 也属于当前读。 溪水静幽：当前读 SQL语句 是否当前读 是否加锁 SELECT * FROM user WHERE id=1 ❌ 否 ❌ 否 SELECT * FROM user WHERE id=1 FOR UPDATE ✅ 是 ✅ 加排他锁 SELECT * FROM user WHERE id=1 LOCK IN SHARE MODE ✅ 是 ✅ 加共享锁 UPDATE user SET ... WHERE id=1 ✅ 是 ✅ 加排他锁 DELETE FROM user WHERE id=1 ✅ 是 ✅ 加排他锁 什么是快照读呢？快照读是 InnoDB 通过 MVCC 实现的一种非阻塞读方式。当事务执行 SELECT 查询时，InnoDB 并不会直接读当前最新的数据，而是根据事务开始时生成的 Read View 去判断每条记录的可见性，从而读取符合条件的历史版本。 爱吃鱼饼的猫：快照读 SQL 是否快照读？ 说明 SELECT * FROM t WHERE id=1 ✅ 是 快照读 SELECT * FROM t WHERE id=1 FOR UPDATE ❌ 否 当前读，读取最新版本并加锁 UPDATE / DELETE ❌ 否 当前读，必须读取当前版本并加锁 INSERT ❌ 否 写操作，不存在历史版本 65.🌟MVCC 了解吗？MVCC 指的是多版本并发控制，每次修改数据时，都会生成一个新的版本，而不是直接在原有数据上进行修改。并且每个事务只能看到在它开始之前已经提交的数据版本。 天瑕：undo log 版本链和 ReadView 这样的话，读操作就不会阻塞写操作，写操作也不会阻塞读操作，从而避免加锁带来的性能损耗。 其底层实现主要依赖于 Undo Log 和 Read View。 每次修改数据前，先将记录拷贝到Undo Log，并且每条记录会包含三个隐藏列，DB_TRX_ID 用来记录修改该行的事务 ID，DB_ROLL_PTR 用来指向 Undo Log 中的前一个版本，DB_ROW_ID 用来唯一标识该行数据（仅无主键时生成）。 guozhchun：额外的存储信息 每次读取数据时，都会生成一个 ReadView，其中记录了当前活跃事务的 ID 集合、最小事务 ID、最大事务 ID 等信息，通过与 DB_TRX_ID 进行对比，判断当前事务是否可以看到该数据版本。 luozhiyun：ReadView 请详细说说什么是版本链？版本链是指 InnoDB 中同一条记录的多个历史版本，通过 DB_ROLL_PTR 字段将它们像链表一样串起来，用来支持 MVCC 的快照读。 二哥的 Java 进阶之路：版本链 假设有一张hero表，表中有这样一行记录，name 为张三，city 为帝都，插入这行记录的事务 id 是 80。 此时，DB_TRX_ID的值就是 80，DB_ROLL_PTR的值就是指向这条 insert undo 日志的指针。 三分恶面渣逆袭：DB_ROLL_PTR 接下来，如果有两个DB_TRX_ID分别为100、200的事务对这条记录进行了update操作，那么这条记录的版本链就会变成下面这样： 三分恶面渣逆袭：update 操作 也就是说，当更新一行数据时，InnoDB 不会直接覆盖原有数据，而是创建一个新的数据版本，并更新 DB_TRX_ID 和 DB_ROLL_PTR，使它们指向前一个版本和相关的 undo 日志。 这样，老版本的数据就不会丢失，可以通过版本链找到。 由于 undo 日志会记录每一次的 update，并且新插入的行数据会记录上一条 undo 日志的指针，所以可以通过 DB_ROLL_PTR 这个指针找到上一条记录，这样就形成了一个版本链。 三分恶面渣逆袭：版本链 请详细说说什么是ReadView？ReadView 是 InnoDB 为每个事务创建的一份“可见性视图”，用于判断在执行快照读时，哪些数据版本是当前这个事务可以看到的，哪些不能看到。 二哥的 Java 进阶之路：ReadView 当事务开始执行时，InnoDB 会为该事务创建一个 ReadView，这个 ReadView 会记录 4 个重要的信息： creator_trx_id：创建该 ReadView 的事务 ID。 m_ids：所有活跃事务的 ID 列表，活跃事务是指那些已经开始但尚未提交的事务。 min_trx_id：所有活跃事务中最小的事务 ID。它是 m_ids 数组中最小的事务 ID。 max_trx_id ：事务 ID 的最大值加一。换句话说，它是下一个将要生成的事务 ID。 ReadView 是如何判断记录的某个版本是否可见的？会通过三个步骤来判断： 二哥的 Java 进阶之路：ReadView判断规则 ①、如果某个数据版本的 DB_TRX_ID 小于 min_trx_id，则该数据版本在生成 ReadView 之前就已经提交，因此对当前事务是可见的。 ②、如果 DB_TRX_ID 大于 max_trx_id，则表示创建该数据版本的事务在生成 ReadView 之后开始，因此对当前事务不可见。 ③、如果 DB_TRX_ID 在 min_trx_id 和 max_trx_id 之间，需要判断 DB_TRX_ID 是否在 m_ids 列表中： 不在，表示创建该数据版本的事务在生成 ReadView 之后已经提交，因此对当前事务也是可见的。 在，表示事务仍然活跃，或者在当前事务生成 ReadView 之后才开始，因此是不可见的。 小许 code：可见性匹配规则 举个实际的例子。 读事务开启了一个 ReadView，这个 ReadView 里面记录了当前活跃事务的 ID 列表（444、555、665），以及最小事务 ID（444）和最大事务 ID（666）。当然还有自己的事务 ID 520，也就是 creator_trx_id。 它要读的这行数据的写事务 ID 是 x，也就是 DB_TRX_ID。 如果 x 110，显然在 ReadView 生成之前就提交了，所以这行数据是可见的。 如果 x 667，显然是未知世界，所以这行数据对读操作是不可见的。 如果 x 519，虽然 519 大于 444 小于 666，但是 519 不在活跃事务列表里，所以这行数据是可见的。因为 519 是在 520 生成 ReadView 之前就提交了。 如果 x 555，虽然 555 大于 444 小于 666，但是 555 在活跃事务列表里，所以这行数据是不可见的。因为 555 不确定有没有提交。 可重复读和读已提交在 ReadView 上的区别是什么？可重复读：在第一次读取数据时生成一个 ReadView，这个 ReadView 会一直保持到事务结束，这样可以保证在事务中多次读取同一行数据时，读取到的数据是一致的。 程序员x：readview 在可重复读和读已提交下的不同 读已提交：每次读取数据前都生成一个 ReadView，这样就能保证每次读取的数据都是最新的。 推荐阅读：搞懂Mysql之InnoDB MVCC 如果两个 AB 事务并发修改一个变量，那么 A 读到的值是什么，怎么分析。事务 A 在读取时是否能读到事务 B 的修改，取决于 A 是快照读还是当前读。如果是快照读，InnoDB 会使用 MVCC 的 ReadView 判断记录版本是否可见，若事务 B 尚未提交或在 A 的视图不可见，则 A 会读到旧值；如果是当前读，则需要加锁，若 B 已提交可直接读取，否则 A 会阻塞直到 B 结束。 高可用66.MySQL数据库读写分离了解吗？读写分离就是把“写操作”交给主库处理，“读操作”分给多个从库处理，从而提升系统并发性能。 三分恶面渣逆袭：读写分离 应用层通过中间件（如 MyCat、ShardingSphere）自动路由请求，将 INSERT UPDATE DELETE 等写操作发送给主库，将 SELECT 查询操作发送给从库。 // 示例：Java中通过不同数据源切换@Transactionalpublic void updateOrder(Order order) masterDataSource.update(order); // 写操作走主库public Order getOrderById(Long id) return slaveDataSource.query(id); // 读操作走从库 主库将数据变更通过 binlog 同步到从库，从而保持数据一致性。 轻风博客：主从同步 主库 dump_thread 线程通过 TCP 将 binlog 推送给从库，从库 io_thread 线程，接收主库 binlog，写入 relay log，从库 sql_thread 线程读取 relay log，并顺序执行 SQL 语句，更新从库数据。 67.读写分离的实现方式有哪些？实现读写分离有三种方式：最简单的是在应用层手动控制主从数据源，适用于小型项目； 三分恶面渣逆袭：业务代码封装 中等项目是通过 Spring + 多数据源插件、AOP 注解自动路由； 大型系统通常使用中间件，如 ShardingSphere、MyCat，支持自动路由、负载均衡、故障转移等功能。 三分恶面渣逆袭：数据库中间件 Mycat 的读写分离功能依赖于 MySQL 的主从复制架构： writeHost: 表示主节点，负责处理所有的 DML SQL 语句，如 INSERT、UPDATE 和 DELETE。 readHost: 表示从节点，负责处理查询 SQL 语句（如 SELECT），以实现读写分离。 正常情况下，Mycat 会将第一个配置的 writeHost 作为默认的写节点。所有的 DML SQL 语句会被发送到此默认写节点执行。 鲲鹏：Mycat for MySQL 读写分离 写节点完成数据写入后，通过 MySQL 的主从复制机制，将数据同步到所有从节点，确保主从数据一致性。 68.主从复制原理了解吗？MySQL 的主从复制是一种数据同步机制，用于将数据从主数据库复制到一个或多个从数据库。 三分恶面渣逆袭：主从复制 主库执行事务提交时，将数据变更以事件形式记录到 Binlog。从库通过 IO 线程从主库的 Binlog 中读取变更事件，并将这些事件写入到本地的中继日志文件中，SQL 线程会实时监控中继日志的内容，按顺序读取并执行这些事件，从而保证从库与主库数据一致。 69.主从同步延迟怎么处理？主从同步延迟是因为从库需要先接收 binlog，再执行 SQL 才能同步主库数据，在高并发写或网络抖动时容易出现延迟，导致读写不一致。 第一种解决方案：对一致性要求高的查询（如支付结果查询）可以直接走主库。 // 伪代码示例public Object query(String sql) if(isWriteQuery(sql) || needStrongConsistency(sql)) return masterDataSource.query(sql); else return slaveDataSource.query(sql); 第二种解决方案：对于非关键业务允许短暂数据不一致，可以提示用户“数据同步中，请稍后刷新”，然后借助异步通知机制替代实时查询。 // 伪代码示例public Object query(String sql) if(isWriteQuery(sql)) return masterDataSource.query(sql); else // 异步通知用户数据已更新 notifyUser(数据同步中，请稍后刷新); return slaveDataSource.query(sql); 第三种解决方案：采用半同步复制，主库在事务提交时，要等至少一个从库确认收到 binlog（但不要求执行完成），才算提交成功。 骏马金龙：半同步复制 请说说半同步复制的流程？第一步，主库安装半同步插件： INSTALL PLUGIN rpl_semi_sync_master SONAME semisync_master.so; 第二步，主库启用半同步复制并设置超时时间： SET GLOBAL rpl_semi_sync_master_enabled = 1;SET GLOBAL rpl_semi_sync_master_timeout = 10000; 主库 my.cnf 配置示例： [mysqld]plugin-load = rpl_semi_sync_master=semisync_master.sorpl_semi_sync_master_enabled = 1rpl_semi_sync_master_timeout = 10000# MySQL 5.7+建议使用无损模式rpl_semi_sync_master_wait_point = AFTER_SYNC 第三步，从库安装半同步插件： INSTALL PLUGIN rpl_semi_sync_slave SONAME semisync_slave.so; 第四步，从库启用半同步复制： SET GLOBAL rpl_semi_sync_slave_enabled = 1; 从库 my.cnf 配置示例： [mysqld]plugin-load = rpl_semi_sync_slave=semisync_slave.sorpl_semi_sync_slave_enabled = 1 70.🌟你们一般是怎么分库的呢？分库的策略有两种，第一种是垂直分库：按照业务模块将不同的表拆分到不同的库中，比如说用户、登录、权限等表放在用户库中，商品、分类、库存放在商品库中，优惠券、满减、秒杀放在活动库中。 三分恶面渣逆袭：垂直分库 第二种是水平分库：按照一定的策略将一个表中的数据拆分到多个库中，比如哈希分片和范围分片，对用户 id 进行取模运算或者范围划分，将数据分散到不同的库中。 三分恶面渣逆袭：水平分库 贴一段使用 ShardingSphere 的 inline 算法定义分片规则： rules:- !SHARDING tables: order: actualDataNodes: db_$0..3.order_$0..15 databaseStrategy: standard: shardingColumn: user_id shardingAlgorithmName: db_hash_mod tableStrategy: standard: shardingColumn: order_time shardingAlgorithmName: table_interval_yearly shardingAlgorithms: db_hash_mod: type: HASH_MOD props: sharding-count: 4 table_interval_yearly: type: INTERVAL props: datetime-pattern: yyyy-MM-dd HH:mm:ss datetime-lower: 2024-01-01 00:00:00 datetime-upper: 2025-01-01 00:00:00 sharding-suffix-pattern: yyyy datetime-interval-amount: 1 datetime-interval-unit: Years 71.🌟那你们是怎么分表的？当单表超过 500 万条数据，就可以考虑水平分表了。比如说我们可以将文章表拆分成多个表，如 article_0、article_9999、article_19999 等。 三分恶面渣逆袭：表拆分 在技术派实战项目中，我们将文章的基本信息和内容详情做了垂直分表处理，因为文章的内容会占用比较大的空间，在只需要查看文章基本信息时把文章详情也带出来的话，就会占用更多的网络 IO 和内存导致查询变慢；而文章的基本信息，如标题、作者、状态等信息占用的空间较小，很适合不需要查询文章详情的场景。 二哥的 Java 进阶之路：文章和详情垂直分表 72.水平分库分表的分片策略有哪几种？常见的分片策略有三种，范围分片、Hash 分片和路由分片。 范围分片是根据某个字段的值范围进行水平拆分。适用于分片键具有连续性的场景。 三分恶面渣逆袭：范围分片 比如说将 user_id 作为分片键： 1 ~ 10000 → db1.user_1 10001 ~ 20000 → db2.user_2 Hash 分片是指通过对分片键的值进行哈希取模，将数据均匀分布到多个库表中，适用于分片键具有离散性的场景。 三分恶面渣逆袭：Hash 分片 比如说我们一开始规划好了 4 个表，那么就可以简单地通过取模来实现分表： public String getTableNameByHash(long userId) int tableIndex = (int) (userId % 4); return user_ + tableIndex; 路由分片是通过路由配置来确定数据应该存储在哪个库表，适用于分片键不规律的场景。 三分恶面渣逆袭：配置路由 比如说我们可以通过 order_router 表来确定订单数据存储在哪个表中： order_id table_id xxxx table_1 yyyy table_2 zzzz table_3 73.不停机扩容怎么实现？第一个阶段：新旧库同时写入，确保数据实时同步；可以借助消息队列实现异步补偿，幂等避免重复写入。读操作仍然走旧库。 三分恶面渣逆袭：数据同步和校验 代码参考： @Transactionalpublic void createOrder(Order order) oldDB.insert(order); // 写入旧库 newDB.insert(order); // 写入新扩容节点 kafka.send(data_sync, order); // 异步补偿通道 第二个阶段，通过 Canal 或者自研脚本将旧库的历史数据同步到新库。关键业务在查询时同时查询新旧库，进行数据校验，确保一致性。 public ListOrder getOrders(Long userId) ListOrder orders = newDB.getOrders(userId); ListOrder oldOrders = oldDB.getOrders(userId); if (!orders.equals(oldOrders)) // 数据不一致，进行补偿 kafka.send(data_sync, oldOrders); 第三个阶段，在确认新库数据一致性后，逐步将读请求切换到新库，然后下线旧库。 三分恶面渣逆袭：下线旧库 74.常用的分库分表中间件有哪些？常用的分库分表中间件有 ShardingSphere 和 Mycat。 ①、ShardingSphere 最初由当当开源，后来贡献给了 Apache，其子项目 Sharding-JDBC 主要在 Java 的 JDBC 层提供额外的服务。无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。 AWS：Sharding-JDBC ②、Mycat 是由阿里巴巴的一款产品 Cobar 衍生而来，可以把它看作一个数据库代理。 piwenfei：mycat 推荐阅读：mycat 介绍 75.你觉得分库分表会带来什么问题呢？第一，跨库事务无法依赖单机 MySQL 的 ACID 特性，需要使用分布式事务解决方案，如 Seata 的 AT 模式、TCC 模式等。 PmHub 项目中 Seata 第二，跨库后无法使用 JOIN 联表查询。可以在业务层进行拼接，或者把需要联表查询的数据放到 ES 中。 // Java 代码示例User user = userService.getUserById(1);ListOrder orders = orderService.getOrdersByUserId(1); 第三，自增 ID 在分片场景下容易冲突，需要使用全局唯一方案。 数据库表被切分后，不能再依赖数据库自身的主键生成机制，所以需要一些手段来保证全局主键唯一。比如说雪花算法、京东的 JD-hotkey。 京东的 JD-hotkey 你们项目中的分布式主键 id 是怎么生成的？在技术派项目中，我们在雪花算法的基础上实现了一套自定义的 ID 生成方案，通过更改时间戳单位、ID 长度、workId 与 dataCenterId 的分配比例，ID 生成的延迟降低了 20%；满足了分布式环境下 ID 的唯一性。 技术派：自定义雪花算法算法 雪花算法具体是怎么实现的？雪花算法是 Twitter 开源的分布式 ID 生成算法，其核心思想是：使用一个 64 位的数字来作为全局唯一 ID。 第 1 位是符号位，永远是 0，表示正数。 接下来的 41 位是时间戳，记录的是当前时间戳减去一个固定的开始时间戳，可以使用 69 年。 然后是 10 位的工作机器 ID。 最后是 12 位的序列号，每毫秒最多可生成 4096 个 ID。 技术派：雪花算法 大致的实现代码如下所示： public class SnowflakeIdGenerator private long datacenterId = 1L; // 数据中心ID private long machineId = 1L; // 机器ID private long sequence = 0L; // 序列号 private long lastTimestamp = -1L; public synchronized long nextId() long timestamp = System.currentTimeMillis(); if (timestamp == lastTimestamp) sequence = (sequence + 1) 4095; if (sequence == 0) while (timestamp == lastTimestamp) timestamp = System.currentTimeMillis(); else sequence = 0; lastTimestamp = timestamp; return ((timestamp - 1609459200000L) 22) | (datacenterId 17) | (machineId 12) | sequence; 运维76.百万级别以上的数据如何删除？在处理百万级别的数据删除时，大范围的 DELETE 语句往往会造成锁表时间长、事务日志膨胀等问题。 可以采用批量删除的方案，将删除操作分成多个小批次进行处理。 public void batchDelete(String tableName, String condition, int batchSize) // 1. 创建线程池 int threadCount = Runtime.getRuntime().availableProcessors(); ExecutorService executor = Executors.newFixedThreadPool(threadCount); CountDownLatch latch = new CountDownLatch(threadCount); // 2. 获取总记录数 long totalCount = getTotalCount(tableName, condition); // 3. 计算每个线程处理的数据量 long perThreadCount = totalCount / threadCount; // 4. 分配任务给线程池 for (int i = 0; i threadCount; i++) long startId = i * perThreadCount; long endId = (i == threadCount - 1) ? totalCount : (startId + perThreadCount); executor.execute(() - try // 分批次删除数据 for (long j = startId; j endId; j += batchSize) String deleteSql = String.format( DELETE FROM %s WHERE %s LIMIT %d, tableName, condition, batchSize ); // 执行删除 jdbcTemplate.update(deleteSql); finally latch.countDown(); ); // 5. 等待所有线程完成 latch.await(); executor.shutdown(); 也可以采用创建新表替换原表的方式，把需要保留的数据迁移到新表中，然后删除旧表。 简单的方案： -- 1. 创建新表结构(包含索引)CREATE TABLE new_table LIKE large_table;-- 2. 插入需要保留的数据INSERT INTO new_table SELECT * FROM large_table WHERE condition;-- 3. 重命名表RENAME TABLE large_table TO old_table, new_table TO large_table;-- 4. 删除旧表DROP TABLE old_table; 加入检查表空间、分批导入数据、验证数据一致性等步骤： -- 1. 在执行之前先检查空间是否足够SELECT table_schema, table_name, round(((data_length + index_length) / 1024 / 1024), 2) Size in MBFROM information_schema.TABLES WHERE table_schema = DATABASE()AND table_name = large_table;-- 2. 创建新表CREATE TABLE new_table LIKE large_table;-- 3. 分批导入数据（避免一次性导入过多数据）SET @batch = 1;SET @batch_size = 10000;SET @total = (SELECT COUNT(*) FROM large_table WHERE condition);REPEAT INSERT INTO new_table SELECT * FROM large_table WHERE condition LIMIT @batch_size; SET @batch = @batch + 1;UNTIL @batch * @batch_size @total END REPEAT;-- 4. 验证数据一致性SELECT COUNT(*) FROM new_table;SELECT COUNT(*) FROM large_table WHERE condition;-- 5. 在业务低峰期执行表切换RENAME TABLE large_table TO old_table, new_table TO large_table;-- 6. 确认无误后再删除旧表（建议不要立即删除）-- DROP TABLE old_table; 77.千万级大表如何添加字段？在低版本的 MySQL 中，千万级数据量的表中添加字段时，直接使用 ALTER TABLE 命令会导致长时间锁表、甚至数据库崩溃等。 可以使用 Percona Toolkit 的 pt-online-schema-change 来完成，它通过创建临时表、逐步同步数据并使用触发器捕获变更来实现。 pt-online-schema-change --alter ADD COLUMN new_column datatype D=database,t=your_table --execute 对于 MySQL 8.0+ 版本，可以直接通过 ALTER TABLE 来完成，因为加入了 INSTAN 算法，添加列并不会长时间锁表。 ALTER TABLE your_table ADD COLUMN new_column datatype; 如果没有指定 ALGORITHM=INSTANT 算法，MySQL 会先尝试 INSTANT 算法；如果无法完成，会切换到 INPLACE 算法；如果仍然无法完成，会尝试 COPY 算法。 截图来自MySQL官网：由腾讯游戏 DBA 团队贡献 78.MySQL 导致 cpu 飙升的话，要怎么处理呢？我通常先通过 top 命令确认是否是 mysqld 的进程占用。 top -pid $(pgrep mysqld) 然后通过 SHOW PROCESSLIST 和慢查询日志定位是否存在耗时 SQL，再配合 explain 和 performance_schema 分析 SQL 是否命中索引，是否存在临时表和排序。 -- 使用 EXPLAIN 分析SQL执行计划EXPLAIN SELECT * FROM large_table WHERE condition;-- 查看表的索引使用情况SHOW INDEX FROM table_name;-- 查看InnoDB状态SHOW ENGINE INNODB STATUS;-- 查看表的统计信息ANALYZE TABLE table_name; 最终通过 SQL 优化、加索引、分批操作等手段逐步优化。 SQL 题79.一张表：id，name，age，sex，class，sql 语句：所有年龄为 18 的人的名字？找到每个班年龄大于 18 有多少人？找到每个班年龄排前两名的人？（补充）第一步，建表： CREATE TABLE students ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(50), age INT, sex CHAR(1), class VARCHAR(50)); 第二步，插入数据： INSERT INTO students (name, age, sex, class) VALUES(沉默王二, 18, 女, 三年二班),(沉默王一, 20, 男, 三年二班),(沉默王三, 19, 男, 三年三班),(沉默王四, 17, 男, 三年三班),(沉默王五, 20, 女, 三年四班),(沉默王六, 21, 男, 三年四班),(沉默王七, 18, 女, 三年四班); 所有年龄为 18 的人的名字？SELECT name FROM students WHERE age = 18; 这条 SQL 语句从表中选择age等于 18 的所有记录，并返回这些记录的name字段。 二哥的 Java 进阶之路：找出age18的记录 如果可以的话，可以给 age 字段加上索引。 ALTER TABLE students ADD INDEX age_index (age); 找到每个班年龄大于 18 有多少人?SELECT class, COUNT(*) AS number_of_studentsFROM studentsWHERE age 18GROUP BY class; 这条 SQL 语句先筛选出年龄大于 18 的记录，然后按class分组，并通过 count 统计每个班的学生数。 二哥的 Java 进阶之路：找出年龄大于 18 的人 找到每个班年龄排前两名的人？这个查询稍微复杂一些，需要使用子查询和去重 DISTINCT。 SELECT a.class, a.name, a.ageFROM students aWHERE ( SELECT COUNT(DISTINCT b.age) FROM students b WHERE b.class = a.class AND b.age a.age) 2ORDER BY a.class, a.age DESC; 这条 SQL 语句首先从students表中选择class、name和age字段，然后使用子查询计算每个班级中年龄排前两名的学生。 二哥的 Java 进阶之路：排名前两名的学生 80.有一个查询需求，MySQL 中有两个表，一个表 1000W 数据，另一个表只有几千数据，要做一个关联查询，如何优化第一步，为关联字段建立索引，确保 on 连接的字段都有索引。 ALTER TABLE big_table ADD INDEX idx_small_id(small_id); 第二步，小表驱动大表，将小表放在 JOIN 的左边（驱动表），大表放在右边。 SELECT ... FROM small_table s JOIN big_table b ON s.id = b.small_id 81.新建一个表结构，创建索引，将百万或千万级的数据使用 insert 导入该表，新建一个表结构，将百万或千万级的数据使用 isnert 导入该表，再创建索引，这两种效率哪个高呢？或者说用时短呢？先说结论： 在大数据量导入场景下，先导入数据，后建索引的效率显著高于先建索引，后导入数据的效率。 来，实操。 先创建一个表，然后创建索引，执行插入语句，来看看执行时间（100 万数据在我本机上执行时间比较长，我们就用 10 万条数据来测试）。 CREATE TABLE test_table ( id BIGINT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, email VARCHAR(255) NOT NULL, created_at DATETIME NOT NULL);CREATE INDEX idx_name ON test_table(name);DELIMITER //CREATE PROCEDURE insert_data()BEGIN DECLARE i INT DEFAULT 0; WHILE i 1000000 DO INSERT INTO test_table(name, email, created_at) VALUES (CONCAT(wanger,i), CONCAT(email, i, @example.com), NOW()); SET i = i + 1; END WHILE;END //DELIMITER ;CALL insert_data(); 总的时间 13.93+0.01+0.01+0.0113.96 秒。 二哥的 Java 进阶之路：先索引再插入 接下来，我们再创建一个表，执行插入操作，然后创建索引。 CREATE TABLE test_table_no_index ( id BIGINT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, email VARCHAR(255) NOT NULL, created_at DATETIME NOT NULL);DELIMITER //CREATE PROCEDURE insert_data_no_index()BEGIN DECLARE i INT DEFAULT 0; WHILE i 1000000 DO INSERT INTO test_table_no_index(name, email, created_at) VALUES (CONCAT(wanger, i), CONCAT(email, i, @example.com), NOW()); SET i = i + 1; END WHILE;END //DELIMITER ;CALL insert_data_no_index();CREATE INDEX idx_name_no_index ON test_table_no_index(name); 来看一下总的时间，0.01+0.00+13.08+0.1813.27 秒。 二哥的 Java 进阶之路：先插入再索引 先插入数据再创建索引的方式比先创建索引再插入数据要快一点。 然后时间差距很微小，主要是因为我们插入的数据少。说一下差别。 先插入数据再创建索引：在没有索引的情况下插入数据，数据库不需要在每次插入时更新索引。 先创建索引再插入数据：数据库需要在每次插入新记录时维护索引结构，随着数据量的增加，索引的维护会导致额外的性能开销。 MySQL是先建立索引好还是先插入数据好？如果是小批量插入，可以先建索引；但在大数据量数据导入场景下，推荐先插入数据再建索引。 因为索引是基于 B+ 树的，大量插入时如果提前建索引，会频繁触发页分裂和索引结构调整，影响性能。 插入完成后统一构建索引，MySQL 会按顺序批量生成索引结构，速度更快、资源消耗更低。 82.什么是深分页，select * from tbn limit 1000000000 这个有什么问题，如果表大或者表小分别什么问题深分页是指在 MySQL 中获取比较靠后的数据页，比如第 1000 页、第 10000 页等。特别是使用 LIMIT offset,count 这种方式，当 offset 特别大，就会带来严重的性能问题。 对于 SELECT * FROM tbn LIMIT 1000000,10，这样的查询语句来说，MySQL 会： 从表中读取第一条记录，判断是否满足 where 条件；如果满足，计数器+1；否则直到 计数器累计到 1000000 时才开始真正取数据 再继续获取 10 条数据，返回 性能会非常差，因为需要从头扫描，无法利用索引优化，并且需要抛弃大量不需要的数据，占用大量的内存和 CPU 资源。 可以借助主键索引分页进行优化： SELECT * FROM tbnWHERE id (SELECT id FROM tbn ORDER BY id LIMIT 1000000, 1)LIMIT 10 或者记住上次分页的最大 ID，然后再查询： SELECT * FROM tbnWHERE id last_page_max_idLIMIT 10 83.SQL 题：一个学生成绩表，字段有学生姓名、班级、成绩，求各班前十名第一步，建表： CREATE TABLE student_scores ( student_name VARCHAR(100), class VARCHAR(50), score INT); 第二步，插入数据： INSERT INTO student_scores (student_name, class, score) VALUES(沉默王二, 三年二班, 88),(沉默王三, 三年二班, 92),(沉默王四, 三年二班, 87),(沉默王五, 三年二班, 85),(沉默王六, 三年二班, 90),(沉默王七, 三年二班, 95),(沉默王八, 三年二班, 82),(沉默王九, 三年二班, 78),(沉默王十, 三年二班, 91),(沉默王十一, 三年二班, 79),(沉默王十二, 三年三班, 84),(沉默王十三, 三年三班, 81),(沉默王十四, 三年三班, 90),(沉默王十五, 三年三班, 88),(沉默王十六, 三年三班, 87),(沉默王十七, 三年三班, 93),(沉默王十八, 三年三班, 89),(沉默王十九, 三年三班, 85),(沉默王二十, 三年三班, 92),(沉默王二十一, 三年三班, 84); 第三步，查询各班前十名。如果 MySQL 是 8.0 以下版本，不支持窗口函数，可以通过在查询中维护班级当前处理状态和排名，实现分组内按成绩排序并打标号，再取前十名。 SET @cur_class = NULL, @cur_rank = 0;SELECT student_name, class, scoreFROM ( SELECT student_name, class, score, @cur_rank := IF(@cur_class = class, @cur_rank + 1, 1) AS rank, @cur_class := class FROM student_scores ORDER BY class, score DESC) AS rankedWHERE ranked.rank = 10; 步骤 解释 @cur_class 变量 记录当前正在处理的班级 @cur_rank 变量 记录当前班级的排名，默认 0 IF(@cur_class = class, @cur_rank + 1, 1) 如果班级没变，就排名 +1；如果换了新班级，排名从 1 重新开始 @cur_class := class 更新当前班级变量，保持班级变化跟踪 ORDER BY class, score DESC 必须先按班级升序、成绩降序排好，才能保证变量正确打排名 外层 WHERE rank = 10 只取每班前十名 ✅ 二哥的 Java 进阶之路：排名前十 如果是 MySQL 8.0+ 版本，可以使用窗口函数来完成： SELECT student_name, class, scoreFROM ( SELECT student_name, class, score, ROW_NUMBER() OVER (PARTITION BY class ORDER BY score DESC) AS rn FROM student_scores) AS tmpWHERE rn = 10; SQL 用到的技术 说明 ROW_NUMBER() OVER (PARTITION BY class ORDER BY score DESC) 给每个班独立打排名，从 1 开始 子查询 tmp 用来临时生成带有 rn（排名）的数据集 外层 WHERE rn = 10 选出每个班排名前 10 的学生 ORDER BY score DESC 成绩高排前面，符合常规排名逻辑 二哥的 Java 进阶之路：窗口函数","tags":["基础","Mysql"],"categories":["Java问答笔记"]},{"title":"2025.6.10学习日记","path":"/2025/06/10/学习日记25年6月/2025.6.10学习笔记/","content":"学习内容最近学习重心想转到算法相关,把随想录和题单刷一遍之后做力扣周赛,太长时间没做需要复健一下. 1. 打卡力扣每日简单的字符串计数找最大最小. 2. 看Mysql基础笔记同步在了Mysql学习笔记中. 3. 代码随想录二叉树的三种遍历和迭代遍历.今天踩了一个小坑:在实现迭代调用的统一写法时,需要在Deque中插入一个null元素作为是否遍历过的标志,但是实现Deque时用的ArrayDeque,这个实现方式不能插入null元素,需要使用LinkedList实现才可以.感觉这几个集合框架还是有必要学的深入一些的,不管是算法还是写项目都有很大帮助. 明天开始跑项目ply生活记录1. 健身今天健身房练上肢 肩 胸 核心","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-06"]},{"title":"JVM学习笔记","path":"/2025/06/09/基础笔记/JVM学习笔记/","content":"运行时数据区域 程序计数器记录正在执行的虚拟机字节码指令的地址,如果是本地方法则为空. Java虚拟机栈每个 Java ⽅法在执⾏的同时会创建⼀个栈帧⽤于存储局部变量表、操作数栈、常量池引⽤等信息。从⽅法调⽤直⾄执⾏完成的过程，对应着⼀个栈帧在 Java 虚拟机栈中⼊栈和出栈的过程。 该区域可能抛出的异常: 当线程请求的栈深度超过最⼤值，会抛出 StackOverflowError 异常； 栈进⾏动态扩展时如果⽆法申请到⾜够内存，会抛出 OutOfMemoryError 异常。 本地方法栈本地方法栈和Java虚拟机栈类似,区别在于本地方法栈为虚拟机使用到的 Native ⽅法服务. 堆所有对象都在这⾥分配内存，是垃圾收集的主要区域（”GC 堆”）。现代垃圾收集器基本都是采⽤分代收集算法，其主要的思想是针对不同类型的对象采取不同的垃圾回收算法。可以将堆分成两块： 新⽣代（Young Generation） ⽼年代（Old Generation）堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 ⽅法区⽤于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。和堆⼀样不需要连续的内存，并且可以动态扩展，动态扩展失败⼀样会抛出 OutOfMemoryError 异常。在 JDK 1.8 之后，原来永久代的数据被分到了堆和元空间中。元空间存储类的元信息，静态变量和常量池等放⼊堆中。 运行时常量池是⽅法区的⼀部分。Class ⽂件中的常量池（编译器⽣成的字⾯量和符号引⽤）会在类加载后被放⼊这个区域。除了在编译期⽣成的常量，还允许动态⽣成，例如 String 类的 intern()。 直接内存JDK 1.4 新引⼊了 NIO 类，它可以使⽤ Native 函数库直接分配堆外内存，然后通过 Java 堆⾥的 DirectByteBuffer对象作为这块内存的引⽤进⾏操作。这样能在⼀些场景中显著提⾼性能，因为避免了在堆内存和堆外内存来回拷⻉数据。 理解运行时的数据区 垃圾收集垃圾收集主要是针对堆和⽅法区进⾏。程序计数器、虚拟机栈和本地⽅法栈这三个区域属于线程私有的，只存在于线程的⽣命周期内，线程结束之后就会消失，因此不需要对这三个区域进⾏垃圾回收。 判断一个对象是否可被回收1. 引⽤计数算法为对象添加⼀个引⽤计数器，当对象增加⼀个引⽤时计数器加 1，引⽤失效时计数器减 1。引⽤计数为 0 的对象可被回收。 但由于对象之间循环引用的存在，引⽤计数器也会失效。 2. 可达性分析算法以 GC Roots 为起始点进⾏搜索，可达的对象都是存活的，不可达的对象可被回收。Java 虚拟机使⽤该算法来判断对象是否可被回收，GC Roots ⼀般包含以下内容： 虚拟机栈中局部变量表中引⽤的对象 本地⽅法栈中 JNI 中引⽤的对象 ⽅法区中类静态属性引⽤的对象 ⽅法区中的常量引⽤的对象 3. 方法区的回收因为⽅法区主要存放永久代对象，⽽永久代对象的回收率⽐新⽣代低很多，所以在⽅法区上进⾏回收性价⽐不⾼。主要是对常量池的回收和对类的卸载。为了避免内存溢出，在⼤量使⽤反射和动态代理的场景都需要虚拟机具备类卸载功能。类的卸载条件很多，需要满⾜以下三个条件，并且满⾜了条件也不⼀定会被卸载： 该类所有的实例都已经被回收，此时堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地⽅被引⽤，也就⽆法在任何地⽅通过反射访问该类⽅法。 4. finalize()类似 C++ 的析构函数，⽤于关闭外部资源。但是⽤ try-finally 可以做得更好，并且 finalize ⽅法运⾏代价很⾼，不确定性⼤，⽆法保证各个对象的调⽤顺序，因此最好不要使⽤。 引用类型⽆论是通过引⽤计数算法判断对象的引⽤数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引⽤有关。Java 提供了四种强度不同的引⽤类型。 1. 强引⽤被强引⽤关联的对象不会被回收。使⽤ new ⼀个新对象的⽅式来创建强引⽤。Object obj = new Object(); 2. 软引⽤被软引⽤关联的对象只有在内存不够的情况下才会被回收。使⽤ SoftReference 类来创建软引⽤。 Object obj = new Object();SoftReferenceObject sf = new SoftReferenceObject(obj);obj = null; // 使对象只被软引⽤关联 3. 弱引⽤被弱引⽤关联的对象⼀定会被回收，也就是说它只能存活到下⼀次垃圾回收发⽣之前。使⽤ WeakReference 类来创建弱引⽤。 Object obj = new Object();WeakReferenceObject wf = new WeakReferenceObject(obj);obj = null; 4. 虚引⽤⼜称为幽灵引⽤或者幻影引⽤，⼀个对象是否有虚引⽤的存在，不会对其⽣存时间造成影响，也⽆法通过虚引⽤得到⼀个对象。为⼀个对象设置虚引⽤的唯⼀⽬的是能在这个对象被回收时收到⼀个系统通知。使⽤ PhantomReference 来创建虚引⽤。 Object obj = new Object();PhantomReferenceObject pf = new PhantomReferenceObject(obj, null);obj = null; 垃圾收集算法1. 标记-清除算法最基础的收集算法，分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。不足: 效率问题，标记和清除两个过程的效率都不⾼； 空间问题，会产⽣⼤量不连续的内存碎⽚，导致⽆法给⼤对象分配内存。 2. 标记-整理算法让所有存活的对象都向⼀端移动，然后直接清理掉端边界以外的内存。 优点: 不会产⽣内存碎⽚ 不⾜: 需要移动⼤量对象，处理效率⽐较低。 3. 复制算法将内存划分为⼤⼩相等的两块，每次只使⽤其中⼀块，当这⼀块内存⽤完了就将还存活的对象复制到另⼀块上⾯，然后再把使⽤过的内存空间进⾏⼀次清理。不⾜是只使⽤了内存的⼀半。 4. 分代收集算法现在的商业虚拟机采⽤分代收集算法，它根据对象存活周期将内存划分为⼏块，不同块采⽤适当的收集算法。⼀般将堆分为新⽣代和⽼年代。 新⽣代使⽤：复制算法 ⽼年代使⽤：标记 - 清除 或者 标记 - 整理 算法 垃圾收集器以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使⽤。 单线程与多线程：单线程指的是垃圾收集器只使⽤⼀个线程，⽽多线程使⽤多个线程； 串⾏与并⾏：串⾏指的是垃圾收集器与⽤户程序交替执⾏，这意味着在执⾏垃圾收集的时候需要停顿⽤户程序；并⾏指的是垃圾收集器和⽤户程序同时执⾏。除了 CMS 和 G1 之外，其它垃圾收集器都是以串⾏的⽅式执⾏。 1. Serial 收集器串行的垃圾收集器，是最基本、历史最悠久的垃圾收集器。单CPU环境下，Serial 收集器由于没有线程交互的开销，可以获得最高的单线程收集效率。 优点: 简单、容易实现 缺点: 单线程、停顿时间⻓ 2. ParNew 收集器ParNew 收集器是 Serial 收集器的多线程版本，它是 Server 场景下默认的新⽣代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合使⽤。 3. Parallel Scavenge 收集器与 ParNew ⼀样是多线程收集器。其它收集器⽬标是尽可能缩短垃圾收集时⽤户线程的停顿时间，⽽它的⽬标是达到⼀个可控制的吞吐量，因此它被称为“吞吐量优先”收集器。这⾥的吞吐量指 CPU ⽤于运⾏⽤户程序的时间占总时间的⽐值。 4. Serial Old 收集器是 Serial 收集器的⽼年代版本，也是给 Client 场景下的虚拟机使⽤。如果⽤在 Server 场景下，它有两⼤⽤途： 在 JDK 1.5 以及之前版本（Parallel Old 诞⽣以前）中与 Parallel Scavenge 收集器搭配使⽤。 作为 CMS 收集器的后备预案，在并发收集发⽣ Concurrent Mode Failure 时使⽤。 5. Parallel Old 收集器是 Parallel Scavenge 收集器的⽼年代版本。在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。分为以下四个流程： 初始标记：仅仅只是标记⼀下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进⾏ GC Roots Tracing 的过程，它在整个回收过程中耗时最⻓，不需要停顿。 重新标记：为了修正并发标记期间因⽤户程序继续运作⽽导致标记产⽣变动的那⼀部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最⻓的并发标记和并发清除过程中，收集器线程都可以与⽤户线程⼀起⼯作，不需要进⾏停顿。具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利⽤率不够⾼。 ⽆法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于⽤户线程继续运⾏⽽产⽣的垃圾，这部分垃圾只能到下⼀次 GC 时才能进⾏回收。由于浮动垃圾的存在，因此需要预留出⼀部分内存，意味着 CMS 收集不能像其它收集器那样等待⽼年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启⽤ Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎⽚，往往出现⽼年代空间剩余，但⽆法找到⾜够⼤连续空间来分配当前对象，不得不提前触发⼀次 Full GC。 7. G1 收集器G1（Garbage-First），它是⼀款⾯向服务端应⽤的垃圾收集器，在多 CPU 和⼤内存的场景下有很好的性能。堆被分为新⽣代和⽼年代，其它收集器进⾏收集的范围都是整个新⽣代或者⽼年代，⽽ G1 可以直接对新⽣代和⽼年代⼀起回收。G1 把堆划分成多个⼤⼩相等的独⽴区域（Region），新⽣代和⽼年代不再物理隔离。通过引⼊ Region 的概念，从⽽将原来的⼀整块内存空间划分成多个的⼩空间，使得每个⼩空间可以单独进⾏垃圾回收。这种划分⽅法带来了很⼤的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护⼀个优先列表，每次根据允许的收集时间，优先回收价值最⼤的 Region。每个 Region 都有⼀个 Remembered Set，⽤来记录该 Region 对象的引⽤对象所在的 Region。通过使⽤Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作⼤致可划分为以下⼏个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因⽤户程序继续运作⽽导致标记产⽣变动的那⼀部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs ⾥⾯，最终标记阶段需要把 Remembered Set Logs的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并⾏执⾏。 筛选回收：⾸先对各个 Region 中的回收价值和成本进⾏排序，根据⽤户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与⽤户程序⼀起并发执⾏，但是因为只回收⼀部分 Region，时间是⽤户可控制的，⽽且停顿⽤户线程将⼤幅度提⾼收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运⾏期间不会产⽣内存空间碎⽚。 可预测的停顿：能让使⽤者明确指定在⼀个⻓度为 M 毫秒的时间⽚段内，消耗在 GC 上的时间不得超过 N 毫秒。 内存分配与回收策略’Minor GC 和 Full GC Minor GC：回收新⽣代，因为新⽣代对象存活时间很短，因此 Minor GC 会频繁执⾏，执⾏的速度⼀般也会⽐较快。 Full GC：回收⽼年代和新⽣代，⽼年代对象存活时间⻓，因此 Full GC 很少执⾏，执⾏速度会⽐ Minor GC 慢很多。 内存分配策略 对象优先在 Eden 分配⼤多数情况下，对象在新⽣代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 ⼤对象直接进⼊⽼年代⼤对象是指需要连续内存空间的对象，最典型的⼤对象是那种很⻓的字符串以及数组。经常出现⼤对象会提前触发垃圾收集以获取⾜够的连续空间分配给⼤对象。-XX:PretenureSizeThreshold，⼤于此值的对象直接在⽼年代分配，避免在 Eden 和 Survivor之间的⼤量内存复制。 ⻓期存活的对象进⼊⽼年代为对象定义年龄计数器，对象在 Eden 出⽣并经过 Minor GC 依然存活，将移动到 Survivor中，年龄就增加 1 岁，增加到⼀定年龄则移动到⽼年代中。-XX:MaxTenuringThreshold ⽤来定义年龄的阈值。 动态对象年龄判定虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升⽼年代，如果在Survivor 中相同年龄所有对象⼤⼩的总和⼤于 Survivor 空间的⼀半，则年龄⼤于或等于该年龄的对象可以直接进⼊⽼年代，⽆需等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保在发⽣ Minor GC 之前，虚拟机先检查⽼年代最⼤可⽤的连续空间是否⼤于新⽣代所有对象总空间，如果条件成⽴的话，那么 Minor GC 可以确认是安全的。如果不成⽴的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查⽼年代最⼤可⽤的连续空间是否⼤于历次晋升到⽼年代对象的平均⼤⼩，如果⼤于，将尝试着进⾏⼀次 Minor GC；如果⼩于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进⾏⼀次 Full GC。 Full GC 的触发条件对于 Minor GC，其触发条件⾮常简单，当 Eden 空间满时，就将触发⼀次 Minor GC。⽽ FullGC 则相对复杂，有以下条件： 调⽤ System.gc()只是建议虚拟机执⾏ Full GC，但是虚拟机不⼀定真正去执⾏。不建议使⽤这种⽅式，⽽是让虚拟机管理内存。 ⽼年代空间不⾜⽼年代空间不⾜的常⻅场景为前⽂所讲的⼤对象直接进⼊⽼年代、⻓期存活的对象进⼊⽼年代等。为了避免以上原因引起的 Full GC，应当尽量不要创建过⼤的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调⼤新⽣代的⼤⼩，让对象尽量在新⽣代被回收掉，不进⼊⽼年代。还可以通过 -XX:MaxTenuringThreshold 调⼤对象进⼊⽼年代的年龄，让对象在新⽣代多存活⼀段时间。 空间分配担保失败使⽤复制算法的 Minor GC 需要⽼年代的内存空间作担保，如果担保失败会执⾏⼀次 FullGC。具体内容请参考上⾯的第 5 ⼩节。 JDK 1.7 及以前的永久代空间不⾜在 JDK 1.7 及以前，HotSpot 虚拟机中的⽅法区是⽤永久代实现的，永久代中存放的为⼀些Class 的信息、常量、静态变量等数据。当系统中要加载的类、反射的类和调⽤的⽅法较多时，永久代可能会被占满，在未配置为采⽤CMS GC 的情况下也会执⾏ Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出java.lang.OutOfMemoryError。为避免以上原因引起的 Full GC，可采⽤的⽅法为增⼤永久代空间或转为使⽤ CMS GC。 Concurrent Mode Failure执⾏ CMS GC 的过程中同时有对象要放⼊⽼年代，⽽此时⽼年代空间不⾜（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不⾜），便会报 Concurrent Mode Failure 错误，并触发Full GC。 类加载机制类是在运⾏期间第⼀次使⽤时动态加载的，⽽不是⼀次性加载所有类。因为如果⼀次性加载，会占⽤很多的内存。 类的生命周期 包括以下 7 个阶段： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使⽤（Using） 卸载（Unloading） 类的加载过程包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载加载是类加载的⼀个阶段，注意不要混淆。加载过程完成以下三件事： 通过类的完全限定名称获取定义该类的⼆进制字节流。 将该字节流表示的静态存储结构转换为⽅法区的运⾏时存储结构。 在内存中⽣成⼀个代表该类的 Class 对象，作为⽅法区中该类各种数据的访问⼊⼝。 其中⼆进制字节流可以从以下⽅式中获取： 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础。 从⽹络中获取，最典型的应⽤是 Applet。 运⾏时计算⽣成，例如动态代理技术，在 java.lang.reflect.Proxy 使⽤ProxyGenerator.generateProxyClass 的代理类的⼆进制字节流。由其他⽂件⽣成，例如由 JSP ⽂件⽣成对应的 Class 类。 2. 验证确保 Class ⽂件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机⾃身的安全。 3. 准备类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使⽤的是⽅法区的内存。应该注意到，实例化不是类加载的⼀个过程，类加载发⽣在所有实例化操作之前，并且类加载只进⾏⼀次，实例化可以进⾏多次。初始值⼀般为 0 值，例如下⾯的类变量 value 被初始化为 0 ⽽不是 123。public static int value = 123;如果类变量是常量，那么它将初始化为表达式所定义的值⽽不是 0。例如下⾯的常量 value 被初始化为 123 ⽽不是 0。public static final int value = 123; 4. 解析将常量池的符号引⽤替换为直接引⽤的过程。其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了⽀持 Java 的动态绑定。 5. 初始化初始化阶段才真正开始执⾏类中定义的 Java 程序代码。 初始化阶段是虚拟机执⾏类构造器clinit() ⽅法的过程。在准备阶段，类变量已经赋过⼀次系统要求的初始值，⽽在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 clinit() 是由编译器⾃动收集类中所有类变量的赋值动作和静态语句块中的语句合并产⽣的，编译器收集的顺序由语句在源⽂件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： public class Test static i = 0; // 给变量赋值可以正常编译通过System.out.print(i); // 这句编译器会提示“⾮法向前引⽤”static int i = 1; 由于⽗类的 () ⽅法先执⾏，也就意味着⽗类中定义的静态语句块的执⾏要优先于⼦类。例如以下代码： static class Parent public static int A = 1;static A = 2;static class Sub extends Parent public static int B = A;public static void main(String[] args) System.out.println(Sub.B); // 2 静态代码块和赋值动作是按照代码的顺序执行的。接⼝中不可以使⽤静态语句块，但仍然有类变量初始化的赋值操作，因此接⼝与类⼀样都会⽣成 clinit() ⽅法。 但接⼝与类不同的是，执⾏接⼝的 clinit() ⽅法不需要先执⾏⽗接⼝的 clinit() ⽅法。只有当⽗接⼝中定义的变量使⽤时，⽗接⼝才会初始化。另外，接⼝的实现类在初始化时也⼀样不会执⾏接⼝的 clinit() ⽅法。虚拟机会保证⼀个类的 clinit() ⽅法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化⼀个类，只会有⼀个线程执⾏这个类的 clinit() ⽅法，其它线程都会阻塞等待，直到活动线程执⾏ clinit() ⽅法完毕。 如果在⼀个类的clinit()⽅法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 类的初始化时机1.主动引用虚拟机规范中并没有强制约束何时进⾏加载，但是规范严格规定了有且只有下列五种情况必须对类进⾏初始化（加载、验证、准备都会随之发⽣）: 遇到 new、getstatic、putstatic 或 invokestatic 这 4 条字节码指令时，如果类没有进⾏过初始化，则需要先触发其初始化。 使⽤ java.lang.reflect 包的方法对类进⾏反射调⽤的时候，如果类没有进⾏过初始化，则需要先触发其初始化。 当初始化⼀个类时，如果发现其父类还没有进⾏过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，⽤于执⾏主类（包含 main() ⽅法的那个类）的初始化。 使⽤ JDK 7 新加入的动态语言⽀持时，如果⼀个 java.lang.invoke.MethodHandle 实例最后的解析结果 REF_getStatic、REF_putStatic、REF_invokeStatic 的⽅法句柄，并且这个⽅法句柄所对应的类没有进⾏过初始化，则需要先出触发其初始化。 2.被动引用以上 5 种场景中的⾏为称为对⼀个类进⾏主动引⽤。除此之外，所有引⽤类的⽅式都不会触发初始化，称为被动引⽤。被动引⽤的常⻅例⼦包括：通过⼦类引⽤⽗类的静态字段，不会导致⼦类初始化。 System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引⽤类，不会触发此类的初始化。该过程会对数组类进⾏初始化，数组类是⼀个由虚拟机⾃动⽣成的、直接继承⾃ Object 的⼦类，其中包含了数组的属性和⽅法。 SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存⼊调⽤类的常量池中，本质上并没有直接引⽤到定义常量的类，因此不会触发定义常量的类的初始化。 System.out.println(ConstClass.HELLOWORLD); 类加载器分类从 Java 虚拟机的⻆度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），使⽤ C++ 实现，是虚拟机⾃身的⼀部分； 所有其它类的加载器，使⽤ Java 实现，独⽴于虚拟机，继承⾃抽象类java.lang.ClassLoader。 从 Java 开发⼯程的⻆度来看，类加载器可以划分得更细致一些： 启动类加载器（Bootstrap ClassLoader）：这个类加载器负责将存放在 JAVA_HOME\\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引⽤，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器去处理，那么直接⽤ null 代替即可。 扩展类加载器（Extension ClassLoader）：这个类加载器是在类 java.lang.ClassLoader 的构造函数中被调⽤的。它负责将 libext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使⽤扩展类加载器。 应用程序类加载器（Application ClassLoader）：这个类加载器是在类 java.lang.ClassLoader 的构造函数中被调⽤的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，所以也被称为系统类加载器。它负责将⽤户类路径（ClassPath）上所指定的类库加载到内存中。开发者可以直接使⽤这个类加载器，如果应⽤程序中没有定义过⾃定义的类加载器，一般情况下这个就是程序中默认的类加载器。 ⼯作过程⼀个类加载器⾸先将类加载请求转发到⽗类加载器，只有当⽗类加载器⽆法完成时才尝试⾃⼰加载。 好处使得 Java 类随着它的类加载器⼀起具有⼀种带有优先级的层次关系，从⽽使得基础类得到统⼀。例如 java.lang.Object 存放在 rt.jar 中，如果编写另外⼀个 java.lang.Object 并放到 ClassPath中，程序可以编译通过。由于双亲委派模型的存在，所以在 rt.jar 中的 Object ⽐在 ClassPath 中的 Object 优先级更⾼，这是因为 rt.jar 中的 Object 使⽤的是启动类加载器，⽽ ClassPath 中的 Object 使⽤的是应⽤程序类加载器。rt.jar 中的 Object 优先级更⾼，那么程序中所有的 Object 都是这个Object。","tags":["基础","JVM"],"categories":["基础笔记"]},{"title":"2025.6.9学习日记","path":"/2025/06/09/学习日记25年6月/2025.6.9学习笔记/","content":"学习内容1. 力扣每日一题 ＋ 昨天的每日一题昨天又忘打卡力扣了,还欠了13天的,得五个月之后才能全续上了.今天的每日是字典序第K数字.昨天的也是字典序相关.做了两道字典序感觉理解了. 2. 学习JVM相关基础学习了JVM相关,并且记了一篇笔记. 3. 把KMP算法实践一下手画梳理了一下流程,然后构造了两遍之后感觉理解很多了.个人理解KMP的核心思想就是让主串索引不后退,匹配串索引通过next数组快速找到前缀相同的下标位置继续匹配.核心就是next数组的构造.这个算法理解后实现也比较复杂,有一堆的边界条件,一个比较好的思路是在主串和匹配串前面加一个空格作为哨兵,然后边界问题会少很多,只需要比较j+1和主串i位置即可. 4. 学习项目文档生活记录1. 足球训练今天七点起床,下楼练球,练颠球和逆足.10","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-06"]},{"title":"Java并发篇","path":"/2025/06/07/基础笔记/Java并发篇/","content":"使用线程三种方法 继承Thread类需要重写run方法，然后调用start方法启动线程。 实现Runnable接口需要重写run方法，然后调用start方法启动线程。 实现Callable接口需要重写call方法，然后调用start方法启动线程。有返回值，通过 FutureTask 进⾏封装。 实现接口还是继承类？实现接口更好一些，java不支持多继承，但可以实现多个接口。 基础线程机制ExecutorExecutor用来管理多个异步操作（多个任务互不干扰，不需要同步操作）。 主要有三种Executor：CachedThreadPool：⼀个任务创建⼀个线程；FixedThreadPool：所有任务只能使⽤固定⼤⼩的线程；SingleThreadExecutor：相当于⼤⼩为 1 的 FixedThreadPool。 DaemonDaemon守护线程是⼀个服务线程，⽤于为其他线程提供服务。所有非守护线程都执⾏完毕后，无论有没有守护线程，程序都会退出。线程启动前可以通过setDaemon() 方法来设置该线程是否为守护线程。 sleep()Thread.sleep() 使当前线程暂停执⾏指定的时间，暂停期间，其他线程可以继续运⾏，不会受到阻塞。sleep()可能会抛出InterruptedException异常，异常不会传回main()，所以必须在Thread类中捕获并处理。 yield()代表线程已经走完了重要的部分，可以让其他线程有机会执行。 中断线程完成会自动关闭，但是如果线程异常也会提前关闭。 InterruptedException 异常线程在 sleep() 或 wait() 时被中断，会抛出 InterruptedException 异常。 interrupted()如果一个线程处于无限循环中，并且没有执行 sleep() 或 wait()，那么可以通过 interrupted() 来判断线程是否被中断。如果线程被中断，interrupted() 会返回 true。如果线程没有被中断，interrupted() 会返回 false。 Executor的中断操作调用shutdown()方法会等待所有任务执行完毕后关闭Executor。调用shutdownNow()方法会中断所有任务，相当于调用每个任务的interrupt()方法，然后关闭Executor。 如果只想中断某一个线程，可以通过submit() 方法提交一个Callable任务，然后调用Future的cancel()方法来中断任务。 互斥同步Java 提供了两种锁机制来控制多个线程对共享资源的互斥访问，第⼀个是 JVM 实现的synchronized，另⼀个是 JDK 实现的 ReentrantLock。 synchronizedsynchronized 是 Java 中的关键字，它可以修饰方法和代码块。修饰方法时，锁的是当前对象。修饰代码块时，锁的是括号中的对象。 ReentrantLockReentrantLock 是 java.util.concurrent（J.U.C）包中的锁。 比较synchronized和ReentrantLock 锁的实现synchronized 是 JVM 实现的，⽽ ReentrantLock 是 JDK 实现的。 性能新版本 Java 对 synchronized 进⾏了很多优化，例如⾃旋锁，synchronized 与ReentrantLock 的性能⼤致相同。 等待可中断当持有锁的线程⻓期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。ReentrantLock 可中断，⽽ synchronized 不⾏。 公平锁公平锁是指多个线程在等待同⼀个锁时，必须按照申请锁的时间顺序来依次获得锁。synchronized 中的锁是⾮公平的，ReentrantLock 默认情况下也是⾮公平的，但是也可以是公平的。 锁绑定多个条件⼀个 ReentrantLock 可以同时绑定多个 Condition 对象。 使用选择除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。因为synchronized 是 JVM 实现的，可以保证⾃⼰的线程安全，⽽ ReentrantLock 需要程序员手动释放锁， 线程之间的协作join()在一个线程中调用另一个线程的 join() 方法，会将当前线程挂起，直到被调用的线程执行完毕。 wait() notify() notifyAll()wait() 使当前线程等待，直到其他线程调⽤ notify() 或 notifyAll() 方法。notify() 随机唤醒⼀个等待线程，notifyAll() 唤醒所有等待线程。wait()必须在 synchronized 块中调⽤,否则会抛出IllegalMonitorStateException异常。 wait()和sleep()的区别 wait() 是 Object 的⽅法，sleep() 是 Thread 的⽅法。 wait() 会释放锁，sleep() 不会释放锁。 wait() 可以被 notify() 或 notifyAll() 唤醒，sleep() 只能被中断。 wait() 必须在 synchronized 块中调⽤，sleep() 可以在任何位置调⽤。 await() signal() signalAll()java.util.concurrent 提供的 Condition 类，可以再Condition 上调⽤ await() 使线程等待. 相比wait()，await() 可以指定时间，超过时间会⾃动唤醒。 线程状态一个线程通常只有一种状态,并且这里特指jvm线程状态,而不是操作系统线程状态.线程状态有6种: 新建(New)创建后尚未启用. 可运行(Runnable)正在Java虚拟机中执行,但是它可能正在等待操作系统分配处理器资源. 阻塞(Blocked)线程被阻塞,等待其他线程完成操作. 等待(Waiting)线程等待其他线程执行特定操作. 计时等待(Timed Waiting)线程等待指定的时间. 终止(Terminated)线程已经完成执行. 状态转换 J.U.C - AQSjava.util.concurrent （J.U.C）⼤⼤提⾼了并发性能，AQS 被认为是 J.U.C 的核⼼。AQS是AbstractQueuedSynchronizer的缩写,是Java并发包中用来实现锁的基础框架. CountDownLatchCountDownLatch用来控制一个或多个线程等待其他线程完成操作. CyclicBarrier用来控制多个线程互相等待，直到到达某个公共屏障点（common barrier point）。和CountDownLatch不同的是，CyclicBarrier的计数器可以被重置后使用，所以它被称为循环屏障。 SemaphoreSemaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 J.U.C - 其它组件FutureTask实现了Future接口和Runnable接口,可以作为Runnable被线程执行,也可以用来获取异步执行的结果.适用于需要异步执行任务,并且需要获取结果的场景. BlockingQueue阻塞队列,可以用来实现生产者-消费者模式. java.util.concurrent.BlockingQueue 接⼝有以下阻塞队列的实现：FIFO 队列 ：LinkedBlockingQueue、ArrayBlockingQueue（固定⻓度）优先级队列 ：PriorityBlockingQueue ForkJoin和MapReduce类似,可以将⼤量的数据拆分成⼩量的数据，然后分⽴计算，最后将结果合并。 Java 内存模型Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到⼀致的内存访问效果。 主内存和工作内存主内存是所有线程共享的内存区域，工作内存是每个线程独有的内存区域。 所有的变量都存储在主内存中，每个线程还有⾃⼰的⼯作内存，⼯作内存存储在⾼速缓存或者寄存器中，保存了该线程使⽤的变量的主内存副本拷⻉。线程只能直接读写⾃⼰的⼯作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。 内存间的交互操作 java内存模型规定了8种操作来完成主内存和工作内存之间的交互操作：read：把⼀个变量的值从主内存传输到⼯作内存中load：在 read 之后执⾏，把 read 得到的值放⼊⼯作内存的变量副本中use：把⼯作内存中⼀个变量的值传递给执⾏引擎assign：把⼀个从执⾏引擎接收到的值赋给⼯作内存的变量store：把⼯作内存的⼀个变量的值传送到主内存中write：在 store 之后执⾏，把 store 得到的值放⼊主内存的变量中lock：作⽤于主内存的变量unlock:作⽤于主内存的变量 内存模型的三大特性原子性java内存模型保证了read、load、use、assign、store、write这6个操作是具有原子性的。但是不保证这6个操作的组合是具有原子性的。AtomicInteger 是⼀个提供原子操作的 Integer 类。除了使用原子类外，还可以通过 synchronized 关键字来保证操作的原子性。 可见性可⻅性指当⼀个线程修改了共享变量的值，其它线程能够⽴即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可⻅性的。主要有三种可见性的实现方式: volatile synchronized，对⼀个变量执⾏ unlock 操作之前，必须把变量值同步回主内存。 final，被 final 关键字修饰的字段在构造⽅法中⼀旦初始化完成，并且没有发⽣ this 逃逸（其它线程通过 this 引⽤访问到初始化了⼀半的对象），那么其它线程就能看⻅ final 字段的值。 有序性有序性指的是在本线程内观察，所有操作都是有序的；如果在⼀个线程观察另⼀个线程，所有操作都是无序的。Java 内存模型是通过禁止指令重排序来保证有序性的。主要有两种有序性的实现方式: volatile synchronized，对⼀个变量执⾏ unlock 操作之前，必须把变量值同步回主内存。 先⾏发⽣原则先⾏发⽣原则是指如果在程序中两个操作的先后顺序与代码中的顺序相同，那么这两个操作就会先⾏发⽣。 1. 单⼀线程原则在单⼀线程中，在程序前⾯的操作先⾏发⽣于后⾯的操作。 2. 管程锁定规则⼀个 unlock 操作先⾏发⽣于后⾯对同⼀个锁的 lock 操作。 3. volatile 变量规则对⼀个 volatile 变量的写操作先⾏发⽣于后⾯对这个变量的读操作。 4. 线程启动规则Thread 对象的 start() ⽅法调⽤先⾏发⽣于此线程的每⼀个动作。 5. 线程加⼊规则Thread 对象的结束先⾏发⽣于 join() ⽅法返回。 6. 线程中断规则对线程interrupt()⽅法的调⽤先⾏发⽣于被中断线程的代码检测到中断事件的发⽣，可以通过 interrupted() ⽅法检测到是否有中断发⽣。 7. 对象终结规则⼀个对象的初始化完成（构造⽅法执⾏结束）先⾏发⽣于它的 finalize() ⽅法的开始。 8. 传递性如果操作 A 先⾏发⽣于操作 B，操作 B 先⾏发⽣于操作 C，那么操作 A 先⾏发⽣于操作 C。 线程安全多个线程不管以何种⽅式访问某个类，并且在主调代码中不需要任何额外的同步或协调，这个类都能表现出正确的⾏为，那么就称这个类是线程安全的。 线程安全有以下几种实现方法:不可变不可变（Immutable）的对象⼀定是线程安全的，不需要再采取任何的线程安全保障措施。只要⼀个不可变的对象被正确地构建出来，永远也不会看到它在多个线程之中处于不⼀致的状态。多线程环境下，应当尽量使对象成为不可变，来满⾜线程安全。 不可变的类型：final 关键字修饰的基本数据类型String枚举类型Number 部分⼦类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等⼤数据类型。但同为 Number 的原⼦类 AtomicInteger 和 AtomicLong 则是可变的。对于集合类型，我们可以使⽤ Collections.unmodifiableXXX() 方法来获取⼀个不可变的集合。 互斥同步synchronized 和 ReentrantLock。 ⾮阻塞同步阻塞同步是一种悲观的并发策略，即认为只要不去做正确的同步措施，那就肯定会出现问题。而非阻塞是一种基于冲突检测的乐观并发策略，即不加锁，但是如果存在冲突，就重试当前操作直到成功。 CAS硬件⽀持的原⼦性操作最典型的是：⽐较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执⾏操作时，只有当 V的值等于 A，才将 V 的值更新为 B。 乐观锁需要操作、冲突检测这两个步骤具备原⼦性，这⾥就不能再使⽤互斥同步来保证了，只能靠硬件来完成。 AtomicIntegerJ.U.C 包⾥⾯的整数原⼦类 AtomicInteger 的⽅法调⽤了 Unsafe 类的 CAS 操作。 ABA如果⼀个变量初次读取的时候是 A 值，它的值被改成了 B，后来⼜被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。J.U.C 包提供了⼀个带有标记的原⼦引⽤类 AtomicStampedReference 来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。⼤部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改⽤传统的互斥同步可能会⽐原⼦类更⾼效。 无同步方案要保证线程安全，并不是⼀定就要进⾏同步。如果⼀个⽅法本来就不涉及共享数据，那它⾃然就⽆须任何同步措施去保证正确性。 栈封闭多个线程访问同⼀个⽅法的局部变量时，不会出现线程安全问题，因为局部变量存储在虚拟机栈中，属于线程私有的。 线程本地存储（Thread Local Storage）如果⼀个变量在线程的⼀个⽅法中被TLS变量存储，并被其他⽅法读取和修改，那么即使两个线程执⾏的是同⼀个代码，它们也会访问到不同的数据。 锁优化主要是针对synchronized关键字的优化。 ⾃旋锁⾃旋锁虽然能避免进⼊阻塞状态从⽽减少开销，但是它需要进⾏忙循环操作占⽤ CPU 时间，它只适⽤于共享数据的锁定状态很短的场景。 锁消除锁消除是指虚拟机即时编译器在运⾏时，对代码进⾏扫描，去除不可能存在共享数据竞争的锁，通过锁消除，可以节省毫无意义的请求锁时间。 锁粗化锁粗化是指虚拟机即时编译器在运⾏时，对代码进⾏扫描，将多个相邻的加锁操作合并为⼀个加锁操作，通过锁粗化，可以节省加锁和释放锁的时间。 轻量级锁JDK 1.6 引⼊了偏向锁和轻量级锁，从⽽让锁拥有了四个状态：⽆锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 轻量级锁相比传统的重量级锁，它使用CAS操作来避免线程阻塞和唤醒的开销，同时也避免了操作系统层面的线程调度。如果轻量级锁自旋或检测到有线程冲突，会升级为重量级锁。 偏向锁偏向锁是指在没有线程竞争的情况下，锁对象会偏向于使⽤它的线程，这样就不需要进⾏额外的加锁和解锁操作。 多线程开发良好的实践 给线程起个有意义的名字，这样可以⽅便找 Bug。 缩⼩同步范围，从⽽减少锁争⽤。例如对于 synchronized，应该尽量使⽤同步块⽽不是同步⽅法。 多⽤同步⼯具少⽤ wait()和 notify() 。⾸先，CountDownLatch, CyclicBarrier, Semaphore 和 Exchanger 这些同步类简化了编码操作，⽽⽤ wait()和 notify() 很难实现复杂控制流；其次，这些同步类是由最好的企业编写和维护，在后续的 JDK 中还会不断优化和完善。 使⽤ BlockingQueue 实现⽣产者消费者问题。 多⽤并发集合少⽤同步集合，例如应该使⽤ ConcurrentHashMap ⽽不是 Hashtable。 使⽤本地变量和不可变类来保证线程安全。 使⽤线程池⽽不是直接创建线程，这是因为创建线程代价很⾼，线程池可以有效地利⽤有限的线程来启动任务。","tags":["基础","并发","Java"],"categories":["基础笔记"]},{"title":"2025.6.7学习日记","path":"/2025/06/07/学习日记25年6月/2025.6.7学习笔记/","content":"学习内容1. 力扣每日一题https://leetcode.cn/problems/lexicographically-minimum-string-after-removing-stars?envType=daily-questionenvId=2025-06-07 先是尝试优先队列，最小堆找前面的最小元素，然后遇到*就删除，但时间复杂度为On2，特殊用例超时了。然后发现可以用栈模拟二十六个字母，栈中存储下标，遇到*出栈最小的字符。优化点：可以使用二进制掩码来表示最小的非空栈在哪里。 2. 学习完java并发篇笔记记录在java并发篇. 3. 把昨天的题用堆来完善一下维护小顶堆求前k大的元素。 4. 学习KMP算法.KMP算法是一种字符串匹配算法，它的时间复杂度为O(n+m)，其中n是文本串的长度，m是模式串的长度。贴一个模板,原理已经理解了，但是代码还需要再写一遍： class Solution // KMP 算法 // ss: 原串(string) pp: 匹配串(pattern) public int strStr(String ss, String pp) if (pp.isEmpty()) return 0; // 分别读取原串和匹配串的长度 int n = ss.length(), m = pp.length(); // 原串和匹配串前面都加空格，使其下标从 1 开始 ss = + ss; pp = + pp; char[] s = ss.toCharArray(); char[] p = pp.toCharArray(); // 构建 next 数组，数组长度为匹配串的长度（next 数组是和匹配串相关的） int[] next = new int[m + 1]; // 构造过程 i = 2，j = 0 开始，i 小于等于匹配串长度 【构造 i 从 2 开始】 for (int i = 2, j = 0; i = m; i++) // 匹配不成功的话，j = next(j) while (j 0 p[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++ if (p[i] == p[j + 1]) j++; // 更新 next[i]，结束本次循环，i++ next[i] = j; // 匹配过程，i = 1，j = 0 开始，i 小于等于原串长度 【匹配 i 从 1 开始】 for (int i = 1, j = 0; i = n; i++) // 匹配不成功 j = next(j) while (j 0 s[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++，结束本次循环后 i++ if (s[i] == p[j + 1]) j++; // 整一段匹配成功，直接返回下标 if (j == m) return i - m; return -1;","tags":["基础","日记","leetcode","项目"],"categories":["学习日记","2025-06"]},{"title":"java30天学习笔记","path":"/2025/06/06/基础笔记/java30天笔记/","content":"杂项最好不使用 clone()来进行复制,可以使用拷贝构造函数或拷贝工厂来复制对象. 抽象类提供了⼀种 IS-A 的关系接⼝更像是⼀种 LIKE-A 关系 Fail-Fast 机制Fail-Fast 机制是 Java 集合(Collection)中的⼀种错误机制。当多个线程对同一个集合的内容进⾏操作时，就可能产⽣线程安全问题。Fail-Fast 机制会⾃动检测到线程安全问题,在操作前后比较集合的结构变化次数是否相同，并抛出 ConcurrentModificationException 异常。 tips:禁⽌在foreach⾥执⾏元素的删除操作 容器的一些解析vector和arraylist的比较vector是同步的，所以开销更大vector每次扩容请求2倍，而arraylist是1.5倍 如果对线程安全有要求的话，可以选择Collections.synchronizedList() 或者使用CopyOnWriteArrayList保证线程安全。 CopyOnWriteArrayList写操作在拷贝的数组上进行，而读操作在原数组上进行。写操作需要加锁，防止并发写操作。适用于读多写少的场景。问题:内存占用约占原数组的两倍。数据一致性问题。 所以CopyOnWriteArrayList更加适合对内存不敏感以及实时性要求很高的场景。 LinkedListArrayList是基于数组实现的，而LinkedList是基于链表实现的。数组支持随机访问，而链表不支持随机访问。数组的插入和删除操作需要移动元素，而链表的插入和删除操作只需要修改指针。数组的空间利用率高，而链表的空间利用率低。LinkedList适用于需要频繁插入和删除元素的场景。 HashMap相当于分成了很多个桶，每个桶里面是一个链表，链表的每个节点是一个键值对，使用头插法插入节点。 ConcurrentHashMapConcurrentHashMap是线程安全的HashMap，它的实现方式是使用分段锁。ConcurrentHashMap将整个HashMap分成了多个段，每个段都是一个HashMap。每个段都有一个锁，当一个线程访问一个段时，其他线程也可以访问其他段。 LinkedHashMapLinkedHashMap是HashMap的子类，它的实现方式是使用双向链表。LinkedHashMap的迭代顺序是插入顺序或者访问顺序。可以通过LinkedHashMap实现LRU缓存。 WeakHashMap主要是用来实现缓存的。WeakHashMap的键是弱引用，当键不再被引用时，键值对会被自动移除。WeakHashMap的迭代器是弱引用的，所以在迭代时可能会出现空指针异常。 ConcurrentCacheConcurrentCache是一个线程安全的缓存，它的实现方式是使用ConcurrentHashMap。","tags":["java"],"categories":["基础笔记"]},{"title":"2025.6.6学习日记","path":"/2025/06/06/学习日记25年6月/2025.6.6学习笔记/","content":"学习内容1. 优化了一下个人博客的图片上传工作流由于大部分笔记在语雀中,导致图片上传后外链无法访问,所以今天通过picgo➕github搭建了一个图床.具体流程如下: 首先需要在github中创建一个仓库,用来存放图片 然后在picgo中配置github的仓库,将图片上传到github中操作如下:mac:shift+control+command+4 截图 然后在vscode中option+command+u上传到图床后会自动生成markdown的链接. 2. 优化了一下自己记笔记发博客的工作流语雀写笔记还是太麻烦了。所以准备以后还是用vscode写笔记,然后直接运行一行命令同步到仓库,而且还有补全工具。 3. 力扣两道每日一题：这题思路很清晰，贪心加一个栈的模拟题，实现有一些小细节需要注意。前K个高频元素：map记录频率，然后小顶堆维护前K个元素。（第一遍做用了单调栈，忘记用小顶堆维护了）做了关于最大最小堆的笔记。 4. 看了java容器相关的八股文记在java30天笔记里面了。 5. 项目部分看了session登录拦截器相关的部分6. 杂谈感觉需要增加基础的学习，项目可以先读一遍文档。","tags":["基础","日记","leetcode","项目","博客","picgo"],"categories":["学习日记","2025-06"]},{"title":"好用的快捷键和操作","path":"/2025/06/06/杂项笔记/好用的快捷键或操作/","content":"MAC系统操作截图到剪切板mac:shift+control+command+4 最小化窗口mac:command+m 关闭窗口mac:command+w 全屏mac:command+control+f IDEAswitch2cursor插件mac:shift + option + o 快速在cursor中打开当前文件mac:shift + option + p 快速在cursor中打开当前工程 VSCODE单行上下移动mac:shift+option+up/down 多行上下移动mac:shift+option+command+up/down 删除本行mac:command+shift+k 开关左侧项目树mac:command+1 重命名文件mac:fn+shift+f6 VSCODE picgo插件上传到图床mac:option+command+u 关闭终端窗口mac:command+j 终端Warp到hexo根目录cd /Users/mac/Blog/JakicDong.github.io hexo清理并且重新上传hexo clean hexo g hexo d 本地部署hexo s Edge浏览器标签页左右移动mac:command+option+left/right 关闭标签页mac:command+w 开发者工具mac:command+option+i fn+f12","tags":["快捷键"],"categories":["杂项笔记"]},{"title":"Hello World","path":"/2025/06/05/杂项笔记/hello-world/","content":"hexo的基本使用和模板常用模版 ---title: 个人博客搭建tags: [博客, hexo]date: 2025-06-01--- 快捷部署: hexo clean hexo g hexo d Quick Start创建一篇文章$ hexo new My New Post 更多信息: Writing 运行服务$ hexo server 更多信息: Server 生成静态文件$ hexo generate 更多信息: Generating 部署到远程站点$ hexo deploy 更多信息: Deployment 文章模版---# 基本信息title: title date: date tags: []categories: []description: # excerpt 也可 # 封面cover: banner: poster: # 海报（可选，全图封面卡片） topic: 标题上方的小字 # 可选 headline: 大标题 # 必选 caption: 标题下方的小字 # 可选 color: 标题颜色 # 可选# 插件sticky: # 数字越大越靠前mermaid:katex: mathjax: # 可选topic: # 专栏 idauthor: references:comments: # 设置 false 禁止评论indexing: # 设置 false 避免被搜索breadcrumb: # 设置 false 隐藏面包屑导航leftbar: rightbar:h1: # 设置为 隐藏标题type: # tech/story---","tags":["欢迎页"],"categories":["杂项笔记"]},{"title":"2025.6.5学习日记","path":"/2025/06/05/学习日记25年6月/2025.6.5学习日记/","content":"学习内容1. hexo轻量化框架搭建个人博客搭建了个人博客网站.简化了一下笔记的流程:直接本地写markdown笔记然后直接运行一行命令同步太仓库,比较方便.todo:后期可以考虑加一个打卡墙. 2. leetcode每日一题并查集的题,太久没做图论有点忘了 class Solution public String smallestEquivalentString(String s1, String s2, String baseStr) int[] fa = new int[26]; for (int i = 0; i 26; i++) fa[i] = i; for (int i = 0; i s1.length(); i++) merge(fa, s1.charAt(i) - a, s2.charAt(i) - a); char[] s = baseStr.toCharArray(); for (int i = 0; i s.length; i++) s[i] = (char) (find(fa, s[i] - a) + a); return new String(s); private int find(int[] fa, int x) if (fa[x] != x) fa[x] = find(fa, fa[x]); return fa[x]; private void merge(int[] fa, int x, int y) int fx = find(fa, x); int fy = find(fa, y); // 把大的代表元指向小的代表元 if (fx fy) fa[fy] = fx; else fa[fx] = fy; 3. 健身练胸日","tags":["日记","leetcode"],"categories":["学习日记","2025-06"]},{"title":"技术派项目笔记","path":"/2025/06/04/项目笔记/技术派项目笔记/","content":"开启新项目要考虑的事情业务模块拆解首先对业务模块进行拆解，除了业务属性纬度以外，还有一个很重要的属性就是参与者角色。 首先对功能、模块划分、概要设计，详细设计有初步的了解。 主要就是功能模块设计 + DB 的设计。 启动项目mysql 启动redis 启动：路径 ： D:\\workTools\\Redis-x64-3.2.100 redis-server.exe redis.windows.conf 启动成功，点击进入首页: http://127.0.0.1:8080 跑环境D:\\sys\\Desktop\\Workplace\\IDEA_Projects\\paicoding下载位置 git clone git@github.com:itwanger/paicoding.git D:\\sys\\Desktop\\Workplace\\IDEA_Projects\\paicodinggit 下载 git clone 出现报错 ， 原因：github ssh 秘钥没有配置 1 打开运行，输入services.msc，确定 找到 OpenSSH Authentication Agent 服务，需开启它. ssh-keygen -t rsa -C “你的邮箱地址” 我用的是Administrator用户，执行完后，可以在 C:\\Users\\Administrator.ssh 目录下生成 id_rsa 和 id_rsa.pub 这两个文件。如果你没有用Administrator用户，也是在类似的目录下 项目结构该项目主要有五个模块，各模块功能如下： paicoding-api： 用于定义一些通用的枚举、实体类，包含 DO（数据对象）、DTO（数据传输对象）、VO（视图对象）等，为不同层之间的数据交互提供统一的格式和规范。 paicoding-core： 核心工具和组件相关的模块，像工具包 util 以及通用的组件都放在这里。按照包路径对模块功能进行拆分，例如搜索、缓存、推荐等功能组件。 paicoding-service： 服务模块，负责业务相关的主要逻辑，数据库（DB）的操作都在这个模块中进行。 paicoding-ui： 存放 HTML 前端资源，包括 JavaScript、CSS、Thymeleaf 等，主要用于构建用户界面。 paicoding-web： Web 模块，是 HTTP 请求的入口，也是项目启动的入口，同时包含权限身份校验、全局异常处理等功能。 web 模块：admin：admin 目录存放和管理后台相关的代码，主要处理管理员对系统的管理操作。 common：common 一般用来存放项目通用的代码，提高代码的复用性和可维护性。 comoinent：TemplateEngineHelper Thymeleaf模版渲染引擎，通过末班引擎进行服务端渲染（SSR），在初次渲染速度方面有显著优势。 configForumDataSourceInitializer 用来进行数据库表的初始化，首次启动时候执行： DbChangeSetLoader 杂项笔记请求参数解析如果一个请求不会引起服务器上任何资源的状态变化，那就可以使用 GET 请求 AOP技术派中的 AOP 记录接口访问日志是放在 paicoding-core 模块下的 mdc 包下。 三层架构为什么要使用微服务而不是单体项目呢？用不用微服务取决于业务量，能用单体的绝对不用微服务，毕竟单体的好处显而易见，当业务简单的时候，部署非常简单，技术架构也简单，不用考虑微服务间的调用什么的，但是随着业务的复杂，单体的缺点也就暴露出来了，例如修改一个模块上线，就要整个服务下线，这在某些业务中是不被允许的，其次单体复杂度高了，部署就缓慢了，出现问题排查也很困难，这些的前提就是业务复杂度提高了。 所以微服务的出现在我看来最初就是为了解决业务复杂单体所出现的问题的，将业务拆分到不同的模块，不同的模块单独部署开发，提高了开发效率，节省了维护时间成本，问题排查也方便了很多，微服务也并不是没有缺点，只不过是维护一个平衡，例如需要引入注册中心，为了方便配置的修改，还需要引入配置中心，不可能修改一个配置重新打包发布，服务间的调用组件，很多都是为了使用微服务而引入的。 所以没有哪种技术更好，只有哪种技术更符合当下的业务，抛开业务谈技术，在我看来并不是那么可靠。 工厂模式创建型设计模式，定义一个创建对象的接口，但让实现这个接口的类决定实例化哪个类。 工厂方法把类的实例化延迟到子类中进行。 @Bean 就是一个工厂方法 各种注解@Slf4j@Slf4j：借助 Lombok 自动添加 SLF4J 日志记录器，简化日志记录代码。 @Value@Configuration@Configuration：把类标记为 Spring 配置类，允许在类里使用 @Bean 注解定义 Spring Bean。 在 Spring 应用启动时，Spring 会扫描带有 @Configuration 注解的类，将其作为配置类来处理，把类里使用 @Bean 注解定义的方法返回的对象注册到 Spring 容器中。 @Component@Component：通用的组件注解，用于标记一个类为 Spring 组件，Spring 会自动扫描并将其注册到容器中。 @Service@Service：@Component 的特殊化注解，通常用于标记服务层类。 @Repository@Repository：@Component 的特殊化注解，通常用于标记数据访问层类。 @Controller@Controller：@Component 的特殊化注解，通常用于标记 Web 控制器类。 @Bean@Bean注册一个实体类 @册一个实体类 实体对象 用 GET 还是 POSTGET - 从指定的资源请求数据。POST - 向指定的资源提交要被处理的数据。 GET 请求是 HTTP 协议中的一种请求方法，通常用于请求访问指定的资源。如果一个请求不会导致服务器上任何资源的状态变化，那你就可以使用 GET 请求。 Filter过滤器 首先进入 filter，执行相关业务逻辑 若判定通行，则进入 Servlet 逻辑，Servlet 执行完毕之后，又返回 Filter，最后在返回给请求方 判定失败，直接返回，不需要将请求发给 Servlet 过滤器的使用如果要使用过滤器，实现 Filter 接口，需要重写 doFilter 方法，在方法中编写过滤逻辑。init: 初始化时执行destory: 销毁时执行doFilter: 重点关注这个，filter 规则命中的请求，都会走进来三个参数，注意第三个 FilterChain，这里是经典的责任链设计模式执行 filterChain.doFilter(servletRequest, servletResponse) 表示会继续将请求执行下去；若不执行这一句，表示这一次的 http 请求到此为止了，后面的走不下去了 过滤器在项目中的应用 身份识别,并保存身份到ReqInfoContext上下文中 记录请求记录 添加跨域支持 跨域问题:跨域问题（CORS）的本质是浏览器的安全限制，而代理服务器是解决该问题的关键方案之一。以下通过​​场景化解析​​帮你彻底理解代理机制. 假如说:前端​​：运行在 http://localhost:5700​​后端​​：运行在 http://localhost:8080​​问题​​：前端直接请求后端接口时，浏览器会拦截并报错： 代理与服务器间的通信属于服务器之间的通信,不受浏览器同源规则的约束. ServletServlet的使用姿势，以及注册自定义的Servelt的四种姿势 ● @WebServlet 注解:在自定义的servlet上添加Servlet3+的注解@WebServlet，来声明这个类是一个Servlet和Fitler的注册方式一样，使用这个注解，需要配合Spring Boot的@ServletComponentScan，否则单纯的添加上面的注解并不会生效 /** * 使用注解的方式来定义并注册一个自定义Servlet */@WebServlet(urlPatterns = /annotation)public class AnnotationServlet extends HttpServlet @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException String name = req.getParameter(name); PrintWriter writer = resp.getWriter(); writer.write([AnnotationServlet] welcome + name); writer.flush(); writer.close(); 还需要配置启动类 @ServletComponentScan@SpringBootApplicationpublic class Application public static void main(String[] args) SpringApplication.run(Application.class); ● ServletRegistrationBean bean定义在Filter的注册中，我们知道有一种方式是定义一个Spring的BeanFilterRegistrationBean来包装我们的自定义Filter，从而让Spring容器来管理我们的过滤器；同样的在Servlet中，也有类似的包装bean: ServletRegistrationBean自定义的bean如下，注意类上没有任何注解: public class RegisterBeanServlet extends HttpServlet @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException String name = req.getParameter(name); PrintWriter writer = resp.getWriter(); writer.write([RegisterBeanServlet] welcome + name); writer.flush(); writer.close(); 接下来我们需要定义一个ServletRegistrationBean，让它持有RegisterBeanServlet的实例 @Beanpublic ServletRegistrationBean servletBean() ServletRegistrationBean registrationBean = new ServletRegistrationBean(); registrationBean.addUrlMappings(/register); registrationBean.setServlet(new RegisterBeanServlet()); return registrationBean; ● ServletContext 动态添加这种姿势，在实际的Servlet注册中，其实用得并不太多，主要思路是在ServletContext初始化后，借助javax.servlet.ServletContext#addServlet(java.lang.String, java.lang.Class? extends javax.servlet.Servlet)方法来主动添加一个Servlet 所以我们需要找一个合适的时机，获取ServletContext实例，并注册Servlet，在SpringBoot生态下，可以借助ServletContextInitializer public class ContextServlet extends HttpServlet @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException String name = req.getParameter(name); PrintWriter writer = resp.getWriter(); writer.write([ContextServlet] welcome + name); writer.flush(); writer.close(); @Componentpublic class SelfServletConfig implements ServletContextInitializer @Override public void onStartup(ServletContext servletContext) throws ServletException ServletRegistration initServlet = servletContext.addServlet(contextServlet, ContextServlet.class); initServlet.addMapping(/context);","tags":["项目","技术派"],"categories":["项目笔记"]},{"title":"个人博客搭建","path":"/2025/06/02/杂项笔记/个人博客搭建/","content":"环境配置: node -v #查看node版本npm -v #查看npm版本 可以使用nvm来管理node版本 安装 hexo: npm install hexo-cli -g 出现权限问题 ,修改: sudo chown -R $(whoami) $(npm config get prefix)/lib/node_modules,bin,share #修改权限范围 初始化个人博客文件夹: hexo init /Users/mac/Blog/JakicDong.github.io # 博客初始化，这里是创建的本地博客文件夹，执行后会自动创建，我这里图简洁，直接用了我网站的地址 hihulu.github.io 为文件名cd /Users/mac/Blog/JakicDong.github.io # 进入本地的博客文件夹hexo server # 打开本地服务器预览 UsersmacJakicDong.github.io 4.Github部署为了部署到Github上，需要安装hexo-deployer-git插件，命令如下： sudo npm install hexo-deployer-git --save 然后找到自己的本地博客文件夹，修改博客根目录下的_config.yml文件中的deploy，修改成： deploy: type: git repo: git@github.com:JakicDong/JakicDong.github.io.git #这个地址是从github仓库复制过来的ssh branch: main （⚠️注意，这里有一个很容易犯错的点，我们在创建“hihulu.github.io”这个仓库的时候，一定要创建和你github用户名相同的仓库，后面加.http://github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是http://xxxx.github.io，其中xxx就是你注册GitHub的用户名。所以上图我的仓库显示的是“hihuluhihulu.github.io”，如果仓库名和用户名不一致，后面是根本打不开这个网站的～） 然后就可以通过以下命令行上传github了 hexo g #hexo generate的简写，即把刚刚做的改动生成更新一下hexo d #hexo deploy，上传到github网站 还有一些常用的命令行： hexo clean #清空一下缓存，有时候博客页面显示不正常也可以试试这个命令行hexo server # 在本地服务器运行，网址默认https://localhost:4000 5.更改主题很多前端大牛博主设计了很多好看的主题，网址https://hexo.io/themes/ ，可以预览并选择你喜爱的主题进行应用。 这里浅以一个蛮火的主题butterfly主题来走一个安装主题的步骤～ 执行以下代码： git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly 运行成功之后，在项目文件夹根目录中可以查看到新的主题themes文件夹：butterfly 在博客的项目文件夹下，修改_config.yml配置文件如下: # theme: landscape 默认主题theme: butterfly 此时主题还不能正常配置使用，需要安装pug 以及stylus 的渲染器： npm install hexo-renderer-pug hexo-renderer-stylus --save 最后推送到github # 清除缓存b.json 和已生成的静态文件 publichexo clean# 生成静态页面到默认设置的 public 文件夹hexo g# 部署到设定的仓库或上传部署至服务端hexo d 至此主题安装end，可访问https://hihulu.github.io/ 查看～ https://JakicDong.github.io/ # 7.4 常用命令hexo new name # 新建文章hexo new page name # 新建页面hexo g # 生成页面hexo d # 部署hexo g -d # 生成页面并部署hexo s # 本地预览hexo clean # 清除缓存和已生成的静态文件hexo help # 帮助","tags":["博客","hexo"],"categories":["杂项笔记"]},{"title":"RabbitMQ项目使用","path":"/2025/06/01/项目笔记/RabbitMQ项目使用/","content":"brew services start rabbitmq# 停止服务 brew services stop rabbitmq# 重启服务brew services restart rabbitmq http://localhost:15672/","tags":["项目","RabbitMQ"],"categories":["项目笔记"]},{"title":"算法学习笔记","path":"/2025/06/01/算法笔记/算法题刷题笔记/","content":"算法笔记一些好的逻辑思路反问题如果一个问题正问题很复杂,可以尝试想一下反问题,比如求有恰好k个相邻数的数组个数,正问题很复杂,但是反问题只需要找到n-k-1个分割线,把数组分割成n-k块.3405. 统计恰好有 K 个相等相邻元素的数组数目 子问题如果一个大的问题可以分解成多个子问题,可以尝试先解决子问题,再解决大问题.这就可以用递归或动态规划的方法来解决. 邻域消除问题对于临项消除问题,并且先后顺序无关,可以考虑使用栈来实现.从左到右来执行. ACM 格式import java.util.*;import java.io.*;public class Main public static void main(String[] args) throws IOException BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); PrintWriter pw = new PrintWriter(System.out); int n = Integer.parseInt(br.readLine()); String[] times = br.readLine().split( ); long prev = parseTime(times[0]); for (int i = 1; i n; i++) long current = parseTime(times[i]); long diff = current - prev; if (diff 0) diff += 24 * 60 * 60; double circles = diff / 60.0; pw.printf(%.2f , circles); prev = current; pw.flush(); private static long parseTime(String time) String[] parts = time.split(:); int hour = Integer.parseInt(parts[0]); int minute = Integer.parseInt(parts[1]); int second = Integer.parseInt(parts[2]); return hour * 3600L + minute * 60L + second; import java.util.*;import java.io.*;public class Main public static void main(String[] args) throws IOException BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); String line; while ((line = br.readLine()) != null) // 逐行读取 int n = Integer.parseInt(line.trim()); int[] nums = new int[n]; String[] parts = br.readLine().split( ); for (int i = 0; i n; i++) nums[i] = Integer.parseInt(parts[i]); // 处理逻辑 // 输出结果 System.out.println(result); public static void main(String[] args) Scanner scanner = new Scanner(System.in); String s = scanner.next(); System.out.println(replaceNumber(s)); scanner.close(); 第二种: import java.util.*;import java.io.*;public static void main(String[] args) Scanner scanner = new Scanner(System.in); String s = scanner.next(); System.out.println(replaceNumber(s)); scanner.close(); 杂项操作绝对值:Math.abs();Int最大值:Integer.MAX_VALUE前导零:Integer.numberOfLeadingZeros(k); 取随机值： import java.util.Random;import java.util.concurrent.ThreadLocalRandom;public class RandomDemo public static void main(String[] args) int n = 10; //0~9 // 方法1: Math.random() int random1 = (int) (Math.random() * n); System.out.println(Math.random(): + random1); // 方法2: Random类 Random rand = new Random(); int random2 = rand.nextInt(n); System.out.println(Random: + random2); // 方法3: ThreadLocalRandom（多线程安全） int random3 = ThreadLocalRandom.current().nextInt(n); System.out.println(ThreadLocalRandom: + random3); Array 常用操作// ========== 数组声明与初始化 ==========int[] arr = new int[5]; // 创建空数组 [0,0,0,0,0]int[] nums = 3,1,4,5,2; // 直接初始化String[] strArr = A, B, C; // 字符串数组int[][] matrix = 1,2, 3,4,5; // 二维数组// ========== 基本操作 ==========// 访问元素int first = nums[0]; nums[2] = 9; // 获取长度int len = nums.length; // 一维数组长度 → 5int cols = matrix[1].length; // 二维数组第二维长度 → 3// ========== 遍历操作 ==========// 普通for循环for(int i=0; inums.length; i++) System.out.print(nums[i] + ); // 3 1 9 5 2// 增强for循环for(int num : nums) System.out.print(num + );// 二维数组遍历for(int[] row : matrix) for(int n : row) System.out.print(n + ); // 1 2 / 3 4 5 数组工具操作 // ========== 数组工具操作 ==========// ========== 排序操作 ==========//升序排序Arrays.sort(array);//降序排序Arrays.sort(Collections.reverseOrder());//自定义排序Arrays.sort(array, (a, b) - a - b); //升序Arrays.sort(array, (a, b) - b - a); //降序// 复制int[] copy1 = Arrays.copyOf(nums, 3); // 复制前3元素 → [1,2,3]int[] copy2 = Arrays.copyOfRange(nums, 1, 4); // 复制1-3索引 → [2,3,5]// 比较boolean isEqual = Arrays.equals(nums, copy1); // false// 填充Arrays.fill(nums, 0); // 全部填充0 → [0,0,0,0,0]Arrays.fill(nums, 1, 3, 5); // 索引1-2填充5 → [0,5,5,0,0]// ========== 类型转换 ==========// 数组转List（固定大小）ListInteger list = Arrays.asList(1,2,3); // 数组转StreamIntStream stream = Arrays.stream(nums);// ========== 其他操作 ==========// 二分查找（需先排序）int index = Arrays.binarySearch(nums, 5); // 返回元素索引// 多维数组操作int[][] matrixCopy = Arrays.copyOf(matrix, matrix.length); // 浅拷贝String deepStr = Arrays.deepToString(matrix); // [[1, 2], [3, 4, 5]]// 反转数组for(int i=0; inums.length/2; i++) int temp = nums[i]; nums[i] = nums[nums.length-1-i]; nums[nums.length-1-i] = temp; 数组带着索引排序例题 public int[] maxSubsequence(int[] nums, int k) int n=nums.length; int ans[]=new int[k]; Integer idx[]=new Integer[n]; Arrays.setAll(idx, i-i); Arrays.sort(idx,(a,b)-nums[a]-nums[b]); int j=0; for(int i=n-k;in;i++) ans[j++]=idx[i]; Arrays.sort(ans); for(int i=0;ik;i++) ans[i]=nums[ans[i]]; return ans; 和 int 数组相互转换： Integer[] arr = 1,2,3;ArrayListInteger list = new ArrayList(Arrays.asList(arr));ArrayListInteger list = new ArrayList();Integer[] arr = list.toArray(new Integer[0]);//但是如何转成int[]数组呢//方法1 arr = list.stream().mapToInt(Integer::valueOf).toArray(); List 常用操作初始化 // ====================== 1. 初始化 List ======================ListString arrayList = new ArrayList(); // 可修改列表ListInteger linkedList = new LinkedList(); ListString immutableList = List.of(A, B, C); // Java9+ 不可变列表 增删改查 // ====================== 2. 添加元素 ======================arrayList.add(Apple); // 末尾添加arrayList.add(0, Banana); // 索引0插入arrayList.addAll(List.of(Orange, Grape)); // 批量添加// ====================== 3. 访问元素 ======================String firstElement = arrayList.get(0); // Bananaboolean containsApple = arrayList.contains(Apple); // trueint indexOfOrange = arrayList.indexOf(Orange); // 2// ====================== 4. 删除元素 ======================arrayList.remove(Banana); // 按值删除arrayList.remove(0); // 按索引删除arrayList.clear(); // 清空列表// ====================== 5. 修改元素 ======================arrayList.add(Mango);arrayList.set(0, Pineapple); // 修改索引0的元素// ====================== 6. 遍历 List ======================Collections.sort(path); // 直接修改原ListCollections.sort(path, Collections.reverseOrder()); // 降序排序// 方式1: 普通for循环for (int i = 0; i arrayList.size(); i++) System.out.println(arrayList.get(i));// 方式2: 增强for循环for (String fruit : arrayList) System.out.println(fruit);// 方式3: 迭代器IteratorString it = arrayList.iterator();while (it.hasNext()) System.out.println(it.next());// 方式4: forEach + LambdaarrayList.forEach(System.out::println); 其他操作 // ====================== 7. 其他操作 ======================int size = arrayList.size(); // 列表长度boolean isEmpty = arrayList.isEmpty(); // 是否为空ListString subList = arrayList.subList(0, 2); // 截取子列表Object[] array = arrayList.toArray(); // 转为数组// ====================== 8. 注意事项 ======================/*1. ArrayList初始容量10，扩容1.5倍 2. LinkedList用节点链接实现3. 线程不安全，多线程环境使用：ListString syncList = Collections.synchronizedList(new ArrayList());4. 快速失败机制(fast-fail)：遍历时修改会抛出ConcurrentModificationException*/ 二维List例子 import java.util.ArrayList;import java.util.Arrays;import java.util.List;public class TwoDListExample public static void main(String[] args) // ====================== // 1. 创建二维List // ====================== ListInteger[] ls = new ArrayList[26]; Arrays.setAll(ls,i-new ArrayList()); // 方法1: 使用Arrays.asList()初始化 ListListInteger matrix1 = new ArrayList(); matrix1.add(Arrays.asList(1, 2, 3)); matrix1.add(Arrays.asList(4, 5, 6)); matrix1.add(Arrays.asList(7, 8, 9)); // 方法2: 动态创建空二维List ListListString matrix2 = new ArrayList(); // 方法3: 使用嵌套循环初始化 ListListCharacter matrix3 = new ArrayList(); for (int i = 0; i 3; i++) ListCharacter row = new ArrayList(); for (int j = 0; j 4; j++) row.add((char) (A + i + j)); matrix3.add(row); // ====================== // 2. 添加元素 // ====================== // 添加新行 matrix2.add(new ArrayList(Arrays.asList(Java, Python))); matrix2.add(new ArrayList(Arrays.asList(C++, JavaScript))); // 在指定行添加元素 matrix2.get(0).add(Ruby); // 第一行添加元素 matrix2.get(1).add(0, Go); // 第二行开头插入元素 // 添加新行（空行） matrix2.add(new ArrayList()); matrix2.get(2).add(Swift); // 给新行添加元素 // ====================== // 3. 访问元素 // ====================== // 访问单个元素 int element = matrix1.get(1).get(2); // 获取第二行第三列元素 → 6 String lang = matrix2.get(0).get(1); // 获取第一行第二列元素 → Python // 获取行数 int rows = matrix1.size(); // 获取列数（特定行） int colsRow0 = matrix1.get(0).size(); int colsRow2 = matrix2.get(2).size(); // ====================== // 4. 修改元素 // ====================== matrix1.get(0).set(0, 100); // 修改第一行第一列: 1 → 100 matrix2.get(1).set(2, TypeScript); // 修改第二行第三列 // ====================== // 5. 删除元素 // ====================== // 删除指定位置的元素 matrix1.get(2).remove(1); // 删除第三行第二列元素(8) // 删除整行 matrix2.remove(2); // 删除第三行 // ====================== // 6. 遍历二维List // ====================== System.out.println( 遍历matrix1:); // 方法1: 索引遍历 for (int i = 0; i matrix1.size(); i++) for (int j = 0; j matrix1.get(i).size(); j++) System.out.print(matrix1.get(i).get(j) + ); System.out.println(); System.out.println( 遍历matrix2:); // 方法2: 增强for循环 for (ListString row : matrix2) for (String item : row) System.out.print(item + ); System.out.println(); System.out.println( 遍历matrix3:); // 方法3: 使用forEach + lambda matrix3.forEach(row - row.forEach(item - System.out.print(item + )); System.out.println(); ); // ====================== // 7. 其他常用操作 // ====================== // 检查是否为空 boolean isEmpty = matrix2.isEmpty(); // 检查是否包含元素 boolean containsPython = matrix2.get(0).contains(Python); // 查找元素位置 int rowIndex = -1, colIndex = -1; for (int i = 0; i matrix1.size(); i++) int index = matrix1.get(i).indexOf(5); if (index != -1) rowIndex = i; colIndex = index; break; // 转换为二维数组 String[][] array2D = new String[matrix2.size()][]; for (int i = 0; i matrix2.size(); i++) ListString row = matrix2.get(i); array2D[i] = row.toArray(new String[0]); ////翻转行 //方法1 使用工具类 ListListInteger matrix = new ArrayList(); // 假设 matrix 已经填充数据 // 翻转行（首行变末行，次行变次末行，依此类推） Collections.reverse(matrix); //方法2 ListListInteger matrix = new ArrayList(); // 假设 matrix 已经填充数据 int left = 0; int right = matrix.size() - 1; while (left right) // 使用临时行 temp 交换左右两行 ListInteger temp = matrix.get(left); matrix.set(left, matrix.get(right)); matrix.set(right, temp); left++; right--; //方法3 Stream ListListInteger matrix = new ArrayList(); // 假设 matrix 已经填充数据 // 翻转行顺序 ListListInteger reversed = IntStream.range(0, matrix.size()) .mapToObj(i - matrix.get(matrix.size() - 1 - i)) .collect(Collectors.toList()); matrix = reversed; // 如果需要原地翻转，需重新赋值 // 打印结果 System.out.println( matrix1: + matrix1); System.out.println(matrix2: + matrix2); System.out.println(matrix3: + matrix3); System.out.println(找到数字5的位置: [ + rowIndex + ][ + colIndex + ]); HashSet 常用操作// =================== HashSet 基础操作 ===================import java.util.Collections;import java.util.HashSet;import java.util.Iterator;import java.util.Set;// 创建对象SetString set = new HashSet(); // 空集合（默认初始容量16，负载因子0.75）SetInteger initSet = new HashSet(32); // 指定初始容量SetString prefilled = new HashSet(Arrays.asList(A, B, C)); // 通过集合初始化// 元素操作boolean added = set.add(Apple); // 添加元素 → true（首次添加）boolean dupAdd = set.add(Apple); // 添加重复元素 → falseboolean hasBanana = set.contains(Apple); // 检查存在 → trueboolean removed = set.remove(Apple); // 删除元素 → true（存在时）set.clear(); // 清空集合// 批量操作SetString fruits = new HashSet(Arrays.asList(Orange, Mango));boolean addedAll = set.addAll(fruits); // 合并集合 → true（集合改变时）boolean retainAll = set.retainAll(Arrays.asList(Mango)); // 保留交集 → true（集合改变时）boolean removeAll = set.removeAll(fruits); // 删除所有匹配元素 → true（集合改变时）// 遍历操作set.add(Apple);set.add(Banana);for (String item : set) // 增强for循环（无序） System.out.print(item + ); // 输出顺序不确定（如 Banana Apple）IteratorString it = set.iterator(); // 迭代器遍历while (it.hasNext()) System.out.print(it.next() + );set.forEach(item - System.out.print(item)); // Java8+ Lambda遍历// 集合信息int size = set.size(); // 元素数量（如 2）boolean isEmpty = set.isEmpty(); // 是否空集合 → falseObject[] array = set.toArray(); // 转Object数组String[] strArray = set.toArray(new String[0]); // 转指定类型数组// 特殊操作SetString cloneSet = (HashSetString) ((HashSetString) set).clone(); // 浅拷贝SetString syncSet = Collections.synchronizedSet(set); // 线程安全包装// 集合运算示例SetString set1 = new HashSet(Arrays.asList(A, B));SetString set2 = new HashSet(Arrays.asList(B, C));SetString union = new HashSet(set1); // 并集 → [A, B, C]union.addAll(set2);SetString intersection = new HashSet(set1); // 交集 → [B]intersection.retainAll(set2);SetString difference = new HashSet(set1); // 差集 → [A]difference.removeAll(set2);/* 核心特性：1. 唯一性：基于 hashCode() 和 equals() 判断重复2. 无序性：遍历顺序不保证与插入顺序一致3. 允许 null 元素（但只能有一个 null）4. 基础操作时间复杂度：add/remove/contains → 平均 O(1)5. 非线程安全：需通过 Collections.synchronizedSet 包装实现线程安全*/ HashMap 常用操作// =================== HashMap 基础操作 ===================import java.util.Collections;import java.util.HashMap;import java.util.Map;import java.util.Set;// 创建对象MapString, Integer map = new HashMap(); // 默认容量16，负载因子0.75MapString, String initMap = new HashMap(32); // 指定初始容量MapString, Integer prefilled = new HashMap(Map.of(A, 1, B, 2)); // Java9+快速初始化// 增删改操作map.put(Apple, 10); // 添加键值对 → Apple=10map.put(Banana, 20); // → Apple=10, Banana=20map.putIfAbsent(Apple, 50); // 仅当键不存在时添加 → 原值10保持不变map.replace(Apple, 15); // 替换已有键的值 → Apple=15, Banana=20map.remove(Banana); // 删除键 → Apple=15map.replace(Apple, 15, 20); // 键值匹配时替换 → Apple=20map.clear(); // 清空映射// 查询操作int count = map.get(Apple); // 获取值（需确保键存在）Integer countSafe = map.get(Orange); // 键不存在时返回nullboolean existsKey = map.containsKey(Apple); // 检查键是否存在 → trueboolean existsValue = map.containsValue(20); // 检查值是否存在 → trueint size = map.size(); // 键值对数量boolean isEmpty = map.isEmpty(); // 是否为空映射// 遍历操作（4种方式）// 1. Entry遍历（推荐）for (Map.EntryString, Integer entry : map.entrySet()) System.out.println(entry.getKey() + : + entry.getValue());// 2. Key遍历for (String key : map.keySet()) System.out.println(key + - + map.get(key));// 3. Value遍历for (Integer value : map.values()) System.out.println(Value: + value);// 4. Java8+ Lambda遍历map.forEach((k, v) - System.out.println(k + = + v));// 批量操作MapString, Integer newItems = Map.of(Cherry, 5, Durian, 8);map.putAll(newItems); // 合并映射 → Apple=20, Cherry=5, Durian=8// 特殊值处理map.put(null, 0); // 允许null键 → null=0map.put(Mango, null); // 允许null值 → null=0, Mango=null// 高级操作（Java8+）map.computeIfAbsent(Orange, k - 3); // 不存在时计算 → 添加 Orange=3map.computeIfPresent(Apple, (k, v) - v + 5); // 存在时更新 → Apple=25map.merge(Apple, 10, (oldVal, newVal) - oldVal + newVal); // 合并值 → Apple=35// 线程安全包装MapString, Integer syncMap = Collections.synchronizedMap(map);// 不可变映射（Java9+）MapString, Integer immutableMap = Map.ofEntries( Map.entry(A, 1), Map.entry(B, 2));/* 核心特性：1. 键唯一性：基于 hashCode() 和 equals() 判断重复2. 无序存储：迭代顺序不保证与插入顺序一致3. 允许一个null键和多个null值4. 基础操作时间复杂度：get/put → 平均 O(1)5. 扩容机制：当元素数量超过（容量*负载因子）时自动翻倍扩容6. 树化优化：当链表长度超过8时转红黑树（Java8+）*/ HashMapCharacter, Integer hm_s = new HashMap();//遍历字符串的所有字符（更清晰的逻辑）for (char c : s.toCharArray()) hm_s.merge(c, 1, Integer::sum);// 使用getOrDefault避免NullPointerExceptionint countS = hm_s.getOrDefault(c, 0); 一个 Key 只能对应一个值，所以如果要实现类似 Multimap 的结构需要如下方式： //初始化HashMapString, ListInteger multiValueMap = new HashMap();/////添加元素if (!multiValueMap.containsKey(scores)) multiValueMap.put(scores, new ArrayList());multiValueMap.get(scores).add(90);// 若键不存在，自动创建空列表multiValueMap.computeIfAbsent(scores, k - new ArrayList()).add(90);multiValueMap.computeIfAbsent(scores, k - new ArrayList()).add(85);////访问元素//获取某个键的所有值ListInteger scores = multiValueMap.get(scores);if (scores != null) for (int num : scores) System.out.println(num); //输出 90, 85 //避免 NullPointerException的情况ListInteger scores = multiValueMap.getOrDefault(scores, new ArrayList());for (int num : scores) System.out.println(num);////删除元素//删除整个键值对multiValueMap.remove(scores);// 删除某个键的特定值ListInteger scores = multiValueMap.get(scores);if (scores != null) scores.remove(Integer.valueOf(90)); // 删除值为 90 的元素 // 如果列表为空，可选删除键 if (scores.isEmpty()) multiValueMap.remove(scores); ////遍历哈希表// 遍历所有键值对for (Map.EntryString, ListInteger entry : multiValueMap.entrySet()) String key = entry.getKey(); ListInteger values = entry.getValue(); System.out.println(key + : + values);//遍历所有键for (String key : multiValueMap.keySet()) System.out.println(Key: + key);//遍历所有值for (ListInteger values : multiValueMap.values()) System.out.println(Values: + values); String的常用操作// =================== String 常用操作 ===================// 创建对象String str1 = Hello; // 直接量创建（字符串常量池）String str2 = new String(World); // 堆内存新对象char[] chars = J,a,v,a;String str3 = new String(chars); // 通过字符数组创建 → Java// 基本操作int len = str1.length(); // 获取长度 → 5char c = str1.charAt(1); // 获取索引1字符 → eString substr1 = str1.substring(2); // 从索引2截取 → lloString substr2 = str1.substring(1,4); // 截取1-3索引 → ellString concatStr = str1.concat( World);// 拼接 → Hello WorldString upper = str1.toUpperCase(); // 转大写 → HELLOString lower = HELLO.toLowerCase(); // 转小写 → helloString trimStr = text .trim(); // 去首尾空格 → textString replaced = str1.replace(l, L);// 替换字符 → HeLLoString replacedAll = a1b2c3.replaceAll(\\\\d, #); // 正则替换 → a#b#c#// 转换与比较char[] arr = str1.toCharArray(); // 转字符数组 → [H,e,l,l,o]byte[] bytes = str1.getBytes(); // 按默认编码转字节数组boolean eq1 = str1.equals(Hello); // 值比较 → trueboolean eq2 = str1.equalsIgnoreCase(hElLo); // 忽略大小写 → trueint cmp = str1.compareTo(Hella); // 字典序比较 → 正数（o a）// 正则处理boolean matches = 123.matches(\\\\d+); // 正则匹配 → trueString[] parts = a,b,c.split(,); // 分割//Arrays.adList()// 字符串 → [a,b,c]String[] regexParts = a1b2c3.split(\\\\d); // 按数字分割 → [a,b,c]// 格式化处理String format1 = String.format(%s-%d, ID, 100); // → ID-100String format2 = String.join(|, A, B, C); // → A|B|C// 特殊判断boolean isEmpty = .isEmpty(); // 空字符串 → trueboolean isBlank = .isBlank(); // 全空白字符（Java 11+） → trueboolean starts = str1.startsWith(He); // 开头判断 → trueboolean ends = str1.endsWith(lo); // 结尾判断 → true/* 核心特性：1. 不可变性：所有操作返回新字符串，原对象不变2. 字符串常量池复用机制：直接量赋值优先使用常量池3. 支持正则表达式操作（split/matches/replaceAll等）4. 包含丰富的格式化方法（format/join等）*/ int转StringString s = String.valueOf(i);String s = Integer.toString(i);String 转 intString str = 123;int num = Integer.parseInt(str); StringBuffer 和 StringBuilderStringBuilder 类在 Java 5 中被提出，它和 StringBuffer 之间的最大不同在于 StringBuilder 的方法不是线程安全的（不能同步访问）。由于 StringBuilder 相较于 StringBuffer 有速度优势，所以多数情况下建议使用 StringBuilder 类。 // =================== StringBuilder ===================// 创建对象StringBuilder sb = new StringBuilder(); // 默认容量16StringBuilder sb2 = new StringBuilder(Hello); // 指定初始内容// 基本操作sb.append( World); // 追加内容 → Hello Worldsb.insert(5, Java); // 指定位置插入 → Hello Java Worldsb.delete(5, 10); // 删除5-9位置 → Hello Worldsb.replace(6, 11, Earth); // 替换指定区间 → Hello Earthsb.reverse(); // 反转 → htraE olleH sb.setLength(5); // 截断保留前5字符 → htraEchar c = sb.charAt(2); // 获取索引2的字符 → rsb.setCharAt(0, H); // 修改索引0字符 → HtraEint len = sb.length(); // 获取当前长度 → 5String s = sb.toString(); // 转换为String// 正向遍历for (int i = 0; i sb.length(); i++) char c = sb.charAt(i); System.out.println(c);// 链式调用StringBuilder sb3 = new StringBuilder() .append(123) // 支持多类型 .append(3.14) .append(true);sb.setLength(prevLen); // 直接回退到添加前的长度// =================== StringBuffer ===================// 创建对象（操作方法与StringBuilder完全一致）StringBuffer sbf = new StringBuffer(); sbf.append(100); // 追加数值sbf.insert(3, new char[]A,B); // 插入字符数组sbf.deleteCharAt(4); // 删除单个字符sbf.setLength(0); // 清空缓冲区（复用对象）// 线程安全示例sbf.append(ThreadSafe); // 所有方法都有synchronized修饰符/* 共性特征：1. 初始容量16，自动扩容（每次扩容2n+2）2. append() 支持所有基础类型/Object类型3. 修改后对象地址不变（与String的不可变性对比）4. 主要方法：append/insert/delete/replace/reverse*/String s1 = buffer.toString(); // StringBuffer → StringString s2 = builder.toString(); // StringBuilder → String PriorityQueue的常用操作record的常用操作相当于定义一个不可变的数据类.由于record是Java 14引入的，所以需要使用Java 14以上的版本。并且通常record是用在优先队列的比较器中，所以直接放在这里.直接举一个例子: record Node(double gain , int a , int b)PriorityQueueNode pq = new PriorityQueue((a, b) - Double.compare(b.gain, a.gain));pq.offer(new Node(1.0, 1, 2));pq.offer(new Node(2.0, 2, 3));pq.offer(new Node(3.0, 3, 4)); System.out.println(pq.poll().gain);System.out.println(pq.poll().gain);System.out.println(pq.poll().gain); 输出: 3.02.01.0 基本用法就是这样. 小顶堆(默认，前 K 大，升序) // 创建小顶堆（默认）PriorityQueueInteger minHeap = new PriorityQueue();// 添加元素minHeap.offer(5);minHeap.add(3); // offer和add功能相同// 查看堆顶元素（不删除）int min = minHeap.peek();// 取出堆顶元素（删除）int removedMin = minHeap.poll();// 获取堆大小int size = minHeap.size();// 检查是否为空boolean isEmpty = minHeap.isEmpty();// 删除指定元素（非堆顶）minHeap.remove(2);// 清空堆minHeap.clear(); 大顶堆(前 K 小，降序) // 创建大顶堆（使用自定义比较器）PriorityQueueInteger maxHeap = new PriorityQueue((a, b) - b - a);// 或 PriorityQueue(Comparator.reverseOrder());// 添加元素maxHeap.offer(8);maxHeap.add(4);// 查看堆顶元素（不删除）int max = maxHeap.peek();// 取出堆顶元素（删除）int removedMax = maxHeap.poll();// 检查元素是否存在boolean contains = maxHeap.contains(5);// 遍历并输出元素for (Integer num : minHeap) System.out.println(num);// 构造按照字典序排列的优先队列PriorityQueueInteger pq = new PriorityQueue((a, b) - // 将 int 转换为 String 后比较字典序 String strA = String.valueOf(a); String strB = String.valueOf(b); return strA.compareTo(strB);); 堆求前k大,然后存储索引(需要保存下标的写法)堆求前k大,然后存储索引 //小根堆，如果当前元素大于堆顶，则加入堆中，堆中的元素还要保存其下标public int[] maxSubsequence(int[] nums, int k) PriorityQueueint[] heap = new PriorityQueue((o1,o2)-o1[0]-o2[0]); int n = nums.length; for(int i = 0; i n; i++) if(heap.size() k) heap.offer(new int[]nums[i], i); else if(nums[i] heap.peek()[0]) heap.poll(); heap.offer(new int[]nums[i], i); int[] idx = new int[k]; for(int i = 0; i k; i++) idx[i] = heap.poll()[1]; Arrays.sort(idx); for(int i = 0; i k; i++) idx[i] = nums[idx[i]]; return idx; 自定义对象堆 // 自定义类class Student String name; int score; // 构造方法等...// 按分数的小顶堆PriorityQueueStudent studentMinHeap = new PriorityQueue( (s1, s2) - s1.score - s2.score);// 按分数的大顶堆 PriorityQueueStudent studentMaxHeap = new PriorityQueue( (s1, s2) - s2.score - s1.score);// 添加自定义对象studentMinHeap.offer(new Student(Alice, 85)); 堆序性质堆类型\t比较条件\t数学表达\tJava比较器实现小顶堆\t父 ≤ 子\ta ≤ b\ta - b 或 a.compareTo(b)大顶堆\t父 ≥ 子\ta ≥ b\tb - a 或 b.compareTo(a)比较器的本质a在堆里代表父节点 b是子节点 a-b 要小于0 a优先级才高 ab 所以是最小堆 比较器定义的是”优先级”关系： 返回负数：第一个参数（a）应该排在前面（更高优先级） 返回正数：第二个参数（b）应该排在前面 返回零：两者优先级相同 使用场景前 K 大的元素：使用最小堆，因为堆顶是存储的最小的元素，如果新增元素比堆顶大，那只需要替换掉堆顶即可。前 K 小的元素：使用最大堆，因为堆顶是存储的最大的元素，如果新增元素比堆顶小，那只需要替换掉堆顶即可。默认小顶堆：升序排序。默认大顶堆：降序排序。 class Solution public int maxEvents(int[][] events) int mx = 0; for (int[] e : events) mx = Math.max(mx, e[1]); // 按照开始时间分组 ListInteger[] groups = new ArrayList[mx + 1]; Arrays.setAll(groups, i - new ArrayList()); for (int[] e : events) groups[e[0]].add(e[1]); int ans = 0; PriorityQueueInteger pq = new PriorityQueue(); for (int i = 0; i = mx; i++) // 删除过期会议 while (!pq.isEmpty() pq.peek() i) pq.poll(); // 新增可以参加的会议 for (int endDay : groups[i]) pq.offer(endDay); // 参加一个结束时间最早的会议 if (!pq.isEmpty()) ans++; pq.poll(); return ans; Deque 的常用操作Deque常用于实现队列和栈.包括维护单调栈. 创建 import java.util.Deque;import java.util.ArrayDeque;import java.util.Iterator;// 初始化 Deque（以 ArrayDeque 为例）DequeString deque = new ArrayDeque(); //不能插入null元素DequeString deque = new LinkedList(); 插入 // 队头插入deque.addFirst(A); // 插入元素到队头（容量满时抛出 IllegalStateException）deque.offerFirst(B); // 插入元素到队头（容量满时返回 false）// 队尾插入deque.addLast(C); // 插入元素到队尾（容量满时抛出 IllegalStateException）deque.offerLast(D); // 插入元素到队尾（容量满时返回 false）// 批量插入（从 Collection 继承）deque.addAll(List.of(E, F)); // 依次插入队尾 删除 // 队头删除String first1 = deque.removeFirst(); // 删除并返回队头元素（空队列抛 NoSuchElementException）String first2 = deque.pollFirst(); // 删除并返回队头元素（空队列返回 null）// 队尾删除String last1 = deque.removeLast(); // 删除并返回队尾元素（空队列抛异常）String last2 = deque.pollLast(); // 删除并返回队尾元素（空队列返回 null）// 删除指定元素（从队头开始搜索）boolean removed1 = deque.remove(E); // 删除第一个出现的 Eboolean removed2 = deque.removeFirstOccurrence(F); // 删除队头方向第一个 Fboolean removed3 = deque.removeLastOccurrence(G); // 删除队尾方向第一个 G 查看 // 查看队头String head1 = deque.getFirst(); // 返回队头元素（空队列抛异常）String head2 = deque.peekFirst(); // 返回队头元素（空队列返回 null）// 查看队尾String tail1 = deque.getLast(); // 返回队尾元素（空队列抛异常）String tail2 = deque.peekLast(); // 返回队尾元素（空队列返回 null） 状态 boolean isEmpty = deque.isEmpty(); // 判断队列是否为空int size = deque.size(); // 返回元素数量boolean exists = deque.contains(A); // 判断是否包含元素 A 遍历 // 迭代器（正向：队头 → 队尾）IteratorString iterator = deque.iterator();while (iterator.hasNext()) String element = iterator.next();// 反向迭代器（队尾 → 队头）IteratorString descendingIterator = deque.descendingIterator();while (descendingIterator.hasNext()) String element = descendingIterator.next(); 其他 // 清空队列deque.clear();// 转换为数组Object[] array1 = deque.toArray(); // 返回 Object[]String[] array2 = deque.toArray(new String[0]); // 指定类型数组// ================== 栈操作（Deque 兼容的额外方法）==================deque.push(X); // 等效于 addFirst()String popped = deque.pop(); // 等效于 removeFirst()// ================== 容量限制队列（如 LinkedBlockingDeque）==================// 阻塞操作示例（需使用线程安全 Deque，此处仅展示方法）/*deque.offerFirst(W, 1, TimeUnit.SECONDS); // 等待1秒尝试插入队头deque.offerLast(Z, 1, TimeUnit.SECONDS); // 等待1秒尝试插入队尾String item = deque.pollFirst(1, TimeUnit.SECONDS); // 等待1秒尝试取出队头*/ TreeMap 的常用操作创建 // 默认按key升序排列TreeMapInteger, String map = new TreeMap();// 按key降序排列TreeMapString, Integer map = new TreeMap(Collections.reverseOrder());// 自定义比较器，按key升序排列TreeMapString, Integer map = new TreeMap((a, b) - a.compareTo(b)); 添加修改 // 添加键值对，如果key已存在则更新valuemap.put(1, A);map.put(2, B);map.put(3, C); 删除 // 删除指定key的键值对，返回被删除的valueString removed = map.remove(1);// 删除并返回第一个（最小的）键值对Map.EntryInteger, String first = map.pollFirstEntry();// 删除并返回最后一个（最大的）键值对Map.EntryInteger, String last = map.pollLastEntry();// 清空所有键值对map.clear(); 遍历 // 遍历键值对for (Map.EntryInteger, String entry : map.entrySet()) System.out.println(entry.getKey() + = + entry.getValue());// 遍历key（按升序）for (Integer key : map.keySet()) System.out.println(key);// 遍历value（按对应key的升序）for (String value : map.values()) System.out.println(value); 有序操作 // 获取第一个（最小的）键值对，不删除Map.EntryInteger, String first = map.firstEntry();// 获取最后一个（最大的）键值对，不删除Map.EntryInteger, String last = map.lastEntry();// 获取第一个（最小的）keyInteger firstKey = map.firstKey();// 获取最后一个（最大的）keyInteger lastKey = map.lastKey();// 获取小于指定key的最大键值对Map.EntryInteger, String lower = map.lowerEntry(5);// 获取大于指定key的最小键值对Map.EntryInteger, String higher = map.higherEntry(5);// 获取小于等于指定key的最大键值对Map.EntryInteger, String floor = map.floorEntry(5);// 获取大于等于指定key的最小键值对Map.EntryInteger, String ceiling = map.ceilingEntry(5); 其他 // 获取键值对数量int size = map.size();// 判断是否为空boolean isEmpty = map.isEmpty();// 获取降序排列的视图NavigableMapInteger, String descMap = map.descendingMap(); 模运算恒等式费马小定理组合数灵神的模运算帖子.respect.https://leetcode.cn/discuss/post/3584387/fen-xiang-gun-mo-yun-suan-de-shi-jie-dan-7xgu/ Java的取模（mod）和取余（% rem），发现我们常用的基本都是正数取余或取模，那带有负数的要怎么计算呢。当x和y的正负相同，取余和取模结果相同，当x和y正负不同，取余结果的符号和x相同，取模结果的符号和y的符号相同。假设：被除数 a 除数 b 商c 余数d 公式 abc…d 可以变形为 da-b*c那么关键就在于这个c取什么值。举个栗子：a5，b-2 ，那么 5÷(-2)-2.5 取模的时候，因为mod 函数采用了 floor 函数，floor函数是向下取整的，所以-2.5向下取整就是-3，那么d5-(-2)*(-3)5-6-1。 取余(%)的时候，因为rem 函数采用 fix 函数，fix函数是向0取整的，所以-2.5向0取整就是-2，那么d5-(-2)*(-2)5-41。 前言某些题目，由于要计算的答案非常大（超出 64 位整数的范围），会要求把答案对 10e9+7 取模。如果没有处理得当的话，会 WA（错误）或者 TLE（超时）。例如计算一堆数字的乘积，如果没有及时取模，乘法会溢出（例如计算结果超出 C++ 中 long long 的最大值），从而得到和预期不符的答案。对于 Python 来说，虽然没有溢出的问题，但大整数（big integer）之间的运算并不是 O(1) 的，可能会导致 TLE。 如何正确的取模呢? 加法和乘法的取模如果让你计算 1234×6789 的个位数，你会如何计算？ 由于只有个位数会影响到乘积的个位数，那么 4×936 的个位数 6 就是答案。 对于 1234+6789 的个位数，同理，4+913 的个位数 3 就是答案。 你能把这个结论抽象成数学等式吗？ 一般涉及到取模的题目，会用到如下两个恒等式，其中 mod 表示取模运算（modulo），即编程语言中的 %。上面计算的是 m10 的情况。 根据这两个恒等式，我们可以在计算过程中（例如循环），对加法和乘法的结果取模，而不是在循环结束后再取模。注：如果涉及到幂运算，指数是不能随意取模的。如果指数在 64 位整数的范围内，可以用快速幂计算，原理见一张图秒懂快速幂；如果指数超出 64 位整数的范围，见欧拉降幂。 如果计算过程中有减法，可能会产生负数，处理不当也会导致 WA。如何正确处理这种情况呢？同余 同余式的移项同余式中的加减法可以移项负数和减法的取模 除法的取模证明: 求模运算总结 代码实现时，上面的加减乘除通常是这样写的： MOD = 1_000_000_007// 加(a + b) % MOD// 减，b 在 [0,MOD-1] 中(a - b + MOD) % MOD// 把任意整数 a 取模到 [0,MOD-1] 中，无论 a 是正是负(a % MOD + MOD) % MOD// 乘（注意使用 64 位整数）a * b % MOD// 多个数相乘，要步步取模，防止溢出a * b % MOD * c % MOD// 除（MOD 是质数且 b 不是 MOD 的倍数）a * qpow(b, MOD - 2, MOD) % MOD 其中 qpow 为快速幂. 总之，如果发现解答错误，可以检查下代码，看看是不是哪里漏掉取模了。附:组合数计算模板代码如下: class Solution private static final int MOD = 1_000_000_007; private static final int MX = 100_001; // 根据题目数据范围修改 private static final long[] F = new long[MX]; // F[i] = i! private static final long[] INV_F = new long[MX]; // INV_F[i] = i!^-1 = pow(i!, MOD-2) static F[0] = 1; for (int i = 1; i MX; i++) F[i] = F[i - 1] * i % MOD; INV_F[MX - 1] = pow(F[MX - 1], MOD - 2); for (int i = MX - 1; i 0; i--) INV_F[i - 1] = INV_F[i] * i % MOD; private static long pow(long x, int n) long res = 1; for (; n 0; n /= 2) if (n % 2 0) res = res * x % MOD; x = x * x % MOD; return res; // 从 n 个数中选 m 个数的方案数 private long comb(int n, int m) return m 0 || m n ? 0 : F[n] * INV_F[m] % MOD * INV_F[n - m] % MOD; public int solve(int[] nums) // 预处理的逻辑写在 static 块中，这样只会初始化一次 快速幂一图流(灵神): 代码实现时，注意 n−2^31的情况，取反后 n2^31超出 int 最大值。可以转成 64 位 int 解决。模版: class Solution public double myPow(double x, int N) double ans = 1; long n = N; if (n 0) // x^-n = (1/x)^n n = -n; x = 1 / x; while (n != 0) // 从低到高枚举 n 的每个比特位 if ((n 1) == 1) // 这个比特位是 1 ans *= x; // 把 x 乘到 ans 中 x *= x; // x 自身平方 n = 1; // 继续枚举下一个比特位 return ans; 二进制从集合论到位运算，常见位运算技巧分类总结 s = 101100 s-1 = 101011 // 最低位的 1 变成 0，同时 1 右边的 0 都取反，变成 1s(s-1) = 101000 特别地，如果 s 是 2 的幂，那么 s(s−1)0。 此外，编程语言提供了一些和二进制有关的库函数，例如： 计算二进制中的 1 的个数，也就是集合大小； 计算二进制长度，减一后得到集合最大元素； 计算二进制尾零个数，也就是集合最小元素。 调用这些函数的时间复杂度都是 O(1)。 s = 101100 ~s = 010011(~s)+1 = 010100 // 根据补码的定义，这就是 -s = s 的最低 1 左侧取反，右侧不变s -s = 000100 // lowbit 遍历集合 for (int i = 0; i n; i++) if (((s i) 1) == 1) // i 在 s 中 // 处理 i 的逻辑 for (int t = s; t 0; t = t - 1) int i = Integer.numberOfTrailingZeros(t); // 处理 i 的逻辑 四、枚举集合§4.1 枚举所有集合设元素范围从 0 到 n−1，从空集 ∅ 枚举到全集 U： for (int s = 0; s (1 n); s++) // 处理 s 的逻辑 §4.2 枚举非空子集设集合为 s，从大到小枚举 s 的所有非空子集 sub： for (int sub = s; sub 0; sub = (sub - 1) s) // 处理 sub 的逻辑 为什么要写成 sub (sub - 1) s 呢？ 暴力做法是从 s 出发，不断减一，直到 0。但这样做，中途会遇到很多并不是 s 的子集的情况。例如 s10101 时，减一得到 10100，这是 s 的子集。但再减一就得到 10011 了，这并不是 s 的子集，下一个子集应该是 10001。 把所有的合法子集按顺序列出来，会发现我们做的相当于「压缩版」的二进制减法，例如 10101→10100→10001→10000→00101→⋯如果忽略掉 10101 中的两个 0，数字的变化和二进制减法是一样的，即 111→110→101→100→011→⋯如何快速跳到下一个子集呢？比如，怎么从 10100 跳到 10001？ 普通的二进制减法，是 10100−110011，也就是把最低位的 1 变成 0，同时把最低位的 1 右边的 0 都变成 1。压缩版的二进制减法也是类似的，对于 10100→10001，也会把最低位的 1 变成 0，对于最低位的 1 右边的 0，并不是都变成 1，只有在 s10101 中的 1 才会变成 1。怎么做到？减一后 10101 就行，也就是 (10100−1) 1010110001。§4.3 枚举子集（包含空集）如果要从大到小枚举 s 的所有子集 sub（从 s 枚举到空集 ∅），可以这样写： int sub = s;do // 处理 sub 的逻辑 sub = (sub - 1) s; while (sub != s); 其中 Java 和 C++ 的原理是，当 sub0 时（空集），再减一就得到 −1，对应的二进制为 111⋯1，再 s 就得到了 s。所以当循环到 subs 时，说明最后一次循环的 sub0（空集），s 的所有子集都枚举到了，退出循环。 注：还可以枚举全集 U 的所有大小恰好为 k 的子集，这一技巧叫做 Gosper’s Hack，具体请看 视频讲解。 §4.4 枚举超集如果 T 是 S 的子集，那么称 S 是 T 的超集（superset）。 枚举超集的原理和上文枚举子集是类似的，这里通过或运算保证枚举的集合 S 一定包含集合 T 中的所有元素。 枚举 S，满足 S 是 T 的超集，也是全集 U{0,1,2,…,n−1} 的子集。 for (int s = t; s (1 n); s = (s + 1) | t) // 处理 s 的逻辑 数组 二分查找https://leetcode.cn/problems/binary-search/通用模板： class Solution public int search(int[] nums, int target) int i = lowerBound(nums, target); // 选择其中一种写法即可 return i nums.length nums[i] == target ? i : -1; // 【下面列了三种写法，选一种自己喜欢的就行】 // lowerBound 返回最小的满足 nums[i] = target 的 i // 如果数组为空，或者所有数都 target，则返回 nums.length // 要求 nums 是非递减的，即 nums[i] = nums[i + 1] // 闭区间写法 private int lowerBound(int[] nums, int target) int left = 0, right = nums.length - 1; // 闭区间 [left, right] while (left = right) // 区间不为空 // 循环不变量： // nums[left-1] target // nums[right+1] = target int mid = left + (right - left) / 2; if (nums[mid] target) left = mid + 1; // 范围缩小到 [mid+1, right] else right = mid - 1; // 范围缩小到 [left, mid-1] return left; // 或者 right+1 // 左闭右开区间写法 private int lowerBound2(int[] nums, int target) int left = 0, right = nums.length; // 左闭右开区间 [left, right) while (left right) // 区间不为空 // 循环不变量： // nums[left-1] target // nums[right] = target int mid = left + (right - left) / 2; if (nums[mid] target) left = mid + 1; // 范围缩小到 [mid+1, right) else right = mid; // 范围缩小到 [left, mid) return left; // 或者 right // 开区间写法 private int lowerBound3(int[] nums, int target) int left = -1, right = nums.length; // 开区间 (left, right) while (left + 1 right) // 区间不为空 // 循环不变量： // nums[left] target // nums[right] = target int mid = left + (right - left) / 2; if (nums[mid] target) left = mid; // 范围缩小到 (mid, right) else right = mid; // 范围缩小到 (left, mid) return right; // 或者 left+1 二分法数组：每次遇到二分法，都是一看就会，一写就废 暴力解法时间复杂度：O(n) 二分法时间复杂度：O(logn) 双指针双指针法 数组：就移除个元素很难么？(opens new window) 双指针法（快慢指针法）：通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。 暴力解法时间复杂度：O(n^2) 双指针时间复杂度：O(n) 滑动窗口滑动窗口 数组：滑动窗口拯救了你(opens new window) 滑动窗口的精妙之处在于根据当前子序列和大小的情况，不断调节子序列的起始位置。从而将O(n^2)的暴力解法降为O(n)。 恰好包含k个对于这种恰好包含k个的题目,可以考虑使用滑动窗口来解决.比如恰好等于k个的,可以通过至少包含k个的,然后减去至少包含k-1个的,就是恰好包含k个的. 模拟行为 数组：这个循环可以转懵很多人 在这道题目中，我们再一次介绍到了循环不变量原则，其实这也是写程序中的重要原则。 相信大家有遇到过这种情况： 感觉题目的边界调节超多，一波接着一波的判断，找边界，拆了东墙补西墙，好不容易运行通过了，代码写的十分冗余，毫无章法，其实真正解决题目的代码都是简洁的，或者有原则性的，大家可以在这道题目中体会到这一点。 前缀和链表JAVA版本 public class ListNode // 结点的值 int val; // 下一个结点 ListNode next; // 节点的构造函数(无参) public ListNode() // 节点的构造函数(有一个参数) public ListNode(int val) this.val = val; // 节点的构造函数(有两个参数) public ListNode(int val, ListNode next) this.val = val; this.next = next; 相交链表题目链接 class Solution public ListNode getIntersectionNode(ListNode headA, ListNode headB) ListNode p = headA; ListNode q = headB; while (p != q) p = p != null ? p.next : headB; q = q != null ? q.next : headA; return p; 反转链表class Solution public ListNode reverseList(ListNode head) if(head == null || head.next == null)return head; ListNode last = reverseList(head.next); head.next.next = head; head.next = null; return last; 递归流程图: 移除链表元素设计链表反转链表原地反转，只需要把每个节点间的指向反转就可以尝试递归调用两两交换链表中的节点删除链表的倒数第N个节点链表相交环形链表II环形链表快慢指针一同走 相遇后差一圈 结论a c从起点和相遇点同时走 就是入口点相似题: https://leetcode.cn/problems/find-the-duplicate-number这个是数组模拟环形链表,非常有意思 class Solution public int findDuplicate(int[] nums) int slow = 0; int fast = 0; slow = nums[slow]; fast = nums[nums[fast]]; while(slow != fast) slow = nums[slow]; fast = nums[nums[fast]]; int pre1 = 0; int pre2 = slow; while(pre1 != pre2) pre1 = nums[pre1]; pre2 = nums[pre2]; return pre1; 哈希表题目242.有效的字母异位词力扣题目链接349.两个数组的交集力扣题目链接第202题. 快乐数力扣题目链接两数之和力扣题目链接第454题.四数相加II力扣题目链接赎金信力扣题目链接第15题. 三数之和力扣题目链接第18题. 四数之和力扣题目链接 字符串KMP算法介绍:KMP 算法是一个快速查找匹配串的算法，它的作用其实就是本题问题：如何快速在「原字符串」中找到「匹配字符串」。 上述的朴素解法，不考虑剪枝的话复杂度是 O(m∗n) 的，而 KMP 算法的复杂度为 O(m+n)。 KMP 之所以能够在 O(m+n) 复杂度内完成查找，是因为其能在「非完全匹配」的过程中提取到有效信息进行复用，以减少「重复匹配」的消耗。个人理解:通过构造一个next数组,使的前缀一样的部分能够快速跳转,前缀一样,但是后面的一个不一样,那就可以直接通过next数组跳转到上一个前缀一样的位置的下标,然后继续匹配.所以这个算法的关键就是构造next数组.看到的一个比较好的实现方式就是在主串和匹配串前面都加上一个空格,这样就可以保证next数组的下标从1开始,这样就可以避免很多边界问题. 先贴一个实现: class Solution // KMP 算法 // ss: 原串(string) pp: 匹配串(pattern) public int strStr(String ss, String pp) if (pp.isEmpty()) return 0; // 分别读取原串和匹配串的长度 int n = ss.length(), m = pp.length(); // 原串和匹配串前面都加空格，使其下标从 1 开始 ss = + ss; pp = + pp; char[] s = ss.toCharArray(); char[] p = pp.toCharArray(); // 构建 next 数组，数组长度为匹配串的长度（next 数组是和匹配串相关的） int[] next = new int[m + 1]; // 构造过程 i = 2，j = 0 开始，i 小于等于匹配串长度 【构造 i 从 2 开始】 for (int i = 2, j = 0; i = m; i++) // 匹配不成功的话，j = next(j) while (j 0 p[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++ if (p[i] == p[j + 1]) j++; // 更新 next[i]，结束本次循环，i++ next[i] = j; // 匹配过程，i = 1，j = 0 开始，i 小于等于原串长度 【匹配 i 从 1 开始】 for (int i = 1, j = 0; i = n; i++) // 匹配不成功 j = next(j) while (j 0 s[i] != p[j + 1]) j = next[j]; // 匹配成功的话，先让 j++，结束本次循环后 i++ if (s[i] == p[j + 1]) j++; // 整一段匹配成功，直接返回下标 if (j == m) return i - m; return -1; next数组构造: 匹配过程: 题目344.反转字符串力扣题目链接 aa^b: 先把a和b中，不相同的位保存到a，现在a中置1的位，代表原始的a和b不相同的位，而0，就是a和b相同的位。 ba^b: 不相同的位是1和原始b异或，就得到原始a的那个位的值；相同的位是0和原始b异或就是原始a或者原始b的值（本来就相同）。现在得到的就是原始a的值，现在存在b中。 aa^b：和上面相同。 a，b已经交换。 1.反转字符串II力扣题目链接 二叉树二叉树基础 满二叉树:如果一棵二叉树只有度为0的结点和度为2的结点，并且度为0的结点在同一层上，则这棵二叉树为满二叉树。 完全二叉树:在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层（h从1开始），则该层包含 1~ 2^(h-1) 个节点。也就是只有最后一层右侧不满,前面都是满的. 二叉搜索树:二叉搜索树是一个有序树 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树 平衡二叉搜索树:平衡二叉搜索树：又被称为AVL（Adelson-Velsky and Landis）树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 二叉树的存储方式二叉树可以链式存储，也可以顺序存储。 二叉树遍历顺序前序遍历：中左右中序遍历：左中右后序遍历：左右中 public class TreeNode int val; TreeNode left; TreeNode right; TreeNode() TreeNode(int val) this.val = val; TreeNode(int val, TreeNode left, TreeNode right) this.val = val; this.left = left; this.right = right; 二叉树递归遍历递归的三要素:1.确定递归函数的参数和返回值： 确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。 2.确定终止条件： 写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。 3.确定单层递归的逻辑： 确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。 递归三部曲 确定递归函数的参数以及返回值 确定终止条件 确定单层递归的逻辑 参数：需要二叉树的根节点，还需要一个计数器，这个计数器用来计算二叉树的一条边之和是否正好是目标和，计数器为int型。再来看返回值，递归函数什么时候需要返回值？什么时候不需要返回值？这里总结如下三点： 如果需要搜索整棵二叉树且不用处理递归返回值，递归函数就不要返回值。（这种情况就是本文下半部分介绍的113.路径总和ii） 如果需要搜索整棵二叉树且需要处理递归返回值，递归函数就需要返回值。 （这种情况我们在236. 二叉树的最近公共祖先 中介绍） 如果要搜索其中一条符合条件的路径，那么递归一定需要返回值，因为遇到符合条件的路径了就要及时返回。（本题的情况） 终止条件终止条件如果是判断叶子节点，递归的过程中就不要让空节点进入递归了。 // 前序遍历·递归·LC144_二叉树的前序遍历class Solution public ListInteger preorderTraversal(TreeNode root) ListInteger result = new ArrayListInteger(); preorder(root, result); return result; public void preorder(TreeNode root, ListInteger result) if (root == null) return; result.add(root.val); preorder(root.left, result); preorder(root.right, result); // 中序遍历·递归·LC94_二叉树的中序遍历class Solution public ListInteger inorderTraversal(TreeNode root) ListInteger res = new ArrayList(); inorder(root, res); return res; void inorder(TreeNode root, ListInteger list) if (root == null) return; inorder(root.left, list); list.add(root.val); // 注意这一句 inorder(root.right, list); // 后序遍历·递归·LC145_二叉树的后序遍历class Solution public ListInteger postorderTraversal(TreeNode root) ListInteger res = new ArrayList(); postorder(root, res); return res; void postorder(TreeNode root, ListInteger list) if (root == null) return; postorder(root.left, list); postorder(root.right, list); list.add(root.val); // 注意这一句 二叉树迭代遍历144.二叉树的前序遍历https://leetcode.cn/problems/binary-tree-preorder-traversal/94.二叉树的中序遍历https://leetcode.cn/problems/binary-tree-inorder-traversal/145.二叉树的后序遍历https://leetcode.cn/problems/binary-tree-postorder-traversal/ 统一写法!!!!!!!!思路:将访问的节点放入栈中，把要处理的节点也放入栈中但是要做标记。实现方式: 方法一：就是要处理的节点放入栈之后，紧接着放入一个空指针作为标记。 这种方法可以叫做空指针标记法。 方法二：加一个 boolean 值跟随每个节点，false (默认值) 表示需要为该节点和它的左右儿子安排在栈中的位次，true 表示该节点的位次之前已经安排过了，可以收割节点了。 这种方法可以叫做boolean 标记法。 这种方法更容易理解，在面试中更容易写出来。前序遍历代码:public ListInteger preorderTraversal(TreeNode root) ListInteger result = new LinkedList(); DequeTreeNode st = new LinkedList(); if (root != null) st.push(root); while (!st.isEmpty()) TreeNode node = st.peek(); if (node != null) st.pop(); // 将该节点弹出，避免重复操作，下面再将右左中节点添加到栈中（前序遍历-中左右，入栈顺序右左中） if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 else // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 return result; 中序遍历代码:public ListInteger inorderTraversal(TreeNode root) ListInteger result = new LinkedList(); DequeTreeNode st = new LinkedList(); if (root != null) st.push(root); while (!st.isEmpty()) TreeNode node = st.peek(); if (node != null) st.pop(); // 将该节点弹出，避免重复操作，下面再将右中左节点添加到栈中（中序遍历-左中右，入栈顺序右中左） if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） else // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 return result; 后序遍历代码:public ListInteger postorderTraversal(TreeNode root) ListInteger result = new LinkedList(); DequeTreeNode st = new LinkedList(); if (root != null) st.push(root); while (!st.isEmpty()) TreeNode node = st.peek(); if (node != null) st.pop(); // 将该节点弹出，避免重复操作，下面再将中右左节点添加到栈中（后序遍历-左右中，入栈顺序中右左） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） else // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 return result; 直接用栈模拟的写法:前序 中序 后序的写法不一样. // 前序遍历顺序：中-左-右，入栈顺序：中-右-左class Solution public ListInteger preorderTraversal(TreeNode root) ListInteger result = new ArrayList(); if (root == null) return result; StackTreeNode stack = new Stack(); stack.push(root); while (!stack.isEmpty()) TreeNode node = stack.pop(); result.add(node.val); if (node.right != null) stack.push(node.right); if (node.left != null) stack.push(node.left); return result; // 中序遍历顺序: 左-中-右 入栈顺序： 左-右class Solution public ListInteger inorderTraversal(TreeNode root) ListInteger result = new ArrayList(); if (root == null) return result; StackTreeNode stack = new Stack(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) if (cur != null) stack.push(cur); cur = cur.left; else cur = stack.pop(); result.add(cur.val); cur = cur.right; return result; // 后序遍历顺序 左-右-中 入栈顺序：中-左-右 出栈顺序：中-右-左， 最后翻转结果class Solution public ListInteger postorderTraversal(TreeNode root) ListInteger result = new ArrayList(); if (root == null) return result; StackTreeNode stack = new Stack(); stack.push(root); while (!stack.isEmpty()) TreeNode node = stack.pop(); result.add(node.val); if (node.left != null) stack.push(node.left); if (node.right != null) stack.push(node.right); Collections.reverse(result); return result; 二叉树的层序遍历(BFS)// 102.二叉树的层序遍历class Solution public ListListInteger resList = new ArrayListListInteger(); public ListListInteger levelOrder(TreeNode root) //checkFun01(root,0); checkFun02(root); return resList; //BFS--递归方式 public void checkFun01(TreeNode node, Integer deep) if (node == null) return; deep++; if (resList.size() deep) //当层级增加时，list的Item也增加，利用list的索引值进行层级界定 ListInteger item = new ArrayListInteger(); resList.add(item); resList.get(deep - 1).add(node.val); checkFun01(node.left, deep); checkFun01(node.right, deep); //BFS--迭代方式--借助队列 public void checkFun02(TreeNode node) if (node == null) return; QueueTreeNode que = new LinkedListTreeNode(); que.offer(node); while (!que.isEmpty()) ListInteger itemList = new ArrayListInteger(); int len = que.size(); while (len 0) TreeNode tmpNode = que.poll(); itemList.add(tmpNode.val); if (tmpNode.left != null) que.offer(tmpNode.left); if (tmpNode.right != null) que.offer(tmpNode.right); len--; resList.add(itemList); Morris遍历学习了一个Morris遍历方式,这种方式可以在O(1)的空间复杂度下完成二叉树的遍历,并且不需要使用栈来存储节点.贴一个自己画的伪代码:前序和中序可以通过调整代码实现,后序可以通过向右的前序然后结果翻转来实现. //前序class Solution public ListInteger preorderTraversal(TreeNode root) ListInteger res = new LinkedList(); TreeNode cur = root; while(cur != null) if(cur.left == null) res.add(cur.val); cur =cur.right; //向右走 else TreeNode pre = cur.left; while(pre.right != null pre.right != cur) pre = pre.right; //开始建立索引 if(pre.right == null) //是一个第一次来的节点 pre.right = cur; res.add(cur.val); cur = cur.left; else //并不是第一次来的节点 pre.right = null; cur = cur.right; return res; //中序class Solution public ListInteger inorderTraversal(TreeNode root) ListInteger res =new LinkedList(); TreeNode cur = root; while(cur != null) if(cur.left == null) res.add(cur.val); cur = cur.right; else TreeNode pre = cur.left; while(pre.right != null pre.right != cur) pre = pre.right; if(pre.right == null) pre.right = cur; cur = cur.left; else pre.right = null; res.add(cur.val); cur = cur.right; return res; //后序import java.util.ArrayList;import java.util.Collections;import java.util.List;class TreeNode int val; TreeNode left; TreeNode right; TreeNode(int x) val = x; public class MorrisPostorderTraversal public ListInteger postorderTraversal(TreeNode root) ListInteger res = new ArrayList(); TreeNode curr = root; TreeNode prev = null; while (curr != null) if (curr.right == null) res.add(curr.val); // 右子树为空，直接访问 curr = curr.left; // 转向左子树 else // 找到右子树的最左节点（即后继节点） prev = curr.right; while (prev.left != null prev.left != curr) prev = prev.left; if (prev.left == null) prev.left = curr; // 建立线索 res.add(curr.val); // 访问当前节点（前序遍历位置） curr = curr.right; // 转向右子树 else prev.left = null; // 恢复树结构 curr = curr.left; // 转向左子树 Collections.reverse(res); // 反转结果，得到后序遍历 return res; 二叉树题目翻转二叉树dfs或者bfs都可做 //DFS递归class Solution /** * 前后序遍历都可以 * 中序不行，因为先左孩子交换孩子，再根交换孩子（做完后，右孩子已经变成了原来的左孩子），再右孩子交换孩子（此时其实是对原来的左孩子做交换） */ public TreeNode invertTree(TreeNode root) if (root == null) return null; invertTree(root.left); invertTree(root.right); swapChildren(root); return root; private void swapChildren(TreeNode root) TreeNode tmp = root.left; root.left = root.right; root.right = tmp; //BFSclass Solution public TreeNode invertTree(TreeNode root) if (root == null) return null; ArrayDequeTreeNode deque = new ArrayDeque(); deque.offer(root); while (!deque.isEmpty()) int size = deque.size(); while (size-- 0) TreeNode node = deque.poll(); swap(node); if (node.left != null) deque.offer(node.left); if (node.right != null) deque.offer(node.right); return root; public void swap(TreeNode root) TreeNode temp = root.left; root.left = root.right; root.right = temp; 对称二叉树这道题递归调用很简单,迭代调用用一个队列就可以,退出条件可以注意一下. /** * 递归法 */public boolean isSymmetric1(TreeNode root) return compare(root.left, root.right);private boolean compare(TreeNode left, TreeNode right) if (left == null right != null) return false; if (left != null right == null) return false; if (left == null right == null) return true; if (left.val != right.val) return false; // 比较外侧 boolean compareOutside = compare(left.left, right.right); // 比较内侧 boolean compareInside = compare(left.right, right.left); return compareOutside compareInside;/** * 迭代法 * 使用双端队列，相当于两个栈 */public boolean isSymmetric2(TreeNode root) DequeTreeNode deque = new LinkedList(); deque.offerFirst(root.left); deque.offerLast(root.right); while (!deque.isEmpty()) TreeNode leftNode = deque.pollFirst(); TreeNode rightNode = deque.pollLast(); if (leftNode == null rightNode == null) continue; // if (leftNode == null rightNode != null) // return false;// // if (leftNode != null rightNode == null) // return false;// // if (leftNode.val != rightNode.val) // return false;// // 以上三个判断条件合并 if (leftNode == null || rightNode == null || leftNode.val != rightNode.val) return false; deque.offerFirst(leftNode.left); deque.offerFirst(leftNode.right); deque.offerLast(rightNode.right); deque.offerLast(rightNode.left); return true;/** * 迭代法 * 使用普通队列 *///public boolean isSymmetric3(TreeNode root) QueueTreeNode deque = new LinkedList(); deque.offer(root.left); deque.offer(root.right); while (!deque.isEmpty()) TreeNode leftNode = deque.poll(); TreeNode rightNode = deque.poll(); if (leftNode == null rightNode == null) continue; // if (leftNode == null rightNode != null) // return false;// // if (leftNode != null rightNode == null) // return false;// // if (leftNode.val != rightNode.val) // return false;// // 以上三个判断条件合并 if (leftNode == null || rightNode == null || leftNode.val != rightNode.val) return false; // 这里顺序与使用Deque不同 deque.offer(leftNode.left); deque.offer(rightNode.right); deque.offer(leftNode.right); deque.offer(rightNode.left); return true; 二叉树最大深度104.二叉树的最大深度 class Solution /** * 递归法 */ public int maxDepth(TreeNode root) if (root == null) return 0; int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); return Math.max(leftDepth, rightDepth) + 1; class Solution /** * 递归法(求深度法) */ //定义最大深度 int maxnum = 0; public int maxDepth(TreeNode root) ans(root,0); return maxnum; //递归求解最大深度 void ans(TreeNode tr,int tmp) if(tr==null) return; tmp++; maxnum = maxnumtmp?tmp:maxnum; ans(tr.left,tmp); ans(tr.right,tmp); tmp--; class Solution /** * 迭代法，使用层序遍历 */ public int maxDepth(TreeNode root) if(root == null) return 0; DequeTreeNode deque = new LinkedList(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) int size = deque.size(); depth++; for (int i = 0; i size; i++) TreeNode node = deque.poll(); if (node.left != null) deque.offer(node.left); if (node.right != null) deque.offer(node.right); return depth; 二叉树的所有路径 https://leetcode.cn/problems/binary-tree-paths/递归和回溯.PS:递归和回溯永远要放在一起!!! //解法一//方式一class Solution /** * 递归法 */ public ListString binaryTreePaths(TreeNode root) ListString res = new ArrayList();// 存最终的结果 if (root == null) return res; ListInteger paths = new ArrayList();// 作为结果中的路径 traversal(root, paths, res); return res; private void traversal(TreeNode root, ListInteger paths, ListString res) paths.add(root.val);// 前序遍历，中 // 遇到叶子结点 if (root.left == null root.right == null) // 输出 StringBuilder sb = new StringBuilder();// StringBuilder用来拼接字符串，速度更快 for (int i = 0; i paths.size() - 1; i++) sb.append(paths.get(i)).append(-); sb.append(paths.get(paths.size() - 1));// 记录最后一个节点 res.add(sb.toString());// 收集一个路径 return; // 递归和回溯是同时进行，所以要放在同一个花括号里 if (root.left != null) // 左 traversal(root.left, paths, res); paths.remove(paths.size() - 1);// 回溯 if (root.right != null) // 右 traversal(root.right, paths, res); paths.remove(paths.size() - 1);// 回溯 //方式二class Solution ListString result = new ArrayList(); public ListString binaryTreePaths(TreeNode root) deal(root, ); return result; public void deal(TreeNode node, String s) if (node == null) return; if (node.left == null node.right == null) result.add(new StringBuilder(s).append(node.val).toString()); return; String tmp = new StringBuilder(s).append(node.val).append(-).toString(); deal(node.left, tmp); deal(node.right, tmp); 迭代法: // 解法二class Solution /** * 迭代法 */ public ListString binaryTreePaths(TreeNode root) ListString result = new ArrayList(); if (root == null) return result; StackObject stack = new Stack(); // 节点和路径同时入栈 stack.push(root); stack.push(root.val + ); while (!stack.isEmpty()) // 节点和路径同时出栈 String path = (String) stack.pop(); TreeNode node = (TreeNode) stack.pop(); // 若找到叶子节点 if (node.left == null node.right == null) result.add(path); //右子节点不为空 if (node.right != null) stack.push(node.right); stack.push(path + - + node.right.val); //左子节点不为空 if (node.left != null) stack.push(node.left); stack.push(path + - + node.left.val); return result; 404.左叶子之和https://leetcode.cn/problems/sum-of-left-leaves/ class Solution public int sumOfLeftLeaves(TreeNode root) if (root == null) return 0; int leftValue = sumOfLeftLeaves(root.left); // 左 int rightValue = sumOfLeftLeaves(root.right); // 右 int midValue = 0; if (root.left != null root.left.left == null root.left.right == null) midValue = root.left.val; int sum = midValue + leftValue + rightValue; // 中 return sum; class Solution public int sumOfLeftLeaves(TreeNode root) if (root == null) return 0; StackTreeNode stack = new Stack (); stack.add(root); int result = 0; while (!stack.isEmpty()) TreeNode node = stack.pop(); if (node.left != null node.left.left == null node.left.right == null) result += node.left.val; if (node.right != null) stack.add(node.right); if (node.left != null) stack.add(node.left); return result; 513.找树左下角的值https://leetcode.cn/problems/find-bottom-left-tree-value/ // 递归法class Solution private int Deep = -1; private int value = 0; public int findBottomLeftValue(TreeNode root) value = root.val; findLeftValue(root,0); return value; private void findLeftValue (TreeNode root,int deep) if (root == null) return; if (root.left == null root.right == null) if (deep Deep) value = root.val; Deep = deep; if (root.left != null) findLeftValue(root.left,deep + 1); if (root.right != null) findLeftValue(root.right,deep + 1); //迭代法class Solution public int findBottomLeftValue(TreeNode root) QueueTreeNode queue = new LinkedList(); queue.offer(root); int res = 0; while (!queue.isEmpty()) int size = queue.size(); for (int i = 0; i size; i++) TreeNode poll = queue.poll(); if (i == 0) res = poll.val; if (poll.left != null) queue.offer(poll.left); if (poll.right != null) queue.offer(poll.right); return res; 路径总和https://leetcode.cn/problems/path-sum/ 106.从中序与后序遍历序列构造二叉树 class Solution MapInteger, Integer map; // 方便根据数值查找位置 public TreeNode buildTree(int[] inorder, int[] postorder) map = new HashMap(); for (int i = 0; i inorder.length; i++) // 用map保存中序序列的数值对应位置 map.put(inorder[i], i); return findNode(inorder, 0, inorder.length, postorder,0, postorder.length); // 前闭后开 public TreeNode findNode(int[] inorder, int inBegin, int inEnd, int[] postorder, int postBegin, int postEnd) // 参数里的范围都是前闭后开 if (inBegin = inEnd || postBegin = postEnd) // 不满足左闭右开，说明没有元素，返回空树 return null; int rootIndex = map.get(postorder[postEnd - 1]); // 找到后序遍历的最后一个元素在中序遍历中的位置 TreeNode root = new TreeNode(inorder[rootIndex]); // 构造结点 int lenOfLeft = rootIndex - inBegin; // 保存中序左子树个数，用来确定后序数列的个数 root.left = findNode(inorder, inBegin, rootIndex, postorder, postBegin, postBegin + lenOfLeft); root.right = findNode(inorder, rootIndex + 1, inEnd, postorder, postBegin + lenOfLeft, postEnd - 1); return root; 106.从前序与后序遍历序列构造二叉树 class Solution MapInteger,Integer mp = new HashMap(); public TreeNode constructFromPrePost(int[] preorder, int[] postorder) int n = preorder.length; for(int i=0 ; in ; ++i) mp.put(postorder[i],i); return dfs(preorder,0,n-1,postorder,0,n-1); private TreeNode dfs(int[] preorder , int pre_beg,int pre_end ,int[] postorder, int pos_beg , int pos_end) //闭区间 先判断是否终止 if(pre_begpre_end || pos_begpos_end) return null; int mid = preorder[pre_beg]; if(pre_beg == pre_end || pos_beg == pos_end) return new TreeNode(mid); int nx_l = preorder[pre_beg+1]; int ind_nxl = mp.get(nx_l); int lenOfLeft = ind_nxl - pos_beg + 1; TreeNode root = new TreeNode(mid); root.left = dfs(preorder,pre_beg+1,pre_beg+lenOfLeft,postorder,pos_beg,pos_beg+lenOfLeft-1); root.right = dfs(preorder,pre_beg+lenOfLeft+1,pre_end,postorder,ind_nxl+1,pos_end-1); return root; 617.合并二叉树https://leetcode.cn/problems/merge-two-binary-trees/ 700.二叉搜索树中的搜索https://leetcode.cn/problems/search-in-a-binary-search-tree/ class Solution // 递归，普通二叉树 public TreeNode searchBST(TreeNode root, int val) if (root == null || root.val == val) return root; TreeNode left = searchBST(root.left, val); if (left != null) return left; return searchBST(root.right, val); class Solution // 递归，利用二叉搜索树特点，优化 public TreeNode searchBST(TreeNode root, int val) if (root == null || root.val == val) return root; if (val root.val) return searchBST(root.left, val); else return searchBST(root.right, val); class Solution // 迭代，普通二叉树 public TreeNode searchBST(TreeNode root, int val) if (root == null || root.val == val) return root; StackTreeNode stack = new Stack(); stack.push(root); while (!stack.isEmpty()) TreeNode pop = stack.pop(); if (pop.val == val) return pop; if (pop.right != null) stack.push(pop.right); if (pop.left != null) stack.push(pop.left); return null; class Solution // 迭代，利用二叉搜索树特点，优化，可以不需要栈 public TreeNode searchBST(TreeNode root, int val) while (root != null) if (val root.val) root = root.left; else if (val root.val) root = root.right; else return root; return null; 98.验证二叉搜索树记住最重要的就是:二叉搜索树的中序遍历是有序的. //使用統一迭代法class Solution public boolean isValidBST(TreeNode root) StackTreeNode stack = new Stack(); TreeNode pre = null; if(root != null) stack.add(root); while(!stack.isEmpty()) TreeNode curr = stack.peek(); if(curr != null) stack.pop(); if(curr.right != null) stack.add(curr.right); stack.add(curr); stack.add(null); if(curr.left != null) stack.add(curr.left); else stack.pop(); TreeNode temp = stack.pop(); if(pre != null pre.val = temp.val) return false; pre = temp; return true; class Solution // 递归 TreeNode max; public boolean isValidBST(TreeNode root) if (root == null) return true; // 左 boolean left = isValidBST(root.left); if (!left) return false; // 中 if (max != null root.val = max.val) return false; max = root; // 右 boolean right = isValidBST(root.right); return right; class Solution // 迭代 public boolean isValidBST(TreeNode root) if (root == null) return true; StackTreeNode stack = new Stack(); TreeNode pre = null; while (root != null || !stack.isEmpty()) while (root != null) stack.push(root); root = root.left;// 左 // 中，处理 TreeNode pop = stack.pop(); if (pre != null pop.val = pre.val) return false; pre = pop; root = pop.right;// 右 return true; // 简洁实现·递归解法class Solution public boolean isValidBST(TreeNode root) return validBST(Long.MIN_VALUE, Long.MAX_VALUE, root); boolean validBST(long lower, long upper, TreeNode root) if (root == null) return true; if (root.val = lower || root.val = upper) return false; return validBST(lower, root.val, root.left) validBST(root.val, upper, root.right); // 简洁实现·中序遍历class Solution private long prev = Long.MIN_VALUE; public boolean isValidBST(TreeNode root) if (root == null) return true; if (!isValidBST(root.left)) return false; if (root.val = prev) // 不满足二叉搜索树条件 return false; prev = root.val; return isValidBST(root.right); 二叉树的最近公共祖先https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/ class Solution public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) if (root == null || root == p || root == q) // 递归结束条件 return root; // 后序遍历 TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left == null right == null) // 若未找到节点 p 或 q return null; else if(left == null right != null) // 若找到一个节点 return right; else if(left != null right == null) // 若找到一个节点 return left; else // 若找到两个节点 return root; //迭代public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) int max = Integer.MAX_VALUE; StackTreeNode st = new Stack(); TreeNode cur = root, pre = null; while (cur != null || !st.isEmpty()) while (cur != null) st.push(cur); cur = cur.left; cur = st.pop(); if (cur.right == null || cur.right == pre) // p/q是 中/左 或者 中/右 , 返回中 if (cur == p || cur == q) if ((cur.left != null cur.left.val == max) || (cur.right != null cur.right.val == max)) return cur; cur.val = max; // p/q是 左/右 , 返回中 if (cur.left != null cur.left.val == max cur.right != null cur.right.val == max) return cur; // MAX_VALUE 往上传递 if ((cur.left != null cur.left.val == max) || (cur.right != null cur.right.val == max)) cur.val = max; pre = cur; cur = null; else st.push(cur); cur = cur.right; return null; 二叉搜索树的最近公共祖先https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-search-tree/ 450.删除二叉搜索树中的节点https://leetcode.cn/problems/delete-node-in-a-bst/ // 解法1(最好理解的版本)class Solution public TreeNode deleteNode(TreeNode root, int key) if (root == null) return root; if (root.val == key) if (root.left == null) return root.right; else if (root.right == null) return root.left; else TreeNode cur = root.right; while (cur.left != null) cur = cur.left; cur.left = root.left; root = root.right; return root; if (root.val key) root.left = deleteNode(root.left, key); if (root.val key) root.right = deleteNode(root.right, key); return root; 修剪二叉搜索树https://leetcode.cn/problems/trim-a-binary-search-tree/ class Solution public: TreeNode* trimBST(TreeNode* root, int low, int high) if (root == nullptr) return nullptr; if (root-val low) return trimBST(root-right, low, high); if (root-val high) return trimBST(root-left, low, high); root-left = trimBST(root-left, low, high); root-right = trimBST(root-right, low, high); return root; ; 回溯算法 回溯的理论基础什么是回溯法回溯法也可以叫做回溯搜索法，它是一种搜索的方式。回溯是递归的副产品，只要有递归就会有回溯。 因为回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，如果想让回溯法高效一些，可以加一些剪枝的操作，但也改不了回溯法就是穷举的本质。 回溯法解决的问题 回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 组合是不强调元素顺序的，排列是强调元素顺序。记住组合无序，排列有序，就可以了。 回溯法模板 回溯函数模板返回值以及参数 在回溯算法中，我的习惯是函数起名字为backtracking，这个起名大家随意。 回溯算法中函数返回值一般为void。再来看一下参数，因为回溯算法需要的参数可不像二叉树递归的时候那么容易一次性确定下来，所以一般是先写逻辑，然后需要什么参数，就填什么参数。回溯函数伪代码如下：void backtracking(参数) 回溯函数终止条件 既然是树形结构，那么我们在讲解二叉树的递归 (opens new window)的时候，就知道遍历树形结构一定要有终止条件。所以回溯也有要终止条件。什么时候达到了终止条件，树中就可以看出，一般来说搜到叶子节点了，也就找到了满足条件的一条答案，把这个答案存放起来，并结束本层递归。所以回溯函数终止条件伪代码如下： if (终止条件) 存放结果; return; 回溯搜索的遍历过程 在上面我们提到了，回溯法一般是在集合中递归搜索，集合的大小构成了树的宽度，递归的深度构成的树的深度。 void backtracking(参数) if (终止条件) 存放结果; return; for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 回溯题目第77题. 组合 贪心算法 选取每阶段的局部最优,最终达到全局最优.怎么判断成不成立呢?最好的办法是举反例,如果举不出来,那就可以尝试一下贪心算法. 解题步骤:贪心算法一般分为如下四步： 将问题分解为若干个子问题 找出适合的贪心策略 求解每一个子问题的最优解 将局部最优解堆叠成全局最优解 动态规划如果某一问题有很多重叠子问题，使用动态规划是最有效的。动态规划中每一个状态一定是由上一个状态推导出来的，这一点就区分于贪心，贪心没有状态推导，而是从局部直接选最优的. 解题步骤:对于动态规划问题，拆解为如下五步曲: 确定dp数组（dp table）以及下标的含义 确定递推公式 dp数组如何初始化 确定遍历顺序 举例推导dp数组 背包问题 0-1 背包由于每个物品只能选一次,所以遍历背包容量时防止重复选择,只能倒序背包容量进行选择. 完全背包由于每个物品没有数量限制,所以可以正序遍历背包容量. 动态规划中排列和组合的区别: 首先从方法论的角度理解: ​排列问题(背包在外层循环) 当外层循环是背包容量，内层循环是物品时，相当于对于每个容量，我们尝试所有可能的物品。这意味着同一个物品可以在不同的位置被多次使用，顺序不同被视为不同的排列。例如：nums [1,2], target 3排列有：(1,1,1), (1,2), (2,1)顺序不同被视为不同解. ​​组合问题(物品在外层循环)​​:当外层循环是物品，内层循环是背包容量时，我们固定了物品的选择顺序。这意味着我们只考虑物品的某种特定顺序的组合，不考虑顺序变化。例如：nums [1,2], target 3组合有：(1,1,1), (1,2)(2,1)不会被计入，因为2已经在1之后考虑了 然后从代码来理解一下:排列问题public int combinationSum4(int[] nums, int target) int[] dp = new int[target + 1]; dp[0] = 1; for (int i = 0; i = target; i++) // 外层循环是背包容量 for (int j = 0; j nums.length; j++) // 内层循环是物品 if (i = nums[j]) dp[i] += dp[i - nums[j]]; return dp[target]; 组合问题 public int combinationSum(int[] nums, int target) int[] dp = new int[target + 1]; dp[0] = 1; for (int j = 0; j nums.length; j++) // 外层循环是物品 for (int i = nums[j]; i = target; i++) // 内层循环是背包容量 dp[i] += dp[i - nums[j]]; return dp[target]; 多重背包有N种物品和一个容量为V 的背包。第i种物品最多有Mi件可用，每件耗费的空间是Ci ，价值是Wi 。求解将哪些物品装入背包可使这些物品的耗费的空间 总和不超过背包容量，且价值总和最大。直接上图:把上面的数量直接展开成一个0-1背包就可以了其实. 数位DP单调栈图论图论基础图的种类整体上一般分为 有向图 和 无向图。加权有向图，就是图中边是有权值的 度无向图中有几条边连接该节点，该节点就有几度。 在有向图中，每个节点有出度和入度。 出度：从该节点出发的边的个数。 入度：指向该节点边的个数。 连通图在无向图中，任何两个节点都是可以到达的，我们称之为连通图 强连通图在有向图中，任何两个节点是可以相互到达的 连通分量在无向图中的极大连通子图称之为该图的一个连通分量。 强连通分量在有向图中极大强连通子图称之为该图的强连通分量。 图的构造一般使用邻接表、邻接矩阵 或者用类来表示。 字典序字典序字典树:我觉得比较关键的点是: 字典树左子树字典序一定比右子树小. 字典树想要到右侧兄弟节点,直接num++就可以. 关键就是要找到以当前数字为根的十叉树的元素总个数.看着这个图就比较好理解了: 这张图也很好: class Solution /** * 以当前数字为根的十叉树的元素总个数 (包括当前数字) * * @param num 当前数字 (需要先 cast 成 long, 因为 num*10 可能导致 int 溢出) * @param n 数字的最大值 * @return */ private int count(long num, int n) int cnt = 0; // 元素总个数 int width = 1; // 当前层数的宽度, 第一层只有 num 一个元素, 所以第一层宽度为 1 while (true) if (num + width - 1 = n) // n 的值大于等于当前层的最大值, 说明当前层数的个数可以全部添加 cnt += width; num *= 10; width *= 10; else // n 的值小于当前层的最大值则只能添加部分个数或者不添加, 并跳出循环 if (n - num = 0) cnt += n - num + 1; break; return cnt; public int findKthNumber(int n, int k) int cnt = 0; // 已经经过的元素个数, 开始一个元素都没有经过, 所以个数为 0 int num = 1; // 第一个元素 (经过 i 个元素, 当前 num 是第 i + 1 元素) // 要找到第 k 个元素, 需要经过 k - 1 个元素 while (true) if (cnt == k - 1) // 经过了 k - 1 个元素找到了第 k 个元素 break; int temp = count((long) num, n); // 以 num 为根, 以 n 为最大值的十叉树的元素总个数 if (cnt + temp = k) // 以 num 为根的十叉树内有第 k 个元素 num *= 10; cnt++; else if (cnt + temp k) // 以 num 为根的十叉树内没有第 k 个元素 num++; cnt += temp; return num; LRU缓存贴一个实现代码: public class LRUCache private static class Node int key, value; Node prev, next; Node(int k, int v) key = k; value = v; private final int capacity; private final Node dummy = new Node(0, 0); // 哨兵节点 private final MapInteger, Node keyToNode = new HashMap(); public LRUCache(int capacity) this.capacity = capacity; dummy.prev = dummy; dummy.next = dummy; public int get(int key) Node node = getNode(key); return node != null ? node.value : -1; public void put(int key, int value) Node node = getNode(key); if (node != null) // 有这本书 node.value = value; // 更新 value return; node = new Node(key, value); // 新书 keyToNode.put(key, node); pushFront(node); // 放在最上面 if (keyToNode.size() capacity) // 书太多了 Node backNode = dummy.prev; keyToNode.remove(backNode.key); remove(backNode); // 去掉最后一本书 private Node getNode(int key) if (!keyToNode.containsKey(key)) // 没有这本书 return null; Node node = keyToNode.get(key); // 有这本书 remove(node); // 把这本书抽出来 pushFront(node); // 放在最上面 return node; // 删除一个节点（抽出一本书） private void remove(Node x) x.prev.next = x.next; x.next.prev = x.prev; // 在链表头添加一个节点（把一本书放在最上面） private void pushFront(Node x) x.prev = dummy; x.next = dummy.next; x.prev.next = x; x.next.prev = x;","tags":["基础","leetcode","算法"],"categories":["算法笔记"]},{"title":"打卡墙","path":"/check-in/index.html","content":"已归档笔记数量: 加载中..."}]